{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rand\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nusers = 100\n",
    "nitems = 100\n",
    "k = 10\n",
    "\n",
    "pu = np.random.rand(k,1)\n",
    "\n",
    "Q = np.random.rand(nitems,k)\n",
    "nQ = np.dot(Q,Q.T)\n",
    "\n",
    "ru = np.dot(Q,pu)\n",
    "D = (np.diag(nQ) + np.diag(nQ.T) - 2*nQ)\n",
    "nitems = len(ru)\n",
    "ntrans=200\n",
    "state = np.ceil(np.random.rand(ntrans,2)*nitems).astype(int)\n",
    "action = np.ceil(np.random.rand(ntrans,1)*nitems).astype(int)\n",
    "next_state = np.ceil(np.random.rand(ntrans,2)*nitems).astype(int)\n",
    "for i in range(0, len(state)):\n",
    "    next_state[i][0]=state[i][1]\n",
    "    next_state[i][1]=action[i][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "act = np.ceil(np.random.rand(ntrans,1)*nitems).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.23155855],\n",
       "       [2.87549914],\n",
       "       [3.25144747],\n",
       "       [1.99483791],\n",
       "       [3.07644234],\n",
       "       [2.28606067],\n",
       "       [2.26859885],\n",
       "       [1.30439348],\n",
       "       [2.19213304],\n",
       "       [2.47494361],\n",
       "       [2.31836866],\n",
       "       [2.70638047],\n",
       "       [2.81320331],\n",
       "       [3.32179939],\n",
       "       [2.6641429 ],\n",
       "       [1.52883047],\n",
       "       [2.20459927],\n",
       "       [2.10680841],\n",
       "       [1.72992872],\n",
       "       [2.27426877],\n",
       "       [2.07780157],\n",
       "       [2.15115147],\n",
       "       [2.28693669],\n",
       "       [2.40990375],\n",
       "       [3.18002488],\n",
       "       [2.02172161],\n",
       "       [1.6009842 ],\n",
       "       [2.40387248],\n",
       "       [3.27079872],\n",
       "       [1.76651873],\n",
       "       [2.6511118 ],\n",
       "       [3.19928971],\n",
       "       [2.76331122],\n",
       "       [2.30083109],\n",
       "       [3.92142845],\n",
       "       [2.14766809],\n",
       "       [1.51133012],\n",
       "       [3.68588451],\n",
       "       [2.10480371],\n",
       "       [1.55870788],\n",
       "       [2.30274985],\n",
       "       [3.0124401 ],\n",
       "       [2.32497651],\n",
       "       [1.65690692],\n",
       "       [1.70669207],\n",
       "       [1.87188319],\n",
       "       [2.9936973 ],\n",
       "       [2.94911191],\n",
       "       [2.34114182],\n",
       "       [2.07434398],\n",
       "       [3.36145824],\n",
       "       [2.38885195],\n",
       "       [2.53110569],\n",
       "       [2.58227845],\n",
       "       [2.33276638],\n",
       "       [1.74450305],\n",
       "       [2.36463863],\n",
       "       [2.55206602],\n",
       "       [2.49143343],\n",
       "       [2.19037511],\n",
       "       [3.01789146],\n",
       "       [1.4032558 ],\n",
       "       [2.61459348],\n",
       "       [2.55445396],\n",
       "       [2.26114941],\n",
       "       [1.57752362],\n",
       "       [2.01732859],\n",
       "       [2.29782885],\n",
       "       [2.4455411 ],\n",
       "       [2.61049121],\n",
       "       [1.60003478],\n",
       "       [1.61409442],\n",
       "       [1.68147393],\n",
       "       [1.58885426],\n",
       "       [2.13851215],\n",
       "       [2.35348556],\n",
       "       [1.99457912],\n",
       "       [2.05664145],\n",
       "       [2.34668398],\n",
       "       [3.12703515],\n",
       "       [1.82014785],\n",
       "       [1.85085199],\n",
       "       [2.78216563],\n",
       "       [2.06559121],\n",
       "       [1.90586675],\n",
       "       [2.11253816],\n",
       "       [2.02043067],\n",
       "       [2.49313677],\n",
       "       [3.2213267 ],\n",
       "       [2.78918887],\n",
       "       [2.81228473],\n",
       "       [2.25665698],\n",
       "       [2.59657026],\n",
       "       [2.62440606],\n",
       "       [1.86166254],\n",
       "       [3.77169338],\n",
       "       [2.223143  ],\n",
       "       [3.12505128],\n",
       "       [3.29115807],\n",
       "       [2.26050006]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame({'state': list(state), 'action': list(action), 'next_state': list(next_state)}, columns=['state', 'action', 'next_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>action</th>\n",
       "      <th>next_state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[90, 10]</td>\n",
       "      <td>[48]</td>\n",
       "      <td>[10, 48]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[60, 37]</td>\n",
       "      <td>[38]</td>\n",
       "      <td>[37, 38]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[19, 89]</td>\n",
       "      <td>[69]</td>\n",
       "      <td>[89, 69]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[20, 3]</td>\n",
       "      <td>[19]</td>\n",
       "      <td>[3, 19]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[82, 47]</td>\n",
       "      <td>[38]</td>\n",
       "      <td>[47, 38]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>[41, 74]</td>\n",
       "      <td>[9]</td>\n",
       "      <td>[74, 9]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>[68, 91]</td>\n",
       "      <td>[71]</td>\n",
       "      <td>[91, 71]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>[4, 68]</td>\n",
       "      <td>[90]</td>\n",
       "      <td>[68, 90]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>[52, 97]</td>\n",
       "      <td>[52]</td>\n",
       "      <td>[97, 52]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>[58, 65]</td>\n",
       "      <td>[22]</td>\n",
       "      <td>[65, 22]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        state action next_state\n",
       "0    [90, 10]   [48]   [10, 48]\n",
       "1    [60, 37]   [38]   [37, 38]\n",
       "2    [19, 89]   [69]   [89, 69]\n",
       "3     [20, 3]   [19]    [3, 19]\n",
       "4    [82, 47]   [38]   [47, 38]\n",
       "..        ...    ...        ...\n",
       "195  [41, 74]    [9]    [74, 9]\n",
       "196  [68, 91]   [71]   [91, 71]\n",
       "197   [4, 68]   [90]   [68, 90]\n",
       "198  [52, 97]   [52]   [97, 52]\n",
       "199  [58, 65]   [22]   [65, 22]\n",
       "\n",
       "[200 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getReward1(ru, D, state, action):\n",
    "    nitems = len(ru)\n",
    "    dist = D.flatten()\n",
    "    try:\n",
    "        reward = ru[action][0]\n",
    "        for i in range(0, len(state)):\n",
    "            if state[i]==-1:\n",
    "                break\n",
    "            else:\n",
    "                reward += (1/((len(state)-i)+1)) * dist[(state[i])*nitems + action] \n",
    "               \n",
    "    except IndexError:\n",
    "        reward = 0\n",
    "        print(\"asr\")\n",
    "    reward=np.ceil(reward)\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getReward(ru, D, state, action):\n",
    "    nitems = len(ru)\n",
    "    dist = D.flatten()\n",
    "    try:\n",
    "        reward = ru[action][0] +  dist[(state[0])*nitems + action] + dist[(state[1])*nitems + action]\n",
    "    except IndexError:\n",
    "        reward = 1.5\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward=[]\n",
    "for i in range(0,len(state)):\n",
    "    reward.append(getReward(ru, D, state[i],action[i]))\n",
    "\n",
    "#reward = ru[action[0]][0] + DD[(state[1])*nitems + action[0]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"reward\"]=reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>action</th>\n",
       "      <th>next_state</th>\n",
       "      <th>reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[90, 10]</td>\n",
       "      <td>[48]</td>\n",
       "      <td>[10, 48]</td>\n",
       "      <td>[9.407260926705952]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[60, 37]</td>\n",
       "      <td>[38]</td>\n",
       "      <td>[37, 38]</td>\n",
       "      <td>[-1.100881261168205]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[19, 89]</td>\n",
       "      <td>[69]</td>\n",
       "      <td>[89, 69]</td>\n",
       "      <td>[6.637259914951207]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[20, 3]</td>\n",
       "      <td>[19]</td>\n",
       "      <td>[3, 19]</td>\n",
       "      <td>[6.499172178467845]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[82, 47]</td>\n",
       "      <td>[38]</td>\n",
       "      <td>[47, 38]</td>\n",
       "      <td>[0.9415734211959603]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>[41, 74]</td>\n",
       "      <td>[9]</td>\n",
       "      <td>[74, 9]</td>\n",
       "      <td>[6.4599879612176965]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>[68, 91]</td>\n",
       "      <td>[71]</td>\n",
       "      <td>[91, 71]</td>\n",
       "      <td>[2.443864935827753]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>[4, 68]</td>\n",
       "      <td>[90]</td>\n",
       "      <td>[68, 90]</td>\n",
       "      <td>[4.332290469804903]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>[52, 97]</td>\n",
       "      <td>[52]</td>\n",
       "      <td>[97, 52]</td>\n",
       "      <td>[2.2477188188716055]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>[58, 65]</td>\n",
       "      <td>[22]</td>\n",
       "      <td>[65, 22]</td>\n",
       "      <td>[8.826742547865921]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        state action next_state                reward\n",
       "0    [90, 10]   [48]   [10, 48]   [9.407260926705952]\n",
       "1    [60, 37]   [38]   [37, 38]  [-1.100881261168205]\n",
       "2    [19, 89]   [69]   [89, 69]   [6.637259914951207]\n",
       "3     [20, 3]   [19]    [3, 19]   [6.499172178467845]\n",
       "4    [82, 47]   [38]   [47, 38]  [0.9415734211959603]\n",
       "..        ...    ...        ...                   ...\n",
       "195  [41, 74]    [9]    [74, 9]  [6.4599879612176965]\n",
       "196  [68, 91]   [71]   [91, 71]   [2.443864935827753]\n",
       "197   [4, 68]   [90]   [68, 90]   [4.332290469804903]\n",
       "198  [52, 97]   [52]   [97, 52]  [2.2477188188716055]\n",
       "199  [58, 65]   [22]   [65, 22]   [8.826742547865921]\n",
       "\n",
       "[200 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-16 19:10:21.901112: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-16 19:10:23.028130: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2023-02-16 19:10:23.028255: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2023-02-16 19:10:23.028271: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "from numpy import int64\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from gym.spaces import Box, Discrete\n",
    "from tqdm import tqdm\n",
    "\n",
    "  \n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n"
     ]
    }
   ],
   "source": [
    "a=Box(low=0, high=99, shape=(1,2), dtype=int64)\n",
    "k=a.sample()\n",
    "print(k.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[86 32]\n",
      "Asr\n"
     ]
    }
   ],
   "source": [
    "observation_space = Box(low=0, high=99, shape=(1,2), dtype=int64)\n",
    "a=observation_space.sample().ravel()\n",
    "print(a)\n",
    "action=89\n",
    "a=np.append(a, action)\n",
    "a[2]\n",
    "if action in a:\n",
    "    print(\"Asr\")\n",
    "else:\n",
    "    print(\"as\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEnv1(Env):\n",
    "    def __init__(self):\n",
    "        self.i=0\n",
    "        self.action_space = Discrete(100)\n",
    "        self.observation_space = Box(low=0, high=99, shape=(1,10), dtype=int64)\n",
    "        self.state = self.observation_space.sample().ravel()\n",
    "        \n",
    "    def step(self,action):\n",
    "        self.i+=1 \n",
    "        for j in range(self.i,10):\n",
    "            self.state[j]=-1\n",
    "        \n",
    "        self.next_state = self.observation_space.sample().ravel()\n",
    "        #self.next_state[0] = self.state[1]\n",
    "        #self.next_state[1]= action\n",
    "        \n",
    "        if action in self.state:\n",
    "            self.state[self.i]=action\n",
    "            reward = -10\n",
    "        else:  \n",
    "            self.state[self.i]=action \n",
    "            reward = getReward1(ru, D, self.state, action)\n",
    "        \n",
    "        if self.i==9:\n",
    "            print(self.state)\n",
    "            done=True\n",
    "        else:\n",
    "            done=False\n",
    "        info={}\n",
    "\n",
    "        return self.state, reward, done, info\n",
    "    def render(self):\n",
    "        # Implement viz\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        self.i=0\n",
    "        self.state = self.observation_space.sample().ravel()\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEnv(Env):\n",
    "    def __init__(self):\n",
    "        self.i=0\n",
    "        self.action_space = Discrete(100)\n",
    "        self.observation_space = Box(low=0, high=99, shape=(1,2), dtype=int64)\n",
    "        self.state = self.observation_space.sample().ravel()\n",
    "        self.a=self.state\n",
    "        \n",
    "    def step(self,action):\n",
    "        self.i+=1\n",
    "        self.next_state = self.observation_space.sample().ravel()\n",
    "        self.next_state[0] = self.state[1]\n",
    "        self.next_state[1]= action\n",
    "        #print(self.next_state)\n",
    "        if action in self.a:\n",
    "            reward=-100\n",
    "        else:\n",
    "            reward= getReward(ru, D, self.state, action)\n",
    "        self.state=self.next_state\n",
    "        self.a=np.append(self.a,action)\n",
    "        if self.i==12:\n",
    "            print(self.a)\n",
    "            done=True\n",
    "        else:\n",
    "            done=False\n",
    "        info={}\n",
    "\n",
    "        return self.state, reward, done, info\n",
    "    def render(self):\n",
    "        # Implement viz\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        self.a=[]\n",
    "        self.i=0\n",
    "        self.state = self.observation_space.sample().ravel()\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CustomEnv1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32  2  9 35 60 89  9 73 77 70]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=env.observation_space.sample().ravel()\n",
    "\n",
    "print(a)\n",
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 655.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[78 36 93 97 82 60 84 23 12 25]\n",
      "Episode:1 Score:40.0\n",
      "[39 39 90 85 61 34 21  9 47 88]\n",
      "Episode:2 Score:31.0\n",
      "[54 87 98 12 26 98 74 18 84 73]\n",
      "Episode:3 Score:19.0\n",
      "[15  2 11 56 80 57 59 24 35 89]\n",
      "Episode:4 Score:37.0\n",
      "[85 23  0 72 55 26 64 38 69 68]\n",
      "Episode:5 Score:30.0\n",
      "[27 85 51 10 58 66 49 44 84 90]\n",
      "Episode:6 Score:35.0\n",
      "[72  4 35 98 69 21 20  1 79  0]\n",
      "Episode:7 Score:38.0\n",
      "[70 90 99  4 24 76 93  1 39 74]\n",
      "Episode:8 Score:39.0\n",
      "[91 79 55 65 90 99 38 44 30 51]\n",
      "Episode:9 Score:37.0\n",
      "[52  8 95 46 10 21 95  3 35 17]\n",
      "Episode:10 Score:16.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "episodes = 10 #20 shower episodes\n",
    "for episode in tqdm(range(1, episodes+1)):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "#from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = np.shape(env.observation_space)\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    model = Sequential()    \n",
    "    model.add(Dense(24, activation='relu', input_shape=states))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(states, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 1, 24)             264       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1, 24)             600       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1, 24)             600       \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 24)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 100)               2500      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,964\n",
      "Trainable params: 3,964\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
    "                  nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2, gamma=0.0)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Adam._name = 'hey'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "2023-02-16 18:14:00.666415: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2023-02-16 18:14:00.666534: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2023-02-16 18:14:00.666627: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2023-02-16 18:14:00.666715: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2023-02-16 18:14:00.666801: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2023-02-16 18:14:00.666887: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2023-02-16 18:14:00.666982: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2023-02-16 18:14:00.667070: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2023-02-16 18:14:00.667104: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-02-16 18:14:00.667580: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-16 18:14:00.688651: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled\n",
      "2023-02-16 18:14:00.739088: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_3_1/kernel/Assign' id:307 op device:{requested: '', assigned: ''} def:{{{node dense_3_1/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_3_1/kernel, dense_3_1/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50001 steps ...\n",
      "[38 22 22 57 22 12 12 22 24 22]\n",
      "     9/50001: episode: 1, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward: -32.000, mean reward: -3.556 [-10.000,  6.000], mean action: 23.889 [12.000, 57.000],  loss: --, mae: --, mean_q: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "2023-02-16 18:14:01.335364: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_3/BiasAdd' id:159 op device:{requested: '', assigned: ''} def:{{{node dense_3/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_3/MatMul, dense_3/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-02-16 18:14:01.360836: W tensorflow/c/c_api.cc:291] Operation '{name:'count_2/Assign' id:495 op device:{requested: '', assigned: ''} def:{{{node count_2/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](count_2, count_2/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 11 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "2023-02-16 18:14:01.434526: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_3_1/BiasAdd' id:317 op device:{requested: '', assigned: ''} def:{{{node dense_3_1/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_3_1/MatMul, dense_3_1/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-02-16 18:14:01.760018: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_3/AddN' id:601 op device:{requested: '', assigned: ''} def:{{{node loss_3/AddN}} = AddN[N=2, T=DT_FLOAT, _has_manual_control_dependencies=true](loss_3/mul, loss_3/mul_1)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-02-16 18:14:01.802893: W tensorflow/c/c_api.cc:291] Operation '{name:'training/Adam/dense_3/bias/m/Assign' id:820 op device:{requested: '', assigned: ''} def:{{{node training/Adam/dense_3/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training/Adam/dense_3/bias/m, training/Adam/dense_3/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 12 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 13 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 14 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 15 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 16 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 17 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 18 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 19 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 20 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 21 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 22 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 23 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 24 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 25 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 26 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 27 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 28 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 29 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 30 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 31 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[98 22 12 22 47 10 22 22 14 22]\n",
      "    18/50001: episode: 2, duration: 0.729s, episode steps:   9, steps per second:  12, episode reward: -21.000, mean reward: -2.333 [-10.000,  5.000], mean action: 21.444 [10.000, 47.000],  loss: 142.527891, mae: 4.241316, mean_q: 12.690224\n",
      "[10 32 47 32 22 22  1 22 57 38]\n",
      "    27/50001: episode: 3, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: -7.000, mean reward: -0.778 [-10.000,  5.000], mean action: 30.333 [1.000, 57.000],  loss: 136.821716, mae: 4.124181, mean_q: 12.119040\n",
      "[28 14 22 64 22 22 57 22 22 22]\n",
      "    36/50001: episode: 4, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: -37.000, mean reward: -4.111 [-10.000,  4.000], mean action: 29.667 [14.000, 64.000],  loss: 112.435417, mae: 3.853518, mean_q: 10.777084\n",
      "[96 22 47 22 81 12 12 12 81 12]\n",
      "    45/50001: episode: 5, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: -35.000, mean reward: -3.889 [-10.000,  5.000], mean action: 33.444 [12.000, 81.000],  loss: 123.999146, mae: 3.948450, mean_q: 11.150828\n",
      "[91 22 47 22 22 57 12 67 22 22]\n",
      "    54/50001: episode: 6, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: -21.000, mean reward: -2.333 [-10.000,  5.000], mean action: 32.556 [12.000, 67.000],  loss: 118.588295, mae: 3.888135, mean_q: 11.028066\n",
      "[62 22 97 22 22 22 14 77  1 22]\n",
      "    63/50001: episode: 7, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: -19.000, mean reward: -2.111 [-10.000,  7.000], mean action: 33.222 [1.000, 97.000],  loss: 111.772446, mae: 3.604028, mean_q: 10.057488\n",
      "[13 22 35 47 81 57 57 22 22 22]\n",
      "    72/50001: episode: 8, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: -23.000, mean reward: -2.556 [-10.000,  4.000], mean action: 40.556 [22.000, 81.000],  loss: 106.522430, mae: 3.617537, mean_q: 9.758508\n",
      "[51 57 10 47 47 47 22 47 14 22]\n",
      "    81/50001: episode: 9, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: -23.000, mean reward: -2.556 [-10.000,  4.000], mean action: 34.778 [10.000, 57.000],  loss: 99.690987, mae: 3.420752, mean_q: 9.455500\n",
      "[ 3 14 23 32 24 75 22 57 42 31]\n",
      "    90/50001: episode: 10, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 35.556 [14.000, 75.000],  loss: 91.817535, mae: 3.258481, mean_q: 8.889484\n",
      "[89 14 81 98 22 77 32 12  4 12]\n",
      "    99/50001: episode: 11, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 39.111 [4.000, 98.000],  loss: 77.212898, mae: 3.115272, mean_q: 8.365219\n",
      "[86 14 22 57 57 14 12 12 81 31]\n",
      "   108/50001: episode: 12, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: -5.000, mean reward: -0.556 [-10.000,  6.000], mean action: 33.333 [12.000, 81.000],  loss: 76.042877, mae: 3.095326, mean_q: 8.270662\n",
      "[91 14 98 69 58 57 57  4 22 77]\n",
      "   117/50001: episode: 13, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 22.000, mean reward:  2.444 [-10.000,  5.000], mean action: 50.667 [4.000, 98.000],  loss: 67.183540, mae: 3.062140, mean_q: 8.196611\n",
      "[17 22 18 69 73 31 10 31 31 22]\n",
      "   126/50001: episode: 14, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: -10.000, mean reward: -1.111 [-10.000,  5.000], mean action: 34.111 [10.000, 73.000],  loss: 58.491978, mae: 2.984498, mean_q: 7.954118\n",
      "[66 22 50  1 69 10 72 24 72 93]\n",
      "   135/50001: episode: 15, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 22.000, mean reward:  2.444 [-10.000,  7.000], mean action: 45.889 [1.000, 93.000],  loss: 56.756199, mae: 2.836498, mean_q: 7.541692\n",
      "[71 22 18  1 25 12 47 47 55 14]\n",
      "   144/50001: episode: 16, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 26.778 [1.000, 55.000],  loss: 51.463169, mae: 2.683703, mean_q: 7.132946\n",
      "[67 22 98 69 12 22 57  2 22 22]\n",
      "   153/50001: episode: 17, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: -5.000, mean reward: -0.556 [-10.000,  5.000], mean action: 36.222 [2.000, 98.000],  loss: 43.660114, mae: 2.536548, mean_q: 6.834163\n",
      "[82 22 24 75 81 81 12 12 12 12]\n",
      "   162/50001: episode: 18, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: -21.000, mean reward: -2.333 [-10.000,  6.000], mean action: 36.778 [12.000, 81.000],  loss: 50.894463, mae: 2.558185, mean_q: 6.692101\n",
      "[26 18 70  1 59 25 32 46 46 46]\n",
      "   171/50001: episode: 19, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward:  8.000, mean reward:  0.889 [-10.000,  6.000], mean action: 38.111 [1.000, 70.000],  loss: 46.389980, mae: 2.324033, mean_q: 6.145535\n",
      "[25 22 51  6 97 31 20 22  9 20]\n",
      "   180/50001: episode: 20, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward:  8.000, mean reward:  0.889 [-10.000,  6.000], mean action: 30.889 [6.000, 97.000],  loss: 44.716717, mae: 2.304542, mean_q: 6.075129\n",
      "[26 32 10 98 98 57 12 12 12 12]\n",
      "   189/50001: episode: 21, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: -18.000, mean reward: -2.000 [-10.000,  6.000], mean action: 38.111 [10.000, 98.000],  loss: 40.253071, mae: 2.231472, mean_q: 5.892956\n",
      "[80 73 47 47 97 47 46  1 47 98]\n",
      "   198/50001: episode: 22, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: -2.000, mean reward: -0.222 [-10.000,  6.000], mean action: 55.889 [1.000, 98.000],  loss: 47.583145, mae: 2.292130, mean_q: 6.045214\n",
      "[88 57 47 47 97 59 69 21 58 12]\n",
      "   207/50001: episode: 23, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 23.000, mean reward:  2.556 [-10.000,  9.000], mean action: 51.889 [12.000, 97.000],  loss: 46.191792, mae: 2.234859, mean_q: 5.744861\n",
      "[24 77 47 57 97 97 57 32 77 12]\n",
      "   216/50001: episode: 24, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: -2.000, mean reward: -0.222 [-10.000,  7.000], mean action: 61.444 [12.000, 97.000],  loss: 36.762741, mae: 2.120976, mean_q: 5.506197\n",
      "[18 86  1 35 23 69 97 47 97 22]\n",
      "   225/50001: episode: 25, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 53.000 [1.000, 97.000],  loss: 38.694267, mae: 2.205975, mean_q: 5.550111\n",
      "[19 15 56 38 38 57 46 86 40 31]\n",
      "   234/50001: episode: 26, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 26.000, mean reward:  2.889 [-10.000,  8.000], mean action: 45.222 [15.000, 86.000],  loss: 30.021189, mae: 2.020328, mean_q: 5.223491\n",
      "[80  4 48  1 75 33 99 11 24 11]\n",
      "   243/50001: episode: 27, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 34.000 [1.000, 99.000],  loss: 30.750502, mae: 2.051396, mean_q: 5.208957\n",
      "[69 86 47 97 97 57  1 57 24 46]\n",
      "   252/50001: episode: 28, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 12.000, mean reward:  1.333 [-10.000,  6.000], mean action: 56.889 [1.000, 97.000],  loss: 38.008930, mae: 2.024691, mean_q: 5.043352\n",
      "[41  1 58 55 77 65 48 15 15 22]\n",
      "   261/50001: episode: 29, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 20.000, mean reward:  2.222 [-10.000,  6.000], mean action: 39.556 [1.000, 77.000],  loss: 38.685249, mae: 2.046269, mean_q: 5.109501\n",
      "[33 35 70 57 35 12  2 58 97  7]\n",
      "   270/50001: episode: 30, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 41.444 [2.000, 97.000],  loss: 34.681267, mae: 2.140883, mean_q: 5.250278\n",
      "[ 7 32 72 38 32 32 97 55 77 77]\n",
      "   279/50001: episode: 31, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: -8.000, mean reward: -0.889 [-10.000,  6.000], mean action: 56.889 [32.000, 97.000],  loss: 30.547255, mae: 1.908262, mean_q: 4.684104\n",
      "[17 57  1 10 97 18 14 10 36 47]\n",
      "   288/50001: episode: 32, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 23.000, mean reward:  2.556 [-10.000,  8.000], mean action: 32.222 [1.000, 97.000],  loss: 29.626865, mae: 1.885917, mean_q: 4.578274\n",
      "[50 24  2 44 89 22 32 95 93 31]\n",
      "   297/50001: episode: 33, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 42.000, mean reward:  4.667 [ 2.000,  8.000], mean action: 48.000 [2.000, 95.000],  loss: 26.459101, mae: 1.819116, mean_q: 4.470289\n",
      "[19  9 46 84 72 33 48  5  3 31]\n",
      "   306/50001: episode: 34, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 38.000, mean reward:  4.222 [ 2.000,  8.000], mean action: 36.778 [3.000, 84.000],  loss: 23.374317, mae: 1.837589, mean_q: 4.547130\n",
      "[85 15 24 94 12 46 32 22  7 24]\n",
      "   315/50001: episode: 35, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 18.000, mean reward:  2.000 [-10.000,  5.000], mean action: 30.667 [7.000, 94.000],  loss: 24.241098, mae: 1.658122, mean_q: 4.085824\n",
      "[83 42 47 97 22 46 33 51 20 51]\n",
      "   324/50001: episode: 36, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 22.000, mean reward:  2.444 [-10.000,  5.000], mean action: 45.444 [20.000, 97.000],  loss: 26.087214, mae: 1.761872, mean_q: 4.346107\n",
      "[32 21 27  1 10 43  4 57 86  9]\n",
      "   333/50001: episode: 37, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 35.000, mean reward:  3.889 [ 2.000,  6.000], mean action: 28.667 [1.000, 86.000],  loss: 21.729753, mae: 1.761080, mean_q: 4.296781\n",
      "[79 14 23 16 70 57 22 69 97 14]\n",
      "   342/50001: episode: 38, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 20.000, mean reward:  2.222 [-10.000,  7.000], mean action: 42.444 [14.000, 97.000],  loss: 21.387964, mae: 1.694273, mean_q: 4.158377\n",
      "[ 8 15 79  2 97 78 57 14 45 12]\n",
      "   351/50001: episode: 39, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 39.000, mean reward:  4.333 [ 2.000,  9.000], mean action: 44.333 [2.000, 97.000],  loss: 24.474327, mae: 1.646176, mean_q: 4.077060\n",
      "[ 4 42 69  1 25  1 70  2 86  4]\n",
      "   360/50001: episode: 40, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward:  4.000, mean reward:  0.444 [-10.000,  6.000], mean action: 33.333 [1.000, 86.000],  loss: 22.690016, mae: 1.636980, mean_q: 4.067377\n",
      "[78 13 39 98 29 64 48 12 77 31]\n",
      "   369/50001: episode: 41, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 45.667 [12.000, 98.000],  loss: 20.914511, mae: 1.542908, mean_q: 3.831679\n",
      "[49 42 64 54 97 33 55 81 62 46]\n",
      "   378/50001: episode: 42, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 36.000, mean reward:  4.000 [ 3.000,  7.000], mean action: 59.333 [33.000, 97.000],  loss: 22.726189, mae: 1.536462, mean_q: 3.831161\n",
      "[82 80 47 75 58 60 75 34 31 20]\n",
      "   387/50001: episode: 43, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 53.333 [20.000, 80.000],  loss: 22.115267, mae: 1.583582, mean_q: 3.909241\n",
      "[94 14 83 47 22 12 70 39 77 46]\n",
      "   396/50001: episode: 44, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 34.000, mean reward:  3.778 [ 2.000,  7.000], mean action: 45.556 [12.000, 83.000],  loss: 22.405466, mae: 1.526805, mean_q: 3.701379\n",
      "[49 24 24 58 19 58 12 46 49 20]\n",
      "   405/50001: episode: 45, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: -3.000, mean reward: -0.333 [-10.000,  6.000], mean action: 34.444 [12.000, 58.000],  loss: 19.063400, mae: 1.569254, mean_q: 3.838342\n",
      "[49 22 74 69 22  1 46 77 46 46]\n",
      "   414/50001: episode: 46, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: -8.000, mean reward: -0.889 [-10.000,  5.000], mean action: 44.778 [1.000, 77.000],  loss: 19.225330, mae: 1.490183, mean_q: 3.643681\n",
      "[67 15  8 52 51 59 79 79 12 14]\n",
      "   423/50001: episode: 47, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 41.000 [8.000, 79.000],  loss: 22.031168, mae: 1.482648, mean_q: 3.604680\n",
      "[44 86 69 67 59 32 25 97 58 77]\n",
      "   432/50001: episode: 48, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 37.000, mean reward:  4.111 [ 3.000,  7.000], mean action: 63.333 [25.000, 97.000],  loss: 19.281115, mae: 1.434351, mean_q: 3.534863\n",
      "[82 94 47 70 25  1 60 47 57 29]\n",
      "   441/50001: episode: 49, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 21.000, mean reward:  2.333 [-10.000,  6.000], mean action: 47.778 [1.000, 94.000],  loss: 19.301229, mae: 1.530939, mean_q: 3.739573\n",
      "[78 58 75 10 97 90 95 27 86 71]\n",
      "   450/50001: episode: 50, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 36.000, mean reward:  4.000 [ 3.000,  7.000], mean action: 67.667 [10.000, 97.000],  loss: 20.432489, mae: 1.572650, mean_q: 3.819781\n",
      "[66  7 21 57 20  7 33 43 20 46]\n",
      "   459/50001: episode: 51, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward:  8.000, mean reward:  0.889 [-10.000,  9.000], mean action: 28.222 [7.000, 57.000],  loss: 16.315371, mae: 1.447163, mean_q: 3.555851\n",
      "[ 3 38 12 58 73 12 60 31 27  7]\n",
      "   468/50001: episode: 52, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 35.333 [7.000, 73.000],  loss: 19.366989, mae: 1.414661, mean_q: 3.431063\n",
      "[70 69 70 22 16 47  1 39 73 88]\n",
      "   477/50001: episode: 53, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 27.000, mean reward:  3.000 [-10.000,  9.000], mean action: 47.222 [1.000, 88.000],  loss: 16.856947, mae: 1.485504, mean_q: 3.604271\n",
      "[41 67 10 75 75  1 97  2 60 86]\n",
      "   486/50001: episode: 54, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 52.556 [1.000, 97.000],  loss: 19.800041, mae: 1.478834, mean_q: 3.552941\n",
      "[96 40 75 48  1 18 98 58  2  1]\n",
      "   495/50001: episode: 55, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 37.889 [1.000, 98.000],  loss: 17.729225, mae: 1.481169, mean_q: 3.594861\n",
      "[85 88 74 18 16 74 57 78 67 18]\n",
      "   504/50001: episode: 56, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward:  6.000, mean reward:  0.667 [-10.000,  5.000], mean action: 54.444 [16.000, 88.000],  loss: 17.266266, mae: 1.420327, mean_q: 3.419052\n",
      "[22 22 69 45 69 57 38 79  2 22]\n",
      "   513/50001: episode: 57, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: -6.000, mean reward: -0.667 [-10.000,  6.000], mean action: 44.778 [2.000, 79.000],  loss: 18.095026, mae: 1.483338, mean_q: 3.595428\n",
      "[69 78 29 25 74  7 69  7  7 74]\n",
      "   522/50001: episode: 58, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: -26.000, mean reward: -2.889 [-10.000,  4.000], mean action: 41.111 [7.000, 78.000],  loss: 16.373974, mae: 1.423508, mean_q: 3.399985\n",
      "[83 24 46 35 75 77 91 54 42  2]\n",
      "   531/50001: episode: 59, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 40.000, mean reward:  4.444 [ 3.000,  8.000], mean action: 49.556 [2.000, 91.000],  loss: 18.496632, mae: 1.389195, mean_q: 3.305641\n",
      "[72 86 52 24 59 58 74 67 15 15]\n",
      "   540/50001: episode: 60, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 20.000, mean reward:  2.222 [-10.000,  5.000], mean action: 50.000 [15.000, 86.000],  loss: 17.159435, mae: 1.440384, mean_q: 3.435427\n",
      "[89 80 74 90 90 59 47 60 84 40]\n",
      "   549/50001: episode: 61, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 69.333 [40.000, 90.000],  loss: 20.795189, mae: 1.389028, mean_q: 3.260314\n",
      "[40 24 74 88 57 67 81 32 77 31]\n",
      "   558/50001: episode: 62, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 37.000, mean reward:  4.111 [ 3.000,  7.000], mean action: 59.000 [24.000, 88.000],  loss: 18.579714, mae: 1.360907, mean_q: 3.269354\n",
      "[24 15 97  2  1 97 67 21 70 51]\n",
      "   567/50001: episode: 63, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 20.000, mean reward:  2.222 [-10.000,  6.000], mean action: 46.778 [1.000, 97.000],  loss: 17.328974, mae: 1.334808, mean_q: 3.193730\n",
      "[76 86 47 18  1 75 57 67 57 46]\n",
      "   576/50001: episode: 64, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 50.444 [1.000, 86.000],  loss: 16.114151, mae: 1.325652, mean_q: 3.167703\n",
      "[73 84 14 14 24 75 22 88 75 80]\n",
      "   585/50001: episode: 65, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 10.000, mean reward:  1.111 [-10.000,  7.000], mean action: 52.889 [14.000, 88.000],  loss: 16.180614, mae: 1.407867, mean_q: 3.328457\n",
      "[63 96 67 67 46 67 50 80 58 20]\n",
      "   594/50001: episode: 66, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 11.000, mean reward:  1.222 [-10.000,  7.000], mean action: 61.222 [20.000, 96.000],  loss: 18.700022, mae: 1.338310, mean_q: 3.169116\n",
      "[ 4 20 32 12 36 44 49 17 96 81]\n",
      "   603/50001: episode: 67, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 32.000, mean reward:  3.556 [ 2.000,  5.000], mean action: 43.000 [12.000, 96.000],  loss: 17.651899, mae: 1.357752, mean_q: 3.258547\n",
      "[55 15 25 59 18 38 19 58 21 20]\n",
      "   612/50001: episode: 68, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 33.000, mean reward:  3.667 [ 2.000,  5.000], mean action: 30.333 [15.000, 59.000],  loss: 16.183403, mae: 1.341630, mean_q: 3.181959\n",
      "[48 45 90 25 58 69 22 63 86 23]\n",
      "   621/50001: episode: 69, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 37.000, mean reward:  4.111 [ 2.000,  7.000], mean action: 53.444 [22.000, 90.000],  loss: 17.931664, mae: 1.270744, mean_q: 3.080518\n",
      "[13 98 88 38  1 97  1 67 46 46]\n",
      "   630/50001: episode: 70, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward:  9.000, mean reward:  1.000 [-10.000,  6.000], mean action: 53.556 [1.000, 98.000],  loss: 17.092215, mae: 1.301039, mean_q: 3.091906\n",
      "[78 58 74 60 12 58 20 11 11 29]\n",
      "   639/50001: episode: 71, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward:  4.000, mean reward:  0.444 [-10.000,  5.000], mean action: 37.000 [11.000, 74.000],  loss: 15.117138, mae: 1.298566, mean_q: 3.116946\n",
      "[80 22 38 61 18  2 56 85  9 59]\n",
      "   648/50001: episode: 72, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 36.000, mean reward:  4.000 [ 2.000,  6.000], mean action: 38.889 [2.000, 85.000],  loss: 17.475863, mae: 1.239313, mean_q: 2.925723\n",
      "[57 15 32 25 44 35 31 73 68  9]\n",
      "   657/50001: episode: 73, duration: 0.057s, episode steps:   9, steps per second: 158, episode reward: 36.000, mean reward:  4.000 [ 2.000,  7.000], mean action: 36.889 [9.000, 73.000],  loss: 15.919529, mae: 1.316817, mean_q: 3.149003\n",
      "[62 68 22 47 74 72 63  3 31 47]\n",
      "   666/50001: episode: 74, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 20.000, mean reward:  2.222 [-10.000,  7.000], mean action: 47.444 [3.000, 74.000],  loss: 17.195822, mae: 1.229525, mean_q: 2.926193\n",
      "[12 86 47 29 70 30 50 56 39 46]\n",
      "   675/50001: episode: 75, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 50.333 [29.000, 86.000],  loss: 15.659400, mae: 1.217256, mean_q: 2.883492\n",
      "[31 16 75 25 90 31 52 75 50 84]\n",
      "   684/50001: episode: 76, duration: 0.057s, episode steps:   9, steps per second: 158, episode reward:  9.000, mean reward:  1.000 [-10.000,  8.000], mean action: 55.333 [16.000, 90.000],  loss: 16.149784, mae: 1.291167, mean_q: 3.037737\n",
      "[50 70 98 18 68  2 58 60  5 85]\n",
      "   693/50001: episode: 77, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 36.000, mean reward:  4.000 [ 2.000,  6.000], mean action: 51.556 [2.000, 98.000],  loss: 16.089891, mae: 1.196552, mean_q: 2.805815\n",
      "[69 48 25 67 24 46 92 90 46 14]\n",
      "   702/50001: episode: 78, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 50.222 [14.000, 92.000],  loss: 16.436892, mae: 1.171923, mean_q: 2.771419\n",
      "[46 97 47  1 90 14 59 10 23 54]\n",
      "   711/50001: episode: 79, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 35.000, mean reward:  3.889 [ 3.000,  5.000], mean action: 43.889 [1.000, 97.000],  loss: 18.901978, mae: 1.211849, mean_q: 2.852885\n",
      "[40 96 64 32 43 23 48 59 60 64]\n",
      "   720/50001: episode: 80, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 54.333 [23.000, 96.000],  loss: 17.446388, mae: 1.128255, mean_q: 2.725043\n",
      "[71 58 74 57 97  5 68 21 36  2]\n",
      "   729/50001: episode: 81, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 35.000, mean reward:  3.889 [ 2.000,  8.000], mean action: 46.444 [2.000, 97.000],  loss: 15.862333, mae: 1.179757, mean_q: 2.775296\n",
      "[93  0 64 11 69 25 78 43 13 24]\n",
      "   738/50001: episode: 82, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 38.000, mean reward:  4.222 [ 2.000,  9.000], mean action: 36.333 [0.000, 78.000],  loss: 17.795650, mae: 1.166759, mean_q: 2.812739\n",
      "[72 83 14 10 25 46 51 65 95 54]\n",
      "   747/50001: episode: 83, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 40.000, mean reward:  4.444 [ 3.000, 10.000], mean action: 49.222 [10.000, 95.000],  loss: 14.878447, mae: 1.179817, mean_q: 2.818930\n",
      "[43  6 77 32 84 12  3 57 10 24]\n",
      "   756/50001: episode: 84, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 37.000, mean reward:  4.111 [ 2.000,  7.000], mean action: 33.889 [3.000, 84.000],  loss: 14.981465, mae: 1.166611, mean_q: 2.816997\n",
      "[33 35 76 53 69 32  2 83 38 41]\n",
      "   765/50001: episode: 85, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 35.000, mean reward:  3.889 [ 2.000,  6.000], mean action: 47.667 [2.000, 83.000],  loss: 18.198631, mae: 1.234171, mean_q: 2.931334\n",
      "[55 88 23 51 16 58 22 98 90 12]\n",
      "   774/50001: episode: 86, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 40.000, mean reward:  4.444 [ 3.000,  8.000], mean action: 50.889 [12.000, 98.000],  loss: 13.782941, mae: 1.181351, mean_q: 2.862237\n",
      "[ 1 88 51 60 97 60 89  8  4 24]\n",
      "   783/50001: episode: 87, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 53.444 [4.000, 97.000],  loss: 14.777459, mae: 1.205250, mean_q: 2.886775\n",
      "[48 89 58 23 12 32 21 89 90 12]\n",
      "   792/50001: episode: 88, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward:  6.000, mean reward:  0.667 [-10.000,  5.000], mean action: 47.333 [12.000, 90.000],  loss: 13.128758, mae: 1.161461, mean_q: 2.798847\n",
      "[89 93  1 16 75  9  7 44 74 93]\n",
      "   801/50001: episode: 89, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 19.000, mean reward:  2.111 [-10.000,  6.000], mean action: 45.778 [1.000, 93.000],  loss: 13.820119, mae: 1.195814, mean_q: 2.843190\n",
      "[17 12 18 80  1  4 58 72  2 31]\n",
      "   810/50001: episode: 90, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 30.889 [1.000, 80.000],  loss: 14.050014, mae: 1.170136, mean_q: 2.843609\n",
      "[24 15 75 11 88 84 31 14 13 70]\n",
      "   819/50001: episode: 91, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 37.000, mean reward:  4.111 [ 2.000,  7.000], mean action: 44.556 [11.000, 88.000],  loss: 14.477795, mae: 1.056342, mean_q: 2.567682\n",
      "[97  2 46 59 67 58 75 14 62 18]\n",
      "   828/50001: episode: 92, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 36.000, mean reward:  4.000 [ 3.000,  5.000], mean action: 44.556 [2.000, 75.000],  loss: 15.136830, mae: 1.107486, mean_q: 2.753531\n",
      "[55 70 18 18 25 44 74 59  3  3]\n",
      "   837/50001: episode: 93, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward:  5.000, mean reward:  0.556 [-10.000,  5.000], mean action: 34.889 [3.000, 74.000],  loss: 13.531555, mae: 1.084616, mean_q: 2.622947\n",
      "[46 22 14  1 79 13 39 72 99 93]\n",
      "   846/50001: episode: 94, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 36.000, mean reward:  4.000 [ 2.000,  7.000], mean action: 48.000 [1.000, 99.000],  loss: 12.103644, mae: 1.188303, mean_q: 2.888195\n",
      "[ 3 62 49 76 69 73 38 96 32 31]\n",
      "   855/50001: episode: 95, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 39.000, mean reward:  4.333 [ 3.000,  8.000], mean action: 58.444 [31.000, 96.000],  loss: 10.965023, mae: 1.123722, mean_q: 2.784437\n",
      "[47 22 69 46 19 44 22 63 48 66]\n",
      "   864/50001: episode: 96, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 26.000, mean reward:  2.889 [-10.000,  8.000], mean action: 44.333 [19.000, 69.000],  loss: 13.378104, mae: 1.230354, mean_q: 3.012414\n",
      "[70 50 74 57 25 84 43 38  2 10]\n",
      "   873/50001: episode: 97, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 34.000, mean reward:  3.778 [ 2.000,  7.000], mean action: 42.556 [2.000, 84.000],  loss: 13.762980, mae: 1.171935, mean_q: 2.866357\n",
      "[34 88 59 23 49  9 25  3 12 31]\n",
      "   882/50001: episode: 98, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 38.000, mean reward:  4.222 [ 3.000,  8.000], mean action: 33.222 [3.000, 88.000],  loss: 12.084544, mae: 1.139390, mean_q: 2.792523\n",
      "[67 63 10 10 75 74 32 73 71 69]\n",
      "   891/50001: episode: 99, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 20.000, mean reward:  2.222 [-10.000,  6.000], mean action: 53.000 [10.000, 75.000],  loss: 14.644307, mae: 1.199483, mean_q: 2.910146\n",
      "[76 12 83 81 15 54 59 40 20 14]\n",
      "   900/50001: episode: 100, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 36.000, mean reward:  4.000 [ 2.000,  7.000], mean action: 42.000 [12.000, 83.000],  loss: 11.621232, mae: 1.156749, mean_q: 2.803399\n",
      "[ 5 14 18 58 77 48 32 73 62 79]\n",
      "   909/50001: episode: 101, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 36.000, mean reward:  4.000 [ 2.000,  7.000], mean action: 51.222 [14.000, 79.000],  loss: 13.590049, mae: 1.125552, mean_q: 2.763201\n",
      "[91 78 51 88 58 72 48 99 86 55]\n",
      "   918/50001: episode: 102, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 34.000, mean reward:  3.778 [ 2.000,  6.000], mean action: 70.556 [48.000, 99.000],  loss: 15.707637, mae: 1.228019, mean_q: 3.032193\n",
      "[32 19 57 33 22 92 88 12 61  9]\n",
      "   927/50001: episode: 103, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 43.667 [9.000, 92.000],  loss: 15.388577, mae: 1.143826, mean_q: 2.836329\n",
      "[44 53 69 16 15 17 46 46  2  1]\n",
      "   936/50001: episode: 104, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 26.000, mean reward:  2.889 [-10.000,  9.000], mean action: 29.444 [1.000, 69.000],  loss: 13.037818, mae: 1.143887, mean_q: 2.791740\n",
      "[89 33 81 57 97 12 43 33 10 27]\n",
      "   945/50001: episode: 105, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 43.667 [10.000, 97.000],  loss: 12.125099, mae: 1.060085, mean_q: 2.546939\n",
      "[79 14  0 24 33 74 22  8 22 30]\n",
      "   954/50001: episode: 106, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 19.000, mean reward:  2.111 [-10.000,  6.000], mean action: 25.222 [0.000, 74.000],  loss: 12.706776, mae: 1.156454, mean_q: 2.828512\n",
      "[93 12 21 81 39 54 75  4 98 31]\n",
      "   963/50001: episode: 107, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 46.111 [4.000, 98.000],  loss: 14.493627, mae: 1.119800, mean_q: 2.712828\n",
      "[58 66 90 97 73 25 32  9 70  4]\n",
      "   972/50001: episode: 108, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 37.000, mean reward:  4.111 [ 2.000,  8.000], mean action: 51.778 [4.000, 97.000],  loss: 12.352024, mae: 1.126321, mean_q: 2.759725\n",
      "[18 14 12  4 86 24 43 15 80 12]\n",
      "   981/50001: episode: 109, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 17.000, mean reward:  1.889 [-10.000,  5.000], mean action: 32.222 [4.000, 86.000],  loss: 13.105840, mae: 1.128065, mean_q: 2.792488\n",
      "[52  9 50  9 83 13 93 22 68 68]\n",
      "   990/50001: episode: 110, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward:  8.000, mean reward:  0.889 [-10.000,  6.000], mean action: 46.111 [9.000, 93.000],  loss: 12.869706, mae: 1.088191, mean_q: 2.681107\n",
      "[87  6 72 33 52 10 36  5 57 21]\n",
      "   999/50001: episode: 111, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 34.000, mean reward:  3.778 [ 2.000,  7.000], mean action: 32.444 [5.000, 72.000],  loss: 12.388146, mae: 1.106021, mean_q: 2.763846\n",
      "[20 22 29 83  9  9 31 31  2  2]\n",
      "  1008/50001: episode: 112, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: -6.000, mean reward: -0.667 [-10.000,  6.000], mean action: 24.222 [2.000, 83.000],  loss: 12.835927, mae: 1.128148, mean_q: 2.796349\n",
      "[ 1  2 45 21  1  5 30 54 58 45]\n",
      "  1017/50001: episode: 113, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward:  7.000, mean reward:  0.778 [-10.000,  6.000], mean action: 29.000 [1.000, 58.000],  loss: 13.718534, mae: 1.032252, mean_q: 2.547551\n",
      "[32 19 56 63 56  9 52 58  9 46]\n",
      "  1026/50001: episode: 114, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward:  8.000, mean reward:  0.889 [-10.000,  7.000], mean action: 40.889 [9.000, 63.000],  loss: 13.135257, mae: 1.168987, mean_q: 2.945539\n",
      "[42 21 50 95 31  5  5 54  5 46]\n",
      "  1035/50001: episode: 115, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 10.000, mean reward:  1.111 [-10.000,  6.000], mean action: 34.667 [5.000, 95.000],  loss: 12.500626, mae: 1.131927, mean_q: 2.792425\n",
      "[58 68  2 76 30 70 79 18 32 46]\n",
      "  1044/50001: episode: 116, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 46.778 [2.000, 79.000],  loss: 14.642748, mae: 1.200188, mean_q: 2.913920\n",
      "[ 9  1 73 88  5  5 60  5 31 31]\n",
      "  1053/50001: episode: 117, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: -4.000, mean reward: -0.444 [-10.000,  6.000], mean action: 33.222 [1.000, 88.000],  loss: 13.611229, mae: 1.183699, mean_q: 2.969813\n",
      "[46 45 88 65 19  1 29 77 97 24]\n",
      "  1062/50001: episode: 118, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 49.444 [1.000, 97.000],  loss: 14.726320, mae: 1.055005, mean_q: 2.533155\n",
      "[79 88 75 33  2 43 46 97 31 77]\n",
      "  1071/50001: episode: 119, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 37.000, mean reward:  4.111 [ 2.000,  6.000], mean action: 54.667 [2.000, 97.000],  loss: 11.557797, mae: 1.157423, mean_q: 2.804431\n",
      "[67 90 60 69 46 49 47 54  2 73]\n",
      "  1080/50001: episode: 120, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 36.000, mean reward:  4.000 [ 3.000,  6.000], mean action: 54.444 [2.000, 90.000],  loss: 12.891409, mae: 1.146380, mean_q: 2.828916\n",
      "[ 5 41 32 11 13 57 30 90 72 90]\n",
      "  1089/50001: episode: 121, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 21.000, mean reward:  2.333 [-10.000,  5.000], mean action: 48.444 [11.000, 90.000],  loss: 13.349418, mae: 1.171848, mean_q: 2.933439\n",
      "[36  7 73 48 84 72 57 59 14  5]\n",
      "  1098/50001: episode: 122, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 37.000, mean reward:  4.111 [ 2.000,  7.000], mean action: 46.556 [5.000, 84.000],  loss: 12.486496, mae: 1.109136, mean_q: 2.756771\n",
      "[30 35 71  8 11 71  2 55 69 58]\n",
      "  1107/50001: episode: 123, duration: 0.070s, episode steps:   9, steps per second: 129, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 42.222 [2.000, 71.000],  loss: 9.714016, mae: 1.162562, mean_q: 2.840851\n",
      "[42  5 68 95 25 56  5 38 45 73]\n",
      "  1116/50001: episode: 124, duration: 0.082s, episode steps:   9, steps per second: 109, episode reward: 17.000, mean reward:  1.889 [-10.000,  6.000], mean action: 45.556 [5.000, 95.000],  loss: 12.379003, mae: 1.153566, mean_q: 2.860497\n",
      "[92 98 50 81 43 57 98 90  2 77]\n",
      "  1125/50001: episode: 125, duration: 0.078s, episode steps:   9, steps per second: 116, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 66.222 [2.000, 98.000],  loss: 12.202737, mae: 1.120627, mean_q: 2.765829\n",
      "[10 51 89 17 54 32 42 84 32 77]\n",
      "  1134/50001: episode: 126, duration: 0.084s, episode steps:   9, steps per second: 107, episode reward: 21.000, mean reward:  2.333 [-10.000,  5.000], mean action: 53.111 [17.000, 89.000],  loss: 11.536059, mae: 1.167966, mean_q: 2.865524\n",
      "[65  9 94 55 56 10 12  5 56 93]\n",
      "  1143/50001: episode: 127, duration: 0.084s, episode steps:   9, steps per second: 107, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 43.333 [5.000, 94.000],  loss: 12.855889, mae: 1.220176, mean_q: 2.994308\n",
      "[97 70 12 29 60 43 59 50 76 36]\n",
      "  1152/50001: episode: 128, duration: 0.084s, episode steps:   9, steps per second: 107, episode reward: 33.000, mean reward:  3.667 [ 2.000,  8.000], mean action: 48.333 [12.000, 76.000],  loss: 13.096418, mae: 1.166864, mean_q: 2.865909\n",
      "[68 66 74 67 97 29 69 17 96 39]\n",
      "  1161/50001: episode: 129, duration: 0.084s, episode steps:   9, steps per second: 107, episode reward: 30.000, mean reward:  3.333 [ 2.000,  5.000], mean action: 61.556 [17.000, 97.000],  loss: 12.284436, mae: 1.096257, mean_q: 2.692958\n",
      "[91 75 48 59 23 88 70 67 77 29]\n",
      "  1170/50001: episode: 130, duration: 0.084s, episode steps:   9, steps per second: 107, episode reward: 33.000, mean reward:  3.667 [ 3.000,  5.000], mean action: 59.556 [23.000, 88.000],  loss: 13.488331, mae: 1.184267, mean_q: 2.954204\n",
      "[29 45 72 25 23 57 29 51 88 59]\n",
      "  1179/50001: episode: 131, duration: 0.084s, episode steps:   9, steps per second: 107, episode reward: 23.000, mean reward:  2.556 [-10.000,  8.000], mean action: 49.889 [23.000, 88.000],  loss: 10.604122, mae: 1.179110, mean_q: 2.955710\n",
      "[96  9 67 80 58 52 33 81 64 23]\n",
      "  1188/50001: episode: 132, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward: 37.000, mean reward:  4.111 [ 3.000,  7.000], mean action: 51.889 [9.000, 81.000],  loss: 11.499831, mae: 1.109883, mean_q: 2.772104\n",
      "[75 21 23 97 10 73  8 31  9 48]\n",
      "  1197/50001: episode: 133, duration: 0.080s, episode steps:   9, steps per second: 112, episode reward: 38.000, mean reward:  4.222 [ 2.000,  8.000], mean action: 35.556 [8.000, 97.000],  loss: 12.707452, mae: 1.159590, mean_q: 2.804275\n",
      "[72 12  2 88 19 59 32 19 79 23]\n",
      "  1206/50001: episode: 134, duration: 0.082s, episode steps:   9, steps per second: 109, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 37.000 [2.000, 88.000],  loss: 14.534451, mae: 1.106983, mean_q: 2.702599\n",
      "[58 52 32 57 60 54 88 23  6 84]\n",
      "  1215/50001: episode: 135, duration: 0.070s, episode steps:   9, steps per second: 129, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 50.667 [6.000, 88.000],  loss: 12.009615, mae: 1.235403, mean_q: 3.068772\n",
      "[ 6  4 67 43 99 48 48 42 83 20]\n",
      "  1224/50001: episode: 136, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 21.000, mean reward:  2.333 [-10.000,  5.000], mean action: 50.444 [4.000, 99.000],  loss: 12.182650, mae: 1.138932, mean_q: 2.812506\n",
      "[37 14  5 22 50 87 84 21 21 69]\n",
      "  1233/50001: episode: 137, duration: 0.076s, episode steps:   9, steps per second: 119, episode reward: 18.000, mean reward:  2.000 [-10.000,  5.000], mean action: 41.444 [5.000, 87.000],  loss: 13.960188, mae: 1.088631, mean_q: 2.657196\n",
      "[62 35 35 77 29  5 46  5 15 31]\n",
      "  1242/50001: episode: 138, duration: 0.069s, episode steps:   9, steps per second: 131, episode reward:  9.000, mean reward:  1.000 [-10.000,  8.000], mean action: 30.889 [5.000, 77.000],  loss: 12.179285, mae: 1.136190, mean_q: 2.840920\n",
      "[88 55 74  2 70 89 45 59 36 53]\n",
      "  1251/50001: episode: 139, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 32.000, mean reward:  3.556 [ 2.000,  7.000], mean action: 53.667 [2.000, 89.000],  loss: 10.017120, mae: 1.119489, mean_q: 2.789797\n",
      "[86  8 46 97 10 80 73 51 20 15]\n",
      "  1260/50001: episode: 140, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 31.000, mean reward:  3.444 [ 2.000,  5.000], mean action: 44.444 [8.000, 97.000],  loss: 11.670054, mae: 1.218550, mean_q: 3.040204\n",
      "[ 4 13 88 58 43 57  2 77 77 20]\n",
      "  1269/50001: episode: 141, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 19.000, mean reward:  2.111 [-10.000,  5.000], mean action: 48.333 [2.000, 88.000],  loss: 12.763721, mae: 1.177988, mean_q: 2.932925\n",
      "[11 48 39 68 25 84 10 96 19  2]\n",
      "  1278/50001: episode: 142, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 35.000, mean reward:  3.889 [ 2.000,  8.000], mean action: 43.444 [2.000, 96.000],  loss: 11.847916, mae: 1.161269, mean_q: 2.896229\n",
      "[ 7 15 65 13 29 32 92  2 15 43]\n",
      "  1287/50001: episode: 143, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 20.000, mean reward:  2.222 [-10.000,  6.000], mean action: 34.000 [2.000, 92.000],  loss: 11.971760, mae: 1.157534, mean_q: 2.914422\n",
      "[65 13 98 78 97 48 73 12 79 22]\n",
      "  1296/50001: episode: 144, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 40.000, mean reward:  4.444 [ 2.000,  6.000], mean action: 57.778 [12.000, 98.000],  loss: 11.516586, mae: 1.201890, mean_q: 3.008942\n",
      "[70 68 75 57 57 60 46 25 88 97]\n",
      "  1305/50001: episode: 145, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 30.000, mean reward:  3.333 [-10.000,  8.000], mean action: 63.667 [25.000, 97.000],  loss: 13.381623, mae: 1.119538, mean_q: 2.785350\n",
      "[46 21 39 77 73  4 33 57  9 14]\n",
      "  1314/50001: episode: 146, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 36.333 [4.000, 77.000],  loss: 10.749317, mae: 1.142325, mean_q: 2.866212\n",
      "[39 88 24 80 88 34 16  7  8 27]\n",
      "  1323/50001: episode: 147, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 22.000, mean reward:  2.444 [-10.000,  7.000], mean action: 41.333 [7.000, 88.000],  loss: 10.942561, mae: 1.166023, mean_q: 2.966805\n",
      "[28 77 55 38 51 60 19 62 77 20]\n",
      "  1332/50001: episode: 148, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 18.000, mean reward:  2.000 [-10.000,  5.000], mean action: 51.000 [19.000, 77.000],  loss: 13.818767, mae: 1.102676, mean_q: 2.768674\n",
      "[41 49 48 85 58 54 38 98 72 82]\n",
      "  1341/50001: episode: 149, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 35.000, mean reward:  3.889 [ 2.000,  7.000], mean action: 64.889 [38.000, 98.000],  loss: 13.810453, mae: 1.167840, mean_q: 2.915564\n",
      "[52  4 56 71 35  4  5 31 59 20]\n",
      "  1350/50001: episode: 150, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 20.000, mean reward:  2.222 [-10.000,  6.000], mean action: 31.667 [4.000, 71.000],  loss: 11.964695, mae: 1.228698, mean_q: 3.103446\n",
      "[72 69 12 90 60  2 29 60  4 85]\n",
      "  1359/50001: episode: 151, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 45.667 [2.000, 90.000],  loss: 12.501680, mae: 1.077822, mean_q: 2.687634\n",
      "[65 37 59 22 72 77 20 78 52 22]\n",
      "  1368/50001: episode: 152, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 19.000, mean reward:  2.111 [-10.000,  6.000], mean action: 48.778 [20.000, 78.000],  loss: 11.525118, mae: 1.184259, mean_q: 2.946223\n",
      "[94 12 78 57 91 10 80 57 10 78]\n",
      "  1377/50001: episode: 153, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: -9.000, mean reward: -1.000 [-10.000,  4.000], mean action: 52.556 [10.000, 91.000],  loss: 13.126826, mae: 1.209604, mean_q: 3.041022\n",
      "[24 48 44 51 51 31 13 31  2 22]\n",
      "  1386/50001: episode: 154, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward:  9.000, mean reward:  1.000 [-10.000,  6.000], mean action: 32.556 [2.000, 51.000],  loss: 14.033986, mae: 1.195870, mean_q: 3.023501\n",
      "[19  4 33 22 55 48 59 84 80 38]\n",
      "  1395/50001: episode: 155, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 31.000, mean reward:  3.444 [ 2.000,  5.000], mean action: 47.000 [4.000, 84.000],  loss: 13.178060, mae: 1.154202, mean_q: 2.922029\n",
      "[70 69 69 92 16 88 21 48 79 29]\n",
      "  1404/50001: episode: 156, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 56.778 [16.000, 92.000],  loss: 13.976021, mae: 1.176004, mean_q: 2.972581\n",
      "[85 84 46 52 60 10 97 40 54 50]\n",
      "  1413/50001: episode: 157, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 45.000, mean reward:  5.000 [ 3.000, 10.000], mean action: 54.778 [10.000, 97.000],  loss: 13.395847, mae: 1.158921, mean_q: 2.835421\n",
      "[85 83 60 60 60 47 39 72 45  8]\n",
      "  1422/50001: episode: 158, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward:  4.000, mean reward:  0.444 [-10.000,  5.000], mean action: 52.667 [8.000, 83.000],  loss: 12.214741, mae: 1.111570, mean_q: 2.777253\n",
      "[13 18 10 98 67  4 78  5 31 58]\n",
      "  1431/50001: episode: 159, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 37.000, mean reward:  4.111 [ 2.000,  7.000], mean action: 41.000 [4.000, 98.000],  loss: 10.501910, mae: 1.201741, mean_q: 2.971845\n",
      "[71 97 57  1  1 57 67 18 21 50]\n",
      "  1440/50001: episode: 160, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 12.000, mean reward:  1.333 [-10.000, 10.000], mean action: 41.000 [1.000, 97.000],  loss: 13.150469, mae: 1.169509, mean_q: 2.907486\n",
      "[10 98 59 19 78 15 73 45 19 54]\n",
      "  1449/50001: episode: 161, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 15.000, mean reward:  1.667 [-10.000,  5.000], mean action: 51.111 [15.000, 98.000],  loss: 12.522328, mae: 1.169803, mean_q: 2.949751\n",
      "[18 26  0 88 50 68 52 61 46 12]\n",
      "  1458/50001: episode: 162, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 42.000, mean reward:  4.667 [ 2.000, 10.000], mean action: 44.778 [0.000, 88.000],  loss: 13.910673, mae: 1.176102, mean_q: 2.971118\n",
      "[36 16 24 35 77 98 14 51 43 54]\n",
      "  1467/50001: episode: 163, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 35.000, mean reward:  3.889 [ 3.000,  5.000], mean action: 45.778 [14.000, 98.000],  loss: 10.302109, mae: 1.143125, mean_q: 2.859090\n",
      "[29 34 30 97 67 60 98 66 58  4]\n",
      "  1476/50001: episode: 164, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 41.000, mean reward:  4.556 [ 3.000,  6.000], mean action: 57.111 [4.000, 98.000],  loss: 10.534674, mae: 1.138242, mean_q: 2.912920\n",
      "[96  6 24 48 33 67 32 38 55 97]\n",
      "  1485/50001: episode: 165, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 37.000, mean reward:  4.111 [ 2.000,  9.000], mean action: 44.444 [6.000, 97.000],  loss: 10.513251, mae: 1.248241, mean_q: 3.143742\n",
      "[68 66 18 12 60 58 25 50 77 90]\n",
      "  1494/50001: episode: 166, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 37.000, mean reward:  4.111 [ 2.000,  7.000], mean action: 50.667 [12.000, 90.000],  loss: 9.533563, mae: 1.166886, mean_q: 2.920374\n",
      "[71 14 98 39 46  4 27 76  7  2]\n",
      "  1503/50001: episode: 167, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 38.000, mean reward:  4.222 [ 2.000,  8.000], mean action: 34.778 [2.000, 98.000],  loss: 9.212944, mae: 1.215551, mean_q: 3.077949\n",
      "[88 72  2 65 97 97 65 79 28 41]\n",
      "  1512/50001: episode: 168, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 11.000, mean reward:  1.222 [-10.000,  6.000], mean action: 60.667 [2.000, 97.000],  loss: 11.853463, mae: 1.113912, mean_q: 2.769379\n",
      "[31 22 60 47  9 37 88  1 58  7]\n",
      "  1521/50001: episode: 169, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: 35.000, mean reward:  3.889 [ 1.000,  6.000], mean action: 36.556 [1.000, 88.000],  loss: 8.265478, mae: 1.195014, mean_q: 3.017913\n",
      "[36 90 88 58 80 38 43 96  2 77]\n",
      "  1530/50001: episode: 170, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 36.000, mean reward:  4.000 [ 2.000,  7.000], mean action: 63.556 [2.000, 96.000],  loss: 12.565111, mae: 1.176929, mean_q: 2.934496\n",
      "[ 6 22 20 29 49 57 96 64 42 22]\n",
      "  1539/50001: episode: 171, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 21.000, mean reward:  2.333 [-10.000,  7.000], mean action: 44.556 [20.000, 96.000],  loss: 12.229305, mae: 1.206918, mean_q: 3.058869\n",
      "[63 46 47 48 34 14 65 82  2  2]\n",
      "  1548/50001: episode: 172, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 37.778 [2.000, 82.000],  loss: 13.287041, mae: 1.180399, mean_q: 2.944680\n",
      "[85 22 37 22 58 52 15 26 10 66]\n",
      "  1557/50001: episode: 173, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 21.000, mean reward:  2.333 [-10.000,  8.000], mean action: 34.222 [10.000, 66.000],  loss: 11.396221, mae: 1.208489, mean_q: 3.085672\n",
      "[97 24 98 70 18 39 16 46 18 10]\n",
      "  1566/50001: episode: 174, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 37.667 [10.000, 98.000],  loss: 12.276406, mae: 1.199830, mean_q: 3.028916\n",
      "[50 48 47 60  1  9 92 97 58 14]\n",
      "  1575/50001: episode: 175, duration: 0.066s, episode steps:   9, steps per second: 135, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 47.333 [1.000, 97.000],  loss: 11.673226, mae: 1.137800, mean_q: 2.903253\n",
      "[51 40 79 51 77 27 20 93 14 86]\n",
      "  1584/50001: episode: 176, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 54.111 [14.000, 93.000],  loss: 10.721745, mae: 1.200547, mean_q: 3.037995\n",
      "[54  6 34 98 73 48  4 32 20 81]\n",
      "  1593/50001: episode: 177, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 38.000, mean reward:  4.222 [ 2.000,  6.000], mean action: 44.000 [4.000, 98.000],  loss: 12.134841, mae: 1.216808, mean_q: 3.065892\n",
      "[24 14 61 86  6 46 32  1 11 57]\n",
      "  1602/50001: episode: 178, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 34.889 [1.000, 86.000],  loss: 9.743282, mae: 1.182711, mean_q: 3.012472\n",
      "[91 14 69 46 67 82 60 71 20 39]\n",
      "  1611/50001: episode: 179, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 33.000, mean reward:  3.667 [ 2.000,  6.000], mean action: 52.000 [14.000, 82.000],  loss: 13.318487, mae: 1.131999, mean_q: 2.875383\n",
      "[73 63 23  2 48 50 16 80 46 57]\n",
      "  1620/50001: episode: 180, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 42.778 [2.000, 80.000],  loss: 12.722754, mae: 1.190031, mean_q: 3.022332\n",
      "[82 53 23 98 51 79 29 23 63 71]\n",
      "  1629/50001: episode: 181, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 17.000, mean reward:  1.889 [-10.000,  5.000], mean action: 54.444 [23.000, 98.000],  loss: 11.508340, mae: 1.241837, mean_q: 3.139858\n",
      "[ 8 21 94  2 31 40 32 80 31 84]\n",
      "  1638/50001: episode: 182, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 22.000, mean reward:  2.444 [-10.000,  5.000], mean action: 46.111 [2.000, 94.000],  loss: 12.879818, mae: 1.209803, mean_q: 3.128555\n",
      "[17 57 38 97 63 32 80 97 32  9]\n",
      "  1647/50001: episode: 183, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward:  9.000, mean reward:  1.000 [-10.000,  6.000], mean action: 56.111 [9.000, 97.000],  loss: 14.043130, mae: 1.183410, mean_q: 2.978260\n",
      "[93 17 23 16 22 80 14 45 27 95]\n",
      "  1656/50001: episode: 184, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 41.000, mean reward:  4.556 [ 3.000, 11.000], mean action: 37.667 [14.000, 95.000],  loss: 12.450873, mae: 1.153394, mean_q: 2.903468\n",
      "[67 41 90 24 16 77 70 11 38 97]\n",
      "  1665/50001: episode: 185, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 37.000, mean reward:  4.111 [ 2.000,  9.000], mean action: 51.556 [11.000, 97.000],  loss: 10.096674, mae: 1.253425, mean_q: 3.141574\n",
      "[99 31 75  1 58 70 59 58 61 67]\n",
      "  1674/50001: episode: 186, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 18.000, mean reward:  2.000 [-10.000,  5.000], mean action: 53.333 [1.000, 75.000],  loss: 12.764568, mae: 1.137944, mean_q: 2.917334\n",
      "[ 5 96 44 29 21 99 38 88 43 59]\n",
      "  1683/50001: episode: 187, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 32.000, mean reward:  3.556 [ 2.000,  7.000], mean action: 57.444 [21.000, 99.000],  loss: 11.303432, mae: 1.165140, mean_q: 2.943187\n",
      "[66  3 88 12 77 93 88 63 54 21]\n",
      "  1692/50001: episode: 188, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 18.000, mean reward:  2.000 [-10.000,  4.000], mean action: 55.444 [3.000, 93.000],  loss: 12.138895, mae: 1.242321, mean_q: 3.245292\n",
      "[40 29 25 21 49 62 51  8 38 50]\n",
      "  1701/50001: episode: 189, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 36.000, mean reward:  4.000 [ 2.000, 10.000], mean action: 37.000 [8.000, 62.000],  loss: 12.884048, mae: 1.149372, mean_q: 2.972902\n",
      "[28 15 75 23 45 96 57  8 95 31]\n",
      "  1710/50001: episode: 190, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 38.000, mean reward:  4.222 [ 2.000,  9.000], mean action: 49.444 [8.000, 96.000],  loss: 10.359892, mae: 1.175718, mean_q: 2.997934\n",
      "[10 54 84 50 58 96 38 69 65 31]\n",
      "  1719/50001: episode: 191, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 36.000, mean reward:  4.000 [ 2.000,  8.000], mean action: 60.556 [31.000, 96.000],  loss: 10.124654, mae: 1.204240, mean_q: 3.085935\n",
      "[43 84 97 32 97 54 21 14  3  3]\n",
      "  1728/50001: episode: 192, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward:  6.000, mean reward:  0.667 [-10.000,  6.000], mean action: 45.000 [3.000, 97.000],  loss: 13.209712, mae: 1.126887, mean_q: 2.852368\n",
      "[48 54 24 23 60 22 37 47 94 18]\n",
      "  1737/50001: episode: 193, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 36.000, mean reward:  4.000 [ 3.000,  6.000], mean action: 42.111 [18.000, 94.000],  loss: 10.747725, mae: 1.174097, mean_q: 2.986529\n",
      "[56 45 48 88 99 37 52 18 53 51]\n",
      "  1746/50001: episode: 194, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 37.000, mean reward:  4.111 [ 2.000,  6.000], mean action: 54.556 [18.000, 99.000],  loss: 11.963364, mae: 1.179242, mean_q: 2.998579\n",
      "[79 54 75 85 51 67 74 91 45 79]\n",
      "  1755/50001: episode: 195, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 18.000, mean reward:  2.000 [-10.000,  5.000], mean action: 69.000 [45.000, 91.000],  loss: 11.191233, mae: 1.167670, mean_q: 2.929159\n",
      "[80 97 88 27 84 15 38  5 54 28]\n",
      "  1764/50001: episode: 196, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 37.000, mean reward:  4.111 [ 2.000,  9.000], mean action: 48.444 [5.000, 97.000],  loss: 11.191982, mae: 1.116158, mean_q: 2.848755\n",
      "[36  2 44 73 67 48 48 57  4 16]\n",
      "  1773/50001: episode: 197, duration: 0.086s, episode steps:   9, steps per second: 105, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 39.889 [2.000, 73.000],  loss: 11.100937, mae: 1.231602, mean_q: 3.144122\n",
      "[64 57 23 24 10 57 30 46 57 89]\n",
      "  1782/50001: episode: 198, duration: 0.070s, episode steps:   9, steps per second: 128, episode reward:  8.000, mean reward:  0.889 [-10.000,  6.000], mean action: 43.667 [10.000, 89.000],  loss: 11.425193, mae: 1.222945, mean_q: 3.120833\n",
      "[31 12 24 70 48 78 50 10 54 54]\n",
      "  1791/50001: episode: 199, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 20.000, mean reward:  2.222 [-10.000,  6.000], mean action: 44.444 [10.000, 78.000],  loss: 8.625576, mae: 1.199305, mean_q: 3.081516\n",
      "[ 0 15 21 98 72 48 48 48 20 86]\n",
      "  1800/50001: episode: 200, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward:  6.000, mean reward:  0.667 [-10.000,  6.000], mean action: 50.667 [15.000, 98.000],  loss: 13.616749, mae: 1.155954, mean_q: 2.952207\n",
      "[76 41 47 88 59  4 58 46 58 86]\n",
      "  1809/50001: episode: 201, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 54.111 [4.000, 88.000],  loss: 12.601580, mae: 1.199690, mean_q: 3.028778\n",
      "[16 48 59 95 23 16 50 25 77 14]\n",
      "  1818/50001: episode: 202, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 45.222 [14.000, 95.000],  loss: 10.827746, mae: 1.174362, mean_q: 3.045840\n",
      "[81  9 64 79 39 80 88 43 42 20]\n",
      "  1827/50001: episode: 203, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 35.000, mean reward:  3.889 [ 2.000,  6.000], mean action: 51.556 [9.000, 88.000],  loss: 10.782368, mae: 1.227417, mean_q: 3.181098\n",
      "[38 88  1 50 23 10 75 57 88 54]\n",
      "  1836/50001: episode: 204, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 24.000, mean reward:  2.667 [-10.000,  5.000], mean action: 49.556 [1.000, 88.000],  loss: 13.115353, mae: 1.170412, mean_q: 2.995044\n",
      "[63  2 22 56 67 32 10  1 32 83]\n",
      "  1845/50001: episode: 205, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 33.889 [1.000, 83.000],  loss: 10.543550, mae: 1.155742, mean_q: 2.997590\n",
      "[46 86 92 73 97 54  2 57 45 40]\n",
      "  1854/50001: episode: 206, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 60.667 [2.000, 97.000],  loss: 10.155359, mae: 1.136368, mean_q: 2.932705\n",
      "[34 74 69 17 70 49 67 27 76 82]\n",
      "  1863/50001: episode: 207, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 35.000, mean reward:  3.889 [ 2.000,  7.000], mean action: 59.000 [17.000, 82.000],  loss: 10.455773, mae: 1.263733, mean_q: 3.254065\n",
      "[58 21 74  1 48 67 88 48 52 50]\n",
      "  1872/50001: episode: 208, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 49.889 [1.000, 88.000],  loss: 11.112241, mae: 1.139026, mean_q: 2.966647\n",
      "[ 0 88 47 19  9 73 88 48  2  1]\n",
      "  1881/50001: episode: 209, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 41.667 [1.000, 88.000],  loss: 9.156736, mae: 1.260626, mean_q: 3.285144\n",
      "[ 5  1 39 14 77 10 62 75  9 96]\n",
      "  1890/50001: episode: 210, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 35.000, mean reward:  3.889 [ 2.000,  6.000], mean action: 42.556 [1.000, 96.000],  loss: 10.073742, mae: 1.214691, mean_q: 3.129812\n",
      "[69 13 47 18 81  5 52 24 17  9]\n",
      "  1899/50001: episode: 211, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 34.000, mean reward:  3.778 [ 2.000,  6.000], mean action: 29.556 [5.000, 81.000],  loss: 12.709036, mae: 1.233782, mean_q: 3.216546\n",
      "[22 46 23 32 29 25 80 31 19 96]\n",
      "  1908/50001: episode: 212, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 32.000, mean reward:  3.556 [ 2.000,  6.000], mean action: 42.333 [19.000, 96.000],  loss: 12.095387, mae: 1.210809, mean_q: 3.112590\n",
      "[19 86 90 22 58 63 46 18 54 21]\n",
      "  1917/50001: episode: 213, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 36.000, mean reward:  4.000 [ 3.000,  5.000], mean action: 50.889 [18.000, 90.000],  loss: 11.012127, mae: 1.206088, mean_q: 3.115567\n",
      "[85  4 13  4 88 72 48 80 18 84]\n",
      "  1926/50001: episode: 214, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 19.000, mean reward:  2.111 [-10.000,  5.000], mean action: 45.667 [4.000, 88.000],  loss: 8.748894, mae: 1.220988, mean_q: 3.183409\n",
      "[72 30 64 89 19 48 50 64  2 55]\n",
      "  1935/50001: episode: 215, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 26.000, mean reward:  2.889 [-10.000,  6.000], mean action: 46.778 [2.000, 89.000],  loss: 11.992338, mae: 1.259897, mean_q: 3.292723\n",
      "[ 0 70 59 57 19 46 67 57 99 14]\n",
      "  1944/50001: episode: 216, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 54.222 [14.000, 99.000],  loss: 11.254986, mae: 1.184455, mean_q: 3.047761\n",
      "[76  5 50 50  4  4 56 19 33 78]\n",
      "  1953/50001: episode: 217, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward:  4.000, mean reward:  0.444 [-10.000,  5.000], mean action: 33.222 [4.000, 78.000],  loss: 10.829276, mae: 1.238682, mean_q: 3.217780\n",
      "[90 16 90 64 10 73 48 67 92 20]\n",
      "  1962/50001: episode: 218, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 53.333 [10.000, 92.000],  loss: 12.909397, mae: 1.225680, mean_q: 3.157649\n",
      "[37 58 39 57 24 51 50 73 58 31]\n",
      "  1971/50001: episode: 219, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 22.000, mean reward:  2.444 [-10.000,  7.000], mean action: 49.000 [24.000, 73.000],  loss: 14.971117, mae: 1.234650, mean_q: 3.184854\n",
      "[55 69 69 79 88  9 73 85  8 27]\n",
      "  1980/50001: episode: 220, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 56.333 [8.000, 88.000],  loss: 11.773189, mae: 1.182160, mean_q: 3.077407\n",
      "[93  6 36 28 68 33  4 88 16 62]\n",
      "  1989/50001: episode: 221, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 36.000, mean reward:  4.000 [ 2.000,  6.000], mean action: 37.889 [4.000, 88.000],  loss: 11.916599, mae: 1.157754, mean_q: 3.000733\n",
      "[97  4 88 29 77 52 88 31 85 35]\n",
      "  1998/50001: episode: 222, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 19.000, mean reward:  2.111 [-10.000,  5.000], mean action: 54.333 [4.000, 88.000],  loss: 10.226936, mae: 1.183681, mean_q: 3.062059\n",
      "[87 13 90  2  4 48 16 46  0 13]\n",
      "  2007/50001: episode: 223, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 19.000, mean reward:  2.111 [-10.000,  5.000], mean action: 25.778 [0.000, 90.000],  loss: 11.182086, mae: 1.230985, mean_q: 3.221930\n",
      "[61 50  9 48 74  5 86 19 19 83]\n",
      "  2016/50001: episode: 224, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 18.000, mean reward:  2.000 [-10.000,  4.000], mean action: 43.667 [5.000, 86.000],  loss: 9.571075, mae: 1.186611, mean_q: 3.106598\n",
      "[29 60 90  2 97 50 63  1 77 81]\n",
      "  2025/50001: episode: 225, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: 40.000, mean reward:  4.444 [ 3.000,  6.000], mean action: 57.889 [1.000, 97.000],  loss: 12.280354, mae: 1.195870, mean_q: 3.115841\n",
      "[33 42 63 97  1 69 67  9 48  9]\n",
      "  2034/50001: episode: 226, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 45.000 [1.000, 97.000],  loss: 13.048044, mae: 1.260143, mean_q: 3.284839\n",
      "[76  7  2 21 48 64 88 74 98 88]\n",
      "  2043/50001: episode: 227, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 54.444 [2.000, 98.000],  loss: 12.988892, mae: 1.187622, mean_q: 3.111609\n",
      "[ 6 12 59 50  2 21 27  1 27  2]\n",
      "  2052/50001: episode: 228, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 10.000, mean reward:  1.111 [-10.000,  7.000], mean action: 22.333 [1.000, 59.000],  loss: 11.880616, mae: 1.146722, mean_q: 2.984254\n",
      "[59 22 65 70 11 19 10 11 19 64]\n",
      "  2061/50001: episode: 229, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward:  5.000, mean reward:  0.556 [-10.000,  6.000], mean action: 32.333 [10.000, 70.000],  loss: 9.939300, mae: 1.191350, mean_q: 3.157335\n",
      "[31 86 51 23 30 96 79 62 97 97]\n",
      "  2070/50001: episode: 230, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 69.000 [23.000, 97.000],  loss: 11.959543, mae: 1.187781, mean_q: 3.163750\n",
      "[46 33 18 74 30 56 92 61 23  9]\n",
      "  2079/50001: episode: 231, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 37.000, mean reward:  4.111 [ 3.000,  6.000], mean action: 44.000 [9.000, 92.000],  loss: 15.124783, mae: 1.195420, mean_q: 3.090093\n",
      "[77 84 24 97 70 22 35 81 59 85]\n",
      "  2088/50001: episode: 232, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 33.000, mean reward:  3.667 [ 2.000,  5.000], mean action: 61.889 [22.000, 97.000],  loss: 9.265190, mae: 1.091049, mean_q: 2.854458\n",
      "[78 57 24 34 50 69 80 96  9  9]\n",
      "  2097/50001: episode: 233, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 21.000, mean reward:  2.333 [-10.000,  5.000], mean action: 47.556 [9.000, 96.000],  loss: 10.641423, mae: 1.218361, mean_q: 3.161662\n",
      "[63 70 23 10 48 46 64 47 63 42]\n",
      "  2106/50001: episode: 234, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 45.889 [10.000, 70.000],  loss: 10.926891, mae: 1.219088, mean_q: 3.198547\n",
      "[55 60 60 21 80  2 15  2 91 56]\n",
      "  2115/50001: episode: 235, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward:  8.000, mean reward:  0.889 [-10.000,  6.000], mean action: 43.000 [2.000, 91.000],  loss: 13.136646, mae: 1.199489, mean_q: 3.152714\n",
      "[73 64 88 57 97  5 40 32 45 34]\n",
      "  2124/50001: episode: 236, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 43.000, mean reward:  4.778 [ 3.000, 10.000], mean action: 51.333 [5.000, 97.000],  loss: 11.759278, mae: 1.229818, mean_q: 3.204645\n",
      "[60  9 12 13 21 55 30  2 41 12]\n",
      "  2133/50001: episode: 237, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 21.667 [2.000, 55.000],  loss: 10.634528, mae: 1.187948, mean_q: 3.101238\n",
      "[ 0 40 30 22 96  9  2 39 45 84]\n",
      "  2142/50001: episode: 238, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 36.000, mean reward:  4.000 [ 3.000,  6.000], mean action: 40.778 [2.000, 96.000],  loss: 11.788286, mae: 1.221014, mean_q: 3.167911\n",
      "[72 48 25 20 32 59 96 46 55 20]\n",
      "  2151/50001: episode: 239, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 20.000, mean reward:  2.222 [-10.000,  6.000], mean action: 44.556 [20.000, 96.000],  loss: 11.311664, mae: 1.236904, mean_q: 3.321094\n",
      "[81 43 37 16 70  5 23 70 93 13]\n",
      "  2160/50001: episode: 240, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 27.000, mean reward:  3.000 [-10.000, 10.000], mean action: 41.111 [5.000, 93.000],  loss: 11.776820, mae: 1.225592, mean_q: 3.215947\n",
      "[ 0 21 97 84 15 72 50  2 54 12]\n",
      "  2169/50001: episode: 241, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 40.000, mean reward:  4.444 [ 2.000,  9.000], mean action: 45.222 [2.000, 97.000],  loss: 9.704220, mae: 1.245568, mean_q: 3.219093\n",
      "[67 74 25 12 23 72 88  1 89 96]\n",
      "  2178/50001: episode: 242, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 38.000, mean reward:  4.222 [ 2.000,  6.000], mean action: 53.333 [1.000, 96.000],  loss: 13.027363, mae: 1.198840, mean_q: 3.160223\n",
      "[30 72 81 56 96 32 16  2 49 43]\n",
      "  2187/50001: episode: 243, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 33.000, mean reward:  3.667 [ 2.000,  6.000], mean action: 49.667 [2.000, 96.000],  loss: 10.658658, mae: 1.249351, mean_q: 3.305691\n",
      "[ 7 51  2 64 30 73 16 48 89  9]\n",
      "  2196/50001: episode: 244, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 37.000, mean reward:  4.111 [ 2.000,  6.000], mean action: 42.444 [2.000, 89.000],  loss: 8.903486, mae: 1.236554, mean_q: 3.226888\n",
      "[66  4 53 83 12 32 48 67 29  4]\n",
      "  2205/50001: episode: 245, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 19.000, mean reward:  2.111 [-10.000,  5.000], mean action: 36.889 [4.000, 83.000],  loss: 11.947118, mae: 1.220377, mean_q: 3.215486\n",
      "[10 42 59 16 59 19 99 58 25 52]\n",
      "  2214/50001: episode: 246, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 18.000, mean reward:  2.000 [-10.000,  5.000], mean action: 47.667 [16.000, 99.000],  loss: 12.415551, mae: 1.174787, mean_q: 3.072608\n",
      "[67 65 47 32 88 49 62 22 54 64]\n",
      "  2223/50001: episode: 247, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 37.000, mean reward:  4.111 [ 2.000,  6.000], mean action: 53.667 [22.000, 88.000],  loss: 11.276214, mae: 1.233311, mean_q: 3.259884\n",
      "[29 32  9 44 39 51 24 48  3 86]\n",
      "  2232/50001: episode: 248, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 33.000, mean reward:  3.667 [ 2.000,  6.000], mean action: 37.333 [3.000, 86.000],  loss: 10.271026, mae: 1.221315, mean_q: 3.216058\n",
      "[25 10 69 19 45 54 88 99 50 90]\n",
      "  2241/50001: episode: 249, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 58.222 [10.000, 99.000],  loss: 10.248816, mae: 1.201235, mean_q: 3.184768\n",
      "[83  9 38 67 46 33  4  4 81 10]\n",
      "  2250/50001: episode: 250, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 32.444 [4.000, 81.000],  loss: 9.235778, mae: 1.220638, mean_q: 3.248455\n",
      "[27 63 59 67 15 57 21 32 97 22]\n",
      "  2259/50001: episode: 251, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 48.111 [15.000, 97.000],  loss: 8.455589, mae: 1.291598, mean_q: 3.426491\n",
      "[56 57 88 17 54 45 43 81 85 14]\n",
      "  2268/50001: episode: 252, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 35.000, mean reward:  3.889 [ 2.000,  8.000], mean action: 53.778 [14.000, 88.000],  loss: 11.779680, mae: 1.208430, mean_q: 3.170744\n",
      "[80 24 98 74 59  9 80 33  4 96]\n",
      "  2277/50001: episode: 253, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 21.000, mean reward:  2.333 [-10.000,  6.000], mean action: 53.000 [4.000, 98.000],  loss: 12.310108, mae: 1.328064, mean_q: 3.517666\n",
      "[ 6 12 82 44  2  2 70 17 99 38]\n",
      "  2286/50001: episode: 254, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 20.000, mean reward:  2.222 [-10.000,  6.000], mean action: 40.667 [2.000, 99.000],  loss: 8.298208, mae: 1.250082, mean_q: 3.262799\n",
      "[10 84 79 99  2 19 19 55 84 46]\n",
      "  2295/50001: episode: 255, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward:  8.000, mean reward:  0.889 [-10.000,  7.000], mean action: 54.111 [2.000, 99.000],  loss: 9.377183, mae: 1.306300, mean_q: 3.450075\n",
      "[ 3 96 59 13  1 52 21 21  2 99]\n",
      "  2304/50001: episode: 256, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 40.444 [1.000, 99.000],  loss: 12.297011, mae: 1.320554, mean_q: 3.478621\n",
      "[ 7 21 21 32 29 43  1 97 83 72]\n",
      "  2313/50001: episode: 257, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 22.000, mean reward:  2.444 [-10.000,  7.000], mean action: 44.333 [1.000, 97.000],  loss: 9.700774, mae: 1.227463, mean_q: 3.237511\n",
      "[94 29 88  2 80  9 41 85  9 94]\n",
      "  2322/50001: episode: 258, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward:  6.000, mean reward:  0.667 [-10.000,  5.000], mean action: 48.556 [2.000, 94.000],  loss: 10.429277, mae: 1.236553, mean_q: 3.309593\n",
      "[70 88 75 57 81 72  2 46 40 53]\n",
      "  2331/50001: episode: 259, duration: 0.057s, episode steps:   9, steps per second: 158, episode reward: 40.000, mean reward:  4.444 [ 2.000,  6.000], mean action: 57.111 [2.000, 88.000],  loss: 11.606853, mae: 1.248442, mean_q: 3.307744\n",
      "[ 9 10 30 85 32 75 57 96 39 66]\n",
      "  2340/50001: episode: 260, duration: 0.057s, episode steps:   9, steps per second: 158, episode reward: 39.000, mean reward:  4.333 [ 3.000,  9.000], mean action: 54.444 [10.000, 96.000],  loss: 12.587646, mae: 1.217409, mean_q: 3.176488\n",
      "[13 49 88 17 46 53 38 46 58 65]\n",
      "  2349/50001: episode: 261, duration: 0.057s, episode steps:   9, steps per second: 158, episode reward: 19.000, mean reward:  2.111 [-10.000,  6.000], mean action: 51.111 [17.000, 88.000],  loss: 9.483452, mae: 1.210209, mean_q: 3.193614\n",
      "[93 68 67 50 10 46 22 29 50 24]\n",
      "  2358/50001: episode: 262, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 40.667 [10.000, 68.000],  loss: 11.251365, mae: 1.238869, mean_q: 3.281234\n",
      "[54 84 79 88 85 55 62 84 18 73]\n",
      "  2367/50001: episode: 263, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 18.000, mean reward:  2.000 [-10.000,  5.000], mean action: 69.778 [18.000, 88.000],  loss: 12.400973, mae: 1.240949, mean_q: 3.297812\n",
      "[47 51 79 25 56 77  8 92 83 15]\n",
      "  2376/50001: episode: 264, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: 29.000, mean reward:  3.222 [ 2.000,  4.000], mean action: 54.000 [8.000, 92.000],  loss: 11.324290, mae: 1.262101, mean_q: 3.295542\n",
      "[ 5  9 54 23 70 76  7 18 26 38]\n",
      "  2385/50001: episode: 265, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: 27.000, mean reward:  3.000 [ 2.000,  4.000], mean action: 35.667 [7.000, 76.000],  loss: 9.894052, mae: 1.296584, mean_q: 3.429936\n",
      "[31 74 69 57 45  2 46 72 58 58]\n",
      "  2394/50001: episode: 266, duration: 0.057s, episode steps:   9, steps per second: 159, episode reward: 19.000, mean reward:  2.111 [-10.000,  5.000], mean action: 53.444 [2.000, 74.000],  loss: 10.340906, mae: 1.252702, mean_q: 3.355507\n",
      "[21 95 22 58 59 49 51 50 48 59]\n",
      "  2403/50001: episode: 267, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 54.556 [22.000, 95.000],  loss: 8.951928, mae: 1.277258, mean_q: 3.428790\n",
      "[ 8 90 38 73 97  2  2 25 32 94]\n",
      "  2412/50001: episode: 268, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 50.333 [2.000, 97.000],  loss: 9.520310, mae: 1.241315, mean_q: 3.296521\n",
      "[98 48 52 74 25  2 80 32 23  9]\n",
      "  2421/50001: episode: 269, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 33.000, mean reward:  3.667 [ 2.000,  5.000], mean action: 38.333 [2.000, 80.000],  loss: 8.772496, mae: 1.231976, mean_q: 3.290680\n",
      "[ 5 88 98  2 73 54  9 38 77  2]\n",
      "  2430/50001: episode: 270, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 18.000, mean reward:  2.000 [-10.000,  5.000], mean action: 49.000 [2.000, 98.000],  loss: 8.598324, mae: 1.282246, mean_q: 3.443630\n",
      "[31 14 59 11 41 67 88 22 83  9]\n",
      "  2439/50001: episode: 271, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 43.778 [9.000, 88.000],  loss: 8.902813, mae: 1.230500, mean_q: 3.291806\n",
      "[63 21 38 79 59 57 10 65 97 12]\n",
      "  2448/50001: episode: 272, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 43.000, mean reward:  4.778 [ 3.000,  9.000], mean action: 48.667 [10.000, 97.000],  loss: 7.886652, mae: 1.281317, mean_q: 3.401892\n",
      "[18 71 24 12  1 58 23 50  2 11]\n",
      "  2457/50001: episode: 273, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 28.000 [1.000, 71.000],  loss: 8.657543, mae: 1.309413, mean_q: 3.482935\n",
      "[10 21 62 34 38 71 88 37 86 31]\n",
      "  2466/50001: episode: 274, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 52.000 [21.000, 88.000],  loss: 11.143021, mae: 1.346094, mean_q: 3.570475\n",
      "[91 10 59 48 46 43 43 34 86 20]\n",
      "  2475/50001: episode: 275, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 25.000, mean reward:  2.778 [-10.000,  9.000], mean action: 43.222 [10.000, 86.000],  loss: 12.257337, mae: 1.261985, mean_q: 3.372073\n",
      "[60 56 59 49 72 72 90 63 59 23]\n",
      "  2484/50001: episode: 276, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward:  8.000, mean reward:  0.889 [-10.000,  7.000], mean action: 60.333 [23.000, 90.000],  loss: 9.155087, mae: 1.255226, mean_q: 3.380843\n",
      "[40 83 44 79 14 46 88 48 73  2]\n",
      "  2493/50001: episode: 277, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 41.000, mean reward:  4.556 [ 2.000,  7.000], mean action: 53.000 [2.000, 88.000],  loss: 12.737100, mae: 1.282107, mean_q: 3.430299\n",
      "[90 97  1 48 23 59 50 50 48 69]\n",
      "  2502/50001: episode: 278, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward:  7.000, mean reward:  0.778 [-10.000,  6.000], mean action: 49.444 [1.000, 97.000],  loss: 8.136697, mae: 1.294296, mean_q: 3.414850\n",
      "[31 54  2 79 33 11 71 37 59 48]\n",
      "  2511/50001: episode: 279, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 40.000, mean reward:  4.444 [ 3.000,  8.000], mean action: 43.778 [2.000, 79.000],  loss: 8.987050, mae: 1.263404, mean_q: 3.375791\n",
      "[65 69 48 33 79 23 17 20 48 76]\n",
      "  2520/50001: episode: 280, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 19.000, mean reward:  2.111 [-10.000,  5.000], mean action: 45.889 [17.000, 79.000],  loss: 10.707362, mae: 1.324594, mean_q: 3.521377\n",
      "[53 70 48 80 30 75 57 51 14 70]\n",
      "  2529/50001: episode: 281, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 23.000, mean reward:  2.556 [-10.000,  5.000], mean action: 55.000 [14.000, 80.000],  loss: 8.644057, mae: 1.293596, mean_q: 3.408281\n",
      "[51  1 32 37 69 72 45 78 16 22]\n",
      "  2538/50001: episode: 282, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 33.000, mean reward:  3.667 [ 2.000,  6.000], mean action: 41.333 [1.000, 78.000],  loss: 9.872835, mae: 1.341268, mean_q: 3.576907\n",
      "[84 41 59 80 24 88 79 30 96 97]\n",
      "  2547/50001: episode: 283, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 43.000, mean reward:  4.778 [ 3.000,  8.000], mean action: 66.000 [24.000, 97.000],  loss: 9.234678, mae: 1.340596, mean_q: 3.593961\n",
      "[34 79 88 25 17 18 29 43 40 73]\n",
      "  2556/50001: episode: 284, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 30.000, mean reward:  3.333 [ 2.000,  6.000], mean action: 45.778 [17.000, 88.000],  loss: 10.611469, mae: 1.348865, mean_q: 3.609051\n",
      "[94 21 14 42 20  2 37 58 14 10]\n",
      "  2565/50001: episode: 285, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 24.222 [2.000, 58.000],  loss: 11.102764, mae: 1.326337, mean_q: 3.504848\n",
      "[48  4 65 69 51 54 76 69 92 31]\n",
      "  2574/50001: episode: 286, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: 21.000, mean reward:  2.333 [-10.000,  7.000], mean action: 56.778 [4.000, 92.000],  loss: 10.503345, mae: 1.276098, mean_q: 3.380728\n",
      "[83  2  2 80 33 48 55 48 31 66]\n",
      "  2583/50001: episode: 287, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 10.000, mean reward:  1.111 [-10.000,  7.000], mean action: 40.556 [2.000, 80.000],  loss: 10.452038, mae: 1.303444, mean_q: 3.472531\n",
      "[54 44 48 48 79 91 22 49 12 42]\n",
      "  2592/50001: episode: 288, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 48.333 [12.000, 91.000],  loss: 7.644867, mae: 1.323152, mean_q: 3.527320\n",
      "[58 42 50  1 72 41 55  9 64 46]\n",
      "  2601/50001: episode: 289, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 42.222 [1.000, 72.000],  loss: 10.956055, mae: 1.345255, mean_q: 3.562657\n",
      "[68 63 18 48 10 59 48 57 11 94]\n",
      "  2610/50001: episode: 290, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 19.000, mean reward:  2.111 [-10.000,  5.000], mean action: 45.333 [10.000, 94.000],  loss: 10.758135, mae: 1.274389, mean_q: 3.404817\n",
      "[29 18 10  9 40 28  4 76 96 24]\n",
      "  2619/50001: episode: 291, duration: 0.069s, episode steps:   9, steps per second: 131, episode reward: 37.000, mean reward:  4.111 [ 2.000,  7.000], mean action: 33.889 [4.000, 96.000],  loss: 9.901360, mae: 1.327146, mean_q: 3.578485\n",
      "[16 49 64  4 50 19 73 50 99 34]\n",
      "  2628/50001: episode: 292, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 25.000, mean reward:  2.778 [-10.000,  9.000], mean action: 49.111 [4.000, 99.000],  loss: 9.968287, mae: 1.260411, mean_q: 3.375672\n",
      "[43 45 92 49 51 84 20 73 86 20]\n",
      "  2637/50001: episode: 293, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 18.000, mean reward:  2.000 [-10.000,  4.000], mean action: 57.778 [20.000, 92.000],  loss: 9.569498, mae: 1.293981, mean_q: 3.482381\n",
      "[34 71 32 50 86 38 50  5 48 44]\n",
      "  2646/50001: episode: 294, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 21.000, mean reward:  2.333 [-10.000,  7.000], mean action: 47.111 [5.000, 86.000],  loss: 8.526882, mae: 1.344943, mean_q: 3.626472\n",
      "[33 96 74 88 24 67 37 21 50 46]\n",
      "  2655/50001: episode: 295, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 43.000, mean reward:  4.778 [ 3.000,  8.000], mean action: 55.889 [21.000, 96.000],  loss: 9.803547, mae: 1.301580, mean_q: 3.504668\n",
      "[21 67 20 50 37 45 46 92 59 26]\n",
      "  2664/50001: episode: 296, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 35.000, mean reward:  3.889 [ 2.000,  6.000], mean action: 49.111 [20.000, 92.000],  loss: 12.417534, mae: 1.347312, mean_q: 3.638164\n",
      "[ 7 96 78 54 50 21 57  2  2 99]\n",
      "  2673/50001: episode: 297, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 51.000 [2.000, 99.000],  loss: 10.209787, mae: 1.346293, mean_q: 3.619210\n",
      "[88 90 46  1 50 58 48 18 47  9]\n",
      "  2682/50001: episode: 298, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 40.778 [1.000, 90.000],  loss: 7.553459, mae: 1.350490, mean_q: 3.665230\n",
      "[20 75 73 56 99 21 50  9 99 24]\n",
      "  2691/50001: episode: 299, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 56.222 [9.000, 99.000],  loss: 8.651071, mae: 1.308098, mean_q: 3.519072\n",
      "[16 10 23 22 48 10 76 45  7 32]\n",
      "  2700/50001: episode: 300, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 19.000, mean reward:  2.111 [-10.000,  9.000], mean action: 30.333 [7.000, 76.000],  loss: 10.106063, mae: 1.363480, mean_q: 3.662333\n",
      "[30 16 88  1 96 13 14 44 13 31]\n",
      "  2709/50001: episode: 301, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 35.111 [1.000, 96.000],  loss: 9.502295, mae: 1.299816, mean_q: 3.531127\n",
      "[22 14 13 38 73 18 54 21 28 55]\n",
      "  2718/50001: episode: 302, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 33.000, mean reward:  3.667 [ 2.000,  8.000], mean action: 34.889 [13.000, 73.000],  loss: 10.266330, mae: 1.357189, mean_q: 3.654496\n",
      "[46 63 79 80 19 54 80 43 23 68]\n",
      "  2727/50001: episode: 303, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 19.000, mean reward:  2.111 [-10.000,  6.000], mean action: 56.556 [19.000, 80.000],  loss: 10.249767, mae: 1.307146, mean_q: 3.511118\n",
      "[53  9 84 30 37 85 21 44 30 86]\n",
      "  2736/50001: episode: 304, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 20.000, mean reward:  2.222 [-10.000,  6.000], mean action: 47.333 [9.000, 86.000],  loss: 11.447818, mae: 1.347222, mean_q: 3.596772\n",
      "[54 12 92 96 10 29 42 80  2 77]\n",
      "  2745/50001: episode: 305, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 35.000, mean reward:  3.889 [ 3.000,  7.000], mean action: 48.889 [2.000, 96.000],  loss: 10.872042, mae: 1.319187, mean_q: 3.541919\n",
      "[41 67 50 16 50 16 50 61  2 59]\n",
      "  2754/50001: episode: 306, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: -5.000, mean reward: -0.556 [-10.000,  6.000], mean action: 41.222 [2.000, 67.000],  loss: 10.437078, mae: 1.301228, mean_q: 3.465421\n",
      "[41 46 68 88 46 14 52 73  2 46]\n",
      "  2763/50001: episode: 307, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward:  9.000, mean reward:  1.000 [-10.000,  6.000], mean action: 48.333 [2.000, 88.000],  loss: 9.520229, mae: 1.311476, mean_q: 3.506677\n",
      "[15 69 12 70 48 88 63 30 14 33]\n",
      "  2772/50001: episode: 308, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 38.000, mean reward:  4.222 [ 2.000,  6.000], mean action: 47.444 [12.000, 88.000],  loss: 9.640079, mae: 1.315297, mean_q: 3.514685\n",
      "[ 6 45 92 42 45 69 50 97 48 46]\n",
      "  2781/50001: episode: 309, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 31.000, mean reward:  3.444 [-10.000,  7.000], mean action: 59.333 [42.000, 97.000],  loss: 10.136383, mae: 1.284717, mean_q: 3.468705\n",
      "[97 79 46 51  2 59 24 50 67 59]\n",
      "  2790/50001: episode: 310, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 48.556 [2.000, 79.000],  loss: 10.674108, mae: 1.340279, mean_q: 3.586037\n",
      "[56 16 61 74 77 65 10 38 51 79]\n",
      "  2799/50001: episode: 311, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 36.000, mean reward:  4.000 [ 2.000,  8.000], mean action: 52.333 [10.000, 79.000],  loss: 8.650762, mae: 1.325480, mean_q: 3.533354\n",
      "[19 43 76 88  1 23  1 32 97 86]\n",
      "  2808/50001: episode: 312, duration: 0.074s, episode steps:   9, steps per second: 122, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 49.667 [1.000, 97.000],  loss: 10.110050, mae: 1.318177, mean_q: 3.515092\n",
      "[ 8 96 51 73 63 67 60  4 90 48]\n",
      "  2817/50001: episode: 313, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 61.333 [4.000, 96.000],  loss: 10.816535, mae: 1.259866, mean_q: 3.335728\n",
      "[53 79 52  2  1 25  2 62 60 51]\n",
      "  2826/50001: episode: 314, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 37.111 [1.000, 79.000],  loss: 7.952692, mae: 1.331683, mean_q: 3.578900\n",
      "[79 96 74 50 10 25 88 46 96 77]\n",
      "  2835/50001: episode: 315, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 62.444 [10.000, 96.000],  loss: 11.339703, mae: 1.278732, mean_q: 3.447901\n",
      "[53 96 51 22 46 24 57 59 60 32]\n",
      "  2844/50001: episode: 316, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 49.667 [22.000, 96.000],  loss: 11.123855, mae: 1.321659, mean_q: 3.501920\n",
      "[32  9 27 39 55 33 80 42  0 83]\n",
      "  2853/50001: episode: 317, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 30.000, mean reward:  3.333 [ 2.000,  5.000], mean action: 40.889 [0.000, 83.000],  loss: 9.495033, mae: 1.263270, mean_q: 3.358980\n",
      "[47 96 58 32 50  2 84 38 50 19]\n",
      "  2862/50001: episode: 318, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 18.000, mean reward:  2.000 [-10.000,  5.000], mean action: 47.667 [2.000, 96.000],  loss: 10.033651, mae: 1.280417, mean_q: 3.450027\n",
      "[77 41 84 50 55  8 30 15 77 31]\n",
      "  2871/50001: episode: 319, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 24.000, mean reward:  2.667 [-10.000,  8.000], mean action: 43.444 [8.000, 84.000],  loss: 10.210506, mae: 1.333210, mean_q: 3.590722\n",
      "[75 51  2 50 79 34 72 94 50 93]\n",
      "  2880/50001: episode: 320, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 58.333 [2.000, 94.000],  loss: 9.298057, mae: 1.370211, mean_q: 3.648300\n",
      "[97 13 33 83 48 12  4 18  1 62]\n",
      "  2889/50001: episode: 321, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 38.000, mean reward:  4.222 [ 3.000,  7.000], mean action: 30.444 [1.000, 83.000],  loss: 9.377945, mae: 1.312429, mean_q: 3.523666\n",
      "[ 1 14 71 50 25 23 73  5 42  8]\n",
      "  2898/50001: episode: 322, duration: 0.069s, episode steps:   9, steps per second: 130, episode reward: 32.000, mean reward:  3.556 [ 2.000,  6.000], mean action: 34.556 [5.000, 73.000],  loss: 10.756407, mae: 1.312595, mean_q: 3.536068\n",
      "[58 20 74 39  7 50 10 86  1 88]\n",
      "  2907/50001: episode: 323, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 41.667 [1.000, 88.000],  loss: 10.596077, mae: 1.354645, mean_q: 3.614258\n",
      "[48 10 21 50 88 33  4 10 88 69]\n",
      "  2916/50001: episode: 324, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward:  8.000, mean reward:  0.889 [-10.000,  5.000], mean action: 41.444 [4.000, 88.000],  loss: 8.732212, mae: 1.336836, mean_q: 3.617835\n",
      "[20 49 60 99 63 32 77 39 40 40]\n",
      "  2925/50001: episode: 325, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 21.000, mean reward:  2.333 [-10.000,  6.000], mean action: 55.444 [32.000, 99.000],  loss: 10.002430, mae: 1.386634, mean_q: 3.758045\n",
      "[57 91 30 48 46 21 57 81 40 32]\n",
      "  2934/50001: episode: 326, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 49.556 [21.000, 91.000],  loss: 8.248697, mae: 1.350616, mean_q: 3.670318\n",
      "[94 98 80 79  1 46 10 79 44 50]\n",
      "  2943/50001: episode: 327, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 26.000, mean reward:  2.889 [-10.000,  9.000], mean action: 54.111 [1.000, 98.000],  loss: 9.396612, mae: 1.342799, mean_q: 3.620710\n",
      "[48 83 72 22 30 99 64 46 61 62]\n",
      "  2952/50001: episode: 328, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 35.000, mean reward:  3.889 [ 2.000,  6.000], mean action: 59.889 [22.000, 99.000],  loss: 10.797773, mae: 1.378979, mean_q: 3.730215\n",
      "[31 46 80 59 21 90  4 73 46 77]\n",
      "  2961/50001: episode: 329, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 19.000, mean reward:  2.111 [-10.000,  5.000], mean action: 55.111 [4.000, 90.000],  loss: 8.782515, mae: 1.387689, mean_q: 3.759177\n",
      "[64 22 48  2 10 63 23 52 87  9]\n",
      "  2970/50001: episode: 330, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 34.000, mean reward:  3.778 [ 3.000,  6.000], mean action: 35.111 [2.000, 87.000],  loss: 9.578693, mae: 1.349595, mean_q: 3.658322\n",
      "[55 83 58 32 32 14 57 97 72 54]\n",
      "  2979/50001: episode: 331, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 55.444 [14.000, 97.000],  loss: 11.280248, mae: 1.362160, mean_q: 3.694953\n",
      "[91 88 59 33 37  2  2 88 60 87]\n",
      "  2988/50001: episode: 332, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward:  9.000, mean reward:  1.000 [-10.000,  6.000], mean action: 50.667 [2.000, 88.000],  loss: 10.882870, mae: 1.354947, mean_q: 3.700017\n",
      "[10  4 11 19 17 58 88 50 14 98]\n",
      "  2997/50001: episode: 333, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 39.889 [4.000, 98.000],  loss: 8.724361, mae: 1.322956, mean_q: 3.581745\n",
      "[60 46 59 55 50 16 96 56  2 32]\n",
      "  3006/50001: episode: 334, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 45.778 [2.000, 96.000],  loss: 9.980283, mae: 1.412773, mean_q: 3.786944\n",
      "[48 49 59 77 67 78 42 93 28 20]\n",
      "  3015/50001: episode: 335, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 57.000 [20.000, 93.000],  loss: 8.804630, mae: 1.394902, mean_q: 3.764820\n",
      "[58 96 22 64 77  1 59 50 59  9]\n",
      "  3024/50001: episode: 336, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 48.556 [1.000, 96.000],  loss: 9.183864, mae: 1.311007, mean_q: 3.496128\n",
      "[48 49 80 94 25 44 50 76 86 40]\n",
      "  3033/50001: episode: 337, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 37.000, mean reward:  4.111 [ 3.000,  7.000], mean action: 60.444 [25.000, 94.000],  loss: 11.246227, mae: 1.347702, mean_q: 3.594206\n",
      "[79 83 79 37 30  1 99 54 25 96]\n",
      "  3042/50001: episode: 338, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 21.000, mean reward:  2.333 [-10.000,  5.000], mean action: 56.000 [1.000, 99.000],  loss: 10.189182, mae: 1.374662, mean_q: 3.693512\n",
      "[70  2 50 80 67 43 62  1 37 86]\n",
      "  3051/50001: episode: 339, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 38.000, mean reward:  4.222 [ 2.000,  8.000], mean action: 47.556 [1.000, 86.000],  loss: 8.494516, mae: 1.342310, mean_q: 3.608657\n",
      "[99 67 16 59 51  1 25 32 83 65]\n",
      "  3060/50001: episode: 340, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 34.000, mean reward:  3.778 [ 3.000,  6.000], mean action: 44.333 [1.000, 83.000],  loss: 9.147937, mae: 1.332954, mean_q: 3.582988\n",
      "[54 83 79 33 14 63  2  2 32 37]\n",
      "  3069/50001: episode: 341, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 27.000, mean reward:  3.000 [-10.000,  8.000], mean action: 38.333 [2.000, 83.000],  loss: 10.068761, mae: 1.378478, mean_q: 3.689501\n",
      "[24  2 18 46 64 33 57 83  9 98]\n",
      "  3078/50001: episode: 342, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 37.000, mean reward:  4.111 [ 2.000,  7.000], mean action: 45.556 [2.000, 98.000],  loss: 11.960819, mae: 1.381913, mean_q: 3.724529\n",
      "[94 26 88 64 10 69 67 61 67 31]\n",
      "  3087/50001: episode: 343, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 22.000, mean reward:  2.444 [-10.000,  8.000], mean action: 53.667 [10.000, 88.000],  loss: 10.595727, mae: 1.284758, mean_q: 3.414168\n",
      "[10 12 33 25 54 57 51 43 22 71]\n",
      "  3096/50001: episode: 344, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 32.000, mean reward:  3.556 [ 3.000,  5.000], mean action: 40.889 [12.000, 71.000],  loss: 9.292708, mae: 1.366209, mean_q: 3.606539\n",
      "[14  5 35 99 48 42 80 58 96 20]\n",
      "  3105/50001: episode: 345, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 31.000, mean reward:  3.444 [ 2.000,  4.000], mean action: 53.667 [5.000, 99.000],  loss: 10.121471, mae: 1.372334, mean_q: 3.624787\n",
      "[70 53 44 10 48 83 51 72 66 85]\n",
      "  3114/50001: episode: 346, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 35.000, mean reward:  3.889 [ 2.000,  7.000], mean action: 56.889 [10.000, 85.000],  loss: 7.692762, mae: 1.314898, mean_q: 3.494774\n",
      "[83 47 60  9 19 60 88 48 30 84]\n",
      "  3123/50001: episode: 347, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 26.000, mean reward:  2.889 [-10.000,  6.000], mean action: 49.444 [9.000, 88.000],  loss: 10.100610, mae: 1.318568, mean_q: 3.547899\n",
      "[58 25 48 79 48 32 46 17 94 48]\n",
      "  3132/50001: episode: 348, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward:  6.000, mean reward:  0.667 [-10.000,  5.000], mean action: 48.556 [17.000, 94.000],  loss: 11.321099, mae: 1.353946, mean_q: 3.620048\n",
      "[12 21 36 65  1 68 19 94  7 40]\n",
      "  3141/50001: episode: 349, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 32.000, mean reward:  3.556 [ 2.000,  7.000], mean action: 39.000 [1.000, 94.000],  loss: 12.965040, mae: 1.300438, mean_q: 3.466788\n",
      "[96  9 83 47 81 52 50 21 63 12]\n",
      "  3150/50001: episode: 350, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 40.000, mean reward:  4.444 [ 3.000,  9.000], mean action: 46.444 [9.000, 83.000],  loss: 9.200348, mae: 1.288207, mean_q: 3.442300\n",
      "[82 42 75 23 97 14 83 22 13 49]\n",
      "  3159/50001: episode: 351, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 38.000, mean reward:  4.222 [ 3.000,  8.000], mean action: 46.444 [13.000, 97.000],  loss: 10.865046, mae: 1.321660, mean_q: 3.557353\n",
      "[30 66 13 18 59 90 42 23 18  3]\n",
      "  3168/50001: episode: 352, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 18.000, mean reward:  2.000 [-10.000,  5.000], mean action: 36.889 [3.000, 90.000],  loss: 10.796288, mae: 1.343823, mean_q: 3.539517\n",
      "[26 89 11 37 23 80 79 88 85 77]\n",
      "  3177/50001: episode: 353, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 63.222 [11.000, 89.000],  loss: 9.625364, mae: 1.318250, mean_q: 3.490989\n",
      "[59 50 76 36 50 21 36 19 43 72]\n",
      "  3186/50001: episode: 354, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward:  2.000, mean reward:  0.222 [-10.000,  4.000], mean action: 44.778 [19.000, 76.000],  loss: 8.454726, mae: 1.333012, mean_q: 3.587385\n",
      "[91 42 35 74 33 67  4 22 92 20]\n",
      "  3195/50001: episode: 355, duration: 0.057s, episode steps:   9, steps per second: 158, episode reward: 36.000, mean reward:  4.000 [ 3.000,  5.000], mean action: 43.222 [4.000, 92.000],  loss: 11.168801, mae: 1.320241, mean_q: 3.581741\n",
      "[46 67 24 79 10  2 58 37 63 14]\n",
      "  3204/50001: episode: 356, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 39.333 [2.000, 79.000],  loss: 10.288770, mae: 1.347661, mean_q: 3.579066\n",
      "[60 48 24 46 30  2 50 63 82 10]\n",
      "  3213/50001: episode: 357, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 40.000, mean reward:  4.444 [ 3.000,  6.000], mean action: 39.444 [2.000, 82.000],  loss: 9.135942, mae: 1.377887, mean_q: 3.677432\n",
      "[33 58 59 84 50  2 74 45 77 88]\n",
      "  3222/50001: episode: 358, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 41.000, mean reward:  4.556 [ 3.000,  9.000], mean action: 59.667 [2.000, 88.000],  loss: 9.742885, mae: 1.430364, mean_q: 3.829878\n",
      "[13  5 71 10 32 68 88 78 62 30]\n",
      "  3231/50001: episode: 359, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 35.000, mean reward:  3.889 [ 2.000,  7.000], mean action: 49.333 [5.000, 88.000],  loss: 9.981595, mae: 1.349318, mean_q: 3.606181\n",
      "[27 50 84 63  2 97 77 61 73 55]\n",
      "  3240/50001: episode: 360, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 29.000, mean reward:  3.222 [ 2.000,  5.000], mean action: 62.444 [2.000, 97.000],  loss: 9.718334, mae: 1.329352, mean_q: 3.516592\n",
      "[66 15 79 84 54 46 39 52 28  9]\n",
      "  3249/50001: episode: 361, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 45.111 [9.000, 84.000],  loss: 7.757436, mae: 1.324632, mean_q: 3.498847\n",
      "[ 1 16 47 29 62 87 46 42 86 90]\n",
      "  3258/50001: episode: 362, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 35.000, mean reward:  3.889 [ 2.000,  7.000], mean action: 56.111 [16.000, 90.000],  loss: 10.439001, mae: 1.364002, mean_q: 3.633229\n",
      "[75 42 32 48 22 24 79 50 83 55]\n",
      "  3267/50001: episode: 363, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 37.000, mean reward:  4.111 [ 3.000,  7.000], mean action: 48.333 [22.000, 83.000],  loss: 9.256506, mae: 1.343909, mean_q: 3.557938\n",
      "[61 16 79 32 77 54 69 92 24  4]\n",
      "  3276/50001: episode: 364, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 38.000, mean reward:  4.222 [ 3.000,  7.000], mean action: 49.667 [4.000, 92.000],  loss: 10.020223, mae: 1.434055, mean_q: 3.836835\n",
      "[50 84 65 21 50 50 80 50  8 55]\n",
      "  3285/50001: episode: 365, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: -15.000, mean reward: -1.667 [-10.000,  4.000], mean action: 51.444 [8.000, 84.000],  loss: 11.066106, mae: 1.415308, mean_q: 3.732558\n",
      "[44 52 32 97 59 54 79 20 86 31]\n",
      "  3294/50001: episode: 366, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 37.000, mean reward:  4.111 [ 3.000,  7.000], mean action: 56.667 [20.000, 97.000],  loss: 6.210166, mae: 1.382772, mean_q: 3.666949\n",
      "[ 2  2 50 43 85 52 22 27 42 48]\n",
      "  3303/50001: episode: 367, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 41.222 [2.000, 85.000],  loss: 8.916227, mae: 1.401756, mean_q: 3.670096\n",
      "[10  2 59 36 18  4  2 38 83  2]\n",
      "  3312/50001: episode: 368, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward:  3.000, mean reward:  0.333 [-10.000,  5.000], mean action: 27.111 [2.000, 83.000],  loss: 11.047286, mae: 1.390054, mean_q: 3.667267\n",
      "[92 63 23 51 50 44 79 85 28  4]\n",
      "  3321/50001: episode: 369, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 37.000, mean reward:  4.111 [ 3.000,  6.000], mean action: 47.444 [4.000, 85.000],  loss: 8.914078, mae: 1.452282, mean_q: 3.898376\n",
      "[85 83 23  2 81 50 34 81 44 93]\n",
      "  3330/50001: episode: 370, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 54.556 [2.000, 93.000],  loss: 9.119055, mae: 1.410993, mean_q: 3.748417\n",
      "[14 88 75 88 96 58 69 70 85  2]\n",
      "  3339/50001: episode: 371, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: 22.000, mean reward:  2.444 [-10.000,  7.000], mean action: 70.111 [2.000, 96.000],  loss: 9.102961, mae: 1.402486, mean_q: 3.720377\n",
      "[87 83 46 43 76 70 37 65 65 65]\n",
      "  3348/50001: episode: 372, duration: 0.057s, episode steps:   9, steps per second: 158, episode reward:  6.000, mean reward:  0.667 [-10.000,  8.000], mean action: 61.111 [37.000, 83.000],  loss: 8.850019, mae: 1.271191, mean_q: 3.375531\n",
      "[73 46 24 37 61 30 47 64 77 84]\n",
      "  3357/50001: episode: 373, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 37.000, mean reward:  4.111 [ 2.000,  6.000], mean action: 52.222 [24.000, 84.000],  loss: 9.989508, mae: 1.347684, mean_q: 3.526797\n",
      "[78 22 90 97 46 76 50 80 59 31]\n",
      "  3366/50001: episode: 374, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 61.222 [22.000, 97.000],  loss: 9.349691, mae: 1.400486, mean_q: 3.636161\n",
      "[32 28 25 96 55 48 20 46 31 40]\n",
      "  3375/50001: episode: 375, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 37.000, mean reward:  4.111 [ 2.000,  6.000], mean action: 43.222 [20.000, 96.000],  loss: 9.202310, mae: 1.392143, mean_q: 3.716455\n",
      "[70 78 44 64 79 80 79 45 67 33]\n",
      "  3384/50001: episode: 376, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 19.000, mean reward:  2.111 [-10.000,  5.000], mean action: 63.222 [33.000, 80.000],  loss: 13.423084, mae: 1.380431, mean_q: 3.664972\n",
      "[43  9 38 64 46 32 48 51  4 24]\n",
      "  3393/50001: episode: 377, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 42.000, mean reward:  4.667 [ 3.000,  6.000], mean action: 35.111 [4.000, 64.000],  loss: 10.628115, mae: 1.372527, mean_q: 3.642297\n",
      "[30 67 32 18 24  2 33 59 25 44]\n",
      "  3402/50001: episode: 378, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 34.000, mean reward:  3.778 [ 3.000,  5.000], mean action: 33.778 [2.000, 67.000],  loss: 11.492459, mae: 1.335940, mean_q: 3.502227\n",
      "[18 23 37 50 97 62 37 41 22 48]\n",
      "  3411/50001: episode: 379, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 46.333 [22.000, 97.000],  loss: 8.450109, mae: 1.278313, mean_q: 3.423732\n",
      "[26 52 39 88 30 51 50 15 19  0]\n",
      "  3420/50001: episode: 380, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 35.000, mean reward:  3.889 [ 2.000,  7.000], mean action: 38.222 [0.000, 88.000],  loss: 10.260415, mae: 1.309417, mean_q: 3.473800\n",
      "[17 77 30 57 88 55 45 79 50  9]\n",
      "  3429/50001: episode: 381, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 54.444 [9.000, 88.000],  loss: 9.226501, mae: 1.338689, mean_q: 3.499581\n",
      "[44  0 95  9 31 99 23 16 41 15]\n",
      "  3438/50001: episode: 382, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 36.000, mean reward:  4.000 [ 2.000,  6.000], mean action: 36.556 [0.000, 99.000],  loss: 8.825973, mae: 1.331517, mean_q: 3.451770\n",
      "[39 99 50 24 50 63 97 16 39 92]\n",
      "  3447/50001: episode: 383, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward:  9.000, mean reward:  1.000 [-10.000,  5.000], mean action: 58.889 [16.000, 99.000],  loss: 7.501383, mae: 1.343638, mean_q: 3.517746\n",
      "[85 86 79 50 30  9 70 23 72 51]\n",
      "  3456/50001: episode: 384, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 52.222 [9.000, 86.000],  loss: 10.484584, mae: 1.373360, mean_q: 3.589614\n",
      "[97 66 44 37 14 80 50 88 65 62]\n",
      "  3465/50001: episode: 385, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 34.000, mean reward:  3.778 [ 2.000,  6.000], mean action: 56.222 [14.000, 88.000],  loss: 9.052013, mae: 1.364973, mean_q: 3.556390\n",
      "[ 5 14 55  0 44 99 96 18 94 97]\n",
      "  3474/50001: episode: 386, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 34.000, mean reward:  3.778 [ 2.000,  9.000], mean action: 57.444 [0.000, 99.000],  loss: 9.629952, mae: 1.378233, mean_q: 3.619670\n",
      "[76 82 23 16 48 18 75 83 56 72]\n",
      "  3483/50001: episode: 387, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 34.000, mean reward:  3.778 [ 2.000,  5.000], mean action: 52.556 [16.000, 83.000],  loss: 10.029336, mae: 1.348779, mean_q: 3.609747\n",
      "[65 51 97 59 58 38 51 73 72 59]\n",
      "  3492/50001: episode: 388, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward:  0.000, mean reward:  0.000 [-10.000,  4.000], mean action: 62.000 [38.000, 97.000],  loss: 11.091177, mae: 1.409520, mean_q: 3.710952\n",
      "[70 55 52 38 10 30 24 16 77 33]\n",
      "  3501/50001: episode: 389, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 37.000, mean reward:  4.111 [ 2.000,  6.000], mean action: 37.222 [10.000, 77.000],  loss: 9.297467, mae: 1.428638, mean_q: 3.730574\n",
      "[63 43 25 96 50 88 84 67 26  7]\n",
      "  3510/50001: episode: 390, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 32.000, mean reward:  3.556 [ 2.000,  6.000], mean action: 54.000 [7.000, 96.000],  loss: 11.456875, mae: 1.358061, mean_q: 3.558257\n",
      "[50 31 43 25 50 50 53 70 55 17]\n",
      "  3519/50001: episode: 391, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward:  2.000, mean reward:  0.222 [-10.000,  4.000], mean action: 43.778 [17.000, 70.000],  loss: 8.337516, mae: 1.389686, mean_q: 3.659926\n",
      "[92 13 32 25 57 76 21 27 27 78]\n",
      "  3528/50001: episode: 392, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward: 18.000, mean reward:  2.000 [-10.000,  5.000], mean action: 39.556 [13.000, 78.000],  loss: 7.540223, mae: 1.422913, mean_q: 3.682700\n",
      "[ 4 32 79 21 15 85 99 32 48 46]\n",
      "  3537/50001: episode: 393, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 22.000, mean reward:  2.444 [-10.000,  7.000], mean action: 50.778 [15.000, 99.000],  loss: 11.711477, mae: 1.443153, mean_q: 3.828920\n",
      "[67 95 10 24  2 63 79 80 57 46]\n",
      "  3546/50001: episode: 394, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 50.667 [2.000, 95.000],  loss: 10.966845, mae: 1.400878, mean_q: 3.654630\n",
      "[23 68 97 50 50  9 73  2 49 75]\n",
      "  3555/50001: episode: 395, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 52.556 [2.000, 97.000],  loss: 8.503130, mae: 1.346457, mean_q: 3.529089\n",
      "[28 76 47 46 17 13  1 25 63 48]\n",
      "  3564/50001: episode: 396, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 37.333 [1.000, 76.000],  loss: 8.117746, mae: 1.429612, mean_q: 3.798856\n",
      "[73 99 39 59 38 60 47 24 46 79]\n",
      "  3573/50001: episode: 397, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 42.000, mean reward:  4.667 [ 2.000,  7.000], mean action: 54.556 [24.000, 99.000],  loss: 9.415182, mae: 1.392420, mean_q: 3.660247\n",
      "[24 14 60 22 23 18 32 69 15 82]\n",
      "  3582/50001: episode: 398, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 34.000, mean reward:  3.778 [ 2.000,  6.000], mean action: 37.222 [14.000, 82.000],  loss: 10.127715, mae: 1.353452, mean_q: 3.595435\n",
      "[47 45 59 16 93 50 25 17 58 79]\n",
      "  3591/50001: episode: 399, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 37.000, mean reward:  4.111 [ 2.000,  8.000], mean action: 49.111 [16.000, 93.000],  loss: 7.638529, mae: 1.365380, mean_q: 3.588359\n",
      "[30  9 74 21 97  6 60 93 67 56]\n",
      "  3600/50001: episode: 400, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 53.667 [6.000, 97.000],  loss: 9.828523, mae: 1.480315, mean_q: 3.898725\n",
      "[72 12 33 41 18 48 34 80 33  2]\n",
      "  3609/50001: episode: 401, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 33.444 [2.000, 80.000],  loss: 9.278858, mae: 1.418958, mean_q: 3.767123\n",
      "[83  4 50 73 33 32 43 20  9 22]\n",
      "  3618/50001: episode: 402, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 36.000, mean reward:  4.000 [ 2.000,  6.000], mean action: 31.778 [4.000, 73.000],  loss: 8.978846, mae: 1.471906, mean_q: 3.844019\n",
      "[12 21  2 61 79 88 96 83 94 97]\n",
      "  3627/50001: episode: 403, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 37.000, mean reward:  4.111 [ 2.000,  8.000], mean action: 69.000 [2.000, 97.000],  loss: 8.340228, mae: 1.505463, mean_q: 3.963422\n",
      "[88 19 96 34 46 32  0 50  0 22]\n",
      "  3636/50001: episode: 404, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 33.222 [0.000, 96.000],  loss: 8.429125, mae: 1.481869, mean_q: 3.941706\n",
      "[72 42 80  1 63 56 21 68 96 25]\n",
      "  3645/50001: episode: 405, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 34.000, mean reward:  3.778 [ 3.000,  5.000], mean action: 50.222 [1.000, 96.000],  loss: 8.685152, mae: 1.461464, mean_q: 3.879261\n",
      "[22 40 68 85 50 13  2 73 52 81]\n",
      "  3654/50001: episode: 406, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 34.000, mean reward:  3.778 [ 2.000,  5.000], mean action: 51.556 [2.000, 85.000],  loss: 9.509992, mae: 1.490588, mean_q: 3.924273\n",
      "[70 98 60 32 84 97 50 50 96 31]\n",
      "  3663/50001: episode: 407, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 66.444 [31.000, 98.000],  loss: 8.364802, mae: 1.444683, mean_q: 3.846992\n",
      "[ 0 84 33 23 88 69 50 50 40 80]\n",
      "  3672/50001: episode: 408, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 57.444 [23.000, 88.000],  loss: 9.745867, mae: 1.488673, mean_q: 3.918077\n",
      "[39 78  2 50 37 51 46 86 94 62]\n",
      "  3681/50001: episode: 409, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 37.000, mean reward:  4.111 [ 3.000,  5.000], mean action: 56.222 [2.000, 94.000],  loss: 10.976951, mae: 1.475661, mean_q: 3.843479\n",
      "[47 79 25 34 76 50 79 88 26 28]\n",
      "  3690/50001: episode: 410, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 53.889 [25.000, 88.000],  loss: 7.421961, mae: 1.481390, mean_q: 3.881502\n",
      "[72 46 71 54 92 63 96 42 78 33]\n",
      "  3699/50001: episode: 411, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 36.000, mean reward:  4.000 [ 2.000,  6.000], mean action: 63.889 [33.000, 96.000],  loss: 9.844189, mae: 1.429587, mean_q: 3.702363\n",
      "[50 45 90 88 58 54 15 46 17 42]\n",
      "  3708/50001: episode: 412, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 37.000, mean reward:  4.111 [ 2.000,  7.000], mean action: 50.556 [15.000, 90.000],  loss: 8.821517, mae: 1.450176, mean_q: 3.758813\n",
      "[12 52 58 44 46  4 22 67 31 88]\n",
      "  3717/50001: episode: 413, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 45.778 [4.000, 88.000],  loss: 8.190205, mae: 1.490507, mean_q: 3.869046\n",
      "[80 30  1 52 46 37 36 63 23 55]\n",
      "  3726/50001: episode: 414, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 37.000, mean reward:  4.111 [ 2.000,  6.000], mean action: 38.111 [1.000, 63.000],  loss: 8.844440, mae: 1.523874, mean_q: 3.954514\n",
      "[40 52 24 11 45 58 46 13 49  9]\n",
      "  3735/50001: episode: 415, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 34.111 [9.000, 58.000],  loss: 9.865909, mae: 1.474847, mean_q: 3.835936\n",
      "[24 63 75 46 58 99  2 21 30 76]\n",
      "  3744/50001: episode: 416, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 36.000, mean reward:  4.000 [ 3.000,  5.000], mean action: 52.222 [2.000, 99.000],  loss: 9.503584, mae: 1.532677, mean_q: 3.936443\n",
      "[52 23 28  2 57 73 96 67 38 20]\n",
      "  3753/50001: episode: 417, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 30.000, mean reward:  3.333 [ 2.000,  5.000], mean action: 44.889 [2.000, 96.000],  loss: 9.731761, mae: 1.446220, mean_q: 3.763565\n",
      "[88  6 64 79 19 76 88 99 58  9]\n",
      "  3762/50001: episode: 418, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 20.000, mean reward:  2.222 [-10.000,  5.000], mean action: 55.333 [6.000, 99.000],  loss: 8.524249, mae: 1.467372, mean_q: 3.813950\n",
      "[ 0  2 22 49 99 78 24 30 86 12]\n",
      "  3771/50001: episode: 419, duration: 0.072s, episode steps:   9, steps per second: 126, episode reward: 41.000, mean reward:  4.556 [ 3.000, 10.000], mean action: 44.667 [2.000, 99.000],  loss: 8.147343, mae: 1.393515, mean_q: 3.654553\n",
      "[27 42 39 66 63 31 97 60 23 14]\n",
      "  3780/50001: episode: 420, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 40.000, mean reward:  4.444 [ 2.000,  6.000], mean action: 48.333 [14.000, 97.000],  loss: 8.612597, mae: 1.508573, mean_q: 3.891472\n",
      "[97 46  9 75 46 75 10 88 98 98]\n",
      "  3789/50001: episode: 421, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: -2.000, mean reward: -0.222 [-10.000,  7.000], mean action: 60.556 [9.000, 98.000],  loss: 8.839025, mae: 1.404924, mean_q: 3.645821\n",
      "[19  4 67 51 54 85  2  4 19 74]\n",
      "  3798/50001: episode: 422, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward:  7.000, mean reward:  0.778 [-10.000,  6.000], mean action: 40.000 [2.000, 85.000],  loss: 8.985830, mae: 1.483601, mean_q: 3.849191\n",
      "[ 3 69 50 23 23 80 48 20  2 86]\n",
      "  3807/50001: episode: 423, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 44.556 [2.000, 86.000],  loss: 9.243910, mae: 1.561400, mean_q: 4.026239\n",
      "[21 51 80 73 97  4 77 49 40 73]\n",
      "  3816/50001: episode: 424, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 20.000, mean reward:  2.222 [-10.000,  6.000], mean action: 60.444 [4.000, 97.000],  loss: 8.691187, mae: 1.478443, mean_q: 3.821440\n",
      "[71 75 57 56 97 55 79 93  4 38]\n",
      "  3825/50001: episode: 425, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 36.000, mean reward:  4.000 [ 2.000,  6.000], mean action: 61.556 [4.000, 97.000],  loss: 8.074841, mae: 1.450832, mean_q: 3.853202\n",
      "[ 5 69 50 17  9 71 62 38  4 49]\n",
      "  3834/50001: episode: 426, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 34.000, mean reward:  3.778 [ 2.000,  7.000], mean action: 41.000 [4.000, 71.000],  loss: 10.093234, mae: 1.410269, mean_q: 3.637298\n",
      "[86 57 37 30 46 28 50 63 83 13]\n",
      "  3843/50001: episode: 427, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 43.000, mean reward:  4.778 [ 3.000,  9.000], mean action: 45.222 [13.000, 83.000],  loss: 6.869676, mae: 1.489583, mean_q: 3.871983\n",
      "[28 23 59  6 65 10 13 21 10 33]\n",
      "  3852/50001: episode: 428, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 19.000, mean reward:  2.111 [-10.000,  6.000], mean action: 26.667 [6.000, 65.000],  loss: 8.046276, mae: 1.437198, mean_q: 3.704690\n",
      "[92  4 50 36 64  5 76 28 13 14]\n",
      "  3861/50001: episode: 429, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 32.222 [4.000, 76.000],  loss: 10.556970, mae: 1.461689, mean_q: 3.742224\n",
      "[67 83 32 48 29 49 92 76 31 28]\n",
      "  3870/50001: episode: 430, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 39.000, mean reward:  4.333 [ 2.000,  8.000], mean action: 52.000 [28.000, 92.000],  loss: 9.689667, mae: 1.494798, mean_q: 3.831895\n",
      "[27  9 18 30 80 25 91 55 50 24]\n",
      "  3879/50001: episode: 431, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 39.000, mean reward:  4.333 [ 2.000,  9.000], mean action: 42.444 [9.000, 91.000],  loss: 8.189331, mae: 1.460640, mean_q: 3.776752\n",
      "[55 43 22 24 24 80 32 73 19 68]\n",
      "  3888/50001: episode: 432, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 19.000, mean reward:  2.111 [-10.000,  5.000], mean action: 42.778 [19.000, 80.000],  loss: 8.243833, mae: 1.507669, mean_q: 3.891562\n",
      "[44  9 36 49 58 58 95 34  4 14]\n",
      "  3897/50001: episode: 433, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 30.000, mean reward:  3.333 [-10.000,  8.000], mean action: 39.667 [4.000, 95.000],  loss: 8.302653, mae: 1.471196, mean_q: 3.748624\n",
      "[19 96 52 57 51 50 16 97 40  4]\n",
      "  3906/50001: episode: 434, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 40.000, mean reward:  4.444 [ 3.000,  6.000], mean action: 51.444 [4.000, 97.000],  loss: 9.257966, mae: 1.462300, mean_q: 3.768866\n",
      "[24 42 25 97  1 36 50 44 94 79]\n",
      "  3915/50001: episode: 435, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 37.000, mean reward:  4.111 [ 2.000,  7.000], mean action: 52.000 [1.000, 97.000],  loss: 7.604799, mae: 1.395487, mean_q: 3.618074\n",
      "[70 42 65 62 44  4 20 58 37 79]\n",
      "  3924/50001: episode: 436, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 42.000, mean reward:  4.667 [ 2.000,  9.000], mean action: 45.667 [4.000, 79.000],  loss: 7.529421, mae: 1.496684, mean_q: 3.866943\n",
      "[40 48 91 36 99 55 24 28  2 96]\n",
      "  3933/50001: episode: 437, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 37.000, mean reward:  4.111 [ 2.000,  6.000], mean action: 53.222 [2.000, 99.000],  loss: 9.648918, mae: 1.499243, mean_q: 3.894119\n",
      "[ 7  4 33 88 46 17 48 94 31  9]\n",
      "  3942/50001: episode: 438, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 42.000, mean reward:  4.667 [ 3.000,  6.000], mean action: 41.111 [4.000, 94.000],  loss: 9.131369, mae: 1.557924, mean_q: 4.032114\n",
      "[36  9 77 17 92 93  4 89 78  9]\n",
      "  3951/50001: episode: 439, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 52.000 [4.000, 93.000],  loss: 8.175573, mae: 1.534002, mean_q: 3.970063\n",
      "[83 86 90 10 73 32 32 32 25 64]\n",
      "  3960/50001: episode: 440, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward:  5.000, mean reward:  0.556 [-10.000,  5.000], mean action: 49.333 [10.000, 90.000],  loss: 8.131665, mae: 1.544931, mean_q: 3.993171\n",
      "[64 10 98 99 58 52 46 62 31 94]\n",
      "  3969/50001: episode: 441, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 61.111 [10.000, 99.000],  loss: 7.910301, mae: 1.545537, mean_q: 3.935958\n",
      "[76 68 75 46 80 17 85 71  4  4]\n",
      "  3978/50001: episode: 442, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 21.000, mean reward:  2.333 [-10.000,  7.000], mean action: 50.000 [4.000, 85.000],  loss: 10.254102, mae: 1.513396, mean_q: 3.910815\n",
      "[65 13 24 49 75 96 74 10 97 97]\n",
      "  3987/50001: episode: 443, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 59.444 [10.000, 97.000],  loss: 10.631982, mae: 1.580138, mean_q: 4.091853\n",
      "[23 27 39  3 71  5  5 27 31 24]\n",
      "  3996/50001: episode: 444, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward:  9.000, mean reward:  1.000 [-10.000,  7.000], mean action: 25.778 [3.000, 71.000],  loss: 8.926304, mae: 1.509733, mean_q: 3.843901\n",
      "[ 9 59 86  2 54 21 46 54  5 99]\n",
      "  4005/50001: episode: 445, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 47.333 [2.000, 99.000],  loss: 9.235989, mae: 1.503852, mean_q: 3.899858\n",
      "[10 30 46 51 58 32 79 19 86  9]\n",
      "  4014/50001: episode: 446, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 37.000, mean reward:  4.111 [ 3.000,  6.000], mean action: 45.556 [9.000, 86.000],  loss: 9.047215, mae: 1.436728, mean_q: 3.731381\n",
      "[12 31 85 45 99  4 38 97  9 14]\n",
      "  4023/50001: episode: 447, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 46.889 [4.000, 99.000],  loss: 9.664444, mae: 1.470239, mean_q: 3.766475\n",
      "[14 22 74 80 10 76 34 38 96 31]\n",
      "  4032/50001: episode: 448, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 36.000, mean reward:  4.000 [ 2.000,  8.000], mean action: 51.222 [10.000, 96.000],  loss: 7.943057, mae: 1.480892, mean_q: 3.831232\n",
      "[35 53 58 50 37 29 23  9 67 10]\n",
      "  4041/50001: episode: 449, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 35.000, mean reward:  3.889 [ 2.000,  5.000], mean action: 37.333 [9.000, 67.000],  loss: 9.438424, mae: 1.423618, mean_q: 3.637015\n",
      "[85 88 64 46 24 49 73 84 29 68]\n",
      "  4050/50001: episode: 450, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 33.000, mean reward:  3.667 [ 2.000,  5.000], mean action: 58.333 [24.000, 88.000],  loss: 8.161829, mae: 1.460646, mean_q: 3.771758\n",
      "[25 86 63  2 50 27 80  9 76 79]\n",
      "  4059/50001: episode: 451, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 52.444 [2.000, 86.000],  loss: 9.224728, mae: 1.422982, mean_q: 3.709308\n",
      "[46 12 77 20 29 40 84 27 45 94]\n",
      "  4068/50001: episode: 452, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 33.000, mean reward:  3.667 [ 2.000,  5.000], mean action: 47.556 [12.000, 94.000],  loss: 8.781310, mae: 1.470485, mean_q: 3.774690\n",
      "[16 88 33 16 92 94 30  2 92 97]\n",
      "  4077/50001: episode: 453, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 12.000, mean reward:  1.333 [-10.000,  8.000], mean action: 60.444 [2.000, 97.000],  loss: 10.441691, mae: 1.443272, mean_q: 3.697309\n",
      "[88 16 52 50 37 32 69 54 61 24]\n",
      "  4086/50001: episode: 454, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 36.000, mean reward:  4.000 [ 3.000,  7.000], mean action: 43.889 [16.000, 69.000],  loss: 9.405367, mae: 1.529654, mean_q: 3.962387\n",
      "[34 42 83 99 55 85 79 67 31 61]\n",
      "  4095/50001: episode: 455, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 32.000, mean reward:  3.556 [ 2.000,  6.000], mean action: 66.889 [31.000, 99.000],  loss: 10.143081, mae: 1.551376, mean_q: 3.980233\n",
      "[22 67 65 19 99 10 56  4 85 56]\n",
      "  4104/50001: episode: 456, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 20.000, mean reward:  2.222 [-10.000,  6.000], mean action: 51.222 [4.000, 99.000],  loss: 7.603548, mae: 1.515088, mean_q: 3.904815\n",
      "[73 28 13 21 79 50 38 45 96 23]\n",
      "  4113/50001: episode: 457, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 43.667 [13.000, 96.000],  loss: 7.955913, mae: 1.505700, mean_q: 3.844801\n",
      "[44 96 59 63 43 80 38 48 77 20]\n",
      "  4122/50001: episode: 458, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 35.000, mean reward:  3.889 [ 3.000,  7.000], mean action: 58.222 [20.000, 96.000],  loss: 9.091446, mae: 1.536083, mean_q: 3.957046\n",
      "[96 72 75 63 37 27 88 61 55 40]\n",
      "  4131/50001: episode: 459, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 37.000, mean reward:  4.111 [ 2.000,  6.000], mean action: 57.556 [27.000, 88.000],  loss: 9.282398, mae: 1.517472, mean_q: 3.896665\n",
      "[ 8 51 40 30 44 32 36 70 20 39]\n",
      "  4140/50001: episode: 460, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 31.000, mean reward:  3.444 [ 2.000,  5.000], mean action: 40.222 [20.000, 70.000],  loss: 8.405931, mae: 1.535429, mean_q: 3.956602\n",
      "[14  4 77 42 54 12 85 21 85 24]\n",
      "  4149/50001: episode: 461, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 21.000, mean reward:  2.333 [-10.000,  7.000], mean action: 44.889 [4.000, 85.000],  loss: 6.803052, mae: 1.544156, mean_q: 4.021631\n",
      "[90 41 50 37 44 50 32 69  8 67]\n",
      "  4158/50001: episode: 462, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 23.000, mean reward:  2.556 [-10.000,  5.000], mean action: 44.222 [8.000, 69.000],  loss: 7.812932, mae: 1.526580, mean_q: 3.882147\n",
      "[48 10 77 37 54 32 32 46 94 20]\n",
      "  4167/50001: episode: 463, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 22.000, mean reward:  2.444 [-10.000,  5.000], mean action: 44.667 [10.000, 94.000],  loss: 8.577559, mae: 1.588397, mean_q: 4.066647\n",
      "[50 38 64 37 51 97 57 83 42 23]\n",
      "  4176/50001: episode: 464, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 38.000, mean reward:  4.222 [ 2.000,  6.000], mean action: 54.667 [23.000, 97.000],  loss: 9.998920, mae: 1.558185, mean_q: 4.030123\n",
      "[26 43 62 79 30 96  1 34 21 16]\n",
      "  4185/50001: episode: 465, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 42.444 [1.000, 96.000],  loss: 8.767871, mae: 1.584975, mean_q: 4.095323\n",
      "[80  0 32 76 78 12 68  4 19 42]\n",
      "  4194/50001: episode: 466, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 38.000, mean reward:  4.222 [ 2.000,  6.000], mean action: 36.778 [0.000, 78.000],  loss: 7.340310, mae: 1.606878, mean_q: 4.172174\n",
      "[66  5 32 64 44 46 51 38 73 66]\n",
      "  4203/50001: episode: 467, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 17.000, mean reward:  1.889 [-10.000,  5.000], mean action: 46.556 [5.000, 73.000],  loss: 7.808652, mae: 1.594570, mean_q: 4.133026\n",
      "[58 83 88 30 50 54  1 49 85 79]\n",
      "  4212/50001: episode: 468, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 57.667 [1.000, 88.000],  loss: 9.832857, mae: 1.593675, mean_q: 4.097409\n",
      "[ 0 42 88 69 50 13 57 25 58 72]\n",
      "  4221/50001: episode: 469, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 33.000, mean reward:  3.667 [ 2.000,  5.000], mean action: 52.667 [13.000, 88.000],  loss: 10.817309, mae: 1.609993, mean_q: 4.090741\n",
      "[42  9 52 85 61 19 62 94 86 14]\n",
      "  4230/50001: episode: 470, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 33.000, mean reward:  3.667 [ 2.000,  8.000], mean action: 53.556 [9.000, 94.000],  loss: 10.353638, mae: 1.579409, mean_q: 4.058702\n",
      "[95 63 24 52 30  1 50 85 94 55]\n",
      "  4239/50001: episode: 471, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 35.000, mean reward:  3.889 [ 3.000,  6.000], mean action: 50.444 [1.000, 94.000],  loss: 9.019348, mae: 1.542883, mean_q: 4.001982\n",
      "[59 62 89  2 88 50  4 81 24 23]\n",
      "  4248/50001: episode: 472, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 40.000, mean reward:  4.444 [ 3.000,  6.000], mean action: 47.000 [2.000, 89.000],  loss: 10.587407, mae: 1.493420, mean_q: 3.871557\n",
      "[ 0 33 88 63 27 57 73 83 85 86]\n",
      "  4257/50001: episode: 473, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 35.000, mean reward:  3.889 [ 3.000,  5.000], mean action: 66.111 [27.000, 88.000],  loss: 7.205331, mae: 1.493672, mean_q: 3.841172\n",
      "[67 89 64 49 49 17  2 91 14 59]\n",
      "  4266/50001: episode: 474, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 48.222 [2.000, 91.000],  loss: 9.276517, mae: 1.535885, mean_q: 3.883620\n",
      "[12 62 65 57 24 31 64 29 42 38]\n",
      "  4275/50001: episode: 475, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 33.000, mean reward:  3.667 [ 2.000,  6.000], mean action: 45.778 [24.000, 65.000],  loss: 4.992963, mae: 1.536100, mean_q: 3.926105\n",
      "[15 97 75 55 32 55 88 20 21 83]\n",
      "  4284/50001: episode: 476, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 20.000, mean reward:  2.222 [-10.000,  6.000], mean action: 58.444 [20.000, 97.000],  loss: 10.131297, mae: 1.549837, mean_q: 3.925647\n",
      "[83 57 76 30 50 23 18 38 32 58]\n",
      "  4293/50001: episode: 477, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 42.444 [18.000, 76.000],  loss: 7.319012, mae: 1.618975, mean_q: 4.106750\n",
      "[60 23 49 97 37  6 46 19 86 59]\n",
      "  4302/50001: episode: 478, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 34.000, mean reward:  3.778 [ 3.000,  5.000], mean action: 46.889 [6.000, 97.000],  loss: 9.203093, mae: 1.597869, mean_q: 4.100042\n",
      "[23 57 37 85 24 85 63 21 92  9]\n",
      "  4311/50001: episode: 479, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 52.556 [9.000, 92.000],  loss: 7.745731, mae: 1.619859, mean_q: 4.135711\n",
      "[57 17 80 75 18 21 62 17  6 29]\n",
      "  4320/50001: episode: 480, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 20.000, mean reward:  2.222 [-10.000,  5.000], mean action: 36.111 [6.000, 80.000],  loss: 8.504225, mae: 1.621005, mean_q: 4.131699\n",
      "[85 82 88 50 86 17 63 77  2  4]\n",
      "  4329/50001: episode: 481, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 52.111 [2.000, 88.000],  loss: 7.973444, mae: 1.677287, mean_q: 4.333658\n",
      "[39 23 98  9 15 27 99 29 61 76]\n",
      "  4338/50001: episode: 482, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 32.000, mean reward:  3.556 [ 2.000,  5.000], mean action: 48.556 [9.000, 99.000],  loss: 8.505250, mae: 1.625672, mean_q: 4.177185\n",
      "[81 41 74  2 70 32  9 12 49 28]\n",
      "  4347/50001: episode: 483, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 41.000, mean reward:  4.556 [ 2.000,  7.000], mean action: 35.222 [2.000, 74.000],  loss: 9.971218, mae: 1.607067, mean_q: 4.109322\n",
      "[28 52 81 27 58 24 85 21 45 34]\n",
      "  4356/50001: episode: 484, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 38.000, mean reward:  4.222 [ 3.000, 11.000], mean action: 47.444 [21.000, 85.000],  loss: 8.660400, mae: 1.559085, mean_q: 4.032880\n",
      "[75 41 33 96 92 23 58 62 85 14]\n",
      "  4365/50001: episode: 485, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 37.000, mean reward:  4.111 [ 3.000,  7.000], mean action: 56.000 [14.000, 96.000],  loss: 9.324078, mae: 1.539982, mean_q: 3.954004\n",
      "[97 60 37 43 37 50 80 59 59 76]\n",
      "  4374/50001: episode: 486, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward:  6.000, mean reward:  0.667 [-10.000,  5.000], mean action: 55.667 [37.000, 80.000],  loss: 7.837639, mae: 1.627030, mean_q: 4.169251\n",
      "[50 62 64 59 77 32 32 49 14 85]\n",
      "  4383/50001: episode: 487, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: 21.000, mean reward:  2.333 [-10.000,  6.000], mean action: 52.667 [14.000, 85.000],  loss: 8.782050, mae: 1.609476, mean_q: 4.108388\n",
      "[68 15 50 11 55 27 25 60  4 49]\n",
      "  4392/50001: episode: 488, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 38.000, mean reward:  4.222 [ 2.000,  6.000], mean action: 32.889 [4.000, 60.000],  loss: 7.400316, mae: 1.519017, mean_q: 3.904634\n",
      "[94 43 98 37 54 91 99 15 31 44]\n",
      "  4401/50001: episode: 489, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 37.000, mean reward:  4.111 [ 2.000,  7.000], mean action: 56.889 [15.000, 99.000],  loss: 8.666036, mae: 1.618179, mean_q: 4.162033\n",
      "[91  4 32 52 12 64 16 64 44 62]\n",
      "  4410/50001: episode: 490, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 21.000, mean reward:  2.333 [-10.000,  5.000], mean action: 38.889 [4.000, 64.000],  loss: 10.115379, mae: 1.564286, mean_q: 4.034840\n",
      "[44 84 98 24 15  9 88 78 25 58]\n",
      "  4419/50001: episode: 491, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 37.000, mean reward:  4.111 [ 2.000,  7.000], mean action: 53.222 [9.000, 98.000],  loss: 10.545531, mae: 1.589339, mean_q: 4.095056\n",
      "[10 94 64 49 46 69 97 92 61 12]\n",
      "  4428/50001: episode: 492, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 39.000, mean reward:  4.333 [ 2.000,  9.000], mean action: 64.889 [12.000, 97.000],  loss: 9.023928, mae: 1.580663, mean_q: 4.087597\n",
      "[48 21 24 21 67 72 57 27 59  9]\n",
      "  4437/50001: episode: 493, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 39.667 [9.000, 72.000],  loss: 9.790369, mae: 1.563974, mean_q: 4.004365\n",
      "[83 22 24  2 72 50 10 34  6  3]\n",
      "  4446/50001: episode: 494, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 37.000, mean reward:  4.111 [ 2.000,  8.000], mean action: 24.778 [2.000, 72.000],  loss: 9.871016, mae: 1.604314, mean_q: 4.112306\n",
      "[22 42 36 43 97  5 14 85 13 17]\n",
      "  4455/50001: episode: 495, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 34.000, mean reward:  3.778 [ 2.000,  8.000], mean action: 39.111 [5.000, 97.000],  loss: 9.723690, mae: 1.583106, mean_q: 4.063102\n",
      "[50 28 63 98 46 43 46 46  2  4]\n",
      "  4464/50001: episode: 496, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 10.000, mean reward:  1.111 [-10.000,  7.000], mean action: 41.778 [2.000, 98.000],  loss: 9.145481, mae: 1.621964, mean_q: 4.118516\n",
      "[15 51 20 97 97 32 74 28 32  4]\n",
      "  4473/50001: episode: 497, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 12.000, mean reward:  1.333 [-10.000,  6.000], mean action: 48.333 [4.000, 97.000],  loss: 9.498793, mae: 1.564579, mean_q: 4.047633\n",
      "[11  4 92 17 17 31 23 18  9 18]\n",
      "  4482/50001: episode: 498, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward:  7.000, mean reward:  0.778 [-10.000,  5.000], mean action: 25.444 [4.000, 92.000],  loss: 7.829089, mae: 1.586061, mean_q: 4.108304\n",
      "[70 54 37  1 73 32 34 44 94  5]\n",
      "  4491/50001: episode: 499, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 41.556 [1.000, 94.000],  loss: 9.031365, mae: 1.597685, mean_q: 4.084982\n",
      "[53 10 75 25  9 99 42 16 72 88]\n",
      "  4500/50001: episode: 500, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 37.000, mean reward:  4.111 [ 2.000,  9.000], mean action: 48.444 [9.000, 99.000],  loss: 7.092264, mae: 1.652705, mean_q: 4.274695\n",
      "[68 10 98 36  2 69  1 86 77 42]\n",
      "  4509/50001: episode: 501, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 37.000, mean reward:  4.111 [ 2.000,  6.000], mean action: 46.778 [1.000, 98.000],  loss: 6.239605, mae: 1.606265, mean_q: 4.146660\n",
      "[73 70 52 32  2 90  2 88  4 42]\n",
      "  4518/50001: episode: 502, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 28.000, mean reward:  3.111 [-10.000,  6.000], mean action: 42.444 [2.000, 90.000],  loss: 9.060486, mae: 1.608082, mean_q: 4.092748\n",
      "[19 96 92 45 96 46 32 15 55  8]\n",
      "  4527/50001: episode: 503, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 19.000, mean reward:  2.111 [-10.000,  6.000], mean action: 53.889 [8.000, 96.000],  loss: 8.049258, mae: 1.575791, mean_q: 4.121502\n",
      "[99 42 14 16 60 13 24 14 14 49]\n",
      "  4536/50001: episode: 504, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward:  8.000, mean reward:  0.889 [-10.000,  5.000], mean action: 27.333 [13.000, 60.000],  loss: 9.235352, mae: 1.624654, mean_q: 4.186413\n",
      "[92 59 10 13 74 14 13 96 18 96]\n",
      "  4545/50001: episode: 505, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward:  5.000, mean reward:  0.556 [-10.000,  5.000], mean action: 43.667 [10.000, 96.000],  loss: 10.398679, mae: 1.555501, mean_q: 3.949367\n",
      "[79 38 33 83  1 37 92 23 23 22]\n",
      "  4554/50001: episode: 506, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 39.111 [1.000, 92.000],  loss: 8.880657, mae: 1.575911, mean_q: 4.011450\n",
      "[47  4 37 42 73 46  1 57 28 86]\n",
      "  4563/50001: episode: 507, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 37.000, mean reward:  4.111 [ 2.000,  6.000], mean action: 41.556 [1.000, 86.000],  loss: 8.352401, mae: 1.599869, mean_q: 4.050649\n",
      "[90 94 25 52 37 24 37 37 70 50]\n",
      "  4572/50001: episode: 508, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 11.000, mean reward:  1.222 [-10.000,  8.000], mean action: 47.333 [24.000, 94.000],  loss: 9.165731, mae: 1.610869, mean_q: 4.086167\n",
      "[53 52 76 92 30 50 92  6  5 72]\n",
      "  4581/50001: episode: 509, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 52.778 [5.000, 92.000],  loss: 9.352221, mae: 1.631532, mean_q: 4.128511\n",
      "[30  4 88 12 23  2 23 30 63 25]\n",
      "  4590/50001: episode: 510, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward:  6.000, mean reward:  0.667 [-10.000,  4.000], mean action: 30.000 [2.000, 88.000],  loss: 7.686356, mae: 1.579305, mean_q: 4.030046\n",
      "[36 68 23 63 27 80 67 37 63 83]\n",
      "  4599/50001: episode: 511, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 22.000, mean reward:  2.444 [-10.000,  8.000], mean action: 56.778 [23.000, 83.000],  loss: 8.171591, mae: 1.572267, mean_q: 4.017012\n",
      "[55 74 65 88 30 50 70 99 14 12]\n",
      "  4608/50001: episode: 512, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 42.000, mean reward:  4.667 [ 2.000,  9.000], mean action: 55.778 [12.000, 99.000],  loss: 9.642856, mae: 1.544202, mean_q: 3.905627\n",
      "[58 45 75 59 30 96 88 51 31 31]\n",
      "  4617/50001: episode: 513, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 56.222 [30.000, 96.000],  loss: 9.084489, mae: 1.602858, mean_q: 4.056091\n",
      "[48 68  1 88 30 30 25 30 46 83]\n",
      "  4626/50001: episode: 514, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward:  9.000, mean reward:  1.000 [-10.000,  6.000], mean action: 44.556 [1.000, 88.000],  loss: 8.733234, mae: 1.529592, mean_q: 3.903715\n",
      "[86 30 24 23 46 96 16 16 78 91]\n",
      "  4635/50001: episode: 515, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 46.667 [16.000, 96.000],  loss: 8.507719, mae: 1.582147, mean_q: 4.005160\n",
      "[92 51 49 74 37 59 21 33 68 87]\n",
      "  4644/50001: episode: 516, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 33.000, mean reward:  3.667 [ 3.000,  6.000], mean action: 53.222 [21.000, 87.000],  loss: 8.746702, mae: 1.605832, mean_q: 4.100286\n",
      "[ 8 10 50 24 85 93  4 41 45  9]\n",
      "  4653/50001: episode: 517, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 37.000, mean reward:  4.111 [ 2.000,  6.000], mean action: 40.111 [4.000, 93.000],  loss: 10.549468, mae: 1.606680, mean_q: 4.119672\n",
      "[22 83 88 57 49 50 32 73 85  4]\n",
      "  4662/50001: episode: 518, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 57.889 [4.000, 88.000],  loss: 7.055548, mae: 1.568386, mean_q: 3.971937\n",
      "[39 14 13 37 13 24 97 32 41 76]\n",
      "  4671/50001: episode: 519, duration: 0.057s, episode steps:   9, steps per second: 158, episode reward: 25.000, mean reward:  2.778 [-10.000,  5.000], mean action: 38.556 [13.000, 97.000],  loss: 8.725094, mae: 1.581796, mean_q: 4.065076\n",
      "[ 4 52 46 13 68 54 59 44 98 65]\n",
      "  4680/50001: episode: 520, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 34.000, mean reward:  3.778 [ 3.000,  7.000], mean action: 55.444 [13.000, 98.000],  loss: 8.498112, mae: 1.590210, mean_q: 4.113894\n",
      "[95 41 12 30 60 49 80 10 60 69]\n",
      "  4689/50001: episode: 521, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 19.000, mean reward:  2.111 [-10.000,  5.000], mean action: 45.667 [10.000, 80.000],  loss: 8.299774, mae: 1.583961, mean_q: 4.085232\n",
      "[ 4  2 28 83 76 31 23 46 86 79]\n",
      "  4698/50001: episode: 522, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 38.000, mean reward:  4.222 [ 3.000,  7.000], mean action: 50.444 [2.000, 86.000],  loss: 7.619766, mae: 1.707224, mean_q: 4.394812\n",
      "[16 83 59 23 55 58  4 86 16 28]\n",
      "  4707/50001: episode: 523, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 23.000, mean reward:  2.556 [-10.000,  8.000], mean action: 45.778 [4.000, 86.000],  loss: 9.345317, mae: 1.647344, mean_q: 4.200658\n",
      "[94 33 23 79 44 37 51 46 28 94]\n",
      "  4716/50001: episode: 524, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 48.333 [23.000, 94.000],  loss: 9.993473, mae: 1.599699, mean_q: 4.072112\n",
      "[77 76 74 99 63 63 13 50 23 62]\n",
      "  4725/50001: episode: 525, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 58.111 [13.000, 99.000],  loss: 6.882293, mae: 1.583268, mean_q: 4.046451\n",
      "[46 51 13 81 98 63 57 15 78 71]\n",
      "  4734/50001: episode: 526, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 30.000, mean reward:  3.333 [ 2.000,  5.000], mean action: 58.556 [13.000, 98.000],  loss: 9.674766, mae: 1.582825, mean_q: 4.083673\n",
      "[82 53 56 49 37 70 72 71  4 30]\n",
      "  4743/50001: episode: 527, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 36.000, mean reward:  4.000 [ 2.000,  7.000], mean action: 49.111 [4.000, 72.000],  loss: 9.513126, mae: 1.632064, mean_q: 4.175127\n",
      "[ 4  0 46 99 21 10 32 10 37 52]\n",
      "  4752/50001: episode: 528, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 24.000, mean reward:  2.667 [-10.000,  8.000], mean action: 34.111 [0.000, 99.000],  loss: 8.225363, mae: 1.613396, mean_q: 4.180546\n",
      "[94 22 50 47 37 53 47  0 11 11]\n",
      "  4761/50001: episode: 529, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward:  5.000, mean reward:  0.556 [-10.000,  5.000], mean action: 30.889 [0.000, 53.000],  loss: 7.947983, mae: 1.679834, mean_q: 4.286139\n",
      "[30 50 55  2 12 59 49 99 29  2]\n",
      "  4770/50001: episode: 530, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 19.000, mean reward:  2.111 [-10.000,  5.000], mean action: 39.667 [2.000, 99.000],  loss: 9.617385, mae: 1.632641, mean_q: 4.204175\n",
      "[19 41 81 25 25 31 48  9 18  2]\n",
      "  4779/50001: episode: 531, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 31.111 [2.000, 81.000],  loss: 9.136281, mae: 1.609930, mean_q: 4.192823\n",
      "[28 63 59 19 76 43 22 78 38 84]\n",
      "  4788/50001: episode: 532, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 32.000, mean reward:  3.556 [ 2.000,  6.000], mean action: 53.556 [19.000, 84.000],  loss: 8.428316, mae: 1.549123, mean_q: 3.967214\n",
      "[22 19 79 71 71 57 58 84  9 42]\n",
      "  4797/50001: episode: 533, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 27.000, mean reward:  3.000 [-10.000,  8.000], mean action: 54.444 [9.000, 84.000],  loss: 9.752354, mae: 1.595679, mean_q: 4.103170\n",
      "[88 94 83  2 24 30  2  2 85 79]\n",
      "  4806/50001: episode: 534, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward:  7.000, mean reward:  0.778 [-10.000,  7.000], mean action: 44.556 [2.000, 94.000],  loss: 9.224051, mae: 1.614925, mean_q: 4.103365\n",
      "[66 95 25 35 37 59 24 79 59 69]\n",
      "  4815/50001: episode: 535, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 22.000, mean reward:  2.444 [-10.000,  5.000], mean action: 53.556 [24.000, 95.000],  loss: 10.944890, mae: 1.563343, mean_q: 4.003355\n",
      "[66 84 12 91 44 59 37 63 33 28]\n",
      "  4824/50001: episode: 536, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 50.111 [12.000, 91.000],  loss: 7.963646, mae: 1.559747, mean_q: 3.960632\n",
      "[96 95 16 32 12 75 88 51 37  8]\n",
      "  4833/50001: episode: 537, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 46.000 [8.000, 95.000],  loss: 7.297205, mae: 1.552083, mean_q: 4.009478\n",
      "[48  0 39 51 48 75 31 86  1 45]\n",
      "  4842/50001: episode: 538, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 20.000, mean reward:  2.222 [-10.000,  7.000], mean action: 41.778 [0.000, 86.000],  loss: 8.517931, mae: 1.543522, mean_q: 3.953131\n",
      "[46  0 61 83 76 52 53 74 28  4]\n",
      "  4851/50001: episode: 539, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 47.889 [0.000, 83.000],  loss: 8.339070, mae: 1.629996, mean_q: 4.148890\n",
      "[77 83 43 97 27 12 92 85 83  4]\n",
      "  4860/50001: episode: 540, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 58.444 [4.000, 97.000],  loss: 9.824774, mae: 1.592226, mean_q: 4.057314\n",
      "[16 42 58  2 49 94 52 11 68 20]\n",
      "  4869/50001: episode: 541, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 33.000, mean reward:  3.667 [ 3.000,  5.000], mean action: 44.000 [2.000, 94.000],  loss: 9.481982, mae: 1.614172, mean_q: 4.105655\n",
      "[31 23 37 98 55 52 21 74 12 42]\n",
      "  4878/50001: episode: 542, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 46.000 [12.000, 98.000],  loss: 7.393757, mae: 1.567301, mean_q: 3.954829\n",
      "[99 78 75 37  2 63 79 50 97 68]\n",
      "  4887/50001: episode: 543, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 61.000 [2.000, 97.000],  loss: 8.224140, mae: 1.606574, mean_q: 4.090421\n",
      "[93 76 61 58 12 52 47 21 63 50]\n",
      "  4896/50001: episode: 544, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 42.000, mean reward:  4.667 [ 2.000, 10.000], mean action: 48.889 [12.000, 76.000],  loss: 9.065572, mae: 1.628162, mean_q: 4.164330\n",
      "[88 46 63 92 37 52 46 19 55 31]\n",
      "  4905/50001: episode: 545, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 21.000, mean reward:  2.333 [-10.000,  6.000], mean action: 49.000 [19.000, 92.000],  loss: 6.998922, mae: 1.578154, mean_q: 4.025369\n",
      "[46 40 75 67 63  1 40 73 37 24]\n",
      "  4914/50001: episode: 546, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 25.000, mean reward:  2.778 [-10.000,  9.000], mean action: 46.667 [1.000, 75.000],  loss: 6.877752, mae: 1.624265, mean_q: 4.162626\n",
      "[90 42 24 47 50 88 51 63 42 61]\n",
      "  4923/50001: episode: 547, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 22.000, mean reward:  2.444 [-10.000,  5.000], mean action: 52.000 [24.000, 88.000],  loss: 7.731489, mae: 1.639043, mean_q: 4.185683\n",
      "[18 96 25 21 24 57 29 88 58 20]\n",
      "  4932/50001: episode: 548, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 46.444 [20.000, 96.000],  loss: 8.088801, mae: 1.745574, mean_q: 4.429541\n",
      "[13 58 22 16 52 96 63 95 22 88]\n",
      "  4941/50001: episode: 549, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 56.889 [16.000, 96.000],  loss: 6.420208, mae: 1.711714, mean_q: 4.415735\n",
      "[14 23 87 92  9 30 77  1 54 79]\n",
      "  4950/50001: episode: 550, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 50.222 [1.000, 92.000],  loss: 12.351922, mae: 1.778947, mean_q: 4.465750\n",
      "[39 56  1 16 30 79 34 30 76 33]\n",
      "  4959/50001: episode: 551, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 39.444 [1.000, 79.000],  loss: 8.298557, mae: 1.716281, mean_q: 4.325225\n",
      "[54 40 37 69 37 84 46 46 10 63]\n",
      "  4968/50001: episode: 552, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward:  8.000, mean reward:  0.889 [-10.000,  5.000], mean action: 48.000 [10.000, 84.000],  loss: 8.493714, mae: 1.674637, mean_q: 4.244356\n",
      "[57 28  1 23 74 68 51 75 68 13]\n",
      "  4977/50001: episode: 553, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 24.000, mean reward:  2.667 [-10.000,  8.000], mean action: 44.556 [1.000, 75.000],  loss: 9.578678, mae: 1.694426, mean_q: 4.303444\n",
      "[76 23 61 55 99  9 23 10 38 64]\n",
      "  4986/50001: episode: 554, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 19.000, mean reward:  2.111 [-10.000,  6.000], mean action: 42.444 [9.000, 99.000],  loss: 8.761878, mae: 1.717507, mean_q: 4.323406\n",
      "[78 34  9 37 63 49  1 46  6  6]\n",
      "  4995/50001: episode: 555, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 27.889 [1.000, 63.000],  loss: 8.438526, mae: 1.730079, mean_q: 4.363783\n",
      "[31 52 90 99 97 56 38 86 88 55]\n",
      "  5004/50001: episode: 556, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 34.000, mean reward:  3.778 [ 2.000,  7.000], mean action: 73.444 [38.000, 99.000],  loss: 9.567673, mae: 1.599584, mean_q: 4.076292\n",
      "[78 75 97 29 11 63 46 24 21 50]\n",
      "  5013/50001: episode: 557, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 40.000, mean reward:  4.444 [ 2.000, 10.000], mean action: 46.222 [11.000, 97.000],  loss: 8.471424, mae: 1.644924, mean_q: 4.180133\n",
      "[ 2 34 63 96 63 58  2 16 62 79]\n",
      "  5022/50001: episode: 558, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward:  9.000, mean reward:  1.000 [-10.000,  7.000], mean action: 52.556 [2.000, 96.000],  loss: 9.789369, mae: 1.611981, mean_q: 4.129329\n",
      "[37  4 64 14 99 60 84 87 48 24]\n",
      "  5031/50001: episode: 559, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 38.000, mean reward:  4.222 [ 3.000,  7.000], mean action: 53.778 [4.000, 99.000],  loss: 6.996738, mae: 1.620915, mean_q: 4.174767\n",
      "[ 4 23 63 55 23 96 34 21 14 42]\n",
      "  5040/50001: episode: 560, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 22.000, mean reward:  2.444 [-10.000,  7.000], mean action: 41.222 [14.000, 96.000],  loss: 8.447502, mae: 1.631599, mean_q: 4.191517\n",
      "[70 94 44 48 81 49  1 97 28 26]\n",
      "  5049/50001: episode: 561, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 36.000, mean reward:  4.000 [ 1.000,  7.000], mean action: 52.000 [1.000, 97.000],  loss: 10.329107, mae: 1.596477, mean_q: 4.101874\n",
      "[75 30 23  2 37 55 16 68 96 44]\n",
      "  5058/50001: episode: 562, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 30.000, mean reward:  3.333 [ 2.000,  5.000], mean action: 41.222 [2.000, 96.000],  loss: 7.653023, mae: 1.607422, mean_q: 4.166214\n",
      "[40 18  6 22 39 78 73 76 86 83]\n",
      "  5067/50001: episode: 563, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 29.000, mean reward:  3.222 [ 2.000,  5.000], mean action: 53.444 [6.000, 86.000],  loss: 8.630263, mae: 1.593245, mean_q: 4.099281\n",
      "[27 96 51 79 98 30 32 88 23 82]\n",
      "  5076/50001: episode: 564, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 64.333 [23.000, 98.000],  loss: 8.696780, mae: 1.608831, mean_q: 4.098882\n",
      "[36 86 64 48 50 49 57 97 94 79]\n",
      "  5085/50001: episode: 565, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 43.000, mean reward:  4.778 [ 3.000,  7.000], mean action: 69.333 [48.000, 97.000],  loss: 8.537029, mae: 1.642069, mean_q: 4.207606\n",
      "[93 10 50 34 81 78 11 78 10 52]\n",
      "  5094/50001: episode: 566, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward:  9.000, mean reward:  1.000 [-10.000,  6.000], mean action: 44.889 [10.000, 81.000],  loss: 8.006783, mae: 1.679592, mean_q: 4.258921\n",
      "[88 59 47  2 52 13 24 13 96 13]\n",
      "  5103/50001: episode: 567, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward:  6.000, mean reward:  0.667 [-10.000,  5.000], mean action: 35.444 [2.000, 96.000],  loss: 9.976613, mae: 1.674958, mean_q: 4.264091\n",
      "[90  9 79 63 56 62 78  1 83 65]\n",
      "  5112/50001: episode: 568, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 37.000, mean reward:  4.111 [ 3.000,  7.000], mean action: 55.111 [1.000, 83.000],  loss: 9.397206, mae: 1.600543, mean_q: 4.088732\n",
      "[ 9 64 58 84 38 67 50 77 40 24]\n",
      "  5121/50001: episode: 569, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 55.778 [24.000, 84.000],  loss: 8.158695, mae: 1.673326, mean_q: 4.327384\n",
      "[36 88  1 69 81 23  9 32 37 65]\n",
      "  5130/50001: episode: 570, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 38.000, mean reward:  4.222 [ 2.000,  8.000], mean action: 45.000 [1.000, 88.000],  loss: 8.040241, mae: 1.588673, mean_q: 4.019439\n",
      "[25 42  2 36 59 92 41 42 42 30]\n",
      "  5139/50001: episode: 571, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward:  7.000, mean reward:  0.778 [-10.000,  6.000], mean action: 42.889 [2.000, 92.000],  loss: 8.540627, mae: 1.605937, mean_q: 4.140310\n",
      "[54 30 25 84 37 27 88 25 28  2]\n",
      "  5148/50001: episode: 572, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 28.000, mean reward:  3.111 [-10.000,  7.000], mean action: 38.444 [2.000, 88.000],  loss: 7.526935, mae: 1.670311, mean_q: 4.236061\n",
      "[17  2 81 58 32 52  7 57 58 42]\n",
      "  5157/50001: episode: 573, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 22.000, mean reward:  2.444 [-10.000,  7.000], mean action: 43.222 [2.000, 81.000],  loss: 10.644061, mae: 1.643075, mean_q: 4.239083\n",
      "[50 84 47 80 76 54 55  2 55  4]\n",
      "  5166/50001: episode: 574, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 22.000, mean reward:  2.444 [-10.000,  8.000], mean action: 50.778 [2.000, 84.000],  loss: 7.996343, mae: 1.675319, mean_q: 4.246339\n",
      "[87 76 33 24 86 49  1 69 48 50]\n",
      "  5175/50001: episode: 575, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 43.000, mean reward:  4.778 [ 3.000,  9.000], mean action: 48.444 [1.000, 86.000],  loss: 8.575541, mae: 1.688411, mean_q: 4.275319\n",
      "[37 63 41 63 28 97 23 34 53  1]\n",
      "  5184/50001: episode: 576, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 29.000, mean reward:  3.222 [-10.000,  7.000], mean action: 44.778 [1.000, 97.000],  loss: 9.113538, mae: 1.701795, mean_q: 4.302472\n",
      "[94 42 34 30 37 33 63 59 31 49]\n",
      "  5193/50001: episode: 577, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 37.000, mean reward:  4.111 [ 3.000,  6.000], mean action: 42.000 [30.000, 63.000],  loss: 7.703928, mae: 1.635457, mean_q: 4.193806\n",
      "[23 60 13 34 64 57 11 51 60 26]\n",
      "  5202/50001: episode: 578, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 18.000, mean reward:  2.000 [-10.000,  5.000], mean action: 41.778 [11.000, 64.000],  loss: 8.760491, mae: 1.715671, mean_q: 4.360600\n",
      "[32 23 96 20 59 23 27 86 28 28]\n",
      "  5211/50001: episode: 579, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward:  7.000, mean reward:  0.778 [-10.000,  7.000], mean action: 43.333 [20.000, 96.000],  loss: 7.812357, mae: 1.642976, mean_q: 4.173158\n",
      "[89 31 39 76 77 58 57 72 94 98]\n",
      "  5220/50001: episode: 580, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 36.000, mean reward:  4.000 [ 2.000,  8.000], mean action: 66.889 [31.000, 98.000],  loss: 8.370682, mae: 1.701722, mean_q: 4.361759\n",
      "[31 51 88 18 34 49 62 50 46  1]\n",
      "  5229/50001: episode: 581, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 43.000, mean reward:  4.778 [ 3.000,  8.000], mean action: 44.333 [1.000, 88.000],  loss: 7.834997, mae: 1.662049, mean_q: 4.222785\n",
      "[24 88 21 61 46 83 80  0 99 69]\n",
      "  5238/50001: episode: 582, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 35.000, mean reward:  3.889 [ 2.000,  6.000], mean action: 60.778 [0.000, 99.000],  loss: 9.222056, mae: 1.687717, mean_q: 4.358888\n",
      "[73  4 56 62 77 76 30 84 28 14]\n",
      "  5247/50001: episode: 583, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 47.889 [4.000, 84.000],  loss: 8.461575, mae: 1.691145, mean_q: 4.290928\n",
      "[47  9 32 76  4 33 64  2 62 97]\n",
      "  5256/50001: episode: 584, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 39.000, mean reward:  4.333 [ 3.000,  8.000], mean action: 42.111 [2.000, 97.000],  loss: 7.894280, mae: 1.700515, mean_q: 4.398549\n",
      "[ 1 51 43 16 34 19 73 96 40  4]\n",
      "  5265/50001: episode: 585, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 36.000, mean reward:  4.000 [ 2.000,  7.000], mean action: 41.778 [4.000, 96.000],  loss: 8.553567, mae: 1.669865, mean_q: 4.297868\n",
      "[ 4 90 83  2 77 13 54 49  9 31]\n",
      "  5274/50001: episode: 586, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 38.000, mean reward:  4.222 [ 3.000,  7.000], mean action: 45.333 [2.000, 90.000],  loss: 8.543632, mae: 1.763973, mean_q: 4.533318\n",
      "[74 45 58 85 99  2 58 40 28 14]\n",
      "  5283/50001: episode: 587, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 47.667 [2.000, 99.000],  loss: 10.084869, mae: 1.628861, mean_q: 4.128255\n",
      "[ 6 83 58 63 58 34 33 50 12 28]\n",
      "  5292/50001: episode: 588, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 31.000, mean reward:  3.444 [-10.000,  7.000], mean action: 46.556 [12.000, 83.000],  loss: 9.311306, mae: 1.640552, mean_q: 4.166786\n",
      "[ 8 88 88 49 55 78  2 80  9 42]\n",
      "  5301/50001: episode: 589, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 54.556 [2.000, 88.000],  loss: 6.907709, mae: 1.659943, mean_q: 4.227792\n",
      "[68 52 34 34 32 70 30  4 17 10]\n",
      "  5310/50001: episode: 590, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 20.000, mean reward:  2.222 [-10.000,  5.000], mean action: 31.444 [4.000, 70.000],  loss: 7.363665, mae: 1.685800, mean_q: 4.320799\n",
      "[67 12 64 79 24  9 48 78 55 24]\n",
      "  5319/50001: episode: 591, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 20.000, mean reward:  2.222 [-10.000,  5.000], mean action: 43.667 [9.000, 79.000],  loss: 8.348652, mae: 1.637070, mean_q: 4.133365\n",
      "[92 78 38 50 76 27 67 23  4 23]\n",
      "  5328/50001: episode: 592, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 42.889 [4.000, 78.000],  loss: 8.556123, mae: 1.630071, mean_q: 4.147705\n",
      "[28 95 50 84 58 44 75 18 63 62]\n",
      "  5337/50001: episode: 593, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 35.000, mean reward:  3.889 [ 2.000,  5.000], mean action: 61.000 [18.000, 95.000],  loss: 7.953044, mae: 1.595447, mean_q: 4.116668\n",
      "[73 48 23 24 52 88 34 80 97 14]\n",
      "  5346/50001: episode: 594, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 51.111 [14.000, 97.000],  loss: 9.221869, mae: 1.677812, mean_q: 4.284882\n",
      "[43 23 90 48 74 27 73 12 40 72]\n",
      "  5355/50001: episode: 595, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 33.000, mean reward:  3.667 [ 1.000,  7.000], mean action: 51.000 [12.000, 90.000],  loss: 7.352373, mae: 1.629643, mean_q: 4.167836\n",
      "[98 59  9 16 37 85 46 34 68 39]\n",
      "  5364/50001: episode: 596, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 35.000, mean reward:  3.889 [ 2.000,  7.000], mean action: 43.667 [9.000, 85.000],  loss: 9.469408, mae: 1.667508, mean_q: 4.237256\n",
      "[55 89 59 40 86 46 14  1 88 50]\n",
      "  5373/50001: episode: 597, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 47.000, mean reward:  5.222 [ 3.000,  9.000], mean action: 52.556 [1.000, 89.000],  loss: 7.406333, mae: 1.693238, mean_q: 4.345522\n",
      "[19 41 24 29 34 53 67 61 16 52]\n",
      "  5382/50001: episode: 598, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 37.000, mean reward:  4.111 [ 2.000,  6.000], mean action: 41.889 [16.000, 67.000],  loss: 7.296699, mae: 1.634158, mean_q: 4.188480\n",
      "[19 14 79 49 50 39  4  9 24  2]\n",
      "  5391/50001: episode: 599, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 38.000, mean reward:  4.222 [ 2.000,  6.000], mean action: 30.000 [2.000, 79.000],  loss: 7.601694, mae: 1.801580, mean_q: 4.546209\n",
      "[74 74 24  1 37 54 52 49 13 18]\n",
      "  5400/50001: episode: 600, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 35.778 [1.000, 74.000],  loss: 8.988303, mae: 1.758694, mean_q: 4.531536\n",
      "[30 78 32 88 11 81  1 88 29 86]\n",
      "  5409/50001: episode: 601, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 20.000, mean reward:  2.222 [-10.000,  6.000], mean action: 54.889 [1.000, 88.000],  loss: 7.712874, mae: 1.711179, mean_q: 4.405423\n",
      "[51 64 71 51 37 34 99 77 40 62]\n",
      "  5418/50001: episode: 602, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 59.444 [34.000, 99.000],  loss: 8.163606, mae: 1.795976, mean_q: 4.612186\n",
      "[84 41 74 50 37 23 27 64 94 14]\n",
      "  5427/50001: episode: 603, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 47.111 [14.000, 94.000],  loss: 8.523822, mae: 1.793277, mean_q: 4.570341\n",
      "[71 76 37 37 86 49 84 29 23  4]\n",
      "  5436/50001: episode: 604, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 47.222 [4.000, 86.000],  loss: 7.770391, mae: 1.750774, mean_q: 4.446930\n",
      "[ 3 94 90 92 55 86 50 67 27 97]\n",
      "  5445/50001: episode: 605, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 43.000, mean reward:  4.778 [ 3.000,  9.000], mean action: 73.111 [27.000, 97.000],  loss: 6.544514, mae: 1.710491, mean_q: 4.341302\n",
      "[50 96 32 37 12 37 12 37 70 24]\n",
      "  5454/50001: episode: 606, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: -6.000, mean reward: -0.667 [-10.000,  6.000], mean action: 39.667 [12.000, 96.000],  loss: 6.289616, mae: 1.717651, mean_q: 4.347174\n",
      "[14 63 95 46 37 99 43 43  4 14]\n",
      "  5463/50001: episode: 607, duration: 0.082s, episode steps:   9, steps per second: 110, episode reward:  9.000, mean reward:  1.000 [-10.000,  6.000], mean action: 49.333 [4.000, 99.000],  loss: 7.098894, mae: 1.783357, mean_q: 4.489781\n",
      "[59 78 69 37 63 32 21 86 28 50]\n",
      "  5472/50001: episode: 608, duration: 0.086s, episode steps:   9, steps per second: 104, episode reward: 42.000, mean reward:  4.667 [ 3.000,  9.000], mean action: 51.556 [21.000, 86.000],  loss: 6.834196, mae: 1.739642, mean_q: 4.419233\n",
      "[58 68 48 58 92 37 34 99 23  9]\n",
      "  5481/50001: episode: 609, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 52.000 [9.000, 99.000],  loss: 9.704431, mae: 1.828882, mean_q: 4.668811\n",
      "[74 83 80 77 92  9 25 49 85 28]\n",
      "  5490/50001: episode: 610, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 35.000, mean reward:  3.889 [ 2.000,  8.000], mean action: 58.667 [9.000, 92.000],  loss: 7.888291, mae: 1.778682, mean_q: 4.496617\n",
      "[32 98  1  1 25 58 98 56 22 63]\n",
      "  5499/50001: episode: 611, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward:  8.000, mean reward:  0.889 [-10.000,  5.000], mean action: 46.889 [1.000, 98.000],  loss: 8.194061, mae: 1.765864, mean_q: 4.391874\n",
      "[ 8 58 36 98 99 32 16 48 28 58]\n",
      "  5508/50001: episode: 612, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 52.556 [16.000, 99.000],  loss: 9.277734, mae: 1.775284, mean_q: 4.479034\n",
      "[12 76 12 32 47 48 67 37 10 38]\n",
      "  5517/50001: episode: 613, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 21.000, mean reward:  2.333 [-10.000,  7.000], mean action: 40.778 [10.000, 76.000],  loss: 8.018963, mae: 1.783951, mean_q: 4.417110\n",
      "[52 89 20 39 34 59 46 48 45 16]\n",
      "  5526/50001: episode: 614, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 35.000, mean reward:  3.889 [ 2.000,  6.000], mean action: 44.000 [16.000, 89.000],  loss: 7.233109, mae: 1.780362, mean_q: 4.395474\n",
      "[66 13 63 34 64 40 85 78  9 38]\n",
      "  5535/50001: episode: 615, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 32.000, mean reward:  3.556 [ 2.000,  5.000], mean action: 47.111 [9.000, 85.000],  loss: 8.691427, mae: 1.723514, mean_q: 4.257845\n",
      "[31 45 37 29 48 23 85 56 23 93]\n",
      "  5544/50001: episode: 616, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 20.000, mean reward:  2.222 [-10.000,  6.000], mean action: 48.778 [23.000, 93.000],  loss: 8.168308, mae: 1.725654, mean_q: 4.316071\n",
      "[35 95  1 59 44  1 37 37 63 46]\n",
      "  5553/50001: episode: 617, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 10.000, mean reward:  1.111 [-10.000,  6.000], mean action: 42.556 [1.000, 95.000],  loss: 8.545144, mae: 1.743958, mean_q: 4.344193\n",
      "[33 68 38 99 37 37 50 50 40 94]\n",
      "  5562/50001: episode: 618, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 10.000, mean reward:  1.111 [-10.000,  6.000], mean action: 57.000 [37.000, 99.000],  loss: 9.985484, mae: 1.650501, mean_q: 4.144429\n",
      "[61 28 29  1 16 33  2 68 72  8]\n",
      "  5571/50001: episode: 619, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 33.000, mean reward:  3.667 [ 2.000,  6.000], mean action: 28.556 [1.000, 72.000],  loss: 9.304146, mae: 1.686005, mean_q: 4.240594\n",
      "[27 58 23 45 17 55 58 78 72 38]\n",
      "  5580/50001: episode: 620, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 13.000, mean reward:  1.444 [-10.000,  4.000], mean action: 49.333 [17.000, 78.000],  loss: 6.839857, mae: 1.676107, mean_q: 4.187983\n",
      "[33 83 97 29 37 58 32 97 59 99]\n",
      "  5589/50001: episode: 621, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 21.000, mean reward:  2.333 [-10.000,  6.000], mean action: 65.667 [29.000, 99.000],  loss: 8.220025, mae: 1.694651, mean_q: 4.234544\n",
      "[53 94 58 57 50 31 79 71  2  2]\n",
      "  5598/50001: episode: 622, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 49.333 [2.000, 94.000],  loss: 8.522574, mae: 1.692822, mean_q: 4.255438\n",
      "[96 85  1 34 59 32 18 24 18 13]\n",
      "  5607/50001: episode: 623, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 27.000, mean reward:  3.000 [-10.000,  9.000], mean action: 31.556 [1.000, 85.000],  loss: 8.854564, mae: 1.685822, mean_q: 4.214145\n",
      "[81 78 40 37  1 49 63 10 62 37]\n",
      "  5616/50001: episode: 624, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 20.000, mean reward:  2.222 [-10.000,  5.000], mean action: 41.889 [1.000, 78.000],  loss: 8.724859, mae: 1.761776, mean_q: 4.446299\n",
      "[58 32 79 51 51 77 61  0 78 97]\n",
      "  5625/50001: episode: 625, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 20.000, mean reward:  2.222 [-10.000,  9.000], mean action: 58.444 [0.000, 97.000],  loss: 8.771435, mae: 1.665702, mean_q: 4.197980\n",
      "[ 0  4 23 58 57 34  4 73 86  9]\n",
      "  5634/50001: episode: 626, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 21.000, mean reward:  2.333 [-10.000,  6.000], mean action: 38.667 [4.000, 86.000],  loss: 7.630131, mae: 1.736896, mean_q: 4.367442\n",
      "[79 69 59 83 37 40 76 17 61 12]\n",
      "  5643/50001: episode: 627, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 38.000, mean reward:  4.222 [ 3.000, 10.000], mean action: 50.444 [12.000, 83.000],  loss: 10.657316, mae: 1.711418, mean_q: 4.272148\n",
      "[46 30 46 74 97 25 62 37  4 42]\n",
      "  5652/50001: episode: 628, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 46.333 [4.000, 97.000],  loss: 8.797273, mae: 1.676241, mean_q: 4.214554\n",
      "[66 39 76 92 62 32 38 26 79  7]\n",
      "  5661/50001: episode: 629, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 33.000, mean reward:  3.667 [ 2.000,  8.000], mean action: 50.111 [7.000, 92.000],  loss: 8.163657, mae: 1.631623, mean_q: 4.063653\n",
      "[87 89 33 52 37 98 28 58 34 34]\n",
      "  5670/50001: episode: 630, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 28.000, mean reward:  3.111 [-10.000,  8.000], mean action: 51.444 [28.000, 98.000],  loss: 8.802073, mae: 1.664425, mean_q: 4.207311\n",
      "[38 34 83 25 73 27 76 88 32 25]\n",
      "  5679/50001: episode: 631, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 51.444 [25.000, 88.000],  loss: 9.012761, mae: 1.692383, mean_q: 4.224120\n",
      "[40 89 58  1 32 56 46 46  1 15]\n",
      "  5688/50001: episode: 632, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward:  5.000, mean reward:  0.556 [-10.000,  5.000], mean action: 38.222 [1.000, 89.000],  loss: 7.642510, mae: 1.675036, mean_q: 4.160865\n",
      "[53 60  1 64 37 17 74 72 14 94]\n",
      "  5697/50001: episode: 633, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 35.000, mean reward:  3.889 [ 2.000,  6.000], mean action: 48.111 [1.000, 94.000],  loss: 9.667843, mae: 1.707752, mean_q: 4.347937\n",
      "[86 52 59 34  1 30 18 99 72 96]\n",
      "  5706/50001: episode: 634, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 36.000, mean reward:  4.000 [ 2.000,  6.000], mean action: 51.222 [1.000, 99.000],  loss: 7.350777, mae: 1.724259, mean_q: 4.294972\n",
      "[57 96 37 29 30  3 59 37 54 81]\n",
      "  5715/50001: episode: 635, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 19.000, mean reward:  2.111 [-10.000,  5.000], mean action: 47.333 [3.000, 96.000],  loss: 6.686609, mae: 1.659868, mean_q: 4.133893\n",
      "[19 88 23 59 99 45 85 63 29  5]\n",
      "  5724/50001: episode: 636, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 31.000, mean reward:  3.444 [ 2.000,  6.000], mean action: 55.111 [5.000, 99.000],  loss: 7.262116, mae: 1.745152, mean_q: 4.295059\n",
      "[ 6 94 32 58 27 33 48 10 51 20]\n",
      "  5733/50001: episode: 637, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 36.000, mean reward:  4.000 [ 3.000,  5.000], mean action: 41.444 [10.000, 94.000],  loss: 9.700131, mae: 1.745605, mean_q: 4.317762\n",
      "[67 22 90 95 37 17 55  6 88 55]\n",
      "  5742/50001: episode: 638, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 24.000, mean reward:  2.667 [-10.000,  8.000], mean action: 51.667 [6.000, 95.000],  loss: 9.442805, mae: 1.702979, mean_q: 4.257837\n",
      "[25 79 60 25 63 50 78 50  8 73]\n",
      "  5751/50001: episode: 639, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward:  6.000, mean reward:  0.667 [-10.000,  6.000], mean action: 54.000 [8.000, 79.000],  loss: 9.087632, mae: 1.737397, mean_q: 4.348936\n",
      "[84 93 58 62 27 60 97 24 61 58]\n",
      "  5760/50001: episode: 640, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 60.000 [24.000, 97.000],  loss: 6.492745, mae: 1.712966, mean_q: 4.297214\n",
      "[94 34 24 89 55 76 51 59 31 94]\n",
      "  5769/50001: episode: 641, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 57.000 [24.000, 94.000],  loss: 7.080164, mae: 1.715447, mean_q: 4.288466\n",
      "[85 54 46 63 94 70 34 46 97 97]\n",
      "  5778/50001: episode: 642, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 11.000, mean reward:  1.222 [-10.000,  8.000], mean action: 66.778 [34.000, 97.000],  loss: 8.763873, mae: 1.664609, mean_q: 4.192808\n",
      "[51 34 29 52 29 46 11 48 52 82]\n",
      "  5787/50001: episode: 643, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 10.000, mean reward:  1.111 [-10.000,  6.000], mean action: 42.556 [11.000, 82.000],  loss: 6.434405, mae: 1.657845, mean_q: 4.132958\n",
      "[30 52 29  8 69  5 56 19  4 50]\n",
      "  5796/50001: episode: 644, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward: 40.000, mean reward:  4.444 [ 2.000,  9.000], mean action: 32.444 [4.000, 69.000],  loss: 8.060435, mae: 1.769242, mean_q: 4.456422\n",
      "[34 41 59 12 69 76 34 83 86  9]\n",
      "  5805/50001: episode: 645, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 19.000, mean reward:  2.111 [-10.000,  6.000], mean action: 52.111 [9.000, 86.000],  loss: 8.000129, mae: 1.736380, mean_q: 4.358766\n",
      "[98 13 46 33 37 76 10 28 51 80]\n",
      "  5814/50001: episode: 646, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 37.000, mean reward:  4.111 [ 3.000,  6.000], mean action: 41.556 [10.000, 80.000],  loss: 8.127793, mae: 1.777183, mean_q: 4.493635\n",
      "[41 91 50 23 36 25 48 84 13 39]\n",
      "  5823/50001: episode: 647, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 37.000, mean reward:  4.111 [ 2.000,  8.000], mean action: 45.444 [13.000, 91.000],  loss: 7.966396, mae: 1.709037, mean_q: 4.265178\n",
      "[43 14 10 63 97 32 88 95 98 24]\n",
      "  5832/50001: episode: 648, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 45.000, mean reward:  5.000 [ 3.000,  8.000], mean action: 57.889 [10.000, 98.000],  loss: 11.069759, mae: 1.768399, mean_q: 4.423939\n",
      "[23 18 37 92 12 76 13 49 86 31]\n",
      "  5841/50001: episode: 649, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 37.000, mean reward:  4.111 [ 2.000,  7.000], mean action: 46.000 [12.000, 92.000],  loss: 7.837306, mae: 1.677671, mean_q: 4.170283\n",
      "[61  9 93 99 46  6 37 62 55 14]\n",
      "  5850/50001: episode: 650, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 46.778 [6.000, 99.000],  loss: 8.515630, mae: 1.743586, mean_q: 4.357645\n",
      "[51 27 70  1  9 63 63 69 97 48]\n",
      "  5859/50001: episode: 651, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 27.000, mean reward:  3.000 [-10.000,  8.000], mean action: 49.667 [1.000, 97.000],  loss: 7.892697, mae: 1.744996, mean_q: 4.286924\n",
      "[56 78 59 24 43 34 28 13 88 50]\n",
      "  5868/50001: episode: 652, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 46.000, mean reward:  5.111 [ 3.000,  8.000], mean action: 46.333 [13.000, 88.000],  loss: 7.995979, mae: 1.740082, mean_q: 4.327811\n",
      "[23 72 12 84 69  9 50 25 94  9]\n",
      "  5877/50001: episode: 653, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 20.000, mean reward:  2.222 [-10.000,  6.000], mean action: 47.111 [9.000, 94.000],  loss: 8.995473, mae: 1.712174, mean_q: 4.325428\n",
      "[64 88 79 92 76 21 25 32  9  9]\n",
      "  5886/50001: episode: 654, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 21.000, mean reward:  2.333 [-10.000,  6.000], mean action: 47.889 [9.000, 92.000],  loss: 9.327552, mae: 1.677159, mean_q: 4.186000\n",
      "[42 54 89 16 99 56 58  3 34 62]\n",
      "  5895/50001: episode: 655, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 38.000, mean reward:  4.222 [ 3.000,  9.000], mean action: 52.333 [3.000, 99.000],  loss: 9.696090, mae: 1.718853, mean_q: 4.313846\n",
      "[91 42 39 98 44 32 21 92 31 12]\n",
      "  5904/50001: episode: 656, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 42.000, mean reward:  4.667 [ 2.000,  9.000], mean action: 45.667 [12.000, 98.000],  loss: 7.579474, mae: 1.673330, mean_q: 4.152003\n",
      "[53 62 50 74 99 49 28 34 31  4]\n",
      "  5913/50001: episode: 657, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 47.889 [4.000, 99.000],  loss: 5.161765, mae: 1.731385, mean_q: 4.249433\n",
      "[50 10 50  7 11 36 43 34 20 29]\n",
      "  5922/50001: episode: 658, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 20.000, mean reward:  2.222 [-10.000,  9.000], mean action: 26.667 [7.000, 50.000],  loss: 8.454730, mae: 1.735002, mean_q: 4.280379\n",
      "[29 88 98 56 63 74 32 44 37 62]\n",
      "  5931/50001: episode: 659, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 39.000, mean reward:  4.333 [ 3.000,  8.000], mean action: 61.556 [32.000, 98.000],  loss: 6.909049, mae: 1.758032, mean_q: 4.363441\n",
      "[79 99 60 15 91 16 60 23 50 76]\n",
      "  5940/50001: episode: 660, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 22.000, mean reward:  2.444 [-10.000,  8.000], mean action: 54.444 [15.000, 99.000],  loss: 7.118289, mae: 1.808104, mean_q: 4.434562\n",
      "[51 46 48 50 34 70 69 97  8 14]\n",
      "  5949/50001: episode: 661, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 38.000, mean reward:  4.222 [ 2.000,  6.000], mean action: 48.444 [8.000, 97.000],  loss: 9.563272, mae: 1.789878, mean_q: 4.505156\n",
      "[70 38 44 57 77 32 32 61 28 28]\n",
      "  5958/50001: episode: 662, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 10.000, mean reward:  1.111 [-10.000,  8.000], mean action: 44.111 [28.000, 77.000],  loss: 8.826939, mae: 1.806344, mean_q: 4.574577\n",
      "[ 5 14 80 70 58 40 48 72 94 40]\n",
      "  5967/50001: episode: 663, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 16.000, mean reward:  1.778 [-10.000,  5.000], mean action: 57.333 [14.000, 94.000],  loss: 8.587548, mae: 1.755059, mean_q: 4.374258\n",
      "[46 80 32 79 17  1 50 12 28  1]\n",
      "  5976/50001: episode: 664, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 33.333 [1.000, 80.000],  loss: 7.761432, mae: 1.699278, mean_q: 4.212234\n",
      "[47 67 37 80 79 57 65 45 78 28]\n",
      "  5985/50001: episode: 665, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 34.000, mean reward:  3.778 [ 2.000,  8.000], mean action: 59.556 [28.000, 80.000],  loss: 7.331336, mae: 1.761798, mean_q: 4.336933\n",
      "[ 2 28 22 76 58 88 41 30 45 30]\n",
      "  5994/50001: episode: 666, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 21.000, mean reward:  2.333 [-10.000,  5.000], mean action: 46.444 [22.000, 88.000],  loss: 7.961399, mae: 1.754321, mean_q: 4.433163\n",
      "[47 15 35 98 37 37 90 18 35 12]\n",
      "  6003/50001: episode: 667, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 10.000, mean reward:  1.111 [-10.000,  8.000], mean action: 41.889 [12.000, 98.000],  loss: 8.988689, mae: 1.698976, mean_q: 4.231299\n",
      "[83 81 25 37 49 34 75 37 69 52]\n",
      "  6012/50001: episode: 668, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 51.000 [25.000, 81.000],  loss: 8.212256, mae: 1.695688, mean_q: 4.188612\n",
      "[71 91  1 23  1 33 50 68  1 89]\n",
      "  6021/50001: episode: 669, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward:  7.000, mean reward:  0.778 [-10.000,  6.000], mean action: 39.667 [1.000, 91.000],  loss: 7.423128, mae: 1.724483, mean_q: 4.275089\n",
      "[96 57 13 13 25 24 37 86 10 15]\n",
      "  6030/50001: episode: 670, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 20.000, mean reward:  2.222 [-10.000,  6.000], mean action: 31.111 [10.000, 86.000],  loss: 7.961791, mae: 1.765329, mean_q: 4.322363\n",
      "[12  4  1 79 12 48 69 30 93 31]\n",
      "  6039/50001: episode: 671, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 40.778 [1.000, 93.000],  loss: 9.105051, mae: 1.664994, mean_q: 4.174967\n",
      "[35 36 73 47 32 94 34 67 52 94]\n",
      "  6048/50001: episode: 672, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 24.000, mean reward:  2.667 [-10.000,  8.000], mean action: 58.778 [32.000, 94.000],  loss: 8.065851, mae: 1.764177, mean_q: 4.350606\n",
      "[ 4 50 33 96 23 40 92 94 85 62]\n",
      "  6057/50001: episode: 673, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 34.000, mean reward:  3.778 [ 3.000,  5.000], mean action: 63.889 [23.000, 96.000],  loss: 6.835061, mae: 1.759239, mean_q: 4.430015\n",
      "[83 30 29 37 37  4 74 83 54 84]\n",
      "  6066/50001: episode: 674, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward:  7.000, mean reward:  0.778 [-10.000,  5.000], mean action: 48.000 [4.000, 84.000],  loss: 8.327845, mae: 1.766687, mean_q: 4.422497\n",
      "[17 10 27 92 33 48 51 65 88  9]\n",
      "  6075/50001: episode: 675, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 39.000, mean reward:  4.333 [ 3.000,  8.000], mean action: 47.000 [9.000, 92.000],  loss: 8.110831, mae: 1.744227, mean_q: 4.316523\n",
      "[52 83 52 20 88 76 27 53 41 82]\n",
      "  6084/50001: episode: 676, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 58.000 [20.000, 88.000],  loss: 9.187136, mae: 1.706024, mean_q: 4.258997\n",
      "[25 89 37 50 55 97 88 78 30 27]\n",
      "  6093/50001: episode: 677, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 61.222 [27.000, 97.000],  loss: 8.151942, mae: 1.745595, mean_q: 4.369474\n",
      "[26  2 64 63 25 60 60 86  2  7]\n",
      "  6102/50001: episode: 678, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward:  4.000, mean reward:  0.444 [-10.000,  5.000], mean action: 41.000 [2.000, 86.000],  loss: 8.935337, mae: 1.732277, mean_q: 4.215026\n",
      "[81  0 32 28 67 24 27 10 84  9]\n",
      "  6111/50001: episode: 679, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 38.000, mean reward:  4.222 [ 2.000,  6.000], mean action: 31.222 [0.000, 84.000],  loss: 7.423129, mae: 1.629304, mean_q: 4.074156\n",
      "[71 10 98 99 12 76 38 50  9 64]\n",
      "  6120/50001: episode: 680, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 37.000, mean reward:  4.111 [ 2.000,  7.000], mean action: 50.667 [9.000, 99.000],  loss: 9.924957, mae: 1.715770, mean_q: 4.320009\n",
      "[67 42 24 20 27 32 90 55 21 71]\n",
      "  6129/50001: episode: 681, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 33.000, mean reward:  3.667 [ 3.000,  5.000], mean action: 42.444 [20.000, 90.000],  loss: 9.711420, mae: 1.681591, mean_q: 4.157794\n",
      "[59 74 44 22 25 37 24 92 76 38]\n",
      "  6138/50001: episode: 682, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 33.000, mean reward:  3.667 [ 2.000,  6.000], mean action: 48.000 [22.000, 92.000],  loss: 7.140604, mae: 1.677149, mean_q: 4.189863\n",
      "[83  1 54 83 48 62 34 46 95 97]\n",
      "  6147/50001: episode: 683, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 34.000, mean reward:  3.778 [-10.000,  9.000], mean action: 57.778 [1.000, 97.000],  loss: 7.641558, mae: 1.683526, mean_q: 4.128463\n",
      "[79 22 24 34 33  9 24 13 52 82]\n",
      "  6156/50001: episode: 684, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 32.556 [9.000, 82.000],  loss: 8.619505, mae: 1.626237, mean_q: 4.085224\n",
      "[88 14 90 12 73 31 43  4 94 50]\n",
      "  6165/50001: episode: 685, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 39.000, mean reward:  4.333 [ 2.000,  9.000], mean action: 45.667 [4.000, 94.000],  loss: 9.159554, mae: 1.650757, mean_q: 4.075042\n",
      "[ 4 97 71 80 83 57 50 62 28 23]\n",
      "  6174/50001: episode: 686, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 38.000, mean reward:  4.222 [ 2.000,  6.000], mean action: 61.222 [23.000, 97.000],  loss: 10.506884, mae: 1.670005, mean_q: 4.215578\n",
      "[15 84 76 49 99 28 23 13 28 85]\n",
      "  6183/50001: episode: 687, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 53.889 [13.000, 99.000],  loss: 8.341931, mae: 1.613865, mean_q: 3.998368\n",
      "[78  4 50 45 54 58 30 67 94 98]\n",
      "  6192/50001: episode: 688, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 55.556 [4.000, 98.000],  loss: 7.480120, mae: 1.700610, mean_q: 4.206190\n",
      "[17 70  2 25 79 48 51 97 91 91]\n",
      "  6201/50001: episode: 689, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 61.556 [2.000, 97.000],  loss: 7.176542, mae: 1.623998, mean_q: 4.048365\n",
      "[15 69 58 58 52 16  2 25  2 43]\n",
      "  6210/50001: episode: 690, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward:  5.000, mean reward:  0.556 [-10.000,  5.000], mean action: 36.111 [2.000, 69.000],  loss: 6.888094, mae: 1.763921, mean_q: 4.431602\n",
      "[37 79 86 21 92 49 88 46 85 72]\n",
      "  6219/50001: episode: 691, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 35.000, mean reward:  3.889 [ 2.000,  6.000], mean action: 68.667 [21.000, 92.000],  loss: 7.440631, mae: 1.778660, mean_q: 4.379495\n",
      "[52 23 48  1 79 49 83  6 52 93]\n",
      "  6228/50001: episode: 692, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 20.000, mean reward:  2.222 [-10.000,  6.000], mean action: 48.222 [1.000, 93.000],  loss: 8.305870, mae: 1.707001, mean_q: 4.237714\n",
      "[ 5  0 44 56 56 60 67 42 59 83]\n",
      "  6237/50001: episode: 693, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 21.000, mean reward:  2.333 [-10.000,  5.000], mean action: 51.889 [0.000, 83.000],  loss: 6.748763, mae: 1.751623, mean_q: 4.323262\n",
      "[75 88 50 24 88 91 34 63 49 89]\n",
      "  6246/50001: episode: 694, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 64.000 [24.000, 91.000],  loss: 9.678666, mae: 1.804803, mean_q: 4.437567\n",
      "[97 39 74 58 55 60 89 95 62 31]\n",
      "  6255/50001: episode: 695, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 39.000, mean reward:  4.333 [ 2.000,  8.000], mean action: 62.556 [31.000, 95.000],  loss: 7.856391, mae: 1.783256, mean_q: 4.433533\n",
      "[90 45 39 63 97 30 43 11 67 50]\n",
      "  6264/50001: episode: 696, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 40.000, mean reward:  4.444 [ 2.000, 10.000], mean action: 49.444 [11.000, 97.000],  loss: 8.946656, mae: 1.737682, mean_q: 4.294785\n",
      "[37 78 58 32 99 14 50 13 28 68]\n",
      "  6273/50001: episode: 697, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 36.000, mean reward:  4.000 [ 2.000,  6.000], mean action: 48.889 [13.000, 99.000],  loss: 8.951904, mae: 1.750362, mean_q: 4.340250\n",
      "[87 26 34 90 33 32 95 28 23 24]\n",
      "  6282/50001: episode: 698, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 41.000, mean reward:  4.556 [ 2.000,  7.000], mean action: 42.778 [23.000, 95.000],  loss: 8.143169, mae: 1.790677, mean_q: 4.409319\n",
      "[22 41 51 75 36 97 76 53 89 31]\n",
      "  6291/50001: episode: 699, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 39.000, mean reward:  4.333 [ 2.000,  8.000], mean action: 61.000 [31.000, 97.000],  loss: 8.074191, mae: 1.778645, mean_q: 4.389748\n",
      "[47 31  9 37 68 13 42 91 91 55]\n",
      "  6300/50001: episode: 700, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 21.000, mean reward:  2.333 [-10.000,  5.000], mean action: 48.556 [9.000, 91.000],  loss: 7.950419, mae: 1.786715, mean_q: 4.393946\n",
      "[38 23 51 96 99 68 45 51 10 24]\n",
      "  6309/50001: episode: 701, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 20.000, mean reward:  2.222 [-10.000,  7.000], mean action: 51.889 [10.000, 99.000],  loss: 10.511647, mae: 1.795511, mean_q: 4.477794\n",
      "[76 69 58 24 29 12 28 97 97 76]\n",
      "  6318/50001: episode: 702, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward:  9.000, mean reward:  1.000 [-10.000,  6.000], mean action: 54.444 [12.000, 97.000],  loss: 8.358321, mae: 1.762902, mean_q: 4.366292\n",
      "[52 83 34 34 24 98 34  1 34 46]\n",
      "  6327/50001: episode: 703, duration: 0.069s, episode steps:   9, steps per second: 131, episode reward: -5.000, mean reward: -0.556 [-10.000,  5.000], mean action: 43.111 [1.000, 98.000],  loss: 6.360886, mae: 1.795007, mean_q: 4.350415\n",
      "[63 34 34  1  2 33 29 51 31 10]\n",
      "  6336/50001: episode: 704, duration: 0.076s, episode steps:   9, steps per second: 118, episode reward: 21.000, mean reward:  2.333 [-10.000,  5.000], mean action: 25.000 [1.000, 51.000],  loss: 5.488516, mae: 1.850723, mean_q: 4.646276\n",
      "[ 0 21 44 68 42 57 22 74  4 28]\n",
      "  6345/50001: episode: 705, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 40.000 [4.000, 74.000],  loss: 10.457674, mae: 1.844607, mean_q: 4.489305\n",
      "[24 99 37  8 57 23 81 27 43 32]\n",
      "  6354/50001: episode: 706, duration: 0.074s, episode steps:   9, steps per second: 121, episode reward: 36.000, mean reward:  4.000 [ 2.000,  7.000], mean action: 45.222 [8.000, 99.000],  loss: 8.260179, mae: 1.802444, mean_q: 4.498559\n",
      "[88 14 98 55 99 23 23 82 98 24]\n",
      "  6363/50001: episode: 707, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward:  8.000, mean reward:  0.889 [-10.000,  6.000], mean action: 57.333 [14.000, 99.000],  loss: 7.059347, mae: 1.794452, mean_q: 4.436294\n",
      "[76 28 74 34 99  5 96 56 36 13]\n",
      "  6372/50001: episode: 708, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 39.000, mean reward:  4.333 [ 2.000,  9.000], mean action: 49.000 [5.000, 99.000],  loss: 6.833689, mae: 1.835074, mean_q: 4.588197\n",
      "[68 83 48 30 34 28 50 98 32 55]\n",
      "  6381/50001: episode: 709, duration: 0.083s, episode steps:   9, steps per second: 109, episode reward: 42.000, mean reward:  4.667 [ 3.000,  6.000], mean action: 50.889 [28.000, 98.000],  loss: 10.074022, mae: 1.824970, mean_q: 4.614067\n",
      "[28 89 88 21 50 63  9 50  9 83]\n",
      "  6390/50001: episode: 710, duration: 0.069s, episode steps:   9, steps per second: 131, episode reward:  8.000, mean reward:  0.889 [-10.000,  5.000], mean action: 51.333 [9.000, 89.000],  loss: 9.444532, mae: 1.786639, mean_q: 4.438367\n",
      "[35  4 44 68 65 57 21 69 86 31]\n",
      "  6399/50001: episode: 711, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 37.000, mean reward:  4.111 [ 2.000,  8.000], mean action: 49.444 [4.000, 86.000],  loss: 8.534370, mae: 1.787754, mean_q: 4.454907\n",
      "[30 52 37 48  9 28 46 27  4 58]\n",
      "  6408/50001: episode: 712, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 40.000, mean reward:  4.444 [ 3.000,  5.000], mean action: 34.333 [4.000, 58.000],  loss: 8.197995, mae: 1.729131, mean_q: 4.350443\n",
      "[55  9 46 50 12 37 98 83 23 32]\n",
      "  6417/50001: episode: 713, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 42.000, mean reward:  4.667 [ 3.000,  6.000], mean action: 43.333 [9.000, 98.000],  loss: 8.227619, mae: 1.774137, mean_q: 4.473795\n",
      "[98  4 50  1 18 52 34 19 92 43]\n",
      "  6426/50001: episode: 714, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 34.000, mean reward:  3.778 [ 2.000,  6.000], mean action: 34.778 [1.000, 92.000],  loss: 6.644350, mae: 1.762453, mean_q: 4.392242\n",
      "[18 95 58 84 58 21 38 99 94 55]\n",
      "  6435/50001: episode: 715, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 20.000, mean reward:  2.222 [-10.000,  5.000], mean action: 66.889 [21.000, 99.000],  loss: 7.084578, mae: 1.784111, mean_q: 4.409074\n",
      "[39 33 76 96  1 62 48 64  2 40]\n",
      "  6444/50001: episode: 716, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 46.889 [1.000, 96.000],  loss: 8.199376, mae: 1.778219, mean_q: 4.420220\n",
      "[78 89 48 79 12 74 79 64 18 55]\n",
      "  6453/50001: episode: 717, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 19.000, mean reward:  2.111 [-10.000,  5.000], mean action: 57.556 [12.000, 89.000],  loss: 8.339729, mae: 1.724951, mean_q: 4.332624\n",
      "[50 57  1 51 34 51 80 44  4 85]\n",
      "  6462/50001: episode: 718, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 20.000, mean reward:  2.222 [-10.000,  6.000], mean action: 45.222 [1.000, 85.000],  loss: 9.399690, mae: 1.800049, mean_q: 4.400054\n",
      "[72 76 75 88 28 92 57 51 20 78]\n",
      "  6471/50001: episode: 719, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 38.000, mean reward:  4.222 [ 3.000,  5.000], mean action: 62.778 [20.000, 92.000],  loss: 8.385754, mae: 1.822928, mean_q: 4.514151\n",
      "[ 9  3 62 38 73 57 62 78 28 62]\n",
      "  6480/50001: episode: 720, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 10.000, mean reward:  1.111 [-10.000,  8.000], mean action: 51.444 [3.000, 78.000],  loss: 6.041609, mae: 1.713566, mean_q: 4.274668\n",
      "[ 5 28 59 99 85  4 62 60 83 82]\n",
      "  6489/50001: episode: 721, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 62.444 [4.000, 99.000],  loss: 8.765686, mae: 1.758393, mean_q: 4.306486\n",
      "[28 52  9 62 36 12 46 10 26 52]\n",
      "  6498/50001: episode: 722, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 18.000, mean reward:  2.000 [-10.000,  6.000], mean action: 33.889 [9.000, 62.000],  loss: 7.406589, mae: 1.717076, mean_q: 4.252281\n",
      "[44 85 98 62 97 34 32 23 86 79]\n",
      "  6507/50001: episode: 723, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 66.222 [23.000, 98.000],  loss: 8.066270, mae: 1.765364, mean_q: 4.381200\n",
      "[85 38  1 34  1 27 10 47 29 20]\n",
      "  6516/50001: episode: 724, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 23.000 [1.000, 47.000],  loss: 8.534610, mae: 1.796333, mean_q: 4.381201\n",
      "[13 50 86 49 37 27 97 38  9  9]\n",
      "  6525/50001: episode: 725, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 20.000, mean reward:  2.222 [-10.000,  5.000], mean action: 44.667 [9.000, 97.000],  loss: 9.158895, mae: 1.810298, mean_q: 4.501266\n",
      "[93  5 21 76 62 68 34 83 62 12]\n",
      "  6534/50001: episode: 726, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 28.000, mean reward:  3.111 [-10.000, 10.000], mean action: 47.000 [5.000, 83.000],  loss: 6.790607, mae: 1.789134, mean_q: 4.463407\n",
      "[97 94 64 30 34 63 88 34 99 74]\n",
      "  6543/50001: episode: 727, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 64.444 [30.000, 99.000],  loss: 8.627854, mae: 1.826472, mean_q: 4.399729\n",
      "[ 5 14 60 47 37 72 15 83 41 46]\n",
      "  6552/50001: episode: 728, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 36.000, mean reward:  4.000 [ 2.000,  7.000], mean action: 46.111 [14.000, 83.000],  loss: 6.032316, mae: 1.793317, mean_q: 4.456360\n",
      "[74 34 79 29 79  5 23 12  9 76]\n",
      "  6561/50001: episode: 729, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 38.444 [5.000, 79.000],  loss: 7.414034, mae: 1.792757, mean_q: 4.442600\n",
      "[45 34 69 81 80  2 48 54 15  9]\n",
      "  6570/50001: episode: 730, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 38.000, mean reward:  4.222 [ 3.000,  7.000], mean action: 43.556 [2.000, 81.000],  loss: 7.151425, mae: 1.765110, mean_q: 4.321807\n",
      "[45 74 50 88 73 74 79 11  1 61]\n",
      "  6579/50001: episode: 731, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 56.778 [1.000, 88.000],  loss: 7.837489, mae: 1.768095, mean_q: 4.334028\n",
      "[31 83 75  1 50 25 24 53 23 24]\n",
      "  6588/50001: episode: 732, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 22.000, mean reward:  2.444 [-10.000,  5.000], mean action: 39.778 [1.000, 83.000],  loss: 7.870480, mae: 1.770890, mean_q: 4.370256\n",
      "[80  4 56 63 34 48 16 73 26 62]\n",
      "  6597/50001: episode: 733, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 32.000, mean reward:  3.556 [ 2.000,  6.000], mean action: 42.444 [4.000, 73.000],  loss: 9.063002, mae: 1.726860, mean_q: 4.253359\n",
      "[13 88 69 99 50 34 34  1 83 48]\n",
      "  6606/50001: episode: 734, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 56.222 [1.000, 99.000],  loss: 11.163402, mae: 1.722878, mean_q: 4.260584\n",
      "[70 70 90  1 88 13 27 56 13 60]\n",
      "  6615/50001: episode: 735, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 17.000, mean reward:  1.889 [-10.000,  7.000], mean action: 46.444 [1.000, 90.000],  loss: 7.198090, mae: 1.762403, mean_q: 4.273600\n",
      "[49 86 32 74 81 15 57  2 77 47]\n",
      "  6624/50001: episode: 736, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 40.000, mean reward:  4.444 [ 3.000,  8.000], mean action: 52.333 [2.000, 86.000],  loss: 6.710908, mae: 1.727421, mean_q: 4.179464\n",
      "[42 79  1 38 85 54 23 78 83 74]\n",
      "  6633/50001: episode: 737, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 36.000, mean reward:  4.000 [ 2.000,  8.000], mean action: 57.222 [1.000, 85.000],  loss: 7.275452, mae: 1.747231, mean_q: 4.289399\n",
      "[57 30 33 90 48 32 67 39 28 39]\n",
      "  6642/50001: episode: 738, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 22.000, mean reward:  2.444 [-10.000,  7.000], mean action: 45.111 [28.000, 90.000],  loss: 7.178369, mae: 1.787049, mean_q: 4.351088\n",
      "[58 12 57 29 33 34 50 44 40 52]\n",
      "  6651/50001: episode: 739, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 38.000, mean reward:  4.222 [ 2.000,  6.000], mean action: 39.000 [12.000, 57.000],  loss: 8.172276, mae: 1.768464, mean_q: 4.348863\n",
      "[62 95 97 76 27 99 34 49 97 62]\n",
      "  6660/50001: episode: 740, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 11.000, mean reward:  1.222 [-10.000,  7.000], mean action: 70.667 [27.000, 99.000],  loss: 10.493325, mae: 1.876926, mean_q: 4.537009\n",
      "[19 50 88 10  1 43 92 20 50 50]\n",
      "  6669/50001: episode: 741, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward:  5.000, mean reward:  0.556 [-10.000,  4.000], mean action: 44.889 [1.000, 92.000],  loss: 6.473960, mae: 1.786825, mean_q: 4.338358\n",
      "[48 40 58 37 63 54 99 78 31  4]\n",
      "  6678/50001: episode: 742, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 38.000, mean reward:  4.222 [ 3.000,  7.000], mean action: 51.556 [4.000, 99.000],  loss: 7.984398, mae: 1.831005, mean_q: 4.503131\n",
      "[60  8 92 27 55  1 85 72 98 44]\n",
      "  6687/50001: episode: 743, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 34.000, mean reward:  3.778 [ 2.000,  7.000], mean action: 53.556 [1.000, 98.000],  loss: 7.019037, mae: 1.821998, mean_q: 4.435655\n",
      "[ 5 34 50 84 99 85 62 60  2 28]\n",
      "  6696/50001: episode: 744, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 56.000 [2.000, 99.000],  loss: 11.653004, mae: 1.809080, mean_q: 4.450581\n",
      "[75 50 25 50 24 34 63 14 28 28]\n",
      "  6705/50001: episode: 745, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 10.000, mean reward:  1.111 [-10.000,  6.000], mean action: 35.111 [14.000, 63.000],  loss: 8.615405, mae: 1.789280, mean_q: 4.360414\n",
      "[83 96 79 24 77 56 34 32 99 98]\n",
      "  6714/50001: episode: 746, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 66.111 [24.000, 99.000],  loss: 7.043274, mae: 1.756561, mean_q: 4.306174\n",
      "[29  0 25 33 33 60 92 52 28 58]\n",
      "  6723/50001: episode: 747, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 42.333 [0.000, 92.000],  loss: 6.496588, mae: 1.724524, mean_q: 4.256556\n",
      "[19 14 38  9 64 64 28 95 62 98]\n",
      "  6732/50001: episode: 748, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 28.000, mean reward:  3.111 [-10.000,  8.000], mean action: 52.444 [9.000, 98.000],  loss: 7.517522, mae: 1.772736, mean_q: 4.350047\n",
      "[96 60 48  2 59  5 28 96 34 76]\n",
      "  6741/50001: episode: 749, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 26.000, mean reward:  2.889 [-10.000,  8.000], mean action: 45.333 [2.000, 96.000],  loss: 7.629446, mae: 1.757098, mean_q: 4.252002\n",
      "[39 14  9 25 72 41 28 36 12  8]\n",
      "  6750/50001: episode: 750, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 38.000, mean reward:  4.222 [ 2.000,  9.000], mean action: 27.222 [8.000, 72.000],  loss: 9.295349, mae: 1.773286, mean_q: 4.376339\n",
      "[25 20 37 58 58 31 83 10 78 13]\n",
      "  6759/50001: episode: 751, duration: 0.069s, episode steps:   9, steps per second: 130, episode reward: 26.000, mean reward:  2.889 [-10.000, 10.000], mean action: 43.111 [10.000, 83.000],  loss: 6.585818, mae: 1.819207, mean_q: 4.423624\n",
      "[19 34 86 49 61  9 68 85 94 52]\n",
      "  6768/50001: episode: 752, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 36.000, mean reward:  4.000 [ 2.000,  6.000], mean action: 59.778 [9.000, 94.000],  loss: 8.494694, mae: 1.792001, mean_q: 4.365554\n",
      "[ 5 31 78 27 55 33 38 75 40  4]\n",
      "  6777/50001: episode: 753, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 42.333 [4.000, 78.000],  loss: 8.310427, mae: 1.785456, mean_q: 4.361034\n",
      "[44 31 16 86 46 52 16 41 68 79]\n",
      "  6786/50001: episode: 754, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 26.000, mean reward:  2.889 [-10.000,  8.000], mean action: 48.333 [16.000, 86.000],  loss: 7.610911, mae: 1.804480, mean_q: 4.381813\n",
      "[81  6 35 38  0 28 55 32 39 31]\n",
      "  6795/50001: episode: 755, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 37.000, mean reward:  4.111 [ 2.000,  8.000], mean action: 29.333 [0.000, 55.000],  loss: 8.091307, mae: 1.793858, mean_q: 4.308751\n",
      "[ 7 30 79 25 54 94 34 41 30 27]\n",
      "  6804/50001: episode: 756, duration: 0.065s, episode steps:   9, steps per second: 137, episode reward: 28.000, mean reward:  3.111 [-10.000,  8.000], mean action: 46.000 [25.000, 94.000],  loss: 9.654741, mae: 1.849629, mean_q: 4.404118\n",
      "[77 14 74 46 81 42 28  4 42 97]\n",
      "  6813/50001: episode: 757, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 47.556 [4.000, 97.000],  loss: 10.946621, mae: 1.791359, mean_q: 4.380436\n",
      "[ 8 34 52 95 11 99 69 70 10 88]\n",
      "  6822/50001: episode: 758, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 38.000, mean reward:  4.222 [ 2.000,  8.000], mean action: 58.667 [10.000, 99.000],  loss: 7.807945, mae: 1.751168, mean_q: 4.241869\n",
      "[46 31 50 58 99 60  4 22 26  4]\n",
      "  6831/50001: episode: 759, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 20.000, mean reward:  2.222 [-10.000,  5.000], mean action: 39.333 [4.000, 99.000],  loss: 10.874083, mae: 1.779662, mean_q: 4.315132\n",
      "[45 67 99 99 78 77 79  4 57 27]\n",
      "  6840/50001: episode: 760, duration: 0.080s, episode steps:   9, steps per second: 112, episode reward: 26.000, mean reward:  2.889 [-10.000,  6.000], mean action: 65.222 [4.000, 99.000],  loss: 6.671482, mae: 1.729387, mean_q: 4.283158\n",
      "[ 3 89 35 69 67 99 67 30 10 14]\n",
      "  6849/50001: episode: 761, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 53.333 [10.000, 99.000],  loss: 8.317467, mae: 1.753229, mean_q: 4.291623\n",
      "[30 74  1 75 72 97 78 17 84 32]\n",
      "  6858/50001: episode: 762, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 37.000, mean reward:  4.111 [ 2.000,  8.000], mean action: 58.889 [1.000, 97.000],  loss: 9.959683, mae: 1.789628, mean_q: 4.387504\n",
      "[32 41 59 68 30 34 24 17 44 97]\n",
      "  6867/50001: episode: 763, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 39.000, mean reward:  4.333 [ 2.000,  8.000], mean action: 46.000 [17.000, 97.000],  loss: 8.916846, mae: 1.730233, mean_q: 4.267791\n",
      "[28 14 57 27 97 10 52 13 24 24]\n",
      "  6876/50001: episode: 764, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 35.333 [10.000, 97.000],  loss: 9.868785, mae: 1.745464, mean_q: 4.286413\n",
      "[44 95 59 58 77 88 70 53 10  9]\n",
      "  6885/50001: episode: 765, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 37.000, mean reward:  4.111 [ 3.000,  6.000], mean action: 57.667 [9.000, 95.000],  loss: 9.571005, mae: 1.765821, mean_q: 4.320351\n",
      "[91 12 62 79 68 32 42 42  4 28]\n",
      "  6894/50001: episode: 766, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 41.000 [4.000, 79.000],  loss: 9.446201, mae: 1.753581, mean_q: 4.279040\n",
      "[50 51 47 96 37 19 64 19 44 14]\n",
      "  6903/50001: episode: 767, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 22.000, mean reward:  2.444 [-10.000,  7.000], mean action: 43.444 [14.000, 96.000],  loss: 8.414315, mae: 1.685190, mean_q: 4.171901\n",
      "[42 41 52 63 30 57 57 34 31 40]\n",
      "  6912/50001: episode: 768, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 29.000, mean reward:  3.222 [-10.000,  8.000], mean action: 45.000 [30.000, 63.000],  loss: 6.767362, mae: 1.730428, mean_q: 4.244184\n",
      "[48 28 12 59 38 20 42 84 28 31]\n",
      "  6921/50001: episode: 769, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 21.000, mean reward:  2.333 [-10.000,  7.000], mean action: 38.000 [12.000, 84.000],  loss: 8.146330, mae: 1.731400, mean_q: 4.216769\n",
      "[77 12 36  1 79 12 45 24  9 62]\n",
      "  6930/50001: episode: 770, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 21.000, mean reward:  2.333 [-10.000,  5.000], mean action: 31.111 [1.000, 79.000],  loss: 7.841470, mae: 1.835977, mean_q: 4.434764\n",
      "[49 20 18 52 99 38  4 59 98 72]\n",
      "  6939/50001: episode: 771, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 35.000, mean reward:  3.889 [ 2.000,  7.000], mean action: 51.111 [4.000, 99.000],  loss: 9.149908, mae: 1.766629, mean_q: 4.339947\n",
      "[94 95 63  9 42 13 49 49 93 25]\n",
      "  6948/50001: episode: 772, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 48.667 [9.000, 95.000],  loss: 5.765485, mae: 1.769774, mean_q: 4.358155\n",
      "[84 68 67 50 30  9 24 61 31 37]\n",
      "  6957/50001: episode: 773, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 43.000, mean reward:  4.778 [ 3.000,  9.000], mean action: 41.889 [9.000, 68.000],  loss: 8.184636, mae: 1.803502, mean_q: 4.394230\n",
      "[18 59 37 84 20  8 17 40 27 28]\n",
      "  6966/50001: episode: 774, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 39.000, mean reward:  4.333 [ 3.000,  8.000], mean action: 35.556 [8.000, 84.000],  loss: 8.942095, mae: 1.817105, mean_q: 4.470812\n",
      "[ 9 42 43 21 23 99 34 34 70 64]\n",
      "  6975/50001: episode: 775, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 20.000, mean reward:  2.222 [-10.000,  7.000], mean action: 47.778 [21.000, 99.000],  loss: 7.201862, mae: 1.778627, mean_q: 4.334746\n",
      "[29 62 74 92 99 57  1 34  8 48]\n",
      "  6984/50001: episode: 776, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 43.000, mean reward:  4.778 [ 3.000,  8.000], mean action: 52.778 [1.000, 99.000],  loss: 10.019764, mae: 1.795226, mean_q: 4.433453\n",
      "[30 34 92 88 27 27 49 97  9 86]\n",
      "  6993/50001: episode: 777, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 56.556 [9.000, 97.000],  loss: 7.468781, mae: 1.771762, mean_q: 4.275069\n",
      "[92  8 32 46 16 68 62  6 66 29]\n",
      "  7002/50001: episode: 778, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 37.000, mean reward:  4.111 [ 3.000,  8.000], mean action: 37.000 [6.000, 68.000],  loss: 7.833722, mae: 1.816251, mean_q: 4.385941\n",
      "[82 31 92 13 37 23 60 67 95 36]\n",
      "  7011/50001: episode: 779, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 39.000, mean reward:  4.333 [ 2.000,  8.000], mean action: 50.444 [13.000, 95.000],  loss: 6.504413, mae: 1.771962, mean_q: 4.309623\n",
      "[10 74 85 99 99 55 32 42 38 40]\n",
      "  7020/50001: episode: 780, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 20.000, mean reward:  2.222 [-10.000,  6.000], mean action: 62.667 [32.000, 99.000],  loss: 6.192052, mae: 1.806799, mean_q: 4.365102\n",
      "[47 10 53 32 37 46 15 67 12  9]\n",
      "  7029/50001: episode: 781, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 38.000, mean reward:  4.222 [ 2.000,  8.000], mean action: 31.222 [9.000, 67.000],  loss: 9.153584, mae: 1.794791, mean_q: 4.390432\n",
      "[99 71 37 86 63 68 20 50 42 22]\n",
      "  7038/50001: episode: 782, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 51.000 [20.000, 86.000],  loss: 8.282668, mae: 1.874199, mean_q: 4.545969\n",
      "[56 30 50 59 48 49 51 52 41 62]\n",
      "  7047/50001: episode: 783, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 36.000, mean reward:  4.000 [ 3.000,  6.000], mean action: 49.111 [30.000, 62.000],  loss: 8.673162, mae: 1.883236, mean_q: 4.500288\n",
      "[68 40 90 32 27  9 86  2 96 51]\n",
      "  7056/50001: episode: 784, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 48.111 [2.000, 96.000],  loss: 9.531159, mae: 1.874068, mean_q: 4.526677\n",
      "[75 61 25 48 30 99 34  4 57 75]\n",
      "  7065/50001: episode: 785, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 48.111 [4.000, 99.000],  loss: 10.973110, mae: 1.802534, mean_q: 4.434454\n",
      "[98 95 38 77 63  1 60 28 63 35]\n",
      "  7074/50001: episode: 786, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 51.111 [1.000, 95.000],  loss: 10.021628, mae: 1.773841, mean_q: 4.225248\n",
      "[74 39 18 90 12 39 20 76 12 31]\n",
      "  7083/50001: episode: 787, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward:  9.000, mean reward:  1.000 [-10.000,  7.000], mean action: 37.444 [12.000, 90.000],  loss: 6.783910, mae: 1.780267, mean_q: 4.280046\n",
      "[54 24 67 99 77 42 48 76 31 97]\n",
      "  7092/50001: episode: 788, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 43.000, mean reward:  4.778 [ 3.000,  8.000], mean action: 62.333 [24.000, 99.000],  loss: 7.371604, mae: 1.745916, mean_q: 4.194264\n",
      "[91  9 58 28 76 13 10 53 55 13]\n",
      "  7101/50001: episode: 789, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 20.000, mean reward:  2.222 [-10.000,  6.000], mean action: 35.000 [9.000, 76.000],  loss: 6.450513, mae: 1.738980, mean_q: 4.215151\n",
      "[32 96 79 88 37 88 50 92  2  8]\n",
      "  7110/50001: episode: 790, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 60.000 [2.000, 96.000],  loss: 8.368632, mae: 1.825411, mean_q: 4.411950\n",
      "[81  8 61 18 68 60 96 52 14 50]\n",
      "  7119/50001: episode: 791, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 40.000, mean reward:  4.444 [ 2.000,  9.000], mean action: 47.444 [8.000, 96.000],  loss: 7.307981, mae: 1.879135, mean_q: 4.522860\n",
      "[90 89 39 52 25 80 62 51 84  9]\n",
      "  7128/50001: episode: 792, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 35.000, mean reward:  3.889 [ 2.000,  6.000], mean action: 54.556 [9.000, 89.000],  loss: 8.130754, mae: 1.797982, mean_q: 4.392447\n",
      "[ 7 62 79 50 78 85  4 68 28 28]\n",
      "  7137/50001: episode: 793, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 53.556 [4.000, 85.000],  loss: 9.543648, mae: 1.845528, mean_q: 4.468709\n",
      "[17 34 69 99 72 37  4 57 12 83]\n",
      "  7146/50001: episode: 794, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 51.889 [4.000, 99.000],  loss: 7.712008, mae: 1.828818, mean_q: 4.426841\n",
      "[63 96 88 88 12 24 30 46 61 46]\n",
      "  7155/50001: episode: 795, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward:  8.000, mean reward:  0.889 [-10.000,  5.000], mean action: 54.556 [12.000, 96.000],  loss: 7.596026, mae: 1.847809, mean_q: 4.521789\n",
      "[30 78 50 32 98 23 33 56 31 88]\n",
      "  7164/50001: episode: 796, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 54.333 [23.000, 98.000],  loss: 6.596952, mae: 1.855750, mean_q: 4.487234\n",
      "[44 34 97 24 11 49  1 34 92 12]\n",
      "  7173/50001: episode: 797, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 26.000, mean reward:  2.889 [-10.000,  8.000], mean action: 39.333 [1.000, 97.000],  loss: 8.722598, mae: 1.858817, mean_q: 4.561998\n",
      "[24 34 75 10 45 56  2 24 57 12]\n",
      "  7182/50001: episode: 798, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 35.000 [2.000, 75.000],  loss: 7.074565, mae: 1.888415, mean_q: 4.597983\n",
      "[81 95 75 88 34  1 97 94 40 12]\n",
      "  7191/50001: episode: 799, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 43.000, mean reward:  4.778 [ 3.000,  7.000], mean action: 59.556 [1.000, 97.000],  loss: 6.378391, mae: 1.827973, mean_q: 4.389228\n",
      "[52 43  1 79 43 39 62 95 66 55]\n",
      "  7200/50001: episode: 800, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 26.000, mean reward:  2.889 [-10.000,  9.000], mean action: 53.667 [1.000, 95.000],  loss: 8.349152, mae: 1.834811, mean_q: 4.478361\n",
      "[52 27 89 53 86 13 37 40 40 98]\n",
      "  7209/50001: episode: 801, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 53.667 [13.000, 98.000],  loss: 7.578608, mae: 1.898595, mean_q: 4.546655\n",
      "[20 14 65 27  1 57 73 37 21  4]\n",
      "  7218/50001: episode: 802, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 39.000, mean reward:  4.333 [ 2.000,  8.000], mean action: 33.222 [1.000, 73.000],  loss: 7.646955, mae: 1.893304, mean_q: 4.586878\n",
      "[53 75 98 37 37 30 63 30  1 12]\n",
      "  7227/50001: episode: 803, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 17.000, mean reward:  1.889 [-10.000,  9.000], mean action: 42.556 [1.000, 98.000],  loss: 8.600948, mae: 1.869013, mean_q: 4.483904\n",
      "[26 95 40 25 30 10 51 15 63 79]\n",
      "  7236/50001: episode: 804, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 45.333 [10.000, 95.000],  loss: 6.626616, mae: 1.883318, mean_q: 4.516939\n",
      "[23 96 68 88  9 17 99 97 60 27]\n",
      "  7245/50001: episode: 805, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 62.333 [9.000, 99.000],  loss: 7.053680, mae: 1.876939, mean_q: 4.463171\n",
      "[38 57 48 52 24 63  4  4  1  2]\n",
      "  7254/50001: episode: 806, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 28.000, mean reward:  3.111 [-10.000,  7.000], mean action: 28.333 [1.000, 63.000],  loss: 9.818574, mae: 1.894700, mean_q: 4.605325\n",
      "[44 75 98 24 99 27 68 38 99 28]\n",
      "  7263/50001: episode: 807, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 22.000, mean reward:  2.444 [-10.000,  8.000], mean action: 61.778 [24.000, 99.000],  loss: 7.296781, mae: 1.902340, mean_q: 4.624339\n",
      "[66  4 99 68 95 77 57 53 40 52]\n",
      "  7272/50001: episode: 808, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 37.000, mean reward:  4.111 [ 3.000,  6.000], mean action: 60.556 [4.000, 99.000],  loss: 7.724302, mae: 1.857780, mean_q: 4.441534\n",
      "[ 1  4  6 50 76 68 99 87 98  8]\n",
      "  7281/50001: episode: 809, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 35.000, mean reward:  3.889 [ 3.000,  6.000], mean action: 55.111 [4.000, 99.000],  loss: 9.553430, mae: 1.841223, mean_q: 4.436772\n",
      "[25 34 12 74 62 61 41 34 14 64]\n",
      "  7290/50001: episode: 810, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 44.000 [12.000, 74.000],  loss: 6.432601, mae: 1.786001, mean_q: 4.278603\n",
      "[80 57 37 48 72  9 49 90 48 19]\n",
      "  7299/50001: episode: 811, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 21.000, mean reward:  2.333 [-10.000,  5.000], mean action: 47.667 [9.000, 90.000],  loss: 7.572912, mae: 1.878703, mean_q: 4.438314\n",
      "[31 14 30 25 99 33 60 69 91  9]\n",
      "  7308/50001: episode: 812, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 36.000, mean reward:  4.000 [ 3.000,  6.000], mean action: 47.778 [9.000, 99.000],  loss: 6.237112, mae: 1.791442, mean_q: 4.337929\n",
      "[80 88 50 55 34  1 91 23 76 77]\n",
      "  7317/50001: episode: 813, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 37.000, mean reward:  4.111 [ 2.000,  6.000], mean action: 55.000 [1.000, 91.000],  loss: 9.033297, mae: 1.866225, mean_q: 4.440647\n",
      "[37 42 46 70 88  4 53 58 83 83]\n",
      "  7326/50001: episode: 814, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 21.000, mean reward:  2.333 [-10.000,  5.000], mean action: 58.556 [4.000, 88.000],  loss: 8.146122, mae: 1.839791, mean_q: 4.400854\n",
      "[76 11 24 32 20 53 34 63 52 12]\n",
      "  7335/50001: episode: 815, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 43.000, mean reward:  4.778 [ 3.000,  9.000], mean action: 33.444 [11.000, 63.000],  loss: 8.152995, mae: 1.854322, mean_q: 4.433352\n",
      "[24 56 48 32 10  6 64 99 97 23]\n",
      "  7344/50001: episode: 816, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 38.000, mean reward:  4.222 [ 3.000,  7.000], mean action: 48.333 [6.000, 99.000],  loss: 6.763144, mae: 1.783633, mean_q: 4.261902\n",
      "[49 84 86 96 37 97 34 42 95  8]\n",
      "  7353/50001: episode: 817, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 44.000, mean reward:  4.889 [ 3.000,  9.000], mean action: 64.333 [8.000, 97.000],  loss: 9.637704, mae: 1.850348, mean_q: 4.480764\n",
      "[41 28 64 26 77  5 14  1 27 78]\n",
      "  7362/50001: episode: 818, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 36.000, mean reward:  4.000 [ 2.000,  6.000], mean action: 35.556 [1.000, 78.000],  loss: 8.086956, mae: 1.797762, mean_q: 4.327814\n",
      "[38 28 58 90 76 42 62 95 38 95]\n",
      "  7371/50001: episode: 819, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 12.000, mean reward:  1.333 [-10.000,  9.000], mean action: 64.889 [28.000, 95.000],  loss: 8.359859, mae: 1.850214, mean_q: 4.484298\n",
      "[88 80 64 34 21 30 13 14 94 90]\n",
      "  7380/50001: episode: 820, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 39.000, mean reward:  4.333 [ 2.000,  6.000], mean action: 48.889 [13.000, 94.000],  loss: 7.727552, mae: 1.777452, mean_q: 4.305003\n",
      "[84 41 75  1 27 76 68 34 61 97]\n",
      "  7389/50001: episode: 821, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 53.333 [1.000, 97.000],  loss: 6.352733, mae: 1.790655, mean_q: 4.265045\n",
      "[94 80 52 88 30 30 20 30 14 63]\n",
      "  7398/50001: episode: 822, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward:  9.000, mean reward:  1.000 [-10.000,  6.000], mean action: 45.222 [14.000, 88.000],  loss: 7.635104, mae: 1.847501, mean_q: 4.471817\n",
      "[79 95 24 29 75 30 37 30 60  9]\n",
      "  7407/50001: episode: 823, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 27.000, mean reward:  3.000 [-10.000,  6.000], mean action: 43.222 [9.000, 95.000],  loss: 7.585517, mae: 1.836418, mean_q: 4.394163\n",
      "[17 85 88 24 88 32 76 39 27 24]\n",
      "  7416/50001: episode: 824, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward:  6.000, mean reward:  0.667 [-10.000,  6.000], mean action: 53.667 [24.000, 88.000],  loss: 8.686398, mae: 1.881595, mean_q: 4.533746\n",
      "[68 95 67 34 92 76 21 34  1 27]\n",
      "  7425/50001: episode: 825, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 49.667 [1.000, 95.000],  loss: 8.473218, mae: 1.857126, mean_q: 4.465919\n",
      "[70  9 97  1 97 76 53 95 95 76]\n",
      "  7434/50001: episode: 826, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: -4.000, mean reward: -0.444 [-10.000,  8.000], mean action: 66.556 [1.000, 97.000],  loss: 8.139089, mae: 1.908996, mean_q: 4.609159\n",
      "[58 82 56 27 46 24 56 13 14 13]\n",
      "  7443/50001: episode: 827, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 12.000, mean reward:  1.333 [-10.000,  7.000], mean action: 36.778 [13.000, 82.000],  loss: 7.672849, mae: 1.849140, mean_q: 4.503053\n",
      "[20 67  1 64 58 24 50 46 82 96]\n",
      "  7452/50001: episode: 828, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 54.222 [1.000, 96.000],  loss: 9.035356, mae: 1.869703, mean_q: 4.512221\n",
      "[52  9 91 13 13 92 83  1  6 46]\n",
      "  7461/50001: episode: 829, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 39.333 [1.000, 92.000],  loss: 7.974941, mae: 1.846076, mean_q: 4.463203\n",
      "[15 62 74 92 69 74 95 28  2 41]\n",
      "  7470/50001: episode: 830, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 28.000, mean reward:  3.111 [-10.000,  8.000], mean action: 59.667 [2.000, 95.000],  loss: 7.842558, mae: 1.807232, mean_q: 4.368898\n",
      "[66 96 25 70 12 37 70 69 25 31]\n",
      "  7479/50001: episode: 831, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 11.000, mean reward:  1.222 [-10.000,  8.000], mean action: 48.333 [12.000, 96.000],  loss: 8.140375, mae: 1.902420, mean_q: 4.633043\n",
      "[26 88 74 73 29 34 35 97 40 85]\n",
      "  7488/50001: episode: 832, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 61.667 [29.000, 97.000],  loss: 8.682925, mae: 1.854837, mean_q: 4.475434\n",
      "[46 52 56 54 63 27 99 41 84 83]\n",
      "  7497/50001: episode: 833, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 34.000, mean reward:  3.778 [ 3.000,  5.000], mean action: 62.111 [27.000, 99.000],  loss: 8.458838, mae: 1.772019, mean_q: 4.231663\n",
      "[49 82 59 21 58 24 46 45  5 73]\n",
      "  7506/50001: episode: 834, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 35.000, mean reward:  3.889 [ 3.000,  6.000], mean action: 45.889 [5.000, 82.000],  loss: 6.179592, mae: 1.833129, mean_q: 4.397325\n",
      "[ 8 27 50 25 15 89 51 95 28 12]\n",
      "  7515/50001: episode: 835, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 42.000, mean reward:  4.667 [ 2.000,  8.000], mean action: 43.556 [12.000, 95.000],  loss: 7.675404, mae: 1.830092, mean_q: 4.362329\n",
      "[44 14  8 34 20 61 34 82 42 40]\n",
      "  7524/50001: episode: 836, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 37.222 [8.000, 82.000],  loss: 7.930354, mae: 1.840430, mean_q: 4.388241\n",
      "[67 88 80 88 81 48  9 11 99 79]\n",
      "  7533/50001: episode: 837, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 64.778 [9.000, 99.000],  loss: 7.081347, mae: 1.882809, mean_q: 4.514311\n",
      "[ 8 82 68 27 25 86 24 21 51  8]\n",
      "  7542/50001: episode: 838, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 43.556 [8.000, 86.000],  loss: 9.158356, mae: 1.926208, mean_q: 4.605665\n",
      "[47 31 89  2 81  5 70 49  5 76]\n",
      "  7551/50001: episode: 839, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 19.000, mean reward:  2.111 [-10.000,  5.000], mean action: 45.333 [2.000, 89.000],  loss: 8.634430, mae: 1.922781, mean_q: 4.623682\n",
      "[91 54 23 32 76 36 57 27 49 21]\n",
      "  7560/50001: episode: 840, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 36.000, mean reward:  4.000 [ 2.000,  5.000], mean action: 41.667 [21.000, 76.000],  loss: 7.509743, mae: 1.904777, mean_q: 4.540585\n",
      "[28 41 56 27 55 32 71 54  2 45]\n",
      "  7569/50001: episode: 841, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 34.000, mean reward:  3.778 [ 2.000,  7.000], mean action: 42.556 [2.000, 71.000],  loss: 7.073732, mae: 1.873369, mean_q: 4.487795\n",
      "[27 44 53 28 94 54 16 21 80 79]\n",
      "  7578/50001: episode: 842, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 36.000, mean reward:  4.000 [ 2.000,  8.000], mean action: 52.111 [16.000, 94.000],  loss: 7.638949, mae: 1.824474, mean_q: 4.347937\n",
      "[21 49 23 24 99 13 19 13 28 79]\n",
      "  7587/50001: episode: 843, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 26.000, mean reward:  2.889 [-10.000,  6.000], mean action: 38.556 [13.000, 99.000],  loss: 8.281381, mae: 1.840204, mean_q: 4.402180\n",
      "[29 15 12 88 33 54 51 68 42 31]\n",
      "  7596/50001: episode: 844, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 43.778 [12.000, 88.000],  loss: 7.261422, mae: 1.887689, mean_q: 4.505120\n",
      "[42 99 77 32 27 12 46 89 27  1]\n",
      "  7605/50001: episode: 845, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 27.000, mean reward:  3.000 [-10.000,  9.000], mean action: 45.556 [1.000, 99.000],  loss: 8.225436, mae: 1.928747, mean_q: 4.588897\n",
      "[58  9 75 40 34 95 89 99 31 31]\n",
      "  7614/50001: episode: 846, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 27.000, mean reward:  3.000 [-10.000,  6.000], mean action: 55.889 [9.000, 99.000],  loss: 6.118811, mae: 1.914100, mean_q: 4.542064\n",
      "[ 9 71 20  9 90 14 24  4 58 33]\n",
      "  7623/50001: episode: 847, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 35.889 [4.000, 90.000],  loss: 8.566418, mae: 1.943322, mean_q: 4.607399\n",
      "[12 93 60 27  2 24 77 76 53 90]\n",
      "  7632/50001: episode: 848, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 35.000, mean reward:  3.889 [ 3.000,  6.000], mean action: 55.778 [2.000, 93.000],  loss: 8.814011, mae: 1.889923, mean_q: 4.539906\n",
      "[85  4 51 48 52 53 69  0 96 93]\n",
      "  7641/50001: episode: 849, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 33.000, mean reward:  3.667 [ 2.000,  6.000], mean action: 51.778 [0.000, 96.000],  loss: 8.876017, mae: 1.880903, mean_q: 4.474110\n",
      "[74  9 24 49 61 60 41 74 40 12]\n",
      "  7650/50001: episode: 850, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 28.000, mean reward:  3.111 [-10.000,  9.000], mean action: 41.111 [9.000, 74.000],  loss: 7.626318, mae: 1.844561, mean_q: 4.395624\n",
      "[ 7 38 10 84 98 32 20 13  9 98]\n",
      "  7659/50001: episode: 851, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 44.667 [9.000, 98.000],  loss: 7.040394, mae: 1.891884, mean_q: 4.449286\n",
      "[72 82 29 52 90 49 21 13 49 73]\n",
      "  7668/50001: episode: 852, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 24.000, mean reward:  2.667 [-10.000,  8.000], mean action: 50.889 [13.000, 90.000],  loss: 7.717225, mae: 1.873199, mean_q: 4.522470\n",
      "[70 95 97  1 37 27 30 27 30 62]\n",
      "  7677/50001: episode: 853, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 11.000, mean reward:  1.222 [-10.000,  5.000], mean action: 45.111 [1.000, 97.000],  loss: 5.684850, mae: 1.913233, mean_q: 4.559517\n",
      "[81 52 90 99 27 12 46 61 97 83]\n",
      "  7686/50001: episode: 854, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 38.000, mean reward:  4.222 [ 3.000,  7.000], mean action: 63.000 [12.000, 99.000],  loss: 10.412833, mae: 1.838896, mean_q: 4.360802\n",
      "[22 81 60 56 37 38 97  2  4 25]\n",
      "  7695/50001: episode: 855, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 35.000, mean reward:  3.889 [ 2.000,  6.000], mean action: 44.444 [2.000, 97.000],  loss: 8.919903, mae: 1.888250, mean_q: 4.489706\n",
      "[19 90 75 48 62 69 96 73 36 64]\n",
      "  7704/50001: episode: 856, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 32.000, mean reward:  3.556 [ 2.000,  6.000], mean action: 68.111 [36.000, 96.000],  loss: 6.044968, mae: 1.868894, mean_q: 4.540518\n",
      "[42 73 98 58 63 88 21 84 14 28]\n",
      "  7713/50001: episode: 857, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 41.000, mean reward:  4.556 [ 2.000,  8.000], mean action: 58.556 [14.000, 98.000],  loss: 7.988322, mae: 1.873678, mean_q: 4.542475\n",
      "[62 28 24 58 10 76 41  0 63 51]\n",
      "  7722/50001: episode: 858, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 38.000, mean reward:  4.222 [ 3.000,  7.000], mean action: 39.000 [0.000, 76.000],  loss: 11.756368, mae: 1.911680, mean_q: 4.556340\n",
      "[59 34 25 50 64 42 88  5  9 14]\n",
      "  7731/50001: episode: 859, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 43.000, mean reward:  4.778 [ 3.000,  7.000], mean action: 36.778 [5.000, 88.000],  loss: 9.939149, mae: 1.918314, mean_q: 4.551068\n",
      "[35 14 58 29 56 93 89 41 31 78]\n",
      "  7740/50001: episode: 860, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 35.000, mean reward:  3.889 [ 2.000,  6.000], mean action: 54.333 [14.000, 93.000],  loss: 8.900797, mae: 1.851742, mean_q: 4.397000\n",
      "[74 91 59 28 52 53 52 46  6 49]\n",
      "  7749/50001: episode: 861, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 48.444 [6.000, 91.000],  loss: 10.941502, mae: 1.824198, mean_q: 4.428668\n",
      "[22  1 26 34 23 52 76 62 13 14]\n",
      "  7758/50001: episode: 862, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 41.000, mean reward:  4.556 [ 2.000,  8.000], mean action: 33.444 [1.000, 76.000],  loss: 6.994890, mae: 1.768792, mean_q: 4.244666\n",
      "[ 7 12 24 62 54 68 95 28 83 31]\n",
      "  7767/50001: episode: 863, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 50.778 [12.000, 95.000],  loss: 7.532927, mae: 1.761832, mean_q: 4.260283\n",
      "[ 5 49 34 90 13 21 50 59 98 46]\n",
      "  7776/50001: episode: 864, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 42.000, mean reward:  4.667 [ 3.000,  6.000], mean action: 51.111 [13.000, 98.000],  loss: 10.564769, mae: 1.851702, mean_q: 4.503729\n",
      "[69 69 48 98 68 65 88 38 84 12]\n",
      "  7785/50001: episode: 865, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 25.000, mean reward:  2.778 [-10.000, 10.000], mean action: 63.333 [12.000, 98.000],  loss: 7.745368, mae: 1.842774, mean_q: 4.419602\n",
      "[80 89 69 32  1 92 35 10 51 20]\n",
      "  7794/50001: episode: 866, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 36.000, mean reward:  4.000 [ 3.000,  5.000], mean action: 44.333 [1.000, 92.000],  loss: 7.345256, mae: 1.832040, mean_q: 4.348760\n",
      "[91 14  4 10 39 95 89 95 33 97]\n",
      "  7803/50001: episode: 867, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 52.889 [4.000, 97.000],  loss: 8.677796, mae: 1.893769, mean_q: 4.502903\n",
      "[16 14 53 52 99 37 76 75 45 83]\n",
      "  7812/50001: episode: 868, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 34.000, mean reward:  3.778 [ 2.000,  6.000], mean action: 59.333 [14.000, 99.000],  loss: 8.833875, mae: 1.864411, mean_q: 4.454309\n",
      "[63 91 47 77 73 90 21 79 30 14]\n",
      "  7821/50001: episode: 869, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 58.000 [14.000, 91.000],  loss: 8.080619, mae: 1.794572, mean_q: 4.269048\n",
      "[32 77 10 59 90 50 32 62 30 52]\n",
      "  7830/50001: episode: 870, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 51.333 [10.000, 90.000],  loss: 6.865104, mae: 1.814328, mean_q: 4.348585\n",
      "[68 98 24 79 27 12 10 25 37 56]\n",
      "  7839/50001: episode: 871, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 40.000, mean reward:  4.444 [ 3.000,  8.000], mean action: 40.889 [10.000, 98.000],  loss: 7.785627, mae: 1.781788, mean_q: 4.279157\n",
      "[35 63 20 28 37 46 52 95 26 28]\n",
      "  7848/50001: episode: 872, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 43.889 [20.000, 95.000],  loss: 7.726211, mae: 1.787692, mean_q: 4.290771\n",
      "[92 71 75 37 32 63  1 34 34 88]\n",
      "  7857/50001: episode: 873, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 48.333 [1.000, 88.000],  loss: 7.456132, mae: 1.813707, mean_q: 4.424569\n",
      "[34 90 59 88 97  9 38 15 23  8]\n",
      "  7866/50001: episode: 874, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 33.000, mean reward:  3.667 [ 2.000,  6.000], mean action: 47.444 [8.000, 97.000],  loss: 8.743783, mae: 1.917474, mean_q: 4.606186\n",
      "[26 14 83  2  1 40 39 34 52 48]\n",
      "  7875/50001: episode: 875, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 42.000, mean reward:  4.667 [ 2.000,  8.000], mean action: 34.778 [1.000, 83.000],  loss: 8.410041, mae: 1.801168, mean_q: 4.296260\n",
      "[23  2 36 62 54 65 11 95 66 28]\n",
      "  7884/50001: episode: 876, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 43.000, mean reward:  4.778 [ 2.000,  9.000], mean action: 46.556 [2.000, 95.000],  loss: 7.489199, mae: 1.799958, mean_q: 4.265055\n",
      "[83  8 62 77 93 60 95 95 96 98]\n",
      "  7893/50001: episode: 877, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 76.000 [8.000, 98.000],  loss: 8.239281, mae: 1.826120, mean_q: 4.295751\n",
      "[61 88 50 98 76 31 40 42 31  4]\n",
      "  7902/50001: episode: 878, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 51.111 [4.000, 98.000],  loss: 7.795490, mae: 1.805308, mean_q: 4.302556\n",
      "[ 7 28 68 10 84 76 82 38 96 50]\n",
      "  7911/50001: episode: 879, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 38.000, mean reward:  4.222 [ 3.000, 10.000], mean action: 59.111 [10.000, 96.000],  loss: 6.882681, mae: 1.810752, mean_q: 4.323346\n",
      "[77 69  1 12 64 59 88 45 14 48]\n",
      "  7920/50001: episode: 880, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 44.444 [1.000, 88.000],  loss: 6.857049, mae: 1.807940, mean_q: 4.295012\n",
      "[77 96  1 32 79 98 59 95 84 71]\n",
      "  7929/50001: episode: 881, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 38.000, mean reward:  4.222 [ 2.000,  8.000], mean action: 68.333 [1.000, 98.000],  loss: 9.542625, mae: 1.841497, mean_q: 4.420129\n",
      "[ 5 76 58 28 29 51 51 50 71 38]\n",
      "  7938/50001: episode: 882, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 18.000, mean reward:  2.000 [-10.000,  7.000], mean action: 50.222 [28.000, 76.000],  loss: 6.894082, mae: 1.847881, mean_q: 4.378713\n",
      "[94  4 24 32 84 89 95 95 18 98]\n",
      "  7947/50001: episode: 883, duration: 0.069s, episode steps:   9, steps per second: 131, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 59.889 [4.000, 98.000],  loss: 6.858978, mae: 1.919737, mean_q: 4.553180\n",
      "[96 96 90 24 88 14 32 32 27  9]\n",
      "  7956/50001: episode: 884, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward: 12.000, mean reward:  1.333 [-10.000,  5.000], mean action: 45.778 [9.000, 96.000],  loss: 10.742835, mae: 1.931098, mean_q: 4.573666\n",
      "[24 90 20 86 24 59 62 68 40 28]\n",
      "  7965/50001: episode: 885, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 53.000 [20.000, 90.000],  loss: 6.251828, mae: 1.910091, mean_q: 4.561239\n",
      "[62  4 76 37 24 81 99 95 95 95]\n",
      "  7974/50001: episode: 886, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 11.000, mean reward:  1.222 [-10.000,  8.000], mean action: 67.333 [4.000, 99.000],  loss: 6.058120, mae: 1.877804, mean_q: 4.473625\n",
      "[17  7 94 30 64 12 46 27 14 40]\n",
      "  7983/50001: episode: 887, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 41.000, mean reward:  4.556 [ 2.000,  6.000], mean action: 37.111 [7.000, 94.000],  loss: 7.711169, mae: 1.925883, mean_q: 4.562435\n",
      "[84 33 64 34 59 72 20 52 86  9]\n",
      "  7992/50001: episode: 888, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 37.000, mean reward:  4.111 [ 2.000,  7.000], mean action: 47.667 [9.000, 86.000],  loss: 6.199910, mae: 1.923758, mean_q: 4.633510\n",
      "[29  9 82 97 88 12 67 78 14 31]\n",
      "  8001/50001: episode: 889, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 53.111 [9.000, 97.000],  loss: 7.448797, mae: 1.873690, mean_q: 4.427542\n",
      "[ 4  4 73 99 58 52 48  4 60 90]\n",
      "  8010/50001: episode: 890, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward:  8.000, mean reward:  0.889 [-10.000,  6.000], mean action: 54.222 [4.000, 99.000],  loss: 8.699825, mae: 1.966528, mean_q: 4.632925\n",
      "[74 34 37 11 46  2 13 76 41 93]\n",
      "  8019/50001: episode: 891, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 40.000, mean reward:  4.444 [ 3.000,  6.000], mean action: 39.222 [2.000, 93.000],  loss: 7.919656, mae: 1.944494, mean_q: 4.597467\n",
      "[97 33 54 59 33 62 48 52 95  9]\n",
      "  8028/50001: episode: 892, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 29.000, mean reward:  3.222 [-10.000, 10.000], mean action: 49.444 [9.000, 95.000],  loss: 8.514650, mae: 1.972523, mean_q: 4.605027\n",
      "[11 95 53 85 12 83 62 50 13 55]\n",
      "  8037/50001: episode: 893, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 56.444 [12.000, 95.000],  loss: 8.920006, mae: 1.978855, mean_q: 4.641239\n",
      "[28 52 76 27 46 32 11 88 14 26]\n",
      "  8046/50001: episode: 894, duration: 0.074s, episode steps:   9, steps per second: 121, episode reward: 37.000, mean reward:  4.111 [ 2.000,  6.000], mean action: 41.333 [11.000, 88.000],  loss: 9.250676, mae: 1.919586, mean_q: 4.546567\n",
      "[53  7 13 62 20 12 50 96 73  8]\n",
      "  8055/50001: episode: 895, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 33.000, mean reward:  3.667 [ 2.000,  6.000], mean action: 37.889 [7.000, 96.000],  loss: 10.138368, mae: 1.909129, mean_q: 4.546108\n",
      "[39 30 50 80 97 85 88 86 82 98]\n",
      "  8064/50001: episode: 896, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 77.333 [30.000, 98.000],  loss: 7.683757, mae: 1.842098, mean_q: 4.353502\n",
      "[42 39 70 67 95 34 80 88 40 28]\n",
      "  8073/50001: episode: 897, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 44.000, mean reward:  4.889 [ 2.000,  7.000], mean action: 60.111 [28.000, 95.000],  loss: 7.392902, mae: 1.804399, mean_q: 4.250938\n",
      "[63  2 51 62 31  6 93 87 39 39]\n",
      "  8082/50001: episode: 898, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 16.000, mean reward:  1.778 [-10.000,  4.000], mean action: 45.556 [2.000, 93.000],  loss: 8.500224, mae: 1.773375, mean_q: 4.190900\n",
      "[62 12 32 20 50 13 42 90 55 96]\n",
      "  8091/50001: episode: 899, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 36.000, mean reward:  4.000 [ 3.000,  5.000], mean action: 45.556 [12.000, 96.000],  loss: 7.741826, mae: 1.784370, mean_q: 4.232012\n",
      "[74 28  4 49 99 41 59 78 27 98]\n",
      "  8100/50001: episode: 900, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 53.667 [4.000, 99.000],  loss: 5.888138, mae: 1.902697, mean_q: 4.452608\n",
      "[43 92 86 34 39 63 79 30 90 95]\n",
      "  8109/50001: episode: 901, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 45.000, mean reward:  5.000 [ 2.000, 11.000], mean action: 67.556 [30.000, 95.000],  loss: 5.545774, mae: 1.915622, mean_q: 4.514436\n",
      "[ 0 82 33 65 44 20 97  4 14 94]\n",
      "  8118/50001: episode: 902, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 50.333 [4.000, 97.000],  loss: 13.024708, mae: 1.911072, mean_q: 4.485501\n",
      "[81 69 64  2 49 34 46 32 60 95]\n",
      "  8127/50001: episode: 903, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 47.000, mean reward:  5.222 [ 3.000, 10.000], mean action: 50.111 [2.000, 95.000],  loss: 9.056527, mae: 1.926134, mean_q: 4.559660\n",
      "[ 7 89 71 96 34 34 99 21 63 62]\n",
      "  8136/50001: episode: 904, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 63.222 [21.000, 99.000],  loss: 8.170747, mae: 1.874502, mean_q: 4.436830\n",
      "[23 83 39  2 23 40 16 28 40 88]\n",
      "  8145/50001: episode: 905, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 11.000, mean reward:  1.222 [-10.000,  8.000], mean action: 39.889 [2.000, 88.000],  loss: 6.257147, mae: 1.813329, mean_q: 4.213795\n",
      "[86 89 98 12 37 85 57 74 78 35]\n",
      "  8154/50001: episode: 906, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 37.000, mean reward:  4.111 [ 3.000,  5.000], mean action: 62.778 [12.000, 98.000],  loss: 7.980000, mae: 1.871504, mean_q: 4.464254\n",
      "[ 8 79 59 77 92 24 23 32 34 37]\n",
      "  8163/50001: episode: 907, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 44.000, mean reward:  4.889 [ 3.000,  8.000], mean action: 50.778 [23.000, 92.000],  loss: 10.137177, mae: 1.888408, mean_q: 4.477181\n",
      "[50 31 86 99 92 57 10 92 28 28]\n",
      "  8172/50001: episode: 908, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward:  7.000, mean reward:  0.778 [-10.000,  6.000], mean action: 58.111 [10.000, 99.000],  loss: 8.042485, mae: 1.889690, mean_q: 4.448778\n",
      "[60 48 13 91 62 54 92 52 95 12]\n",
      "  8181/50001: episode: 909, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 41.000, mean reward:  4.556 [ 3.000,  9.000], mean action: 57.667 [12.000, 95.000],  loss: 9.228088, mae: 1.851001, mean_q: 4.358659\n",
      "[87 96 50 16 52 48 34  1 20 50]\n",
      "  8190/50001: episode: 910, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 40.778 [1.000, 96.000],  loss: 7.260808, mae: 1.843525, mean_q: 4.380395\n",
      "[60 28 53 15 40 21 89 76  9 76]\n",
      "  8199/50001: episode: 911, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 21.000, mean reward:  2.333 [-10.000,  6.000], mean action: 45.222 [9.000, 89.000],  loss: 7.230341, mae: 1.855353, mean_q: 4.294703\n",
      "[75 28 60  4 24 63 21 84 85 50]\n",
      "  8208/50001: episode: 912, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 41.000, mean reward:  4.556 [ 3.000, 10.000], mean action: 46.556 [4.000, 85.000],  loss: 5.854024, mae: 1.870596, mean_q: 4.461435\n",
      "[ 2 17 89 33 84 40 28 59 24 38]\n",
      "  8217/50001: episode: 913, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 35.000, mean reward:  3.889 [ 2.000,  6.000], mean action: 45.778 [17.000, 89.000],  loss: 7.611014, mae: 1.846084, mean_q: 4.341158\n",
      "[24 12 71 81 24 76 36 51 52 12]\n",
      "  8226/50001: episode: 914, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward:  4.000, mean reward:  0.444 [-10.000,  5.000], mean action: 46.111 [12.000, 81.000],  loss: 7.347751, mae: 1.885612, mean_q: 4.443340\n",
      "[37 53 51 95 13 97 50  0 91 10]\n",
      "  8235/50001: episode: 915, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 34.000, mean reward:  3.778 [ 1.000,  5.000], mean action: 51.111 [0.000, 97.000],  loss: 8.799569, mae: 1.889640, mean_q: 4.465797\n",
      "[21  2 76 20 43 90  8 52 12 62]\n",
      "  8244/50001: episode: 916, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 40.000, mean reward:  4.444 [ 2.000,  9.000], mean action: 40.556 [2.000, 90.000],  loss: 6.645194, mae: 1.890759, mean_q: 4.421358\n",
      "[80 84 24 90 24 24 20 24 92 79]\n",
      "  8253/50001: episode: 917, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: -6.000, mean reward: -0.667 [-10.000,  6.000], mean action: 51.222 [20.000, 92.000],  loss: 9.632441, mae: 1.983594, mean_q: 4.629394\n",
      "[85 61 16 68 98 11 32 27 46 21]\n",
      "  8262/50001: episode: 918, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 38.000, mean reward:  4.222 [ 2.000,  6.000], mean action: 42.222 [11.000, 98.000],  loss: 9.174138, mae: 1.943617, mean_q: 4.575672\n",
      "[55 61 12 92 29 39 50 74 28  5]\n",
      "  8271/50001: episode: 919, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 43.333 [5.000, 92.000],  loss: 9.351871, mae: 1.883306, mean_q: 4.345722\n",
      "[95 31 29  9  1 23 77  9 50 19]\n",
      "  8280/50001: episode: 920, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 22.000, mean reward:  2.444 [-10.000,  8.000], mean action: 27.556 [1.000, 77.000],  loss: 8.973661, mae: 1.924998, mean_q: 4.562613\n",
      "[43 24 96 33 33  1 58 40 28 12]\n",
      "  8289/50001: episode: 921, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 30.000, mean reward:  3.333 [-10.000,  8.000], mean action: 36.111 [1.000, 96.000],  loss: 7.860739, mae: 1.877646, mean_q: 4.383636\n",
      "[31 91 50 13 76 27 69 99 41 60]\n",
      "  8298/50001: episode: 922, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 58.444 [13.000, 99.000],  loss: 7.124171, mae: 1.911811, mean_q: 4.441978\n",
      "[35 85 79 50 25 14 63 39 27 27]\n",
      "  8307/50001: episode: 923, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 45.444 [14.000, 85.000],  loss: 7.799425, mae: 1.929963, mean_q: 4.541663\n",
      "[82 32 50 88 34 70 46 45 68 18]\n",
      "  8316/50001: episode: 924, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 35.000, mean reward:  3.889 [ 2.000,  5.000], mean action: 50.111 [18.000, 88.000],  loss: 7.371592, mae: 1.958176, mean_q: 4.585381\n",
      "[67 31 77 20 24 54 67 60 12 28]\n",
      "  8325/50001: episode: 925, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 29.000, mean reward:  3.222 [-10.000,  8.000], mean action: 41.444 [12.000, 77.000],  loss: 8.851749, mae: 1.933826, mean_q: 4.561068\n",
      "[ 0 48 90 27 68 90 62 61 41 28]\n",
      "  8334/50001: episode: 926, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 26.000, mean reward:  2.889 [-10.000,  9.000], mean action: 57.222 [27.000, 90.000],  loss: 6.631726, mae: 1.970817, mean_q: 4.557127\n",
      "[73  3 67 11 22 34 39 11 70 21]\n",
      "  8343/50001: episode: 927, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 22.000, mean reward:  2.444 [-10.000,  7.000], mean action: 30.889 [3.000, 70.000],  loss: 10.724964, mae: 1.912579, mean_q: 4.529814\n",
      "[18 53 47 27  2 94 45 88 32 42]\n",
      "  8352/50001: episode: 928, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 47.778 [2.000, 94.000],  loss: 8.648281, mae: 1.875599, mean_q: 4.450012\n",
      "[85 13 38 53  1  6 27 37 81 31]\n",
      "  8361/50001: episode: 929, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 31.889 [1.000, 81.000],  loss: 9.135515, mae: 1.920742, mean_q: 4.508309\n",
      "[94 83 64 25 58 54 91 18 82 69]\n",
      "  8370/50001: episode: 930, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 37.000, mean reward:  4.111 [ 3.000,  6.000], mean action: 60.444 [18.000, 91.000],  loss: 8.694662, mae: 1.982752, mean_q: 4.695388\n",
      "[74 51 28 53 77 99 99 28 95  5]\n",
      "  8379/50001: episode: 931, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 11.000, mean reward:  1.222 [-10.000,  9.000], mean action: 59.444 [5.000, 99.000],  loss: 8.551110, mae: 1.949283, mean_q: 4.484365\n",
      "[28 62 12  1 32 90 34 34 42  3]\n",
      "  8388/50001: episode: 932, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 34.444 [1.000, 90.000],  loss: 8.398533, mae: 1.883306, mean_q: 4.523285\n",
      "[42 41 12 12 18 56 75 59 50 79]\n",
      "  8397/50001: episode: 933, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 44.667 [12.000, 79.000],  loss: 8.179864, mae: 1.871013, mean_q: 4.510065\n",
      "[28 31 67 36 14 44 88  9 96 88]\n",
      "  8406/50001: episode: 934, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 21.000, mean reward:  2.333 [-10.000,  6.000], mean action: 52.556 [9.000, 96.000],  loss: 5.995302, mae: 1.916189, mean_q: 4.507370\n",
      "[58  4  6 67 37 62 51  4 95 85]\n",
      "  8415/50001: episode: 935, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 45.667 [4.000, 95.000],  loss: 7.101576, mae: 1.881076, mean_q: 4.538981\n",
      "[36 79 63 20 84 60 50 68 82 93]\n",
      "  8424/50001: episode: 936, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 66.556 [20.000, 93.000],  loss: 8.395695, mae: 1.916955, mean_q: 4.571545\n",
      "[ 4 40 78 63 63 48 95 28  2  4]\n",
      "  8433/50001: episode: 937, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 13.000, mean reward:  1.444 [-10.000,  7.000], mean action: 46.778 [2.000, 95.000],  loss: 5.110449, mae: 1.924852, mean_q: 4.484034\n",
      "[73 21 64 88 77 57 46  0 51 21]\n",
      "  8442/50001: episode: 938, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 47.222 [0.000, 88.000],  loss: 7.826968, mae: 1.915186, mean_q: 4.537977\n",
      "[42 97 85 55 92 56 84 53 12  5]\n",
      "  8451/50001: episode: 939, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 59.889 [5.000, 97.000],  loss: 8.833118, mae: 1.941619, mean_q: 4.496266\n",
      "[40 15 18 29 62 41 51  8 30 34]\n",
      "  8460/50001: episode: 940, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 43.000, mean reward:  4.778 [ 2.000, 11.000], mean action: 32.000 [8.000, 62.000],  loss: 8.756536, mae: 1.889162, mean_q: 4.432578\n",
      "[99 62 90 27  1 54 28 63 34 52]\n",
      "  8469/50001: episode: 941, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 45.667 [1.000, 90.000],  loss: 7.060489, mae: 1.900525, mean_q: 4.559060\n",
      "[34 99 90 16 20 69  9 34 68 50]\n",
      "  8478/50001: episode: 942, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 21.000, mean reward:  2.333 [-10.000,  8.000], mean action: 50.556 [9.000, 99.000],  loss: 7.178623, mae: 1.961768, mean_q: 4.540833\n",
      "[83  4 88 50 37 12 52 40 95 39]\n",
      "  8487/50001: episode: 943, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 46.333 [4.000, 95.000],  loss: 6.113410, mae: 1.968311, mean_q: 4.642589\n",
      "[ 3  4 97 50 57 34 28 23 85 24]\n",
      "  8496/50001: episode: 944, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 40.000, mean reward:  4.444 [ 3.000,  6.000], mean action: 44.667 [4.000, 97.000],  loss: 4.697902, mae: 1.924145, mean_q: 4.613985\n",
      "[92 27 98 69  1 37 51 95 66 81]\n",
      "  8505/50001: episode: 945, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 58.333 [1.000, 98.000],  loss: 8.352729, mae: 1.963898, mean_q: 4.616280\n",
      "[18 82 88 50 71 90 88 30 14 14]\n",
      "  8514/50001: episode: 946, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 11.000, mean reward:  1.222 [-10.000,  6.000], mean action: 58.556 [14.000, 90.000],  loss: 8.362929, mae: 1.961388, mean_q: 4.713246\n",
      "[10 50 58 50 86 24 90 23 88 79]\n",
      "  8523/50001: episode: 947, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 28.000, mean reward:  3.111 [-10.000,  7.000], mean action: 60.889 [23.000, 90.000],  loss: 7.563650, mae: 1.892603, mean_q: 4.555799\n",
      "[45 27 83 83 48 52  1 62 12 10]\n",
      "  8532/50001: episode: 948, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 27.000, mean reward:  3.000 [-10.000,  8.000], mean action: 42.000 [1.000, 83.000],  loss: 9.917965, mae: 1.938299, mean_q: 4.495788\n",
      "[68 28 24 63 33 57 89 51 76 12]\n",
      "  8541/50001: episode: 949, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 40.000, mean reward:  4.444 [ 3.000,  9.000], mean action: 48.111 [12.000, 89.000],  loss: 8.463184, mae: 1.916732, mean_q: 4.553424\n",
      "[59 53 98 99 99 85 38 13 28 23]\n",
      "  8550/50001: episode: 950, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 59.556 [13.000, 99.000],  loss: 6.210876, mae: 1.904324, mean_q: 4.507379\n",
      "[59  4 38 48 80 52 80 14  9  2]\n",
      "  8559/50001: episode: 951, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 36.333 [2.000, 80.000],  loss: 7.913888, mae: 1.971889, mean_q: 4.626402\n",
      "[12  4 51 83 20 58 89 50 28  4]\n",
      "  8568/50001: episode: 952, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 43.000 [4.000, 89.000],  loss: 9.846751, mae: 1.865991, mean_q: 4.455504\n",
      "[ 9 53 90 95  1 37 16  9 37 22]\n",
      "  8577/50001: episode: 953, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward:  9.000, mean reward:  1.000 [-10.000,  5.000], mean action: 40.000 [1.000, 95.000],  loss: 10.453420, mae: 1.946470, mean_q: 4.598467\n",
      "[ 1 52 55 96 63 76 38 57 28 21]\n",
      "  8586/50001: episode: 954, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 35.000, mean reward:  3.889 [ 2.000,  7.000], mean action: 54.000 [21.000, 96.000],  loss: 9.870812, mae: 1.880176, mean_q: 4.457018\n",
      "[68 40  8  1 12 97 95 92 20 62]\n",
      "  8595/50001: episode: 955, duration: 0.074s, episode steps:   9, steps per second: 121, episode reward: 37.000, mean reward:  4.111 [ 3.000,  7.000], mean action: 47.444 [1.000, 97.000],  loss: 10.466743, mae: 1.809773, mean_q: 4.336254\n",
      "[11 52 47 68 84 56 84 31  4 28]\n",
      "  8604/50001: episode: 956, duration: 0.077s, episode steps:   9, steps per second: 117, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 50.444 [4.000, 84.000],  loss: 7.289041, mae: 1.913329, mean_q: 4.512123\n",
      "[34  7 43 83  6 24 79 30 83 24]\n",
      "  8613/50001: episode: 957, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward:  6.000, mean reward:  0.667 [-10.000,  6.000], mean action: 42.111 [6.000, 83.000],  loss: 8.899459, mae: 1.838632, mean_q: 4.336979\n",
      "[98 60 50 23 52 86 46 89  7 27]\n",
      "  8622/50001: episode: 958, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 36.000, mean reward:  4.000 [ 2.000,  7.000], mean action: 48.889 [7.000, 89.000],  loss: 8.802402, mae: 1.893861, mean_q: 4.498476\n",
      "[20 74 64 13 33 94  4 88  8 14]\n",
      "  8631/50001: episode: 959, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 37.000, mean reward:  4.111 [ 3.000,  6.000], mean action: 43.556 [4.000, 94.000],  loss: 9.913927, mae: 1.851080, mean_q: 4.395890\n",
      "[18 46 74 24 69 49 14 62 13 52]\n",
      "  8640/50001: episode: 960, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 40.000, mean reward:  4.444 [ 3.000,  8.000], mean action: 44.778 [13.000, 74.000],  loss: 6.802642, mae: 1.844298, mean_q: 4.358427\n",
      "[37  5 71 10  2 27 97 14 56  1]\n",
      "  8649/50001: episode: 961, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 31.444 [1.000, 97.000],  loss: 7.481404, mae: 1.869847, mean_q: 4.379913\n",
      "[78 31 52 97 99  4 32  3 12 83]\n",
      "  8658/50001: episode: 962, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 45.889 [3.000, 99.000],  loss: 9.068807, mae: 1.851251, mean_q: 4.379936\n",
      "[95 97 51 68 99  1 15 27 40 21]\n",
      "  8667/50001: episode: 963, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 34.000, mean reward:  3.778 [ 2.000,  5.000], mean action: 46.556 [1.000, 99.000],  loss: 5.274427, mae: 1.855379, mean_q: 4.354002\n",
      "[90 14 92 44 68 60 42 17 97 65]\n",
      "  8676/50001: episode: 964, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 34.000, mean reward:  3.778 [ 2.000,  7.000], mean action: 55.444 [14.000, 97.000],  loss: 8.586697, mae: 1.813898, mean_q: 4.340088\n",
      "[33 28 76 68 99 31 28 76 28 48]\n",
      "  8685/50001: episode: 965, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: -4.000, mean reward: -0.444 [-10.000,  7.000], mean action: 53.556 [28.000, 99.000],  loss: 9.684652, mae: 1.871502, mean_q: 4.446609\n",
      "[14 62 55 83 77 48 79 31 56 77]\n",
      "  8694/50001: episode: 966, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 63.111 [31.000, 83.000],  loss: 9.176998, mae: 1.819617, mean_q: 4.345869\n",
      "[14 31 98 23 75 55 42 97  4 23]\n",
      "  8703/50001: episode: 967, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 49.778 [4.000, 98.000],  loss: 8.676191, mae: 1.915140, mean_q: 4.508859\n",
      "[85 21 92 85 36 95 34 96 28 66]\n",
      "  8712/50001: episode: 968, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 30.000, mean reward:  3.333 [-10.000,  8.000], mean action: 61.444 [21.000, 96.000],  loss: 7.338268, mae: 1.847456, mean_q: 4.404542\n",
      "[27 41 79 53 27 94 42 99 14  1]\n",
      "  8721/50001: episode: 969, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 26.000, mean reward:  2.889 [-10.000,  8.000], mean action: 50.000 [1.000, 99.000],  loss: 8.514718, mae: 1.852385, mean_q: 4.374830\n",
      "[95 48 92 28 59 99 34 11  1 89]\n",
      "  8730/50001: episode: 970, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 35.000, mean reward:  3.889 [ 3.000,  6.000], mean action: 51.222 [1.000, 99.000],  loss: 8.365226, mae: 1.789070, mean_q: 4.286803\n",
      "[56 92 22 76 24 62 48 61 17 44]\n",
      "  8739/50001: episode: 971, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 34.000, mean reward:  3.778 [ 3.000,  6.000], mean action: 49.556 [17.000, 92.000],  loss: 8.781937, mae: 1.895879, mean_q: 4.392210\n",
      "[72 14 90 44 62  9 42 59 28 14]\n",
      "  8748/50001: episode: 972, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 40.222 [9.000, 90.000],  loss: 9.283652, mae: 1.912130, mean_q: 4.480295\n",
      "[49 52  6 56 24 80 45 91 89 12]\n",
      "  8757/50001: episode: 973, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 41.000, mean reward:  4.556 [ 3.000, 10.000], mean action: 50.556 [6.000, 91.000],  loss: 10.376451, mae: 1.946409, mean_q: 4.490351\n",
      "[ 6 28 34 50 92  4 65  1 28 13]\n",
      "  8766/50001: episode: 974, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 27.000, mean reward:  3.000 [-10.000,  8.000], mean action: 35.000 [1.000, 92.000],  loss: 6.790916, mae: 1.964076, mean_q: 4.564905\n",
      "[80 62 42 42 37 25 34 63 97 23]\n",
      "  8775/50001: episode: 975, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 47.222 [23.000, 97.000],  loss: 7.872311, mae: 1.860634, mean_q: 4.349339\n",
      "[86 69 77 53 86 32 43 76 31 48]\n",
      "  8784/50001: episode: 976, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 27.000, mean reward:  3.000 [-10.000,  9.000], mean action: 57.222 [31.000, 86.000],  loss: 9.394253, mae: 1.897498, mean_q: 4.386003\n",
      "[38  4 61 20  1  4 46 21 55 22]\n",
      "  8793/50001: episode: 977, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 21.000, mean reward:  2.333 [-10.000,  6.000], mean action: 26.000 [1.000, 61.000],  loss: 9.201382, mae: 1.906809, mean_q: 4.374681\n",
      "[62 95 68 72 24 27 14 88 94 62]\n",
      "  8802/50001: episode: 978, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 60.444 [14.000, 95.000],  loss: 8.627902, mae: 1.923746, mean_q: 4.538046\n",
      "[ 4 98 48 64 83 80 46 80 82 90]\n",
      "  8811/50001: episode: 979, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 74.556 [46.000, 98.000],  loss: 7.906020, mae: 1.864681, mean_q: 4.412699\n",
      "[29 23 59 92  9 60 79 38 40 24]\n",
      "  8820/50001: episode: 980, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 47.111 [9.000, 92.000],  loss: 7.810988, mae: 1.925229, mean_q: 4.467187\n",
      "[ 1 25 36 62 97  6  1 60 23 79]\n",
      "  8829/50001: episode: 981, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 43.222 [1.000, 97.000],  loss: 9.296530, mae: 1.917323, mean_q: 4.402795\n",
      "[62  6 71 39 15  6  9 27 39 44]\n",
      "  8838/50001: episode: 982, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward:  7.000, mean reward:  0.778 [-10.000,  6.000], mean action: 28.444 [6.000, 71.000],  loss: 8.752205, mae: 1.941815, mean_q: 4.590766\n",
      "[17 52 87 19 13 51 78 24 38 79]\n",
      "  8847/50001: episode: 983, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 35.000, mean reward:  3.889 [ 2.000,  7.000], mean action: 49.000 [13.000, 87.000],  loss: 6.795571, mae: 1.885473, mean_q: 4.402274\n",
      "[55 41 10 44 37 77 90 68 28 28]\n",
      "  8856/50001: episode: 984, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 47.000 [10.000, 90.000],  loss: 8.227306, mae: 1.936599, mean_q: 4.543116\n",
      "[52 30 20 42 23 88 74  0 89 23]\n",
      "  8865/50001: episode: 985, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 20.000, mean reward:  2.222 [-10.000,  5.000], mean action: 43.222 [0.000, 89.000],  loss: 9.551833, mae: 1.969826, mean_q: 4.606596\n",
      "[98 88 23 23 97 13 49 52 74 13]\n",
      "  8874/50001: episode: 986, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward:  8.000, mean reward:  0.889 [-10.000,  5.000], mean action: 48.000 [13.000, 97.000],  loss: 8.226223, mae: 1.926651, mean_q: 4.467128\n",
      "[29 51 69 12 31 47 57  8 48 20]\n",
      "  8883/50001: episode: 987, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 35.000, mean reward:  3.889 [ 3.000,  6.000], mean action: 38.111 [8.000, 69.000],  loss: 8.957760, mae: 1.893657, mean_q: 4.432170\n",
      "[37 28 18 65 69 85 11 84 68 28]\n",
      "  8892/50001: episode: 988, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 17.000, mean reward:  1.889 [-10.000,  4.000], mean action: 50.667 [11.000, 85.000],  loss: 8.757421, mae: 1.880610, mean_q: 4.356711\n",
      "[56 83 39 39  1  2 71 99 63 21]\n",
      "  8901/50001: episode: 989, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 21.000, mean reward:  2.333 [-10.000,  5.000], mean action: 46.444 [1.000, 99.000],  loss: 8.009571, mae: 1.903270, mean_q: 4.412218\n",
      "[20 14  7 73 74  1  6 75 96 93]\n",
      "  8910/50001: episode: 990, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 36.000, mean reward:  4.000 [ 2.000,  7.000], mean action: 48.778 [1.000, 96.000],  loss: 6.611277, mae: 1.803905, mean_q: 4.215528\n",
      "[76 41 23 33 27 52 30 64 14 21]\n",
      "  8919/50001: episode: 991, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 37.000, mean reward:  4.111 [ 3.000,  6.000], mean action: 33.889 [14.000, 64.000],  loss: 8.375872, mae: 1.895306, mean_q: 4.344084\n",
      "[31 50 84 21 55 53 52 59 42 62]\n",
      "  8928/50001: episode: 992, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 34.000, mean reward:  3.778 [ 3.000,  5.000], mean action: 53.111 [21.000, 84.000],  loss: 7.063755, mae: 1.908983, mean_q: 4.469827\n",
      "[57 24 95 63 44 34 57  4  2 45]\n",
      "  8937/50001: episode: 993, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 40.889 [2.000, 95.000],  loss: 9.561597, mae: 1.928098, mean_q: 4.512327\n",
      "[93 89 98  1 96 13 13 34 56 63]\n",
      "  8946/50001: episode: 994, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 51.444 [1.000, 98.000],  loss: 10.346947, mae: 1.952762, mean_q: 4.542653\n",
      "[87 95 97 24 27 24 88 99 45 94]\n",
      "  8955/50001: episode: 995, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 23.000, mean reward:  2.556 [-10.000,  5.000], mean action: 65.889 [24.000, 99.000],  loss: 10.355220, mae: 1.921831, mean_q: 4.518104\n",
      "[31 23 50 43 28 46 88 88 14  4]\n",
      "  8964/50001: episode: 996, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 26.000, mean reward:  2.889 [-10.000,  6.000], mean action: 42.667 [4.000, 88.000],  loss: 7.589571, mae: 1.854524, mean_q: 4.332668\n",
      "[81 62 79 51 56 90 76 95  3 34]\n",
      "  8973/50001: episode: 997, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 42.000, mean reward:  4.667 [ 2.000, 10.000], mean action: 60.667 [3.000, 95.000],  loss: 6.947992, mae: 1.831994, mean_q: 4.188009\n",
      "[49 32 73 68 28 23 11 20 46  9]\n",
      "  8982/50001: episode: 998, duration: 0.069s, episode steps:   9, steps per second: 131, episode reward: 38.000, mean reward:  4.222 [ 2.000,  6.000], mean action: 34.444 [9.000, 73.000],  loss: 7.666211, mae: 1.839301, mean_q: 4.274492\n",
      "[56 95 60 58 51 58 10 63 10 76]\n",
      "  8991/50001: episode: 999, duration: 0.069s, episode steps:   9, steps per second: 131, episode reward:  6.000, mean reward:  0.667 [-10.000,  5.000], mean action: 53.444 [10.000, 95.000],  loss: 5.753921, mae: 1.884201, mean_q: 4.361734\n",
      "[ 1 95 47 55 20 97 48 91  8 45]\n",
      "  9000/50001: episode: 1000, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 32.000, mean reward:  3.556 [ 2.000,  5.000], mean action: 56.222 [8.000, 97.000],  loss: 7.492911, mae: 1.909683, mean_q: 4.424069\n",
      "[32 84 79 21 76  9 79 77 44 83]\n",
      "  9009/50001: episode: 1001, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 19.000, mean reward:  2.111 [-10.000,  5.000], mean action: 61.333 [9.000, 84.000],  loss: 9.435316, mae: 1.922958, mean_q: 4.495214\n",
      "[35 97 46 32 34 75 88 62  4 41]\n",
      "  9018/50001: episode: 1002, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 41.000, mean reward:  4.556 [ 3.000,  6.000], mean action: 53.222 [4.000, 97.000],  loss: 6.310305, mae: 1.889645, mean_q: 4.348247\n",
      "[50  3 61 10 22 10 55  6 77 79]\n",
      "  9027/50001: episode: 1003, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 19.000, mean reward:  2.111 [-10.000,  8.000], mean action: 35.889 [3.000, 79.000],  loss: 7.785557, mae: 1.867610, mean_q: 4.305609\n",
      "[72 48 94 60 53 31 71 42  1 31]\n",
      "  9036/50001: episode: 1004, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 47.889 [1.000, 94.000],  loss: 9.965965, mae: 1.884759, mean_q: 4.322586\n",
      "[ 8 82 50 34  9  8 12 94 23 58]\n",
      "  9045/50001: episode: 1005, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 41.111 [8.000, 94.000],  loss: 7.477295, mae: 1.881125, mean_q: 4.410078\n",
      "[98 32 21 92 11 37 56 42 98 12]\n",
      "  9054/50001: episode: 1006, duration: 0.074s, episode steps:   9, steps per second: 122, episode reward: 26.000, mean reward:  2.889 [-10.000,  8.000], mean action: 44.556 [11.000, 98.000],  loss: 7.490989, mae: 1.905112, mean_q: 4.338683\n",
      "[51 52 49 38 37 23 63 38 31 55]\n",
      "  9063/50001: episode: 1007, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 42.889 [23.000, 63.000],  loss: 8.851803, mae: 1.968568, mean_q: 4.572789\n",
      "[90 94 32 77 30 32 34 25 57 81]\n",
      "  9072/50001: episode: 1008, duration: 0.091s, episode steps:   9, steps per second:  99, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 51.333 [25.000, 94.000],  loss: 6.323204, mae: 1.957456, mean_q: 4.524070\n",
      "[91 99 92 40 42 90 51 12 74 55]\n",
      "  9081/50001: episode: 1009, duration: 0.079s, episode steps:   9, steps per second: 114, episode reward: 35.000, mean reward:  3.889 [ 3.000,  6.000], mean action: 61.667 [12.000, 99.000],  loss: 8.035549, mae: 1.964906, mean_q: 4.463206\n",
      "[92 89 48 86 20 37 50  1 68 48]\n",
      "  9090/50001: episode: 1010, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 49.667 [1.000, 89.000],  loss: 6.593180, mae: 2.026639, mean_q: 4.645071\n",
      "[25 11 50 81 68 57 95 28  2 28]\n",
      "  9099/50001: episode: 1011, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 46.667 [2.000, 95.000],  loss: 5.607627, mae: 1.966136, mean_q: 4.577388\n",
      "[50 83 63 90 83 57 76 86 12 28]\n",
      "  9108/50001: episode: 1012, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 27.000, mean reward:  3.000 [-10.000,  8.000], mean action: 64.222 [12.000, 90.000],  loss: 8.046993, mae: 2.054414, mean_q: 4.749825\n",
      "[99  1 75 28  1 28 19 51 83 23]\n",
      "  9117/50001: episode: 1013, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward:  7.000, mean reward:  0.778 [-10.000,  6.000], mean action: 34.333 [1.000, 83.000],  loss: 6.742445, mae: 2.039830, mean_q: 4.694073\n",
      "[99 38 62 23 15 53 23 51 33 64]\n",
      "  9126/50001: episode: 1014, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 20.000, mean reward:  2.222 [-10.000,  5.000], mean action: 40.222 [15.000, 64.000],  loss: 7.332588, mae: 1.974501, mean_q: 4.585217\n",
      "[31 51 90 88 13 24 56 68  2 77]\n",
      "  9135/50001: episode: 1015, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 36.000, mean reward:  4.000 [ 3.000,  5.000], mean action: 52.111 [2.000, 90.000],  loss: 6.588451, mae: 1.991346, mean_q: 4.707495\n",
      "[38 14 61  1 11 57 22 50 13 62]\n",
      "  9144/50001: episode: 1016, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 42.000, mean reward:  4.667 [ 2.000,  7.000], mean action: 32.333 [1.000, 62.000],  loss: 9.506963, mae: 2.012833, mean_q: 4.746747\n",
      "[ 9 82 32 60 50 24 11 10 83 20]\n",
      "  9153/50001: episode: 1017, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 36.000, mean reward:  4.000 [ 3.000,  5.000], mean action: 41.333 [10.000, 83.000],  loss: 8.831520, mae: 1.946206, mean_q: 4.479798\n",
      "[56 79 98 86 37  1 20 46 12  2]\n",
      "  9162/50001: episode: 1018, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 43.000, mean reward:  4.778 [ 3.000,  7.000], mean action: 42.333 [1.000, 98.000],  loss: 6.458334, mae: 1.918682, mean_q: 4.479124\n",
      "[27 79 69 63 61 97  8 57 28 34]\n",
      "  9171/50001: episode: 1019, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 41.000, mean reward:  4.556 [ 2.000,  9.000], mean action: 55.111 [8.000, 97.000],  loss: 12.530405, mae: 1.946679, mean_q: 4.530936\n",
      "[30 52 74 80 50 32 48 34 40 88]\n",
      "  9180/50001: episode: 1020, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 41.000, mean reward:  4.556 [ 2.000,  7.000], mean action: 55.333 [32.000, 88.000],  loss: 7.874433, mae: 1.943542, mean_q: 4.521861\n",
      "[85 96 37 48 98 43 43 85  5  8]\n",
      "  9189/50001: episode: 1021, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward:  8.000, mean reward:  0.889 [-10.000,  6.000], mean action: 51.444 [5.000, 98.000],  loss: 8.000751, mae: 1.904144, mean_q: 4.450971\n",
      "[95 82 34 46 90 34  1 11  1  1]\n",
      "  9198/50001: episode: 1022, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: -6.000, mean reward: -0.667 [-10.000,  5.000], mean action: 33.333 [1.000, 90.000],  loss: 7.730796, mae: 1.919337, mean_q: 4.478453\n",
      "[40 75 20 78 62 38 20 63 94 48]\n",
      "  9207/50001: episode: 1023, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 24.000, mean reward:  2.667 [-10.000,  9.000], mean action: 55.333 [20.000, 94.000],  loss: 7.975095, mae: 1.878786, mean_q: 4.343615\n",
      "[82 28 64  1 97 95 67 74 86 68]\n",
      "  9216/50001: episode: 1024, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 36.000, mean reward:  4.000 [ 3.000,  6.000], mean action: 64.444 [1.000, 97.000],  loss: 8.698080, mae: 1.928675, mean_q: 4.354708\n",
      "[38 51 52 28 28 27 24 92 60 76]\n",
      "  9225/50001: episode: 1025, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 48.667 [24.000, 92.000],  loss: 8.679195, mae: 1.861126, mean_q: 4.361101\n",
      "[73  4 44 94 53 75 53 68 28 26]\n",
      "  9234/50001: episode: 1026, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 21.000, mean reward:  2.333 [-10.000,  7.000], mean action: 49.444 [4.000, 94.000],  loss: 6.368489, mae: 1.899074, mean_q: 4.358079\n",
      "[35 34 77 76 24 92 76 52 31 27]\n",
      "  9243/50001: episode: 1027, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 28.000, mean reward:  3.111 [-10.000,  7.000], mean action: 54.333 [24.000, 92.000],  loss: 7.281200, mae: 1.925840, mean_q: 4.433559\n",
      "[38 28 52  9 72 60 91 86 58 17]\n",
      "  9252/50001: episode: 1028, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 34.000, mean reward:  3.778 [ 2.000,  5.000], mean action: 52.556 [9.000, 91.000],  loss: 5.449821, mae: 1.931784, mean_q: 4.438110\n",
      "[12 84 69 32 36 53 88 13 90 48]\n",
      "  9261/50001: episode: 1029, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 57.000 [13.000, 90.000],  loss: 7.343809, mae: 1.963130, mean_q: 4.500654\n",
      "[93 98 30 50 12 37 98 92 23  8]\n",
      "  9270/50001: episode: 1030, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 24.000, mean reward:  2.667 [-10.000,  5.000], mean action: 49.778 [8.000, 98.000],  loss: 6.009161, mae: 1.981203, mean_q: 4.598849\n",
      "[63 26 50 20 88 76 72 46 64 74]\n",
      "  9279/50001: episode: 1031, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 57.333 [20.000, 88.000],  loss: 8.149387, mae: 1.939062, mean_q: 4.462000\n",
      "[97 31 80 13 34  1 37  4 85 21]\n",
      "  9288/50001: episode: 1032, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 35.000, mean reward:  3.889 [ 2.000,  5.000], mean action: 34.000 [1.000, 85.000],  loss: 8.329856, mae: 1.918981, mean_q: 4.394768\n",
      "[10  5 63 44 14  6 63 99 21 14]\n",
      "  9297/50001: episode: 1033, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward:  4.000, mean reward:  0.444 [-10.000,  5.000], mean action: 36.556 [5.000, 99.000],  loss: 8.457816, mae: 2.000209, mean_q: 4.601657\n",
      "[43 34 98 89 13 99 50 99 42 62]\n",
      "  9306/50001: episode: 1034, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 26.000, mean reward:  2.889 [-10.000,  6.000], mean action: 65.111 [13.000, 99.000],  loss: 9.727153, mae: 1.960075, mean_q: 4.557036\n",
      "[ 5 28  1 12 98 50 68 69 66  5]\n",
      "  9315/50001: episode: 1035, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 44.111 [1.000, 98.000],  loss: 8.499891, mae: 1.985284, mean_q: 4.607302\n",
      "[ 8  9 97 77 11  2 68 58 85  1]\n",
      "  9324/50001: episode: 1036, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 38.000, mean reward:  4.222 [ 3.000,  8.000], mean action: 45.333 [1.000, 97.000],  loss: 8.202041, mae: 1.930067, mean_q: 4.431873\n",
      "[15 95 38 10 45 91 11 84 28 73]\n",
      "  9333/50001: episode: 1037, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 35.000, mean reward:  3.889 [ 2.000,  7.000], mean action: 52.778 [10.000, 95.000],  loss: 7.463106, mae: 1.892836, mean_q: 4.432882\n",
      "[58 53 39 24 12 59 20 51 95  9]\n",
      "  9342/50001: episode: 1038, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 40.000, mean reward:  4.444 [ 2.000,  9.000], mean action: 40.222 [9.000, 95.000],  loss: 7.061455, mae: 1.900181, mean_q: 4.414055\n",
      "[56 91 91 86 68 88 42 45 95 66]\n",
      "  9351/50001: episode: 1039, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 31.000, mean reward:  3.444 [-10.000, 10.000], mean action: 74.667 [42.000, 95.000],  loss: 9.440196, mae: 1.879676, mean_q: 4.376449\n",
      "[74  3  9 28 54 75 34 62 86 79]\n",
      "  9360/50001: episode: 1040, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 47.778 [3.000, 86.000],  loss: 7.997393, mae: 1.907569, mean_q: 4.494012\n",
      "[53 53 16 77 27 30 98 18  4 40]\n",
      "  9369/50001: episode: 1041, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 40.333 [4.000, 98.000],  loss: 10.693663, mae: 1.858981, mean_q: 4.315549\n",
      "[51 47 59  4 25 32 17 30 28 73]\n",
      "  9378/50001: episode: 1042, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 35.000, mean reward:  3.889 [ 2.000,  7.000], mean action: 35.000 [4.000, 73.000],  loss: 9.461433, mae: 1.849671, mean_q: 4.252635\n",
      "[96 23 83 80 11 87 42 17 69 50]\n",
      "  9387/50001: episode: 1043, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 39.000, mean reward:  4.333 [ 3.000, 10.000], mean action: 51.333 [11.000, 87.000],  loss: 9.451893, mae: 1.884883, mean_q: 4.288461\n",
      "[99  0 83 69 95 46 95  1  1  1]\n",
      "  9396/50001: episode: 1044, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: -4.000, mean reward: -0.444 [-10.000,  6.000], mean action: 43.444 [0.000, 95.000],  loss: 9.203571, mae: 1.886045, mean_q: 4.345053\n",
      "[24 52 11 22 16 48 57 83 93 62]\n",
      "  9405/50001: episode: 1045, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 36.000, mean reward:  4.000 [ 3.000,  6.000], mean action: 49.333 [11.000, 93.000],  loss: 8.292434, mae: 1.831637, mean_q: 4.304509\n",
      "[54 82 29 20 12 23 98 50 19 23]\n",
      "  9414/50001: episode: 1046, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 39.556 [12.000, 98.000],  loss: 6.028864, mae: 1.895203, mean_q: 4.433327\n",
      "[68 95 34 25 90 38 74 17 40 87]\n",
      "  9423/50001: episode: 1047, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 35.000, mean reward:  3.889 [ 2.000,  6.000], mean action: 55.556 [17.000, 95.000],  loss: 8.703307, mae: 1.931487, mean_q: 4.463419\n",
      "[16 42 63 34 50 31 88  2 90  9]\n",
      "  9432/50001: episode: 1048, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 41.000, mean reward:  4.556 [ 3.000,  6.000], mean action: 45.444 [2.000, 90.000],  loss: 9.307755, mae: 1.932379, mean_q: 4.457747\n",
      "[66 89 99 40 34 16 37 24 23 53]\n",
      "  9441/50001: episode: 1049, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 46.111 [16.000, 99.000],  loss: 8.715241, mae: 1.907040, mean_q: 4.351794\n",
      "[83 50 80 34 40 88  4 34 52 79]\n",
      "  9450/50001: episode: 1050, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 26.000, mean reward:  2.889 [-10.000,  6.000], mean action: 51.222 [4.000, 88.000],  loss: 8.080853, mae: 1.825681, mean_q: 4.221949\n",
      "[ 6 22 93 34 28 44 34  1 25  8]\n",
      "  9459/50001: episode: 1051, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 32.111 [1.000, 93.000],  loss: 7.388087, mae: 1.861034, mean_q: 4.224413\n",
      "[96 89 63 94 12  1 34 54 40 74]\n",
      "  9468/50001: episode: 1052, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 51.222 [1.000, 94.000],  loss: 8.455638, mae: 1.905568, mean_q: 4.368755\n",
      "[13 57 13 49 41 79 88 51 74  9]\n",
      "  9477/50001: episode: 1053, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 51.222 [9.000, 88.000],  loss: 7.132112, mae: 1.944595, mean_q: 4.368231\n",
      "[74 99 53 52 11 28 79 30 68 88]\n",
      "  9486/50001: episode: 1054, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 39.000, mean reward:  4.333 [ 3.000,  8.000], mean action: 56.444 [11.000, 99.000],  loss: 8.420693, mae: 1.952640, mean_q: 4.501029\n",
      "[32 80 13 32 12 34 20 10 37 63]\n",
      "  9495/50001: episode: 1055, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 33.444 [10.000, 80.000],  loss: 5.882933, mae: 1.960606, mean_q: 4.602389\n",
      "[75 33 79  1 97 27 99 31 41 54]\n",
      "  9504/50001: episode: 1056, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 37.000, mean reward:  4.111 [ 3.000,  5.000], mean action: 51.333 [1.000, 99.000],  loss: 9.599412, mae: 2.010563, mean_q: 4.520144\n",
      "[59 10 50 44 46 23 83 64 23 14]\n",
      "  9513/50001: episode: 1057, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 39.667 [10.000, 83.000],  loss: 10.561992, mae: 2.049254, mean_q: 4.693041\n",
      "[84 52 69 20 20  2 17 57 30 52]\n",
      "  9522/50001: episode: 1058, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward:  8.000, mean reward:  0.889 [-10.000,  6.000], mean action: 35.444 [2.000, 69.000],  loss: 7.535856, mae: 2.013139, mean_q: 4.579377\n",
      "[17 57 32 18 79 54 68 92 16 96]\n",
      "  9531/50001: episode: 1059, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 34.000, mean reward:  3.778 [ 3.000,  5.000], mean action: 56.889 [16.000, 96.000],  loss: 8.342303, mae: 2.055583, mean_q: 4.709864\n",
      "[25 62 74 39 17 61 10 73 14 12]\n",
      "  9540/50001: episode: 1060, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 38.000, mean reward:  4.222 [ 2.000, 10.000], mean action: 40.222 [10.000, 74.000],  loss: 10.527171, mae: 1.914096, mean_q: 4.427631\n",
      "[32 95 48 37 58 92 95 52 69 78]\n",
      "  9549/50001: episode: 1061, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 18.000, mean reward:  2.000 [-10.000,  5.000], mean action: 69.333 [37.000, 95.000],  loss: 5.553009, mae: 1.935821, mean_q: 4.508599\n",
      "[81 52 74 34 28 42 62 50 27 96]\n",
      "  9558/50001: episode: 1062, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 51.667 [27.000, 96.000],  loss: 7.802178, mae: 1.924671, mean_q: 4.429099\n",
      "[62 50 16 45 90 25 21  8 90  9]\n",
      "  9567/50001: episode: 1063, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 20.000, mean reward:  2.222 [-10.000,  6.000], mean action: 39.333 [8.000, 90.000],  loss: 8.451717, mae: 1.972404, mean_q: 4.499034\n",
      "[65 34  9 16  1 13 55 48 82 96]\n",
      "  9576/50001: episode: 1064, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 36.000, mean reward:  4.000 [ 2.000,  6.000], mean action: 39.333 [1.000, 96.000],  loss: 10.571377, mae: 1.934283, mean_q: 4.462746\n",
      "[49 34 94 99 11 76 24 24 80 64]\n",
      "  9585/50001: episode: 1065, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 22.000, mean reward:  2.444 [-10.000,  5.000], mean action: 56.222 [11.000, 99.000],  loss: 7.903239, mae: 1.920584, mean_q: 4.395903\n",
      "[85 27  6 34 33 10 13 88 14 31]\n",
      "  9594/50001: episode: 1066, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 43.000, mean reward:  4.778 [ 3.000,  6.000], mean action: 28.444 [6.000, 88.000],  loss: 7.985392, mae: 1.873462, mean_q: 4.325021\n",
      "[30 41 39 49 34 53 97 68  4 23]\n",
      "  9603/50001: episode: 1067, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 39.000, mean reward:  4.333 [ 2.000,  6.000], mean action: 45.333 [4.000, 97.000],  loss: 7.613327, mae: 1.867986, mean_q: 4.290281\n",
      "[ 5 75 32 74 63 99 34 46 95 90]\n",
      "  9612/50001: episode: 1068, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 44.000, mean reward:  4.889 [ 3.000,  9.000], mean action: 67.556 [32.000, 99.000],  loss: 6.930310, mae: 1.941186, mean_q: 4.416968\n",
      "[70 60 34 79 28 69 33 19 86 50]\n",
      "  9621/50001: episode: 1069, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 39.000, mean reward:  4.333 [ 3.000,  9.000], mean action: 50.889 [19.000, 86.000],  loss: 7.076365, mae: 1.949775, mean_q: 4.427143\n",
      "[19  4 94 38 99 30 74 40 57 17]\n",
      "  9630/50001: episode: 1070, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 36.000, mean reward:  4.000 [ 2.000,  6.000], mean action: 50.333 [4.000, 99.000],  loss: 9.542557, mae: 1.897581, mean_q: 4.338549\n",
      "[97 63 24 83 18 59 79 67 82 48]\n",
      "  9639/50001: episode: 1071, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 41.000, mean reward:  4.556 [ 3.000,  9.000], mean action: 58.111 [18.000, 83.000],  loss: 8.628394, mae: 1.837879, mean_q: 4.164110\n",
      "[28 89 47 67 83 53 24 98 14  5]\n",
      "  9648/50001: episode: 1072, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 53.333 [5.000, 98.000],  loss: 9.270989, mae: 1.879912, mean_q: 4.324915\n",
      "[59 11 47 53 55 60 80 95 95  4]\n",
      "  9657/50001: episode: 1073, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 23.000, mean reward:  2.556 [-10.000,  8.000], mean action: 55.556 [4.000, 95.000],  loss: 9.942096, mae: 1.885785, mean_q: 4.335283\n",
      "[71 69 48 25 75 27 90 54 27 52]\n",
      "  9666/50001: episode: 1074, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 51.889 [25.000, 90.000],  loss: 7.729202, mae: 1.865188, mean_q: 4.268071\n",
      "[80 27 50 53 27 67 20 23 11 34]\n",
      "  9675/50001: episode: 1075, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 26.000, mean reward:  2.889 [-10.000, 10.000], mean action: 34.667 [11.000, 67.000],  loss: 6.328373, mae: 1.871366, mean_q: 4.354361\n",
      "[77 12 46 97 20 32  4 77 14 81]\n",
      "  9684/50001: episode: 1076, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 42.556 [4.000, 97.000],  loss: 7.382527, mae: 1.919042, mean_q: 4.407288\n",
      "[34 41 59 28 66  9 23  8 74 29]\n",
      "  9693/50001: episode: 1077, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 31.000, mean reward:  3.444 [ 2.000,  4.000], mean action: 37.444 [8.000, 74.000],  loss: 7.514585, mae: 1.956568, mean_q: 4.434748\n",
      "[ 7 13  3 12 67 24 79 82 98 24]\n",
      "  9702/50001: episode: 1078, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 44.667 [3.000, 98.000],  loss: 7.662466, mae: 1.942427, mean_q: 4.433854\n",
      "[25 51 79 95 95 32 46 57 86  4]\n",
      "  9711/50001: episode: 1079, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 60.556 [4.000, 95.000],  loss: 5.935504, mae: 1.951670, mean_q: 4.460072\n",
      "[96 12 38 46 95 51 88 51 31 96]\n",
      "  9720/50001: episode: 1080, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 12.000, mean reward:  1.333 [-10.000,  6.000], mean action: 56.444 [12.000, 96.000],  loss: 9.373143, mae: 2.086518, mean_q: 4.752448\n",
      "[84 57 15 49 50 78 48  6 95 50]\n",
      "  9729/50001: episode: 1081, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 26.000, mean reward:  2.889 [-10.000,  9.000], mean action: 49.778 [6.000, 95.000],  loss: 7.416242, mae: 2.003060, mean_q: 4.514543\n",
      "[62 97 79 20 34 33 24 48 58 32]\n",
      "  9738/50001: episode: 1082, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 41.000, mean reward:  4.556 [ 3.000,  6.000], mean action: 47.222 [20.000, 97.000],  loss: 9.649666, mae: 2.066226, mean_q: 4.653607\n",
      "[ 6 10 59 79 92 13  1 46 82 20]\n",
      "  9747/50001: episode: 1083, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 40.000, mean reward:  4.444 [ 3.000,  6.000], mean action: 44.667 [1.000, 92.000],  loss: 6.363717, mae: 1.978643, mean_q: 4.536719\n",
      "[95 86 52 16 13  9 98  9 45 21]\n",
      "  9756/50001: episode: 1084, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 19.000, mean reward:  2.111 [-10.000,  5.000], mean action: 38.778 [9.000, 98.000],  loss: 6.458632, mae: 1.962997, mean_q: 4.529877\n",
      "[43 89 97 77 81 32 57 23  2 97]\n",
      "  9765/50001: episode: 1085, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 61.667 [2.000, 97.000],  loss: 5.603422, mae: 1.990814, mean_q: 4.623900\n",
      "[65 75 46 79 52 14 50 34 89 10]\n",
      "  9774/50001: episode: 1086, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 49.889 [10.000, 89.000],  loss: 8.930121, mae: 1.962375, mean_q: 4.500649\n",
      "[97 83 92  1 24 14 60 34 54 32]\n",
      "  9783/50001: episode: 1087, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 43.778 [1.000, 92.000],  loss: 8.056975, mae: 2.021756, mean_q: 4.622453\n",
      "[79 93 44 69 79 67 34 21  1 27]\n",
      "  9792/50001: episode: 1088, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 48.333 [1.000, 93.000],  loss: 8.015459, mae: 1.966765, mean_q: 4.462658\n",
      "[76 91 22 98 30 30 95 48  2 40]\n",
      "  9801/50001: episode: 1089, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 30.000, mean reward:  3.333 [-10.000,  7.000], mean action: 50.667 [2.000, 98.000],  loss: 8.099582, mae: 2.020495, mean_q: 4.617099\n",
      "[19 59 71 31 99 56  8 52  4 45]\n",
      "  9810/50001: episode: 1090, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 34.000, mean reward:  3.778 [ 2.000,  6.000], mean action: 47.222 [4.000, 99.000],  loss: 7.250434, mae: 1.973254, mean_q: 4.457565\n",
      "[ 2 28 24 99 46 43 34 52 31 52]\n",
      "  9819/50001: episode: 1091, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 45.444 [24.000, 99.000],  loss: 6.896385, mae: 2.012042, mean_q: 4.668768\n",
      "[14 88 63 49 63 65 42 84 82 48]\n",
      "  9828/50001: episode: 1092, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 28.000, mean reward:  3.111 [-10.000,  9.000], mean action: 64.889 [42.000, 88.000],  loss: 8.323757, mae: 1.979040, mean_q: 4.476151\n",
      "[ 0 95 33 58 92 42 51 57 28 84]\n",
      "  9837/50001: episode: 1093, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 60.000 [28.000, 95.000],  loss: 8.678493, mae: 1.941327, mean_q: 4.377967\n",
      "[41 68 20 19 80 91 95 84 74 69]\n",
      "  9846/50001: episode: 1094, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 37.000, mean reward:  4.111 [ 3.000,  8.000], mean action: 66.667 [19.000, 95.000],  loss: 9.651466, mae: 1.949723, mean_q: 4.525117\n",
      "[57 30 50 79 62 48 74 52 10 95]\n",
      "  9855/50001: episode: 1095, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 42.000, mean reward:  4.667 [ 3.000, 10.000], mean action: 55.556 [10.000, 95.000],  loss: 7.415611, mae: 1.942609, mean_q: 4.495244\n",
      "[ 9 95 48 70 59 60 32 50 50 24]\n",
      "  9864/50001: episode: 1096, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 54.222 [24.000, 95.000],  loss: 4.629231, mae: 1.954865, mean_q: 4.511707\n",
      "[79 29 60 46 63 96 95 95 31 82]\n",
      "  9873/50001: episode: 1097, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 66.333 [29.000, 96.000],  loss: 8.006869, mae: 2.017892, mean_q: 4.594221\n",
      "[67 31 12 88 95 12 13 14 28 42]\n",
      "  9882/50001: episode: 1098, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 27.000, mean reward:  3.000 [-10.000,  6.000], mean action: 37.222 [12.000, 95.000],  loss: 9.237376, mae: 1.982279, mean_q: 4.533651\n",
      "[50 72  2 90 10 97  3 95 99 81]\n",
      "  9891/50001: episode: 1099, duration: 0.075s, episode steps:   9, steps per second: 121, episode reward: 39.000, mean reward:  4.333 [ 2.000,  8.000], mean action: 61.000 [2.000, 99.000],  loss: 10.392042, mae: 1.958728, mean_q: 4.455520\n",
      "[81 89 44 28 58 42  4 79 82 18]\n",
      "  9900/50001: episode: 1100, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 39.000, mean reward:  4.333 [ 2.000,  6.000], mean action: 49.333 [4.000, 89.000],  loss: 7.518378, mae: 1.925506, mean_q: 4.461651\n",
      "[70 63 79 60 24 72 46 84 31 12]\n",
      "  9909/50001: episode: 1101, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 43.000, mean reward:  4.778 [ 2.000,  9.000], mean action: 52.333 [12.000, 84.000],  loss: 7.167055, mae: 1.910637, mean_q: 4.338856\n",
      "[71 62 46 83 34 46 51 99 31 14]\n",
      "  9918/50001: episode: 1102, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 28.000, mean reward:  3.111 [-10.000,  6.000], mean action: 51.778 [14.000, 99.000],  loss: 8.290730, mae: 1.970427, mean_q: 4.434908\n",
      "[18 48 59 57 89 77 76 55 41 28]\n",
      "  9927/50001: episode: 1103, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 38.000, mean reward:  4.222 [ 3.000,  9.000], mean action: 58.889 [28.000, 89.000],  loss: 8.103517, mae: 1.998332, mean_q: 4.538199\n",
      "[24  4 52 27 44 23 87 84 21  9]\n",
      "  9936/50001: episode: 1104, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 36.000, mean reward:  4.000 [ 3.000,  7.000], mean action: 39.000 [4.000, 87.000],  loss: 8.897331, mae: 1.967456, mean_q: 4.513961\n",
      "[61 10 95 27 76 13 60  5 93 98]\n",
      "  9945/50001: episode: 1105, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 53.000 [5.000, 98.000],  loss: 7.141668, mae: 1.939335, mean_q: 4.463460\n",
      "[61 98  4 68 94 53 27 41 59 97]\n",
      "  9954/50001: episode: 1106, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 40.000, mean reward:  4.444 [ 3.000,  8.000], mean action: 60.111 [4.000, 98.000],  loss: 9.063583, mae: 1.895767, mean_q: 4.315198\n",
      "[47 88 41 90 28 99 95 59 40 82]\n",
      "  9963/50001: episode: 1107, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 69.111 [28.000, 99.000],  loss: 7.292088, mae: 1.853310, mean_q: 4.270160\n",
      "[17 48 86 42 63 96 41 83 28 64]\n",
      "  9972/50001: episode: 1108, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 40.000, mean reward:  4.444 [ 3.000,  8.000], mean action: 61.222 [28.000, 96.000],  loss: 8.147995, mae: 1.892796, mean_q: 4.314724\n",
      "[ 0 14 90 91 33 15 97 54 31  2]\n",
      "  9981/50001: episode: 1109, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 47.444 [2.000, 97.000],  loss: 8.182948, mae: 1.941111, mean_q: 4.426261\n",
      "[22 89 97 73  1 27 34 63  2 88]\n",
      "  9990/50001: episode: 1110, duration: 0.074s, episode steps:   9, steps per second: 122, episode reward: 42.000, mean reward:  4.667 [ 2.000,  7.000], mean action: 52.667 [1.000, 97.000],  loss: 7.866398, mae: 1.927646, mean_q: 4.430933\n",
      "[35 14  7 96 32 61 61 57 44 31]\n",
      "  9999/50001: episode: 1111, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 24.000, mean reward:  2.667 [-10.000,  8.000], mean action: 44.778 [7.000, 96.000],  loss: 5.648828, mae: 1.895862, mean_q: 4.363492\n",
      "[75 97 52 85  1 11 32 36 99 48]\n",
      " 10008/50001: episode: 1112, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 38.000, mean reward:  4.222 [ 1.000,  8.000], mean action: 51.222 [1.000, 99.000],  loss: 6.695094, mae: 1.926313, mean_q: 4.425611\n",
      "[31 40  0 98 97 79 76 61 84 31]\n",
      " 10017/50001: episode: 1113, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 20.000, mean reward:  2.222 [-10.000,  5.000], mean action: 62.889 [0.000, 98.000],  loss: 10.122668, mae: 1.968670, mean_q: 4.497987\n",
      "[40  4 68  7 93 56 10 41 52  9]\n",
      " 10026/50001: episode: 1114, duration: 0.081s, episode steps:   9, steps per second: 111, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 37.778 [4.000, 93.000],  loss: 8.162712, mae: 1.939990, mean_q: 4.397771\n",
      "[16 26 98 59 29 17 28 86 12 12]\n",
      " 10035/50001: episode: 1115, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward: 23.000, mean reward:  2.556 [-10.000,  8.000], mean action: 40.778 [12.000, 98.000],  loss: 8.717779, mae: 1.924254, mean_q: 4.363225\n",
      "[73 60 92 34 98 24 50 58 41 72]\n",
      " 10044/50001: episode: 1116, duration: 0.069s, episode steps:   9, steps per second: 130, episode reward: 38.000, mean reward:  4.222 [ 2.000,  6.000], mean action: 58.778 [24.000, 98.000],  loss: 10.990398, mae: 1.931473, mean_q: 4.435669\n",
      "[88 97 23 34 90 13 32 40 77 71]\n",
      " 10053/50001: episode: 1117, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 34.000, mean reward:  3.778 [ 2.000,  5.000], mean action: 53.000 [13.000, 97.000],  loss: 6.786559, mae: 1.897727, mean_q: 4.311890\n",
      "[63 41 27 81 58  8 48 67 23 54]\n",
      " 10062/50001: episode: 1118, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 35.000, mean reward:  3.889 [ 3.000,  5.000], mean action: 45.222 [8.000, 81.000],  loss: 7.779626, mae: 1.883733, mean_q: 4.333148\n",
      "[95 69 48  1 63 23 76 90 50 41]\n",
      " 10071/50001: episode: 1119, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 38.000, mean reward:  4.222 [ 3.000,  8.000], mean action: 51.222 [1.000, 90.000],  loss: 8.661997, mae: 1.923908, mean_q: 4.407522\n",
      "[59 42 43 27 37 53 99 45 31 40]\n",
      " 10080/50001: episode: 1120, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 46.333 [27.000, 99.000],  loss: 5.610903, mae: 1.930332, mean_q: 4.363819\n",
      "[50 88 48 36 42  8 79 72 68 79]\n",
      " 10089/50001: episode: 1121, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 18.000, mean reward:  2.000 [-10.000,  6.000], mean action: 57.778 [8.000, 88.000],  loss: 7.296107, mae: 1.963202, mean_q: 4.462471\n",
      "[47 84 72 24 11 24 89 97 82 48]\n",
      " 10098/50001: episode: 1122, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 59.000 [11.000, 97.000],  loss: 8.253002, mae: 2.015838, mean_q: 4.615868\n",
      "[16 34 44 95 68 56 74 35 95 12]\n",
      " 10107/50001: episode: 1123, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 26.000, mean reward:  2.889 [-10.000,  8.000], mean action: 57.000 [12.000, 95.000],  loss: 7.050654, mae: 1.883496, mean_q: 4.345116\n",
      "[56 93 44 58 90 85 51 32 41 20]\n",
      " 10116/50001: episode: 1124, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 38.000, mean reward:  4.222 [ 2.000,  6.000], mean action: 57.111 [20.000, 93.000],  loss: 9.173257, mae: 1.967812, mean_q: 4.463588\n",
      "[72 12 64 81 34 68 73 97 82  7]\n",
      " 10125/50001: episode: 1125, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 33.000, mean reward:  3.667 [ 1.000,  6.000], mean action: 57.556 [7.000, 97.000],  loss: 5.077092, mae: 1.957921, mean_q: 4.540204\n",
      "[12 67 44 90 74 62 38  1 68 88]\n",
      " 10134/50001: episode: 1126, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 39.000, mean reward:  4.333 [ 2.000,  9.000], mean action: 59.111 [1.000, 90.000],  loss: 10.954158, mae: 2.003721, mean_q: 4.497763\n",
      "[40 13 16 84 10 79 33 10 11 84]\n",
      " 10143/50001: episode: 1127, duration: 0.079s, episode steps:   9, steps per second: 114, episode reward:  5.000, mean reward:  0.556 [-10.000,  5.000], mean action: 37.778 [10.000, 84.000],  loss: 8.082269, mae: 1.984580, mean_q: 4.519656\n",
      "[ 8 95 95 23 62 40 24 51 27 40]\n",
      " 10152/50001: episode: 1128, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 10.000, mean reward:  1.111 [-10.000,  6.000], mean action: 50.778 [23.000, 95.000],  loss: 7.449036, mae: 1.957250, mean_q: 4.488108\n",
      "[37 26 28  1 63 52 48 46 41  6]\n",
      " 10161/50001: episode: 1129, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 37.000, mean reward:  4.111 [ 2.000,  5.000], mean action: 34.556 [1.000, 63.000],  loss: 8.469595, mae: 1.971627, mean_q: 4.501017\n",
      "[31 51 23 81 53 48 42 53 93 79]\n",
      " 10170/50001: episode: 1130, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 21.000, mean reward:  2.333 [-10.000,  7.000], mean action: 58.111 [23.000, 93.000],  loss: 7.501378, mae: 1.936634, mean_q: 4.401476\n",
      "[16 53 58 27 99 54 91 96 95 50]\n",
      " 10179/50001: episode: 1131, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 44.000, mean reward:  4.889 [ 3.000, 10.000], mean action: 69.222 [27.000, 99.000],  loss: 7.893815, mae: 1.940059, mean_q: 4.413836\n",
      "[29  5 92 76 76 57 28 45  4 42]\n",
      " 10188/50001: episode: 1132, duration: 0.082s, episode steps:   9, steps per second: 110, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 47.222 [4.000, 92.000],  loss: 6.220785, mae: 1.964187, mean_q: 4.504272\n",
      "[86 51 50 12 44 90 36 34 41 41]\n",
      " 10197/50001: episode: 1133, duration: 0.081s, episode steps:   9, steps per second: 111, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 44.333 [12.000, 90.000],  loss: 7.497366, mae: 1.963857, mean_q: 4.480881\n",
      "[75 82 30 44 28 42 98 12 76 79]\n",
      " 10206/50001: episode: 1134, duration: 0.079s, episode steps:   9, steps per second: 115, episode reward: 43.000, mean reward:  4.778 [ 3.000,  7.000], mean action: 54.556 [12.000, 98.000],  loss: 8.169307, mae: 2.015313, mean_q: 4.563930\n",
      "[45 53 20 51 73 31 53 41 82 50]\n",
      " 10215/50001: episode: 1135, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 28.000, mean reward:  3.111 [-10.000, 10.000], mean action: 50.444 [20.000, 82.000],  loss: 8.406274, mae: 2.058958, mean_q: 4.655689\n",
      "[93 11 58 34 30 27 77 31 31 19]\n",
      " 10224/50001: episode: 1136, duration: 0.081s, episode steps:   9, steps per second: 112, episode reward: 20.000, mean reward:  2.222 [-10.000,  5.000], mean action: 35.333 [11.000, 77.000],  loss: 8.627971, mae: 1.975233, mean_q: 4.447115\n",
      "[ 6 53 85 44 34 57 23  0 21 88]\n",
      " 10233/50001: episode: 1137, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 39.000, mean reward:  4.333 [ 2.000,  9.000], mean action: 45.000 [0.000, 88.000],  loss: 8.342556, mae: 1.961472, mean_q: 4.497165\n",
      "[35 90 50 68 83 79 13 62 14  9]\n",
      " 10242/50001: episode: 1138, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 41.000, mean reward:  4.556 [ 3.000,  6.000], mean action: 52.000 [9.000, 90.000],  loss: 8.607071, mae: 1.948471, mean_q: 4.387952\n",
      "[41 49 94  9 24 13  4 90 35  1]\n",
      " 10251/50001: episode: 1139, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 35.444 [1.000, 94.000],  loss: 6.782515, mae: 1.853870, mean_q: 4.264480\n",
      "[95 41 38 80 34 77 68 84 66 26]\n",
      " 10260/50001: episode: 1140, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 33.000, mean reward:  3.667 [ 2.000,  7.000], mean action: 57.111 [26.000, 84.000],  loss: 6.616171, mae: 1.885403, mean_q: 4.281255\n",
      "[29 52 59 88 68 34 34  1 69 48]\n",
      " 10269/50001: episode: 1141, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 50.333 [1.000, 88.000],  loss: 8.086138, mae: 1.958182, mean_q: 4.569047\n",
      "[79 69 39 97 97 96 62 34  6 99]\n",
      " 10278/50001: episode: 1142, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 66.556 [6.000, 99.000],  loss: 6.978469, mae: 1.978503, mean_q: 4.493745\n",
      "[29 98 45 29 68 74 68 28 89 32]\n",
      " 10287/50001: episode: 1143, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 13.000, mean reward:  1.444 [-10.000,  8.000], mean action: 59.000 [28.000, 98.000],  loss: 7.244017, mae: 2.029123, mean_q: 4.674025\n",
      "[44 27 56 28 43 32 34 84 61  4]\n",
      " 10296/50001: episode: 1144, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 41.000 [4.000, 84.000],  loss: 6.387075, mae: 2.004451, mean_q: 4.578467\n",
      "[22 89 33 81 24  5 20 63 12 60]\n",
      " 10305/50001: episode: 1145, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 43.000 [5.000, 89.000],  loss: 8.836108, mae: 2.057654, mean_q: 4.634086\n",
      "[81 95 97 37 97 31 34 48 40  9]\n",
      " 10314/50001: episode: 1146, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 27.000, mean reward:  3.000 [-10.000,  6.000], mean action: 54.222 [9.000, 97.000],  loss: 5.786982, mae: 1.954808, mean_q: 4.462101\n",
      "[61 41 79 68 76 25  4 68 28 31]\n",
      " 10323/50001: episode: 1147, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 46.667 [4.000, 79.000],  loss: 9.698437, mae: 1.993919, mean_q: 4.537278\n",
      "[87 50 98 70 34  1  4  8 63 28]\n",
      " 10332/50001: episode: 1148, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 41.000, mean reward:  4.556 [ 2.000,  7.000], mean action: 39.556 [1.000, 98.000],  loss: 5.533462, mae: 1.989562, mean_q: 4.499248\n",
      "[ 9 52 52 57 95  2 32 40 27 75]\n",
      " 10341/50001: episode: 1149, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 28.000, mean reward:  3.111 [-10.000,  7.000], mean action: 48.000 [2.000, 95.000],  loss: 8.102437, mae: 1.986375, mean_q: 4.535740\n",
      "[89 95 85 34 95 27 34 28 28 40]\n",
      " 10350/50001: episode: 1150, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: -3.000, mean reward: -0.333 [-10.000,  5.000], mean action: 51.778 [27.000, 95.000],  loss: 8.109967, mae: 2.045938, mean_q: 4.695031\n",
      "[92 63 21 77 34 99  4 95 89 50]\n",
      " 10359/50001: episode: 1151, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 44.000, mean reward:  4.889 [ 3.000,  8.000], mean action: 59.111 [4.000, 99.000],  loss: 6.445295, mae: 2.019173, mean_q: 4.607882\n",
      "[73 51 95 37 34 12 33 29 40  9]\n",
      " 10368/50001: episode: 1152, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 38.000, mean reward:  4.222 [ 2.000,  5.000], mean action: 37.778 [9.000, 95.000],  loss: 4.645083, mae: 2.039022, mean_q: 4.676266\n",
      "[39 95 20 57 81 89 60 23 91 21]\n",
      " 10377/50001: episode: 1153, duration: 0.069s, episode steps:   9, steps per second: 131, episode reward: 37.000, mean reward:  4.111 [ 3.000,  5.000], mean action: 59.667 [20.000, 95.000],  loss: 8.800750, mae: 2.096756, mean_q: 4.797263\n",
      "[73 99 13 37 58 77 34 41 57 16]\n",
      " 10386/50001: episode: 1154, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 48.000 [13.000, 99.000],  loss: 7.951968, mae: 2.070321, mean_q: 4.633780\n",
      "[32 99 98 27 53 37 48 88 82 24]\n",
      " 10395/50001: episode: 1155, duration: 0.075s, episode steps:   9, steps per second: 120, episode reward: 41.000, mean reward:  4.556 [ 3.000,  6.000], mean action: 61.778 [24.000, 99.000],  loss: 8.368114, mae: 2.021943, mean_q: 4.624810\n",
      "[58 89 32 92 53 59 61  2 10 61]\n",
      " 10404/50001: episode: 1156, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 20.000, mean reward:  2.222 [-10.000,  6.000], mean action: 51.000 [2.000, 92.000],  loss: 8.731438, mae: 2.051466, mean_q: 4.653559\n",
      "[14 41 92 24 11 94 88 50 51 95]\n",
      " 10413/50001: episode: 1157, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 45.000, mean reward:  5.000 [ 3.000, 10.000], mean action: 60.667 [11.000, 95.000],  loss: 7.986245, mae: 2.059316, mean_q: 4.686195\n",
      "[75 88 56 54 34 28  2 57 28 58]\n",
      " 10422/50001: episode: 1158, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 45.000 [2.000, 88.000],  loss: 7.062188, mae: 2.017268, mean_q: 4.534228\n",
      "[73 52 90  1 62 70 33 20 99  9]\n",
      " 10431/50001: episode: 1159, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 37.000, mean reward:  4.111 [ 2.000,  6.000], mean action: 48.444 [1.000, 99.000],  loss: 6.051607, mae: 1.969799, mean_q: 4.454492\n",
      "[26 91 47 74 97  9 25 76 91 88]\n",
      " 10440/50001: episode: 1160, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 25.000, mean reward:  2.778 [-10.000,  9.000], mean action: 66.444 [9.000, 97.000],  loss: 7.574902, mae: 2.042087, mean_q: 4.646203\n",
      "[66 13 81 16 12 81 41 95 12 27]\n",
      " 10449/50001: episode: 1161, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 11.000, mean reward:  1.222 [-10.000,  8.000], mean action: 42.000 [12.000, 95.000],  loss: 9.808874, mae: 2.022284, mean_q: 4.630279\n",
      "[90 74 56 17 63 27 24 46 31 69]\n",
      " 10458/50001: episode: 1162, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 45.222 [17.000, 74.000],  loss: 6.694116, mae: 2.007184, mean_q: 4.540421\n",
      "[87 21 18  8 12 48 79 21 50 68]\n",
      " 10467/50001: episode: 1163, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 36.111 [8.000, 79.000],  loss: 8.154021, mae: 1.904681, mean_q: 4.359223\n",
      "[23 53 69 69 11  1 44  2 63 62]\n",
      " 10476/50001: episode: 1164, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 20.000, mean reward:  2.222 [-10.000,  6.000], mean action: 41.556 [1.000, 69.000],  loss: 7.697610, mae: 1.929123, mean_q: 4.399002\n",
      "[25  6 34  4 99 14 89 13 50 93]\n",
      " 10485/50001: episode: 1165, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 43.000, mean reward:  4.778 [ 3.000,  7.000], mean action: 44.667 [4.000, 99.000],  loss: 7.509777, mae: 2.002854, mean_q: 4.472874\n",
      "[89 14 37  6 91 75 51 95 97 50]\n",
      " 10494/50001: episode: 1166, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 44.000, mean reward:  4.889 [ 3.000,  8.000], mean action: 57.333 [6.000, 97.000],  loss: 6.071206, mae: 1.995696, mean_q: 4.476722\n",
      "[89 41 51 97 20 62 13 88 66  2]\n",
      " 10503/50001: episode: 1167, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 42.000, mean reward:  4.667 [ 3.000,  6.000], mean action: 48.889 [2.000, 97.000],  loss: 10.063034, mae: 1.979291, mean_q: 4.458889\n",
      "[37 84 88 13 42 88 32 11 34 11]\n",
      " 10512/50001: episode: 1168, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward:  9.000, mean reward:  1.000 [-10.000,  8.000], mean action: 44.778 [11.000, 88.000],  loss: 7.070408, mae: 1.951261, mean_q: 4.459535\n",
      "[ 0 30 19 53 77 60 89 36 27 83]\n",
      " 10521/50001: episode: 1169, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 36.000, mean reward:  4.000 [ 2.000,  7.000], mean action: 52.667 [19.000, 89.000],  loss: 7.311097, mae: 2.026687, mean_q: 4.565171\n",
      "[39  4 44 28 77 34 16 80 28 49]\n",
      " 10530/50001: episode: 1170, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 20.000, mean reward:  2.222 [-10.000,  6.000], mean action: 40.000 [4.000, 80.000],  loss: 9.630627, mae: 2.019683, mean_q: 4.599298\n",
      "[58 41 52 97 61 10 48 37 67 62]\n",
      " 10539/50001: episode: 1171, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 52.778 [10.000, 97.000],  loss: 5.738377, mae: 2.074868, mean_q: 4.665573\n",
      "[48 98 64 63 76 88 67 63 28 51]\n",
      " 10548/50001: episode: 1172, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 66.444 [28.000, 98.000],  loss: 8.754940, mae: 2.096034, mean_q: 4.745303\n",
      "[53 62 40 34 23 28  1 99 15 55]\n",
      " 10557/50001: episode: 1173, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 34.000, mean reward:  3.778 [ 2.000,  5.000], mean action: 39.667 [1.000, 99.000],  loss: 8.737081, mae: 2.079556, mean_q: 4.609361\n",
      "[47 51 24 65 95  8 88 41 62 41]\n",
      " 10566/50001: episode: 1174, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 52.778 [8.000, 95.000],  loss: 5.360180, mae: 2.062455, mean_q: 4.637604\n",
      "[58 48 76  2 24 34 34 37 40 48]\n",
      " 10575/50001: episode: 1175, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 10.000, mean reward:  1.111 [-10.000,  6.000], mean action: 38.111 [2.000, 76.000],  loss: 6.517367, mae: 2.112521, mean_q: 4.757563\n",
      "[36 62 97 44 94 42  1 57 28 82]\n",
      " 10584/50001: episode: 1176, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 56.333 [1.000, 97.000],  loss: 6.748812, mae: 2.068449, mean_q: 4.655692\n",
      "[ 9  2 93 90 34 24 62 57 67 66]\n",
      " 10593/50001: episode: 1177, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 55.000 [2.000, 93.000],  loss: 8.796304, mae: 2.039167, mean_q: 4.575388\n",
      "[ 8 67 58 43 33 24 50 89 23 23]\n",
      " 10602/50001: episode: 1178, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 45.556 [23.000, 89.000],  loss: 5.673100, mae: 2.008362, mean_q: 4.535299\n",
      "[18 89 90 42 11 85 45 79 10  4]\n",
      " 10611/50001: episode: 1179, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 50.556 [4.000, 90.000],  loss: 8.249818, mae: 1.997706, mean_q: 4.507069\n",
      "[92 89 57 13 34 34  2 52 94 50]\n",
      " 10620/50001: episode: 1180, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 26.000, mean reward:  2.889 [-10.000,  8.000], mean action: 47.222 [2.000, 94.000],  loss: 9.476630, mae: 1.984006, mean_q: 4.412830\n",
      "[12 20 40 10 45 49 83 95 41 41]\n",
      " 10629/50001: episode: 1181, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 23.000, mean reward:  2.556 [-10.000,  9.000], mean action: 47.111 [10.000, 95.000],  loss: 6.910684, mae: 2.022671, mean_q: 4.531842\n",
      "[ 4 28 16 44 56 46 51 10 52 23]\n",
      " 10638/50001: episode: 1182, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 36.222 [10.000, 56.000],  loss: 8.500658, mae: 2.049555, mean_q: 4.623825\n",
      "[ 5 28 71 59  9 92 34 78 12 97]\n",
      " 10647/50001: episode: 1183, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 42.000, mean reward:  4.667 [ 2.000,  8.000], mean action: 53.333 [9.000, 97.000],  loss: 7.085711, mae: 2.006679, mean_q: 4.534188\n",
      "[82  2 34 52  1 37 95 82 50 42]\n",
      " 10656/50001: episode: 1184, duration: 0.069s, episode steps:   9, steps per second: 131, episode reward: 29.000, mean reward:  3.222 [-10.000,  6.000], mean action: 43.889 [1.000, 95.000],  loss: 8.321256, mae: 2.019790, mean_q: 4.579846\n",
      "[26  4 87 16 78 60 39 88 40  3]\n",
      " 10665/50001: episode: 1185, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 37.000, mean reward:  4.111 [ 3.000,  7.000], mean action: 46.111 [3.000, 88.000],  loss: 7.429608, mae: 2.031060, mean_q: 4.555116\n",
      "[ 7 46 47 78 40 62 21 20 42 13]\n",
      " 10674/50001: episode: 1186, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 43.000, mean reward:  4.778 [ 3.000, 10.000], mean action: 41.000 [13.000, 78.000],  loss: 5.610155, mae: 1.994389, mean_q: 4.523542\n",
      "[68 82 87  4 58 12 34 60 53 21]\n",
      " 10683/50001: episode: 1187, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 36.000, mean reward:  4.000 [ 3.000,  6.000], mean action: 45.667 [4.000, 87.000],  loss: 6.488373, mae: 2.016144, mean_q: 4.552503\n",
      "[ 4 23 64 14 98 84 59 67 96 48]\n",
      " 10692/50001: episode: 1188, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 37.000, mean reward:  4.111 [ 3.000,  8.000], mean action: 61.444 [14.000, 98.000],  loss: 5.054824, mae: 2.055500, mean_q: 4.631212\n",
      "[39 95 25 13 86 50 27 82 10 91]\n",
      " 10701/50001: episode: 1189, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 40.000, mean reward:  4.444 [ 3.000,  6.000], mean action: 53.222 [10.000, 95.000],  loss: 9.716400, mae: 2.059292, mean_q: 4.614727\n",
      "[44 95 78 21 49  1  2 24 31 88]\n",
      " 10710/50001: episode: 1190, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 45.000, mean reward:  5.000 [ 3.000,  8.000], mean action: 43.222 [1.000, 95.000],  loss: 6.673406, mae: 2.058414, mean_q: 4.675901\n",
      "[12 76 90 95 55 98 67  4 58  4]\n",
      " 10719/50001: episode: 1191, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 22.000, mean reward:  2.444 [-10.000,  5.000], mean action: 60.778 [4.000, 98.000],  loss: 7.879819, mae: 2.029382, mean_q: 4.586622\n",
      "[43 34 60 55 84 60 53 41 96 90]\n",
      " 10728/50001: episode: 1192, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 63.667 [34.000, 96.000],  loss: 6.334428, mae: 2.018256, mean_q: 4.602628\n",
      "[27 75 28 44  9 34 79  2 23 12]\n",
      " 10737/50001: episode: 1193, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 34.000 [2.000, 79.000],  loss: 6.642998, mae: 2.066644, mean_q: 4.729120\n",
      "[46 76 63 77 37 96 34 66 88 12]\n",
      " 10746/50001: episode: 1194, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 46.000, mean reward:  5.111 [ 3.000,  8.000], mean action: 61.000 [12.000, 96.000],  loss: 7.113051, mae: 2.069873, mean_q: 4.653226\n",
      "[67 83 38 13 67 89 34 17  2 78]\n",
      " 10755/50001: episode: 1195, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 46.778 [2.000, 89.000],  loss: 10.880260, mae: 2.111723, mean_q: 4.716928\n",
      "[22 34 44 25 77 88 95 95 28 93]\n",
      " 10764/50001: episode: 1196, duration: 0.071s, episode steps:   9, steps per second: 126, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 64.333 [25.000, 95.000],  loss: 8.833291, mae: 2.038079, mean_q: 4.574385\n",
      "[32 38 59  2 14 44 69 68 40 28]\n",
      " 10773/50001: episode: 1197, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 37.000, mean reward:  4.111 [ 3.000,  7.000], mean action: 40.222 [2.000, 69.000],  loss: 8.218038, mae: 2.033633, mean_q: 4.604924\n",
      "[20 51 65 58 95 34  1 56 82 50]\n",
      " 10782/50001: episode: 1198, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 43.000, mean reward:  4.778 [ 2.000,  8.000], mean action: 54.667 [1.000, 95.000],  loss: 7.268145, mae: 1.932886, mean_q: 4.427079\n",
      "[63 31 37 14  8 13 46 42 44 24]\n",
      " 10791/50001: episode: 1199, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 28.778 [8.000, 46.000],  loss: 8.716035, mae: 1.926371, mean_q: 4.376320\n",
      "[71 27 90 84 34 49 50  2 62 82]\n",
      " 10800/50001: episode: 1200, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 53.333 [2.000, 90.000],  loss: 9.453780, mae: 1.984329, mean_q: 4.505919\n",
      "[80 62 28 20 23 90 84 11 76 50]\n",
      " 10809/50001: episode: 1201, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 39.000, mean reward:  4.333 [ 3.000,  9.000], mean action: 49.333 [11.000, 90.000],  loss: 7.870602, mae: 1.900857, mean_q: 4.365399\n",
      "[32 93 39 78 54 92 96 30 95 20]\n",
      " 10818/50001: episode: 1202, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 39.000, mean reward:  4.333 [ 2.000, 10.000], mean action: 66.333 [20.000, 96.000],  loss: 5.212883, mae: 1.903816, mean_q: 4.389306\n",
      "[65 34 92 97 34 27 55  1 34 94]\n",
      " 10827/50001: episode: 1203, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward:  9.000, mean reward:  1.000 [-10.000,  6.000], mean action: 52.000 [1.000, 97.000],  loss: 8.453320, mae: 2.028635, mean_q: 4.629629\n",
      "[42  5 48 86 80 46 50 59  2  4]\n",
      " 10836/50001: episode: 1204, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 42.222 [2.000, 86.000],  loss: 9.112604, mae: 1.980807, mean_q: 4.472018\n",
      "[92 68 48 90 64 49 83 99  4  9]\n",
      " 10845/50001: episode: 1205, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 36.000, mean reward:  4.000 [ 3.000,  6.000], mean action: 57.111 [4.000, 99.000],  loss: 7.152094, mae: 2.020132, mean_q: 4.514097\n",
      "[93 13 64 47 12 16 94 13 55 93]\n",
      " 10854/50001: episode: 1206, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward:  4.000, mean reward:  0.444 [-10.000,  5.000], mean action: 45.222 [12.000, 94.000],  loss: 7.201231, mae: 1.911693, mean_q: 4.309616\n",
      "[98 34  4 48 27 79 95 50 50 50]\n",
      " 10863/50001: episode: 1207, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 12.000, mean reward:  1.333 [-10.000,  6.000], mean action: 48.556 [4.000, 95.000],  loss: 8.016084, mae: 2.061637, mean_q: 4.736585\n",
      "[89 95 79 67 46 14 95 40 40 21]\n",
      " 10872/50001: episode: 1208, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward:  9.000, mean reward:  1.000 [-10.000,  5.000], mean action: 55.222 [14.000, 95.000],  loss: 8.059895, mae: 2.053134, mean_q: 4.561266\n",
      "[ 5 53 90 34 43 53 88 68 90 62]\n",
      " 10881/50001: episode: 1209, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward:  8.000, mean reward:  0.889 [-10.000,  6.000], mean action: 64.556 [34.000, 90.000],  loss: 9.594798, mae: 2.055162, mean_q: 4.597045\n",
      "[20 32 18 60 69 24 58 60  4 41]\n",
      " 10890/50001: episode: 1210, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 40.667 [4.000, 69.000],  loss: 5.640716, mae: 2.118715, mean_q: 4.809855\n",
      "[68 13 31 49 34 17 53 50 28 12]\n",
      " 10899/50001: episode: 1211, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 44.000, mean reward:  4.889 [ 2.000,  8.000], mean action: 31.889 [12.000, 53.000],  loss: 6.804297, mae: 2.077085, mean_q: 4.694091\n",
      "[33 83 86 95 80 27 57 23 57 88]\n",
      " 10908/50001: episode: 1212, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 28.000, mean reward:  3.111 [-10.000,  9.000], mean action: 66.222 [23.000, 95.000],  loss: 7.779278, mae: 2.130068, mean_q: 4.806067\n",
      "[18 46 68 17 88 88 32 41 41  4]\n",
      " 10917/50001: episode: 1213, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 12.000, mean reward:  1.333 [-10.000,  7.000], mean action: 47.222 [4.000, 88.000],  loss: 7.320476, mae: 2.087113, mean_q: 4.802729\n",
      "[82 34 63 92 34 42 11 92 27 77]\n",
      " 10926/50001: episode: 1214, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward:  8.000, mean reward:  0.889 [-10.000,  6.000], mean action: 52.444 [11.000, 92.000],  loss: 7.825068, mae: 2.094744, mean_q: 4.693781\n",
      "[65 41 31 64 54 49 16 63 88 74]\n",
      " 10935/50001: episode: 1215, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 53.333 [16.000, 88.000],  loss: 7.451853, mae: 2.047856, mean_q: 4.555919\n",
      "[69 28 83 12  1 76 80 95  9 61]\n",
      " 10944/50001: episode: 1216, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 37.000, mean reward:  4.111 [ 3.000,  8.000], mean action: 49.444 [1.000, 95.000],  loss: 5.839182, mae: 2.147519, mean_q: 4.909589\n",
      "[ 3  4 57 97 60 60 21 21  4  4]\n",
      " 10953/50001: episode: 1217, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: -21.000, mean reward: -2.333 [-10.000,  5.000], mean action: 36.444 [4.000, 97.000],  loss: 7.536473, mae: 2.185081, mean_q: 4.891104\n",
      "[87 96 79 98 37 37 28 59 31 97]\n",
      " 10962/50001: episode: 1218, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 26.000, mean reward:  2.889 [-10.000,  6.000], mean action: 62.444 [28.000, 98.000],  loss: 8.792431, mae: 2.190452, mean_q: 4.957867\n",
      "[28 95 47 34 37 50 48 21 46  4]\n",
      " 10971/50001: episode: 1219, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 40.000, mean reward:  4.444 [ 3.000,  5.000], mean action: 42.444 [4.000, 95.000],  loss: 8.444652, mae: 2.063103, mean_q: 4.655970\n",
      "[76 27 23  1 46 71 95 74  6 44]\n",
      " 10980/50001: episode: 1220, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 37.000, mean reward:  4.111 [ 2.000,  8.000], mean action: 43.000 [1.000, 95.000],  loss: 7.316266, mae: 2.008120, mean_q: 4.558348\n",
      "[28 68 56 99 95 95 51 91 31 68]\n",
      " 10989/50001: episode: 1221, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward:  8.000, mean reward:  0.889 [-10.000,  6.000], mean action: 72.667 [31.000, 99.000],  loss: 7.423404, mae: 1.977226, mean_q: 4.537455\n",
      "[49 41  4 69 97 41 88 53 57 13]\n",
      " 10998/50001: episode: 1222, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 29.000, mean reward:  3.222 [-10.000,  8.000], mean action: 51.444 [4.000, 97.000],  loss: 6.721716, mae: 2.018205, mean_q: 4.519980\n",
      "[ 7 34 46 95 12 62  9 67  4 42]\n",
      " 11007/50001: episode: 1223, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 43.000, mean reward:  4.778 [ 3.000,  6.000], mean action: 41.222 [4.000, 95.000],  loss: 7.668538, mae: 2.058671, mean_q: 4.722026\n",
      "[30 52 74 34 14 13 97 32 90 48]\n",
      " 11016/50001: episode: 1224, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 43.000, mean reward:  4.778 [ 3.000,  7.000], mean action: 50.444 [13.000, 97.000],  loss: 8.223370, mae: 2.046681, mean_q: 4.559932\n",
      "[89 42 16 90 46  2 61  9 50 95]\n",
      " 11025/50001: episode: 1225, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 45.000, mean reward:  5.000 [ 3.000, 10.000], mean action: 45.667 [2.000, 95.000],  loss: 9.072659, mae: 1.990404, mean_q: 4.532902\n",
      "[96 93 88  1 34 54 34  1 37 47]\n",
      " 11034/50001: episode: 1226, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 13.000, mean reward:  1.444 [-10.000,  7.000], mean action: 43.222 [1.000, 93.000],  loss: 7.946263, mae: 2.015164, mean_q: 4.579994\n",
      "[92 22 58 56 34 60 68 53 52 62]\n",
      " 11043/50001: episode: 1227, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 35.000, mean reward:  3.889 [ 3.000,  6.000], mean action: 51.667 [22.000, 68.000],  loss: 8.177449, mae: 2.013177, mean_q: 4.502486\n",
      "[71 41 50  2 25 21 24 92  8 57]\n",
      " 11052/50001: episode: 1228, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 38.000, mean reward:  4.222 [ 3.000,  7.000], mean action: 35.556 [2.000, 92.000],  loss: 6.019032, mae: 1.954047, mean_q: 4.454649\n",
      "[88 28 69 34 34  1  4 68  9 34]\n",
      " 11061/50001: episode: 1229, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward:  6.000, mean reward:  0.667 [-10.000,  5.000], mean action: 31.222 [1.000, 69.000],  loss: 7.213898, mae: 1.950648, mean_q: 4.405492\n",
      "[15 43 93  9 97 91 99 28 31 95]\n",
      " 11070/50001: episode: 1230, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 45.000, mean reward:  5.000 [ 2.000, 10.000], mean action: 65.111 [9.000, 99.000],  loss: 7.861715, mae: 1.976544, mean_q: 4.494221\n",
      "[65 79 55 10  2 46 88 88 72 79]\n",
      " 11079/50001: episode: 1231, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward:  7.000, mean reward:  0.778 [-10.000,  6.000], mean action: 57.667 [2.000, 88.000],  loss: 8.225506, mae: 2.020599, mean_q: 4.583871\n",
      "[92 41 13 86 69  8 46 41 82  9]\n",
      " 11088/50001: episode: 1232, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 43.889 [8.000, 86.000],  loss: 6.934544, mae: 2.035334, mean_q: 4.675042\n",
      "[ 1 42 37 75 17 86 72 38 96 12]\n",
      " 11097/50001: episode: 1233, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 36.000, mean reward:  4.000 [ 2.000, 10.000], mean action: 52.778 [12.000, 96.000],  loss: 6.846616, mae: 2.012482, mean_q: 4.511122\n",
      "[ 4 23 21 48 51 58 89 88 88  4]\n",
      " 11106/50001: episode: 1234, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward:  7.000, mean reward:  0.778 [-10.000,  6.000], mean action: 52.222 [4.000, 89.000],  loss: 6.906280, mae: 1.995480, mean_q: 4.499779\n",
      "[50 60 13  2 34 31 44 12 89 97]\n",
      " 11115/50001: episode: 1235, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 42.444 [2.000, 97.000],  loss: 8.987392, mae: 2.007402, mean_q: 4.469648\n",
      "[85 37  4 53 51 49  4 95 90 98]\n",
      " 11124/50001: episode: 1236, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 26.000, mean reward:  2.889 [-10.000,  8.000], mean action: 53.444 [4.000, 98.000],  loss: 6.923195, mae: 2.022805, mean_q: 4.643604\n",
      "[93 60 90 27 53 46 57 40 46 37]\n",
      " 11133/50001: episode: 1237, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 29.000, mean reward:  3.222 [-10.000,  9.000], mean action: 50.667 [27.000, 90.000],  loss: 8.879432, mae: 2.083197, mean_q: 4.649767\n",
      "[93 47 41 32 34 12 63 12 88 31]\n",
      " 11142/50001: episode: 1238, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 27.000, mean reward:  3.000 [-10.000,  6.000], mean action: 40.000 [12.000, 88.000],  loss: 8.330930, mae: 2.014384, mean_q: 4.624204\n",
      "[65 51 84 15 11 67 51 48  9 10]\n",
      " 11151/50001: episode: 1239, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 38.444 [9.000, 84.000],  loss: 7.159320, mae: 1.933904, mean_q: 4.382432\n",
      "[10  0 33 76 46 59 35 28  6 28]\n",
      " 11160/50001: episode: 1240, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 22.000, mean reward:  2.444 [-10.000,  7.000], mean action: 34.556 [0.000, 76.000],  loss: 8.757869, mae: 1.872770, mean_q: 4.363361\n",
      "[35 94 48 55 53 53 79 34 31 50]\n",
      " 11169/50001: episode: 1241, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 30.000, mean reward:  3.333 [-10.000,  8.000], mean action: 55.222 [31.000, 94.000],  loss: 6.321092, mae: 1.993855, mean_q: 4.483390\n",
      "[61  4 89 61 52 62 74 37 52 31]\n",
      " 11178/50001: episode: 1242, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 14.000, mean reward:  1.556 [-10.000,  7.000], mean action: 51.333 [4.000, 89.000],  loss: 10.034397, mae: 1.960627, mean_q: 4.514345\n",
      "[89 46 14 83 50 38 50 11 21 53]\n",
      " 11187/50001: episode: 1243, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 21.000, mean reward:  2.333 [-10.000,  5.000], mean action: 40.667 [11.000, 83.000],  loss: 7.422855, mae: 1.972777, mean_q: 4.518848\n",
      "[39 27 12 34 61 43 41 10 28 53]\n",
      " 11196/50001: episode: 1244, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 38.000, mean reward:  4.222 [ 2.000,  8.000], mean action: 34.333 [10.000, 61.000],  loss: 6.625213, mae: 1.941383, mean_q: 4.472942\n",
      "[ 3 14 15 38 67 31 88 28 23 82]\n",
      " 11205/50001: episode: 1245, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 42.000, mean reward:  4.667 [ 2.000,  6.000], mean action: 42.889 [14.000, 88.000],  loss: 7.397169, mae: 1.971312, mean_q: 4.424227\n",
      "[32 76 41 46 92 32 32 68 12 98]\n",
      " 11214/50001: episode: 1246, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 12.000, mean reward:  1.333 [-10.000,  7.000], mean action: 55.222 [12.000, 98.000],  loss: 6.142024, mae: 1.980435, mean_q: 4.474443\n",
      "[87 51 58 51 29  9 24 95 62 58]\n",
      " 11223/50001: episode: 1247, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward:  9.000, mean reward:  1.000 [-10.000,  8.000], mean action: 48.556 [9.000, 95.000],  loss: 7.476871, mae: 1.997391, mean_q: 4.650089\n",
      "[79 95 37 76 11 18 24 76 82 82]\n",
      " 11232/50001: episode: 1248, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward:  7.000, mean reward:  0.778 [-10.000,  5.000], mean action: 55.667 [11.000, 95.000],  loss: 8.886101, mae: 2.024168, mean_q: 4.584616\n",
      "[76 41 20 52 37 21 34 94 93 14]\n",
      " 11241/50001: episode: 1249, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 45.111 [14.000, 94.000],  loss: 6.216085, mae: 2.075852, mean_q: 4.673337\n",
      "[49 62 53  4 90 13 76 21 18 31]\n",
      " 11250/50001: episode: 1250, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 39.000, mean reward:  4.333 [ 3.000,  8.000], mean action: 40.889 [4.000, 90.000],  loss: 7.266416, mae: 2.033184, mean_q: 4.612480\n",
      "[99 50 28 98  6 52 95 32 88 31]\n",
      " 11259/50001: episode: 1251, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 53.333 [6.000, 98.000],  loss: 4.736946, mae: 2.035682, mean_q: 4.603549\n",
      "[10 89 49 68 92 88 27 56 42 42]\n",
      " 11268/50001: episode: 1252, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 61.444 [27.000, 92.000],  loss: 8.490574, mae: 2.044368, mean_q: 4.633506\n",
      "[60 60 94 20 68 34 92 31  9 40]\n",
      " 11277/50001: episode: 1253, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 49.778 [9.000, 94.000],  loss: 6.748751, mae: 2.084224, mean_q: 4.662835\n",
      "[83 88 24 25 37  8 34 28 32 56]\n",
      " 11286/50001: episode: 1254, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 36.889 [8.000, 88.000],  loss: 9.084397, mae: 2.051636, mean_q: 4.626098\n",
      "[93 58 31 34 34  9 92  8 58  3]\n",
      " 11295/50001: episode: 1255, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward:  4.000, mean reward:  0.444 [-10.000,  5.000], mean action: 36.333 [3.000, 92.000],  loss: 8.706998, mae: 2.043980, mean_q: 4.569749\n",
      "[13  9 33 40 29 92 60 78 98 79]\n",
      " 11304/50001: episode: 1256, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 57.556 [9.000, 98.000],  loss: 7.808618, mae: 2.054695, mean_q: 4.607181\n",
      "[70  4 32 69 55 35 95 41 95 66]\n",
      " 11313/50001: episode: 1257, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 27.000, mean reward:  3.000 [-10.000,  8.000], mean action: 54.667 [4.000, 95.000],  loss: 7.513452, mae: 2.065539, mean_q: 4.732678\n",
      "[53 41 13 85 14 10 92 92 82 85]\n",
      " 11322/50001: episode: 1258, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward:  8.000, mean reward:  0.889 [-10.000,  5.000], mean action: 57.111 [10.000, 92.000],  loss: 8.584249, mae: 1.990527, mean_q: 4.532349\n",
      "[68  4 90 59 80 37 34 23 88  8]\n",
      " 11331/50001: episode: 1259, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 47.000 [4.000, 90.000],  loss: 9.641182, mae: 2.007217, mean_q: 4.527518\n",
      "[44 40 86 42 98 98 34 83 31 62]\n",
      " 11340/50001: episode: 1260, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 63.778 [31.000, 98.000],  loss: 4.491491, mae: 1.956574, mean_q: 4.433044\n",
      "[ 8 86 94 91 34 34 97 28 62 88]\n",
      " 11349/50001: episode: 1261, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 68.222 [28.000, 97.000],  loss: 6.220330, mae: 2.011039, mean_q: 4.491937\n",
      "[41 35 58 81 53 95 51  5  1 13]\n",
      " 11358/50001: episode: 1262, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 43.556 [1.000, 95.000],  loss: 8.445485, mae: 2.099779, mean_q: 4.646959\n",
      "[25 16 51  2 67  5 91  3 13 28]\n",
      " 11367/50001: episode: 1263, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 39.000, mean reward:  4.333 [ 3.000,  8.000], mean action: 30.667 [2.000, 91.000],  loss: 8.635129, mae: 2.096291, mean_q: 4.642364\n",
      "[92 50 58 30 92 54 62 84 23 82]\n",
      " 11376/50001: episode: 1264, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 59.444 [23.000, 92.000],  loss: 7.526510, mae: 2.061816, mean_q: 4.611393\n",
      "[23 41 40 97 73 48 98 34 84  4]\n",
      " 11385/50001: episode: 1265, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 57.667 [4.000, 98.000],  loss: 9.680640, mae: 2.020261, mean_q: 4.495606\n",
      "[98 91 20 90 98 27 84 52  4 50]\n",
      " 11394/50001: episode: 1266, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 26.000, mean reward:  2.889 [-10.000,  8.000], mean action: 57.333 [4.000, 98.000],  loss: 8.952290, mae: 1.980113, mean_q: 4.479601\n",
      "[79 76 88 91 95 34 24 85  4 26]\n",
      " 11403/50001: episode: 1267, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 36.000, mean reward:  4.000 [ 2.000,  6.000], mean action: 58.111 [4.000, 95.000],  loss: 8.655421, mae: 1.954482, mean_q: 4.401339\n",
      "[54  1 69 80 54 78  1 88 52 97]\n",
      " 11412/50001: episode: 1268, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 12.000, mean reward:  1.333 [-10.000,  7.000], mean action: 57.778 [1.000, 97.000],  loss: 6.841837, mae: 2.025865, mean_q: 4.584597\n",
      "[61 74 68 88 64 23 88 29 54 77]\n",
      " 11421/50001: episode: 1269, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 20.000, mean reward:  2.222 [-10.000,  5.000], mean action: 62.778 [23.000, 88.000],  loss: 5.826081, mae: 1.991161, mean_q: 4.477157\n",
      "[32 28  4 25 22 77 27 10 51 62]\n",
      " 11430/50001: episode: 1270, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 35.000, mean reward:  3.889 [ 2.000,  5.000], mean action: 34.000 [4.000, 77.000],  loss: 8.352039, mae: 2.035197, mean_q: 4.596453\n",
      "[28 92 69  4 23  9 14 99 90 18]\n",
      " 11439/50001: episode: 1271, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 34.000, mean reward:  3.778 [ 3.000,  5.000], mean action: 46.444 [4.000, 99.000],  loss: 8.610209, mae: 2.011040, mean_q: 4.529860\n",
      "[98  3 88 95 34  1 95 12 34 31]\n",
      " 11448/50001: episode: 1272, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 14.000, mean reward:  1.556 [-10.000,  6.000], mean action: 43.667 [1.000, 95.000],  loss: 7.868329, mae: 2.049718, mean_q: 4.540023\n",
      "[50 41 34 48 11  4 27 75 14 82]\n",
      " 11457/50001: episode: 1273, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 39.000, mean reward:  4.333 [ 3.000,  5.000], mean action: 37.333 [4.000, 82.000],  loss: 8.450668, mae: 2.077092, mean_q: 4.614872\n",
      "[94 87  4 91 48 46 94 82 26 31]\n",
      " 11466/50001: episode: 1274, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 56.556 [4.000, 94.000],  loss: 9.558750, mae: 2.027719, mean_q: 4.550135\n",
      "[71 50 84 57 63 77 95 42 82 82]\n",
      " 11475/50001: episode: 1275, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 70.222 [42.000, 95.000],  loss: 7.908442, mae: 1.997321, mean_q: 4.525210\n",
      "[55  6 37 34 21 68 23 51 95 12]\n",
      " 11484/50001: episode: 1276, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 44.000, mean reward:  4.889 [ 3.000,  9.000], mean action: 38.556 [6.000, 95.000],  loss: 8.718700, mae: 1.984627, mean_q: 4.446012\n",
      "[81 88 37 15 28 30 94 20 96 69]\n",
      " 11493/50001: episode: 1277, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 36.000, mean reward:  4.000 [ 2.000,  5.000], mean action: 53.000 [15.000, 96.000],  loss: 6.329636, mae: 1.998576, mean_q: 4.464788\n",
      "[56 53 50 69 37 68 88  9 30 78]\n",
      " 11502/50001: episode: 1278, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 36.000, mean reward:  4.000 [ 3.000,  5.000], mean action: 53.556 [9.000, 88.000],  loss: 7.651217, mae: 1.997410, mean_q: 4.555614\n",
      "[63 79 98 77 84 45 67 32 95 87]\n",
      " 11511/50001: episode: 1279, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 38.000, mean reward:  4.222 [ 2.000,  9.000], mean action: 73.778 [32.000, 98.000],  loss: 8.609794, mae: 2.033562, mean_q: 4.531176\n",
      "[88 59 57 28 97 12 34  8 40  4]\n",
      " 11520/50001: episode: 1280, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward: 40.000, mean reward:  4.444 [ 3.000,  6.000], mean action: 37.667 [4.000, 97.000],  loss: 6.471354, mae: 1.993521, mean_q: 4.420331\n",
      "[63 96 41 55 25 62 42 89 99 83]\n",
      " 11529/50001: episode: 1281, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 37.000, mean reward:  4.111 [ 2.000,  6.000], mean action: 65.778 [25.000, 99.000],  loss: 7.072963, mae: 2.053607, mean_q: 4.576019\n",
      "[88 63 84 97 28 14 92 34 37 27]\n",
      " 11538/50001: episode: 1282, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 52.889 [14.000, 97.000],  loss: 6.014148, mae: 2.071255, mean_q: 4.672565\n",
      "[79 12 57 24 34 96 36 51  6  6]\n",
      " 11547/50001: episode: 1283, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 17.000, mean reward:  1.889 [-10.000,  5.000], mean action: 35.778 [6.000, 96.000],  loss: 7.529123, mae: 2.128605, mean_q: 4.771465\n",
      "[35 95 90 69 24 55 62 46  1 12]\n",
      " 11556/50001: episode: 1284, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 41.000, mean reward:  4.556 [ 2.000,  8.000], mean action: 50.444 [1.000, 95.000],  loss: 8.767978, mae: 2.071311, mean_q: 4.587477\n",
      "[18  9 37 34 40 73 10 41 76 23]\n",
      " 11565/50001: episode: 1285, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 38.111 [9.000, 76.000],  loss: 9.718162, mae: 2.125138, mean_q: 4.748366\n",
      "[11 12 62  4 67 30 69 62 35 97]\n",
      " 11574/50001: episode: 1286, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 24.000, mean reward:  2.667 [-10.000,  8.000], mean action: 48.667 [4.000, 97.000],  loss: 5.718158, mae: 2.124767, mean_q: 4.739405\n",
      "[51  7 50 34 34 97 74 95 41 52]\n",
      " 11583/50001: episode: 1287, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 27.000, mean reward:  3.000 [-10.000,  8.000], mean action: 53.778 [7.000, 97.000],  loss: 7.899300, mae: 2.086694, mean_q: 4.667627\n",
      "[54 18  8  9 91 13 22  8 91  9]\n",
      " 11592/50001: episode: 1288, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: -7.000, mean reward: -0.778 [-10.000,  6.000], mean action: 29.889 [8.000, 91.000],  loss: 7.720821, mae: 2.063215, mean_q: 4.520010\n",
      "[93 11 76 34 68 88 95 51 44 12]\n",
      " 11601/50001: episode: 1289, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 43.000, mean reward:  4.778 [ 3.000,  9.000], mean action: 53.222 [11.000, 95.000],  loss: 9.420166, mae: 2.018004, mean_q: 4.495533\n",
      "[31 41 94 37 28 20 63 37 31  4]\n",
      " 11610/50001: episode: 1290, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 10.000, mean reward:  1.111 [-10.000,  6.000], mean action: 39.444 [4.000, 94.000],  loss: 9.253543, mae: 1.961138, mean_q: 4.420763\n",
      "[52 87 97 34 94 49 14 48 88 88]\n",
      " 11619/50001: episode: 1291, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 66.556 [14.000, 97.000],  loss: 5.416790, mae: 1.898494, mean_q: 4.252341\n",
      "[31 92 81 17 37 81 62 91 95 55]\n",
      " 11628/50001: episode: 1292, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 26.000, mean reward:  2.889 [-10.000, 10.000], mean action: 67.889 [17.000, 95.000],  loss: 8.501675, mae: 1.937695, mean_q: 4.446527\n",
      "[34 18 41 49 47 12 79 32 94 50]\n",
      " 11637/50001: episode: 1293, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 41.000, mean reward:  4.556 [ 2.000,  9.000], mean action: 46.889 [12.000, 94.000],  loss: 9.006176, mae: 1.978271, mean_q: 4.380335\n",
      "[ 4 80 70 84 28 27 99 59 62 48]\n",
      " 11646/50001: episode: 1294, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 37.000, mean reward:  4.111 [ 2.000,  8.000], mean action: 61.889 [27.000, 99.000],  loss: 6.539952, mae: 1.943474, mean_q: 4.349422\n",
      "[82 54  0 90 86 21 51 41 27 89]\n",
      " 11655/50001: episode: 1295, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 51.000 [0.000, 90.000],  loss: 6.615121, mae: 2.018272, mean_q: 4.516128\n",
      "[ 9 88 53 77 59 20 10  4 98 48]\n",
      " 11664/50001: episode: 1296, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 40.000, mean reward:  4.444 [ 3.000,  8.000], mean action: 50.778 [4.000, 98.000],  loss: 7.196284, mae: 1.992532, mean_q: 4.471194\n",
      "[80  6 32 81 55 53 88 34 82 12]\n",
      " 11673/50001: episode: 1297, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 45.000, mean reward:  5.000 [ 3.000,  9.000], mean action: 49.222 [6.000, 88.000],  loss: 9.513607, mae: 2.066693, mean_q: 4.705314\n",
      "[15 50 23 60 50 49 34  7 95 33]\n",
      " 11682/50001: episode: 1298, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 25.000, mean reward:  2.778 [-10.000,  9.000], mean action: 44.556 [7.000, 95.000],  loss: 7.772541, mae: 2.005108, mean_q: 4.458387\n",
      "[95 69 50 88 52 32 75 50 44 34]\n",
      " 11691/50001: episode: 1299, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 25.000, mean reward:  2.778 [-10.000,  9.000], mean action: 54.889 [32.000, 88.000],  loss: 9.728947, mae: 2.029193, mean_q: 4.531529\n",
      "[ 0 78 79 40 46 44 89 57 40 48]\n",
      " 11700/50001: episode: 1300, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 28.000, mean reward:  3.111 [-10.000,  8.000], mean action: 57.889 [40.000, 89.000],  loss: 5.526781, mae: 1.936008, mean_q: 4.383372\n",
      "[12 95 53 84 20 95 24 68 44 50]\n",
      " 11709/50001: episode: 1301, duration: 0.075s, episode steps:   9, steps per second: 120, episode reward: 23.000, mean reward:  2.556 [-10.000,  9.000], mean action: 59.222 [20.000, 95.000],  loss: 11.526661, mae: 2.023281, mean_q: 4.521708\n",
      "[67 51 28 47 40 75 59 36 57 87]\n",
      " 11718/50001: episode: 1302, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 35.000, mean reward:  3.889 [ 2.000,  6.000], mean action: 53.333 [28.000, 87.000],  loss: 6.847912, mae: 1.934087, mean_q: 4.395563\n",
      "[31 50 48 25 28 53 69 50 44 86]\n",
      " 11727/50001: episode: 1303, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 17.000, mean reward:  1.889 [-10.000,  4.000], mean action: 50.333 [25.000, 86.000],  loss: 5.174884, mae: 2.042629, mean_q: 4.515707\n",
      "[49 86 98 99 20 44 50 64 12 31]\n",
      " 11736/50001: episode: 1304, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 43.000, mean reward:  4.778 [ 3.000,  8.000], mean action: 56.000 [12.000, 99.000],  loss: 8.300501, mae: 2.023640, mean_q: 4.549325\n",
      "[80 89 84 34 15 94 88 78  2 21]\n",
      " 11745/50001: episode: 1305, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 36.000, mean reward:  4.000 [ 2.000,  6.000], mean action: 56.111 [2.000, 94.000],  loss: 8.251131, mae: 1.965648, mean_q: 4.401916\n",
      "[37  7 34 13 16  5 33 36 32 13]\n",
      " 11754/50001: episode: 1306, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 20.000, mean reward:  2.222 [-10.000,  6.000], mean action: 21.000 [5.000, 36.000],  loss: 9.633037, mae: 2.041057, mean_q: 4.546713\n",
      "[57 31 50 41 40 48 13 51 31 83]\n",
      " 11763/50001: episode: 1307, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 43.111 [13.000, 83.000],  loss: 8.898075, mae: 2.031574, mean_q: 4.561734\n",
      "[ 4 72 64 57 18 39 46 37  1  2]\n",
      " 11772/50001: episode: 1308, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 42.000, mean reward:  4.667 [ 2.000,  8.000], mean action: 37.333 [1.000, 72.000],  loss: 8.209268, mae: 2.024582, mean_q: 4.476934\n",
      "[66 26  4 81 24 46 16 89 10 55]\n",
      " 11781/50001: episode: 1309, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 35.000, mean reward:  3.889 [ 2.000,  5.000], mean action: 39.000 [4.000, 89.000],  loss: 6.184083, mae: 1.996197, mean_q: 4.504694\n",
      "[60 74 67 69 98 48 79 72 40 94]\n",
      " 11790/50001: episode: 1310, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 36.000, mean reward:  4.000 [ 2.000,  5.000], mean action: 71.222 [40.000, 98.000],  loss: 8.601722, mae: 2.031723, mean_q: 4.598597\n",
      "[53 69 75 45 69 12 92 60 53 13]\n",
      " 11799/50001: episode: 1311, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 12.000, mean reward:  1.333 [-10.000,  9.000], mean action: 54.222 [12.000, 92.000],  loss: 7.263244, mae: 2.040938, mean_q: 4.542329\n",
      "[56 82 23  4 38 48 46 28 15 34]\n",
      " 11808/50001: episode: 1312, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 40.000, mean reward:  4.444 [ 2.000, 10.000], mean action: 35.333 [4.000, 82.000],  loss: 7.086329, mae: 2.056183, mean_q: 4.572381\n",
      "[95  6 48 49 95  3 21 31 82 93]\n",
      " 11817/50001: episode: 1313, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 47.556 [3.000, 95.000],  loss: 6.915371, mae: 2.111494, mean_q: 4.651313\n",
      "[53 33 92 58 24 63 21 78  1 97]\n",
      " 11826/50001: episode: 1314, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 39.000, mean reward:  4.333 [ 3.000,  8.000], mean action: 51.889 [1.000, 97.000],  loss: 6.968457, mae: 2.036252, mean_q: 4.497630\n",
      "[23 53 80 28 77 37  4 50 31 88]\n",
      " 11835/50001: episode: 1315, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 40.000, mean reward:  4.444 [ 2.000,  6.000], mean action: 49.778 [4.000, 88.000],  loss: 6.526290, mae: 2.121817, mean_q: 4.697095\n",
      "[22 95 75 47 69 24 32 98 14 44]\n",
      " 11844/50001: episode: 1316, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 38.000, mean reward:  4.222 [ 3.000,  5.000], mean action: 55.333 [14.000, 98.000],  loss: 8.551707, mae: 2.112389, mean_q: 4.650626\n",
      "[57 75 11 59 32 60 21 20 75 33]\n",
      " 11853/50001: episode: 1317, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 20.000, mean reward:  2.222 [-10.000,  5.000], mean action: 42.889 [11.000, 75.000],  loss: 6.172789, mae: 2.087976, mean_q: 4.620363\n",
      "[58 79 90  8 48 88 76 73 28 31]\n",
      " 11862/50001: episode: 1318, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 57.889 [8.000, 90.000],  loss: 9.163923, mae: 2.119170, mean_q: 4.673691\n",
      "[79 50 54 77 14 24 12 92 82 28]\n",
      " 11871/50001: episode: 1319, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 48.111 [12.000, 92.000],  loss: 6.999575, mae: 2.138860, mean_q: 4.691025\n",
      "[89 76  1 20 61 34 95 85 31 41]\n",
      " 11880/50001: episode: 1320, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 41.000, mean reward:  4.556 [ 2.000,  7.000], mean action: 49.333 [1.000, 95.000],  loss: 5.724114, mae: 2.046879, mean_q: 4.500963\n",
      "[83 58 29 27 98 49  1 34 57 69]\n",
      " 11889/50001: episode: 1321, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward: 42.000, mean reward:  4.667 [ 2.000,  8.000], mean action: 46.889 [1.000, 98.000],  loss: 7.260107, mae: 2.107434, mean_q: 4.685153\n",
      "[62 14 62 14 44  6 32 53 27 42]\n",
      " 11898/50001: episode: 1322, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 12.000, mean reward:  1.333 [-10.000,  6.000], mean action: 32.667 [6.000, 62.000],  loss: 8.537500, mae: 2.078902, mean_q: 4.595612\n",
      "[11 89 76 95 34 50 46 29 92 48]\n",
      " 11907/50001: episode: 1323, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 42.000, mean reward:  4.667 [ 2.000,  8.000], mean action: 62.111 [29.000, 95.000],  loss: 10.708493, mae: 2.127185, mean_q: 4.771395\n",
      "[40 89 98 24 34 59 72 20  4 68]\n",
      " 11916/50001: episode: 1324, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 35.000, mean reward:  3.889 [ 2.000,  6.000], mean action: 52.000 [4.000, 98.000],  loss: 7.032789, mae: 1.945672, mean_q: 4.360287\n",
      "[13 41 95 44 98  4 42 82 24 18]\n",
      " 11925/50001: episode: 1325, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 38.000, mean reward:  4.222 [ 2.000,  6.000], mean action: 49.778 [4.000, 98.000],  loss: 7.367408, mae: 2.050265, mean_q: 4.533749\n",
      "[ 9 98 50 23 42 96 45 53 26 28]\n",
      " 11934/50001: episode: 1326, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 36.000, mean reward:  4.000 [ 2.000,  8.000], mean action: 51.222 [23.000, 98.000],  loss: 6.299417, mae: 2.045549, mean_q: 4.564714\n",
      "[ 7 14 50 56 43 80 30 87 77 12]\n",
      " 11943/50001: episode: 1327, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 39.000, mean reward:  4.333 [ 2.000,  9.000], mean action: 49.889 [12.000, 87.000],  loss: 6.940431, mae: 2.049330, mean_q: 4.529390\n",
      "[21 88 68 86 34 45 68 50 77 88]\n",
      " 11952/50001: episode: 1328, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 10.000, mean reward:  1.111 [-10.000,  7.000], mean action: 67.111 [34.000, 88.000],  loss: 8.955722, mae: 2.036527, mean_q: 4.583791\n",
      "[45 28 42 62 11 88 95 88 12 50]\n",
      " 11961/50001: episode: 1329, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 32.000, mean reward:  3.556 [-10.000,  8.000], mean action: 52.889 [11.000, 95.000],  loss: 8.458803, mae: 2.033325, mean_q: 4.505541\n",
      "[ 3 41 74 46 10 95 95 51 14 44]\n",
      " 11970/50001: episode: 1330, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 52.222 [10.000, 95.000],  loss: 10.080421, mae: 2.031521, mean_q: 4.564580\n",
      "[29 26 58 25  1 79 88 50 84 78]\n",
      " 11979/50001: episode: 1331, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 54.333 [1.000, 88.000],  loss: 6.682594, mae: 1.997819, mean_q: 4.430173\n",
      "[13 74 32 69 73 55 34 53 12 23]\n",
      " 11988/50001: episode: 1332, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 37.000, mean reward:  4.111 [ 2.000,  7.000], mean action: 47.222 [12.000, 74.000],  loss: 7.587411, mae: 2.054978, mean_q: 4.486034\n",
      "[93 41 58 15  2 99 10 88 87 89]\n",
      " 11997/50001: episode: 1333, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 36.000, mean reward:  4.000 [ 2.000,  6.000], mean action: 54.333 [2.000, 99.000],  loss: 6.943626, mae: 2.005834, mean_q: 4.408781\n",
      "[23 14 82  1 33 24 48  4  2 11]\n",
      " 12006/50001: episode: 1334, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 35.000, mean reward:  3.889 [ 3.000,  5.000], mean action: 24.333 [1.000, 82.000],  loss: 8.674844, mae: 1.996223, mean_q: 4.424749\n",
      "[39 41 13 40 74 13 89 28  4 14]\n",
      " 12015/50001: episode: 1335, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 27.000, mean reward:  3.000 [-10.000,  6.000], mean action: 35.111 [4.000, 89.000],  loss: 5.737997, mae: 2.071164, mean_q: 4.568306\n",
      "[84 57 64 34 12  1 21 63 31 65]\n",
      " 12024/50001: episode: 1336, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 38.667 [1.000, 65.000],  loss: 6.584798, mae: 2.072596, mean_q: 4.611719\n",
      "[24 80 41 32 80 89 51 95 82 12]\n",
      " 12033/50001: episode: 1337, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 31.000, mean reward:  3.444 [-10.000,  9.000], mean action: 62.444 [12.000, 95.000],  loss: 8.524002, mae: 2.087830, mean_q: 4.576209\n",
      "[88 41 94 68  1 98 51 11 67  9]\n",
      " 12042/50001: episode: 1338, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 37.000, mean reward:  4.111 [ 3.000,  6.000], mean action: 48.889 [1.000, 98.000],  loss: 8.761254, mae: 2.074302, mean_q: 4.562652\n",
      "[34 57 50 37 83 60 96 56 97 31]\n",
      " 12051/50001: episode: 1339, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 63.000 [31.000, 97.000],  loss: 6.718071, mae: 2.042130, mean_q: 4.448641\n",
      "[51 93 72 25 37 34 32 54  4 46]\n",
      " 12060/50001: episode: 1340, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 41.000, mean reward:  4.556 [ 2.000,  6.000], mean action: 44.111 [4.000, 93.000],  loss: 6.962683, mae: 2.002691, mean_q: 4.432512\n",
      "[76 28 98 37 14 30 39 38 21 12]\n",
      " 12069/50001: episode: 1341, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 39.000, mean reward:  4.333 [ 2.000, 10.000], mean action: 35.222 [12.000, 98.000],  loss: 7.006772, mae: 2.050856, mean_q: 4.511902\n",
      "[82  7 88  9 11 91 96 95 40 95]\n",
      " 12078/50001: episode: 1342, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 27.000, mean reward:  3.000 [-10.000,  9.000], mean action: 59.111 [7.000, 96.000],  loss: 6.567398, mae: 2.055310, mean_q: 4.577281\n",
      "[26 28 24 34 59 62 57  1 45 67]\n",
      " 12087/50001: episode: 1343, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 37.000, mean reward:  4.111 [ 2.000,  6.000], mean action: 41.889 [1.000, 67.000],  loss: 10.214596, mae: 2.102067, mean_q: 4.609373\n",
      "[ 5 26 55 12 97 56 41 52 52 14]\n",
      " 12096/50001: episode: 1344, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 45.000 [12.000, 97.000],  loss: 8.810143, mae: 2.039778, mean_q: 4.444993\n",
      "[39 89 98 69 99 31 90 57 56 14]\n",
      " 12105/50001: episode: 1345, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 42.000, mean reward:  4.667 [ 4.000,  7.000], mean action: 67.000 [14.000, 99.000],  loss: 6.606602, mae: 1.997683, mean_q: 4.447155\n",
      "[80 96  6 97 29 14 97 57  8 24]\n",
      " 12114/50001: episode: 1346, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 47.556 [6.000, 97.000],  loss: 7.460577, mae: 2.080127, mean_q: 4.547505\n",
      "[65 52 46 49 13 94 89 95 86 21]\n",
      " 12123/50001: episode: 1347, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 39.000, mean reward:  4.333 [ 3.000,  9.000], mean action: 60.556 [13.000, 95.000],  loss: 9.174644, mae: 2.046161, mean_q: 4.505494\n",
      "[44 95 79 88 86 34 50  4 34 48]\n",
      " 12132/50001: episode: 1348, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 31.000, mean reward:  3.444 [-10.000,  7.000], mean action: 57.556 [4.000, 95.000],  loss: 6.743525, mae: 2.015441, mean_q: 4.470088\n",
      "[33 41 80 57 77 14 47  2 52 38]\n",
      " 12141/50001: episode: 1349, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 36.000, mean reward:  4.000 [ 2.000,  6.000], mean action: 45.333 [2.000, 80.000],  loss: 10.829976, mae: 2.032090, mean_q: 4.468075\n",
      "[33 84 74 23  4 49 50 46 21 42]\n",
      " 12150/50001: episode: 1350, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 43.667 [4.000, 84.000],  loss: 7.307832, mae: 2.052596, mean_q: 4.457205\n",
      "[11  8 95 54 56 31 78 69 53 83]\n",
      " 12159/50001: episode: 1351, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 36.000, mean reward:  4.000 [ 3.000,  5.000], mean action: 58.556 [8.000, 95.000],  loss: 8.415918, mae: 2.018824, mean_q: 4.449133\n",
      "[22 50 97  9  2 95 51 24 95 62]\n",
      " 12168/50001: episode: 1352, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 53.889 [2.000, 97.000],  loss: 6.461199, mae: 1.983397, mean_q: 4.324467\n",
      "[49 68 47 48 27 20  8 76  8 28]\n",
      " 12177/50001: episode: 1353, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 22.000, mean reward:  2.444 [-10.000,  8.000], mean action: 36.667 [8.000, 76.000],  loss: 7.942191, mae: 1.946137, mean_q: 4.216215\n",
      "[24 88 84 12 79 32 48 25 94 16]\n",
      " 12186/50001: episode: 1354, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 32.000, mean reward:  3.556 [ 2.000,  5.000], mean action: 53.111 [12.000, 94.000],  loss: 8.306017, mae: 2.038514, mean_q: 4.464161\n",
      "[69 12 32 48 21 34 11 80 12 37]\n",
      " 12195/50001: episode: 1355, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 24.000, mean reward:  2.667 [-10.000,  8.000], mean action: 31.889 [11.000, 80.000],  loss: 8.786711, mae: 1.964445, mean_q: 4.300886\n",
      "[42 86 47 50 84 14 96 66 43 31]\n",
      " 12204/50001: episode: 1356, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 38.000, mean reward:  4.222 [ 2.000,  8.000], mean action: 57.444 [14.000, 96.000],  loss: 11.889560, mae: 1.941234, mean_q: 4.283979\n",
      "[16 79 31 92 97 12  9 52 28 88]\n",
      " 12213/50001: episode: 1357, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 54.222 [9.000, 97.000],  loss: 5.921110, mae: 1.918342, mean_q: 4.182973\n",
      "[21 14 65 97 57 95 51 34 96 31]\n",
      " 12222/50001: episode: 1358, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 41.000, mean reward:  4.556 [ 2.000,  7.000], mean action: 60.000 [14.000, 97.000],  loss: 7.343371, mae: 1.950690, mean_q: 4.348027\n",
      "[26 36 73 40  1 33 77 54 42 90]\n",
      " 12231/50001: episode: 1359, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 49.556 [1.000, 90.000],  loss: 6.515667, mae: 1.947206, mean_q: 4.292147\n",
      "[77 36 88 56 14 77 67 28 50 31]\n",
      " 12240/50001: episode: 1360, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 27.000, mean reward:  3.000 [-10.000,  8.000], mean action: 49.667 [14.000, 88.000],  loss: 8.635159, mae: 2.063895, mean_q: 4.495680\n",
      "[75 13 77 50 81 33 14 39 34 77]\n",
      " 12249/50001: episode: 1361, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 23.000, mean reward:  2.556 [-10.000,  9.000], mean action: 46.444 [13.000, 81.000],  loss: 8.902885, mae: 1.954140, mean_q: 4.273111\n",
      "[22 88 32 50 15 89 34  6  9 91]\n",
      " 12258/50001: episode: 1362, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 46.000 [6.000, 91.000],  loss: 5.835093, mae: 1.992193, mean_q: 4.337427\n",
      "[86 21 50  2 89 13 96 41 66 32]\n",
      " 12267/50001: episode: 1363, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 44.000, mean reward:  4.889 [ 3.000,  7.000], mean action: 45.556 [2.000, 96.000],  loss: 7.472370, mae: 2.046682, mean_q: 4.455292\n",
      "[16 67 79 67 99  4 21 23 27 37]\n",
      " 12276/50001: episode: 1364, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 29.000, mean reward:  3.222 [-10.000,  9.000], mean action: 47.111 [4.000, 99.000],  loss: 8.566676, mae: 2.088747, mean_q: 4.578077\n",
      "[41 34 64 28 46 88 96 44 95 88]\n",
      " 12285/50001: episode: 1365, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 26.000, mean reward:  2.889 [-10.000,  9.000], mean action: 64.778 [28.000, 96.000],  loss: 9.100537, mae: 2.078136, mean_q: 4.574446\n",
      "[ 7 54 95 24 27 92 68 24 95 95]\n",
      " 12294/50001: episode: 1366, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: -6.000, mean reward: -0.667 [-10.000,  6.000], mean action: 63.778 [24.000, 95.000],  loss: 6.459035, mae: 2.084598, mean_q: 4.556889\n",
      "[58 19 50 42 34 80 95 62 50 31]\n",
      " 12303/50001: episode: 1367, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 51.444 [19.000, 95.000],  loss: 5.892865, mae: 2.098803, mean_q: 4.615138\n",
      "[35  3 85 89 60 71 41 41 82 12]\n",
      " 12312/50001: episode: 1368, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 29.000, mean reward:  3.222 [-10.000, 11.000], mean action: 53.778 [3.000, 89.000],  loss: 8.932273, mae: 2.082089, mean_q: 4.541625\n",
      "[ 3 97 32 97 97 24 88 50  4  4]\n",
      " 12321/50001: episode: 1369, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: -1.000, mean reward: -0.111 [-10.000,  7.000], mean action: 54.778 [4.000, 97.000],  loss: 7.700520, mae: 2.053098, mean_q: 4.525392\n",
      "[99 72 78 37 25  9 67 12 40  4]\n",
      " 12330/50001: episode: 1370, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 38.222 [4.000, 78.000],  loss: 9.005515, mae: 2.045157, mean_q: 4.428240\n",
      "[89 89 16 73 90 67 57  1 16 95]\n",
      " 12339/50001: episode: 1371, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 15.000, mean reward:  1.667 [-10.000, 11.000], mean action: 56.000 [1.000, 95.000],  loss: 5.967974, mae: 2.024838, mean_q: 4.427384\n",
      "[75 99 50 90 77 46 79  4 14 40]\n",
      " 12348/50001: episode: 1372, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 40.000, mean reward:  4.444 [ 3.000,  5.000], mean action: 55.444 [4.000, 99.000],  loss: 6.841558, mae: 2.019613, mean_q: 4.355242\n",
      "[70  4 88 97 34 46 20 37 62 12]\n",
      " 12357/50001: episode: 1373, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 44.000, mean reward:  4.889 [ 3.000,  9.000], mean action: 44.444 [4.000, 97.000],  loss: 6.437055, mae: 2.067634, mean_q: 4.502489\n",
      "[60 31 42 89 12 79 86  5 96 50]\n",
      " 12366/50001: episode: 1374, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward: 40.000, mean reward:  4.444 [ 3.000,  9.000], mean action: 54.444 [5.000, 96.000],  loss: 8.159527, mae: 2.161084, mean_q: 4.680463\n",
      "[70 76 98 17 37 86  0 24 34  1]\n",
      " 12375/50001: episode: 1375, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 47.000, mean reward:  5.222 [ 3.000,  9.000], mean action: 41.444 [0.000, 98.000],  loss: 4.739219, mae: 2.041436, mean_q: 4.456269\n",
      "[87 37 37  4 58 21 59 12 21 74]\n",
      " 12384/50001: episode: 1376, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 11.000, mean reward:  1.222 [-10.000,  7.000], mean action: 35.889 [4.000, 74.000],  loss: 7.997366, mae: 2.095269, mean_q: 4.500973\n",
      "[82 50 13 21 79 53 88 90 82 75]\n",
      " 12393/50001: episode: 1377, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 61.222 [13.000, 90.000],  loss: 6.846846, mae: 2.162291, mean_q: 4.643152\n",
      "[81 70 48 92 68 42 38 48 31 14]\n",
      " 12402/50001: episode: 1378, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 21.000, mean reward:  2.333 [-10.000,  6.000], mean action: 50.111 [14.000, 92.000],  loss: 8.479315, mae: 2.082608, mean_q: 4.574508\n",
      "[60 52 22 71 99 34  2 42 85 69]\n",
      " 12411/50001: episode: 1379, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 36.000, mean reward:  4.000 [ 2.000,  7.000], mean action: 52.889 [2.000, 99.000],  loss: 8.816032, mae: 2.168528, mean_q: 4.653283\n",
      "[17 84 79 21 67 58 16 94 82 10]\n",
      " 12420/50001: episode: 1380, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 36.000, mean reward:  4.000 [ 3.000,  6.000], mean action: 56.778 [10.000, 94.000],  loss: 9.354960, mae: 2.053150, mean_q: 4.426620\n",
      "[12 12 96 37 31 62 21 88 94 98]\n",
      " 12429/50001: episode: 1381, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 59.889 [12.000, 98.000],  loss: 8.329672, mae: 2.025985, mean_q: 4.462182\n",
      "[68 42  4  4 97 84 95 97 79 50]\n",
      " 12438/50001: episode: 1382, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 13.000, mean reward:  1.444 [-10.000,  7.000], mean action: 61.333 [4.000, 97.000],  loss: 7.969263, mae: 2.001398, mean_q: 4.327115\n",
      "[52 93 37 38 92 92 48 78 31 57]\n",
      " 12447/50001: episode: 1383, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 62.889 [31.000, 93.000],  loss: 9.320095, mae: 2.022800, mean_q: 4.382794\n",
      "[56 57 21 12 62  3 62 31 64  8]\n",
      " 12456/50001: episode: 1384, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 21.000, mean reward:  2.333 [-10.000,  6.000], mean action: 35.556 [3.000, 64.000],  loss: 8.441834, mae: 1.991326, mean_q: 4.357118\n",
      "[44  6 68 20 79 60 52 68  3 98]\n",
      " 12465/50001: episode: 1385, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 50.444 [3.000, 98.000],  loss: 7.055547, mae: 1.977135, mean_q: 4.302791\n",
      "[ 6 34 34 42 60  9  2 75 14 94]\n",
      " 12474/50001: episode: 1386, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 25.000, mean reward:  2.778 [-10.000,  5.000], mean action: 40.444 [2.000, 94.000],  loss: 6.695330, mae: 2.021824, mean_q: 4.380499\n",
      "[55 32  2 88 50 79 95 89 66 93]\n",
      " 12483/50001: episode: 1387, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 44.000, mean reward:  4.889 [ 3.000,  7.000], mean action: 66.000 [2.000, 95.000],  loss: 9.762594, mae: 2.068287, mean_q: 4.494016\n",
      "[28 59 22  1 42 38 23 93 99 71]\n",
      " 12492/50001: episode: 1388, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 33.000, mean reward:  3.667 [ 2.000,  5.000], mean action: 49.778 [1.000, 99.000],  loss: 5.348109, mae: 2.077584, mean_q: 4.545928\n",
      "[38 42 59 53 98 95 34 46 78 12]\n",
      " 12501/50001: episode: 1389, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 46.000, mean reward:  5.111 [ 3.000,  9.000], mean action: 57.444 [12.000, 98.000],  loss: 7.783985, mae: 2.089730, mean_q: 4.633637\n",
      "[88 31 95 86 92 46  1 66 48 81]\n",
      " 12510/50001: episode: 1390, duration: 0.071s, episode steps:   9, steps per second: 127, episode reward: 40.000, mean reward:  4.444 [ 2.000,  6.000], mean action: 60.667 [1.000, 95.000],  loss: 8.403958, mae: 2.148758, mean_q: 4.681409\n",
      "[37 83 97 97 92 28 40 12 12 27]\n",
      " 12519/50001: episode: 1391, duration: 0.069s, episode steps:   9, steps per second: 130, episode reward: 10.000, mean reward:  1.111 [-10.000,  6.000], mean action: 54.222 [12.000, 97.000],  loss: 7.291110, mae: 2.151805, mean_q: 4.685366\n",
      "[31  4 44 59  6 10 25  2 90 32]\n",
      " 12528/50001: episode: 1392, duration: 0.078s, episode steps:   9, steps per second: 115, episode reward: 37.000, mean reward:  4.111 [ 2.000,  7.000], mean action: 30.222 [2.000, 90.000],  loss: 7.372893, mae: 2.123616, mean_q: 4.642626\n",
      "[17 11 83 25 53 97 95 45 34 66]\n",
      " 12537/50001: episode: 1393, duration: 0.076s, episode steps:   9, steps per second: 118, episode reward: 46.000, mean reward:  5.111 [ 2.000,  9.000], mean action: 56.556 [11.000, 97.000],  loss: 5.971662, mae: 2.078579, mean_q: 4.522922\n",
      "[58 42 97 48  6 23  4 97 28 88]\n",
      " 12546/50001: episode: 1394, duration: 0.086s, episode steps:   9, steps per second: 105, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 48.111 [4.000, 97.000],  loss: 7.514089, mae: 2.186756, mean_q: 4.708628\n",
      "[36 28 50 48 69 94 11 95 46 97]\n",
      " 12555/50001: episode: 1395, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward: 43.000, mean reward:  4.778 [ 3.000,  8.000], mean action: 59.778 [11.000, 97.000],  loss: 11.789859, mae: 2.184254, mean_q: 4.687811\n",
      "[ 6 28 80 54 37 90 10 13 21 34]\n",
      " 12564/50001: episode: 1396, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 43.000, mean reward:  4.778 [ 2.000, 10.000], mean action: 40.778 [10.000, 90.000],  loss: 7.920353, mae: 2.125328, mean_q: 4.631312\n",
      "[43 17 20 66  2 44 87 95 75 14]\n",
      " 12573/50001: episode: 1397, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 43.000, mean reward:  4.778 [ 3.000,  9.000], mean action: 46.667 [2.000, 95.000],  loss: 6.448404, mae: 2.101062, mean_q: 4.522498\n",
      "[10 26 28  0 55 48 13 67 24  4]\n",
      " 12582/50001: episode: 1398, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 29.444 [0.000, 67.000],  loss: 7.097575, mae: 2.084794, mean_q: 4.453395\n",
      "[16 89 10 89 91 79 41 76 28 48]\n",
      " 12591/50001: episode: 1399, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 30.000, mean reward:  3.333 [-10.000,  8.000], mean action: 61.222 [10.000, 91.000],  loss: 6.735960, mae: 2.125659, mean_q: 4.559400\n",
      "[76 82 83  1 86 92 15 63 52 53]\n",
      " 12600/50001: episode: 1400, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 58.556 [1.000, 92.000],  loss: 8.499437, mae: 2.115344, mean_q: 4.525043\n",
      "[68 83 76 57 25 58 48 53 96 88]\n",
      " 12609/50001: episode: 1401, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 41.000, mean reward:  4.556 [ 3.000,  9.000], mean action: 64.889 [25.000, 96.000],  loss: 8.960854, mae: 2.142701, mean_q: 4.623404\n",
      "[47  2 50 55 55 87 34 46 34 10]\n",
      " 12618/50001: episode: 1402, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward:  9.000, mean reward:  1.000 [-10.000,  7.000], mean action: 41.444 [2.000, 87.000],  loss: 7.390872, mae: 2.023513, mean_q: 4.400217\n",
      "[48 50 60 47 11  0 44 59 86 88]\n",
      " 12627/50001: episode: 1403, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 36.000, mean reward:  4.000 [ 2.000,  9.000], mean action: 49.444 [0.000, 88.000],  loss: 7.456166, mae: 2.057613, mean_q: 4.548270\n",
      "[ 6 43  1 94 51 44 41 95 66 28]\n",
      " 12636/50001: episode: 1404, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 45.000, mean reward:  5.000 [ 2.000,  9.000], mean action: 51.444 [1.000, 95.000],  loss: 6.761254, mae: 2.068354, mean_q: 4.419914\n",
      "[16 24 58 77 44 58 88 27 95 12]\n",
      " 12645/50001: episode: 1405, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 33.000, mean reward:  3.667 [-10.000, 10.000], mean action: 53.667 [12.000, 95.000],  loss: 6.438915, mae: 2.147479, mean_q: 4.694854\n",
      "[79 96  9 97 92 42 21 48 31 42]\n",
      " 12654/50001: episode: 1406, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 53.111 [9.000, 97.000],  loss: 6.539074, mae: 2.161067, mean_q: 4.743961\n",
      "[17 71 23 33 63 62 51 59 28 96]\n",
      " 12663/50001: episode: 1407, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward: 37.000, mean reward:  4.111 [ 2.000,  7.000], mean action: 54.000 [23.000, 96.000],  loss: 8.708232, mae: 2.098513, mean_q: 4.531236\n",
      "[41 34 59 96 75 62 34 88 27 31]\n",
      " 12672/50001: episode: 1408, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 27.000, mean reward:  3.000 [-10.000,  6.000], mean action: 56.222 [27.000, 96.000],  loss: 6.165116, mae: 2.184936, mean_q: 4.744809\n",
      "[39 88 56 37 62 62 38 10 60 16]\n",
      " 12681/50001: episode: 1409, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 47.667 [10.000, 88.000],  loss: 8.634111, mae: 2.134034, mean_q: 4.634182\n",
      "[30 14 49 90 38 75 20 46 10 95]\n",
      " 12690/50001: episode: 1410, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 44.000, mean reward:  4.889 [ 3.000, 11.000], mean action: 48.556 [10.000, 95.000],  loss: 4.863063, mae: 2.089457, mean_q: 4.556599\n",
      "[62 68 90 94 34 50 50 37 12 40]\n",
      " 12699/50001: episode: 1411, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 28.000, mean reward:  3.111 [-10.000,  6.000], mean action: 52.778 [12.000, 94.000],  loss: 8.605316, mae: 2.141824, mean_q: 4.684916\n",
      "[25 34 85 77 25 14 79 57 37 93]\n",
      " 12708/50001: episode: 1412, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 30.000, mean reward:  3.333 [-10.000,  8.000], mean action: 55.667 [14.000, 93.000],  loss: 7.131699, mae: 2.191567, mean_q: 4.767261\n",
      "[41 50 43 34 38 37  4 13 28 73]\n",
      " 12717/50001: episode: 1413, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 38.000, mean reward:  4.222 [ 2.000,  6.000], mean action: 35.556 [4.000, 73.000],  loss: 6.309446, mae: 2.223788, mean_q: 4.821054\n",
      "[66 48 51 13 10 84 14 41 95 36]\n",
      " 12726/50001: episode: 1414, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 35.000, mean reward:  3.889 [ 2.000,  8.000], mean action: 43.556 [10.000, 95.000],  loss: 7.935380, mae: 2.182715, mean_q: 4.666498\n",
      "[24 41 21 62 54 76 21 63 86  4]\n",
      " 12735/50001: episode: 1415, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 24.000, mean reward:  2.667 [-10.000,  8.000], mean action: 47.556 [4.000, 86.000],  loss: 7.135280, mae: 2.123483, mean_q: 4.567389\n",
      "[30 50 47  4 24 23 57  2 79 79]\n",
      " 12744/50001: episode: 1416, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 24.000, mean reward:  2.667 [-10.000,  5.000], mean action: 40.556 [2.000, 79.000],  loss: 6.084160, mae: 2.148599, mean_q: 4.637563\n",
      "[84 55 79 34 95 94 28 96 55 96]\n",
      " 12753/50001: episode: 1417, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward:  9.000, mean reward:  1.000 [-10.000,  6.000], mean action: 70.222 [28.000, 96.000],  loss: 5.607759, mae: 2.155915, mean_q: 4.594987\n",
      "[37  8  2 25 33 79  1 60 12 93]\n",
      " 12762/50001: episode: 1418, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 34.778 [1.000, 93.000],  loss: 8.443055, mae: 2.238001, mean_q: 4.750482\n",
      "[79 13 50 98 54 32 79 96 37 40]\n",
      " 12771/50001: episode: 1419, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 26.000, mean reward:  2.889 [-10.000,  8.000], mean action: 55.444 [13.000, 98.000],  loss: 8.275494, mae: 2.204399, mean_q: 4.693238\n",
      "[84 74 90 81  2 89  4 28 95 20]\n",
      " 12780/50001: episode: 1420, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 43.000, mean reward:  4.778 [ 3.000,  9.000], mean action: 53.667 [2.000, 95.000],  loss: 6.847255, mae: 2.124336, mean_q: 4.561776\n",
      "[20 83 38 10 45 95 78 68 24 59]\n",
      " 12789/50001: episode: 1421, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 38.000, mean reward:  4.222 [ 3.000,  7.000], mean action: 55.556 [10.000, 95.000],  loss: 6.705487, mae: 2.081663, mean_q: 4.460835\n",
      "[88 48 51 88 83 95 40 51 95 87]\n",
      " 12798/50001: episode: 1422, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: -8.000, mean reward: -0.889 [-10.000,  6.000], mean action: 70.889 [40.000, 95.000],  loss: 9.416136, mae: 2.146785, mean_q: 4.565023\n",
      "[26 32  6 62 24  4 57 30 10 28]\n",
      " 12807/50001: episode: 1423, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 44.000, mean reward:  4.889 [ 3.000,  8.000], mean action: 28.111 [4.000, 62.000],  loss: 7.361758, mae: 2.109262, mean_q: 4.555039\n",
      "[40 54 47 79 30  4 79 68 41  4]\n",
      " 12816/50001: episode: 1424, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward:  8.000, mean reward:  0.889 [-10.000,  5.000], mean action: 45.111 [4.000, 79.000],  loss: 8.856064, mae: 2.065381, mean_q: 4.417698\n",
      "[76 57 24 75 27 49 76 31 10 94]\n",
      " 12825/50001: episode: 1425, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 49.222 [10.000, 94.000],  loss: 6.430058, mae: 2.077338, mean_q: 4.462239\n",
      "[57 12 76 52 37 42 46 93 12 66]\n",
      " 12834/50001: episode: 1426, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 28.000, mean reward:  3.111 [-10.000,  8.000], mean action: 48.444 [12.000, 93.000],  loss: 6.110613, mae: 2.140910, mean_q: 4.565300\n",
      "[12 85 88 88 64 21 24  8 59 14]\n",
      " 12843/50001: episode: 1427, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 22.000, mean reward:  2.444 [-10.000,  7.000], mean action: 50.111 [8.000, 88.000],  loss: 8.726909, mae: 2.122426, mean_q: 4.561698\n",
      "[16 37 89 19 56 12 95 73 96  2]\n",
      " 12852/50001: episode: 1428, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 53.222 [2.000, 96.000],  loss: 7.257309, mae: 2.149087, mean_q: 4.699949\n",
      "[27 20 91 46 44  2 27 12 86 79]\n",
      " 12861/50001: episode: 1429, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 45.222 [2.000, 91.000],  loss: 7.986704, mae: 2.204999, mean_q: 4.853665\n",
      "[ 4  8 37 38 84 82 76  9 84 38]\n",
      " 12870/50001: episode: 1430, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward:  5.000, mean reward:  0.556 [-10.000,  5.000], mean action: 50.667 [8.000, 84.000],  loss: 8.806874, mae: 2.144577, mean_q: 4.562258\n",
      "[73 63 27 14  1 53  4 51 57 11]\n",
      " 12879/50001: episode: 1431, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 37.000, mean reward:  4.111 [ 3.000,  5.000], mean action: 31.222 [1.000, 63.000],  loss: 6.875677, mae: 2.141119, mean_q: 4.555039\n",
      "[31 30 47 78 37 14 15 92 81 82]\n",
      " 12888/50001: episode: 1432, duration: 0.072s, episode steps:   9, steps per second: 126, episode reward: 34.000, mean reward:  3.778 [ 2.000,  5.000], mean action: 52.889 [14.000, 92.000],  loss: 8.257909, mae: 2.133776, mean_q: 4.584146\n",
      "[ 1 11  3 80 32 45 31 88 44 40]\n",
      " 12897/50001: episode: 1433, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 41.556 [3.000, 88.000],  loss: 8.101939, mae: 2.130182, mean_q: 4.614688\n",
      "[19 34 38 77 11 32 79 42 58 48]\n",
      " 12906/50001: episode: 1434, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 42.000, mean reward:  4.667 [ 2.000,  8.000], mean action: 46.556 [11.000, 79.000],  loss: 6.559999, mae: 2.155048, mean_q: 4.633102\n",
      "[21  2 24 11 81 79 10 95 98 98]\n",
      " 12915/50001: episode: 1435, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 26.000, mean reward:  2.889 [-10.000,  8.000], mean action: 55.333 [2.000, 98.000],  loss: 7.607414, mae: 2.083847, mean_q: 4.492796\n",
      "[45 31 12 32 61 41 28 16 31  7]\n",
      " 12924/50001: episode: 1436, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward: 19.000, mean reward:  2.111 [-10.000,  6.000], mean action: 28.778 [7.000, 61.000],  loss: 7.454302, mae: 2.166548, mean_q: 4.625390\n",
      "[57 57 97 14 37 44 91 96 50 62]\n",
      " 12933/50001: episode: 1437, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 24.000, mean reward:  2.667 [-10.000,  8.000], mean action: 60.889 [14.000, 97.000],  loss: 8.203702, mae: 2.153369, mean_q: 4.589352\n",
      "[57 32 79  8 94 14 28  4 81  9]\n",
      " 12942/50001: episode: 1438, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 35.000, mean reward:  3.889 [ 3.000,  5.000], mean action: 38.778 [4.000, 94.000],  loss: 7.774858, mae: 2.084973, mean_q: 4.446397\n",
      "[70 91 34 19 86 48 24 63 34 52]\n",
      " 12951/50001: episode: 1439, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 24.000, mean reward:  2.667 [-10.000,  5.000], mean action: 50.111 [19.000, 91.000],  loss: 7.159237, mae: 2.090714, mean_q: 4.460150\n",
      "[73 75 20 34 96 28 36 13 57 73]\n",
      " 12960/50001: episode: 1440, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 48.000 [13.000, 96.000],  loss: 5.834630, mae: 2.110206, mean_q: 4.590131\n",
      "[68  1 32 28 34 84 78 27 95  5]\n",
      " 12969/50001: episode: 1441, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 42.000, mean reward:  4.667 [ 3.000,  9.000], mean action: 42.667 [1.000, 95.000],  loss: 7.036616, mae: 2.197476, mean_q: 4.756058\n",
      "[12 30 86 63 34 97 53 11 44 53]\n",
      " 12978/50001: episode: 1442, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 21.000, mean reward:  2.333 [-10.000,  6.000], mean action: 52.333 [11.000, 97.000],  loss: 8.096045, mae: 2.168259, mean_q: 4.794913\n",
      "[50 83 37 56 10 69 88 31 31 88]\n",
      " 12987/50001: episode: 1443, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward:  9.000, mean reward:  1.000 [-10.000,  6.000], mean action: 54.778 [10.000, 88.000],  loss: 6.380034, mae: 2.198437, mean_q: 4.756133\n",
      "[24 40 73 12 73  4 68 56 49 21]\n",
      " 12996/50001: episode: 1444, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 19.000, mean reward:  2.111 [-10.000,  5.000], mean action: 44.000 [4.000, 73.000],  loss: 7.449835, mae: 2.144351, mean_q: 4.630497\n",
      "[69 63 98 27 28 24 88 25  2 12]\n",
      " 13005/50001: episode: 1445, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 43.000, mean reward:  4.778 [ 3.000,  8.000], mean action: 40.778 [2.000, 98.000],  loss: 7.974674, mae: 2.087388, mean_q: 4.516996\n",
      "[32 67 45  6 56 27 88 34 31 15]\n",
      " 13014/50001: episode: 1446, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 39.000, mean reward:  4.333 [ 2.000,  8.000], mean action: 41.000 [6.000, 88.000],  loss: 8.115442, mae: 2.111655, mean_q: 4.620594\n",
      "[71 14 27 98 59 95 51 20 68 64]\n",
      " 13023/50001: episode: 1447, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 36.000, mean reward:  4.000 [ 3.000,  7.000], mean action: 55.111 [14.000, 98.000],  loss: 7.209818, mae: 2.152474, mean_q: 4.660774\n",
      "[ 1 48  2 97 84 64 34 53  8 40]\n",
      " 13032/50001: episode: 1448, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 37.000, mean reward:  4.111 [ 3.000,  7.000], mean action: 47.778 [2.000, 97.000],  loss: 6.247679, mae: 2.139517, mean_q: 4.767221\n",
      "[34 66 20  4 38 88  4 95 91  4]\n",
      " 13041/50001: episode: 1449, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward:  9.000, mean reward:  1.000 [-10.000,  8.000], mean action: 45.556 [4.000, 95.000],  loss: 7.011408, mae: 2.194589, mean_q: 4.686219\n",
      "[46 40 14 28 86 56 57  1 60 51]\n",
      " 13050/50001: episode: 1450, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 40.000, mean reward:  4.444 [ 3.000,  6.000], mean action: 43.667 [1.000, 86.000],  loss: 6.088293, mae: 2.181293, mean_q: 4.691671\n",
      "[85 39 79 21  9 23 91 18 50 74]\n",
      " 13059/50001: episode: 1451, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 40.000, mean reward:  4.444 [ 2.000,  9.000], mean action: 44.889 [9.000, 91.000],  loss: 6.957976, mae: 2.175132, mean_q: 4.693181\n",
      "[22  1 19 40 70 75 54 84 66 88]\n",
      " 13068/50001: episode: 1452, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 39.000, mean reward:  4.333 [ 2.000,  9.000], mean action: 55.222 [1.000, 88.000],  loss: 7.807205, mae: 2.182574, mean_q: 4.668364\n",
      "[53 95 47 59 89 88  4 34 68 73]\n",
      " 13077/50001: episode: 1453, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 37.000, mean reward:  4.111 [ 2.000,  7.000], mean action: 61.889 [4.000, 95.000],  loss: 8.717253, mae: 2.172234, mean_q: 4.712491\n",
      "[28 13 38 34 91 60 95 97 98 55]\n",
      " 13086/50001: episode: 1454, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 64.556 [13.000, 98.000],  loss: 8.415739, mae: 2.147811, mean_q: 4.692682\n",
      "[40 63 90 57 76 28 97  4 57 95]\n",
      " 13095/50001: episode: 1455, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 29.000, mean reward:  3.222 [-10.000,  9.000], mean action: 63.000 [4.000, 97.000],  loss: 5.979460, mae: 2.132898, mean_q: 4.627766\n",
      "[90 98 20  1 86 30 74 53 41 88]\n",
      " 13104/50001: episode: 1456, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 54.556 [1.000, 98.000],  loss: 7.349535, mae: 2.106460, mean_q: 4.499849\n",
      "[87 93 20 89 37 37 88 94 39 31]\n",
      " 13113/50001: episode: 1457, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 58.667 [20.000, 94.000],  loss: 8.100373, mae: 2.160671, mean_q: 4.607558\n",
      "[13 48 39 61 13 49  0 54 68 52]\n",
      " 13122/50001: episode: 1458, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 16.000, mean reward:  1.778 [-10.000,  6.000], mean action: 42.667 [0.000, 68.000],  loss: 9.364765, mae: 2.186645, mean_q: 4.643599\n",
      "[71 79 29 28 84 32 24 29 69  9]\n",
      " 13131/50001: episode: 1459, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 42.556 [9.000, 84.000],  loss: 5.351429, mae: 2.136993, mean_q: 4.589899\n",
      "[84 89 59 14 43 30 12 34 53 44]\n",
      " 13140/50001: episode: 1460, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 38.000, mean reward:  4.222 [ 2.000,  8.000], mean action: 42.000 [12.000, 89.000],  loss: 8.789329, mae: 2.123267, mean_q: 4.540033\n",
      "[69 88 50 50  1  2 97 31 19  2]\n",
      " 13149/50001: episode: 1461, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward:  7.000, mean reward:  0.778 [-10.000,  5.000], mean action: 37.778 [1.000, 97.000],  loss: 8.231545, mae: 2.145393, mean_q: 4.595125\n",
      "[30 67 44 48 80 53 33 84 82 50]\n",
      " 13158/50001: episode: 1462, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 40.000, mean reward:  4.444 [ 3.000,  9.000], mean action: 60.111 [33.000, 84.000],  loss: 7.198611, mae: 2.153801, mean_q: 4.604804\n",
      "[ 7 86 32 13 38 23 11 83 61 32]\n",
      " 13167/50001: episode: 1463, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 19.000, mean reward:  2.111 [-10.000,  5.000], mean action: 42.111 [11.000, 86.000],  loss: 6.629022, mae: 2.107867, mean_q: 4.461252\n",
      "[49 51 47 81 30 13 50 44 14 37]\n",
      " 13176/50001: episode: 1464, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 40.778 [13.000, 81.000],  loss: 6.920650, mae: 2.119580, mean_q: 4.545095\n",
      "[96 31 50  2 28 10 18 52 98 31]\n",
      " 13185/50001: episode: 1465, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 35.556 [2.000, 98.000],  loss: 6.445352, mae: 2.148907, mean_q: 4.579148\n",
      "[98 84 34 16 43 72 91 88 91 10]\n",
      " 13194/50001: episode: 1466, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 22.000, mean reward:  2.444 [-10.000,  7.000], mean action: 58.778 [10.000, 91.000],  loss: 8.257288, mae: 2.171896, mean_q: 4.594123\n",
      "[66  2 30 34 59 38 89  4 98 21]\n",
      " 13203/50001: episode: 1467, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 36.000, mean reward:  4.000 [ 2.000,  6.000], mean action: 41.667 [2.000, 98.000],  loss: 7.253865, mae: 2.182193, mean_q: 4.605013\n",
      "[95 72 15 83 97 56 98 82 66 82]\n",
      " 13212/50001: episode: 1468, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 72.333 [15.000, 98.000],  loss: 7.828125, mae: 2.119252, mean_q: 4.492103\n",
      "[66 84 32 80 11 88 48  0 38 50]\n",
      " 13221/50001: episode: 1469, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 36.000, mean reward:  4.000 [ 2.000, 10.000], mean action: 47.889 [0.000, 88.000],  loss: 8.661202, mae: 2.209389, mean_q: 4.717448\n",
      "[87 41 88 34 98 54 11 91 55 98]\n",
      " 13230/50001: episode: 1470, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 21.000, mean reward:  2.333 [-10.000,  5.000], mean action: 63.333 [11.000, 98.000],  loss: 6.184877, mae: 2.102932, mean_q: 4.552833\n",
      "[36 21 49 90 48 37 88 50 95  8]\n",
      " 13239/50001: episode: 1471, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 44.000, mean reward:  4.889 [ 2.000,  8.000], mean action: 54.000 [8.000, 95.000],  loss: 5.276745, mae: 2.103984, mean_q: 4.503820\n",
      "[85 31 30 33 53 46 42 18 21  2]\n",
      " 13248/50001: episode: 1472, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 30.667 [2.000, 53.000],  loss: 7.631329, mae: 2.199464, mean_q: 4.672030\n",
      "[68 41 32 11 85 21 99  4 32 75]\n",
      " 13257/50001: episode: 1473, duration: 0.087s, episode steps:   9, steps per second: 103, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 44.444 [4.000, 99.000],  loss: 8.660007, mae: 2.206055, mean_q: 4.607886\n",
      "[78 54 15 52 66 99 28  8 90 10]\n",
      " 13266/50001: episode: 1474, duration: 0.106s, episode steps:   9, steps per second:  85, episode reward: 38.000, mean reward:  4.222 [ 2.000,  6.000], mean action: 46.889 [8.000, 99.000],  loss: 7.416473, mae: 2.159659, mean_q: 4.549509\n",
      "[24 22  4 59  2 46 48 46 13 61]\n",
      " 13275/50001: episode: 1475, duration: 0.095s, episode steps:   9, steps per second:  95, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 33.444 [2.000, 61.000],  loss: 9.591373, mae: 2.113578, mean_q: 4.552969\n",
      "[39 50 78 37 68 14 23 84 26 34]\n",
      " 13284/50001: episode: 1476, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward: 39.000, mean reward:  4.333 [ 2.000, 10.000], mean action: 46.000 [14.000, 84.000],  loss: 9.741105, mae: 2.082540, mean_q: 4.418228\n",
      "[68 66 89 34 59 86  2  1 34 42]\n",
      " 13293/50001: episode: 1477, duration: 0.076s, episode steps:   9, steps per second: 118, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 45.889 [1.000, 89.000],  loss: 6.635071, mae: 2.041520, mean_q: 4.325050\n",
      "[50  6 44 67 46 75 16 11 34 95]\n",
      " 13302/50001: episode: 1478, duration: 0.085s, episode steps:   9, steps per second: 106, episode reward: 47.000, mean reward:  5.222 [ 3.000, 10.000], mean action: 43.778 [6.000, 95.000],  loss: 8.216621, mae: 2.002076, mean_q: 4.229877\n",
      "[49 88 50 97 74 44 48 62  9 90]\n",
      " 13311/50001: episode: 1479, duration: 0.081s, episode steps:   9, steps per second: 111, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 62.444 [9.000, 97.000],  loss: 9.422514, mae: 2.071015, mean_q: 4.411410\n",
      "[67 50  9 59 42 44 50 92 39 14]\n",
      " 13320/50001: episode: 1480, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 20.000, mean reward:  2.222 [-10.000,  7.000], mean action: 44.333 [9.000, 92.000],  loss: 7.948377, mae: 1.955855, mean_q: 4.213021\n",
      "[78 45 47  8 16  2 26 50 40 64]\n",
      " 13329/50001: episode: 1481, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 35.000, mean reward:  3.889 [ 2.000,  7.000], mean action: 33.111 [2.000, 64.000],  loss: 9.639389, mae: 2.017150, mean_q: 4.311878\n",
      "[ 0 50 68 34 11 28 62 97  5 12]\n",
      " 13338/50001: episode: 1482, duration: 0.070s, episode steps:   9, steps per second: 129, episode reward: 44.000, mean reward:  4.889 [ 3.000,  8.000], mean action: 40.778 [5.000, 97.000],  loss: 9.512089, mae: 1.979664, mean_q: 4.277593\n",
      "[58 14 88 13 36 36 51 40  4 54]\n",
      " 13347/50001: episode: 1483, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 37.333 [4.000, 88.000],  loss: 7.641490, mae: 1.976050, mean_q: 4.315741\n",
      "[ 9 49 72 62 29  1 35 28 27 14]\n",
      " 13356/50001: episode: 1484, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 43.000, mean reward:  4.778 [ 2.000,  7.000], mean action: 35.222 [1.000, 72.000],  loss: 7.101147, mae: 1.970126, mean_q: 4.231236\n",
      "[70 88 90 20 20 50 48 61 42 28]\n",
      " 13365/50001: episode: 1485, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 28.000, mean reward:  3.111 [-10.000,  8.000], mean action: 49.667 [20.000, 90.000],  loss: 7.617393, mae: 2.020696, mean_q: 4.383906\n",
      "[32 24 97 14  1  6 97 21  8 21]\n",
      " 13374/50001: episode: 1486, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward:  5.000, mean reward:  0.556 [-10.000,  4.000], mean action: 32.111 [1.000, 97.000],  loss: 8.478479, mae: 2.065176, mean_q: 4.507256\n",
      "[20  8 74  6 91 23 72 16  5 10]\n",
      " 13383/50001: episode: 1487, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 34.000, mean reward:  3.778 [ 2.000,  6.000], mean action: 33.889 [5.000, 91.000],  loss: 7.594950, mae: 2.097008, mean_q: 4.434875\n",
      "[82 17 49 42 94 27 39 96 24 48]\n",
      " 13392/50001: episode: 1488, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 40.000, mean reward:  4.444 [ 2.000,  9.000], mean action: 48.444 [17.000, 96.000],  loss: 8.592314, mae: 2.131346, mean_q: 4.606416\n",
      "[41 27 55 78 12 37 30 79 40 94]\n",
      " 13401/50001: episode: 1489, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 50.222 [12.000, 94.000],  loss: 7.091619, mae: 2.038902, mean_q: 4.365778\n",
      "[11 98 68  2 50 44 74  2 28 28]\n",
      " 13410/50001: episode: 1490, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward:  9.000, mean reward:  1.000 [-10.000,  6.000], mean action: 43.778 [2.000, 98.000],  loss: 7.560084, mae: 2.079362, mean_q: 4.397775\n",
      "[ 4 97 78 38 67 12 92  8 33 30]\n",
      " 13419/50001: episode: 1491, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 35.000, mean reward:  3.889 [ 2.000,  6.000], mean action: 50.556 [8.000, 97.000],  loss: 7.584477, mae: 2.156410, mean_q: 4.714425\n",
      "[64  2 32 59 48 28 72 52 82 93]\n",
      " 13428/50001: episode: 1492, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 38.000, mean reward:  4.222 [ 1.000,  7.000], mean action: 52.000 [2.000, 93.000],  loss: 7.147442, mae: 2.107546, mean_q: 4.525612\n",
      "[70 13 41 56 69 67 84 93 14 66]\n",
      " 13437/50001: episode: 1493, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 44.000, mean reward:  4.889 [ 3.000,  9.000], mean action: 55.889 [13.000, 93.000],  loss: 12.646777, mae: 2.134016, mean_q: 4.557916\n",
      "[31 66 85  4 25 73 32 62  9 57]\n",
      " 13446/50001: episode: 1494, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 36.000, mean reward:  4.000 [ 2.000,  7.000], mean action: 45.889 [4.000, 85.000],  loss: 8.535833, mae: 2.107159, mean_q: 4.515632\n",
      "[22 91 60 44 30 34 61 58 52 33]\n",
      " 13455/50001: episode: 1495, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 38.000, mean reward:  4.222 [ 3.000,  7.000], mean action: 51.444 [30.000, 91.000],  loss: 6.962891, mae: 1.972741, mean_q: 4.161043\n",
      "[24  1 10 32 92 89 88 93 84 24]\n",
      " 13464/50001: episode: 1496, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 57.000 [1.000, 93.000],  loss: 7.471926, mae: 2.053137, mean_q: 4.365762\n",
      "[98 14 60 53 75 53 96  5 27 81]\n",
      " 13473/50001: episode: 1497, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 21.000, mean reward:  2.333 [-10.000,  6.000], mean action: 51.556 [5.000, 96.000],  loss: 7.229621, mae: 2.103304, mean_q: 4.406626\n",
      "[49 13 64 61 95 87 28 33 62 12]\n",
      " 13482/50001: episode: 1498, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 41.000, mean reward:  4.556 [ 2.000,  9.000], mean action: 50.556 [12.000, 95.000],  loss: 8.119315, mae: 2.077053, mean_q: 4.465663\n",
      "[95 46 75 89 37 48 64 92 66 14]\n",
      " 13491/50001: episode: 1499, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 59.000 [14.000, 92.000],  loss: 6.751184, mae: 2.041532, mean_q: 4.382444\n",
      "[57 95 47 25 97 28 89  4 57 39]\n",
      " 13500/50001: episode: 1500, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 22.000, mean reward:  2.444 [-10.000,  5.000], mean action: 53.444 [4.000, 97.000],  loss: 4.880087, mae: 2.109173, mean_q: 4.506058\n",
      "[75 97 28 74 46 94 88 24 27 13]\n",
      " 13509/50001: episode: 1501, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 43.000, mean reward:  4.778 [ 3.000,  8.000], mean action: 54.556 [13.000, 97.000],  loss: 6.767297, mae: 2.101968, mean_q: 4.487639\n",
      "[27 13  4 14 80  1 64 41  5 50]\n",
      " 13518/50001: episode: 1502, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 30.222 [1.000, 80.000],  loss: 9.150500, mae: 2.188502, mean_q: 4.683326\n",
      "[73 50 27 37 29 94  1 95 44 41]\n",
      " 13527/50001: episode: 1503, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 46.444 [1.000, 95.000],  loss: 9.888256, mae: 2.101893, mean_q: 4.506477\n",
      "[43 30 91 67 34 27  6 94 94 81]\n",
      " 13536/50001: episode: 1504, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 21.000, mean reward:  2.333 [-10.000,  6.000], mean action: 58.222 [6.000, 94.000],  loss: 6.684803, mae: 2.070620, mean_q: 4.517185\n",
      "[26 73  4 11 99 40 28 46 74 53]\n",
      " 13545/50001: episode: 1505, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 41.000, mean reward:  4.556 [ 2.000,  6.000], mean action: 47.556 [4.000, 99.000],  loss: 7.581236, mae: 2.089924, mean_q: 4.450281\n",
      "[41  4 59 98  6 54 20 83 12 98]\n",
      " 13554/50001: episode: 1506, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 23.000, mean reward:  2.556 [-10.000,  8.000], mean action: 48.222 [4.000, 98.000],  loss: 7.685718, mae: 2.105601, mean_q: 4.491394\n",
      "[19 34 37 70 66 42 42 88 98 50]\n",
      " 13563/50001: episode: 1507, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 30.000, mean reward:  3.333 [-10.000,  8.000], mean action: 58.556 [34.000, 98.000],  loss: 6.761166, mae: 2.129773, mean_q: 4.526914\n",
      "[31 14 94 80 14 34 37 68 63 64]\n",
      " 13572/50001: episode: 1508, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 20.000, mean reward:  2.222 [-10.000,  6.000], mean action: 52.000 [14.000, 94.000],  loss: 7.499662, mae: 2.114378, mean_q: 4.495931\n",
      "[46 66 96 13 99 78 28  1 95 79]\n",
      " 13581/50001: episode: 1509, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 42.000, mean reward:  4.667 [ 3.000,  9.000], mean action: 61.667 [1.000, 99.000],  loss: 7.282988, mae: 2.171828, mean_q: 4.593647\n",
      "[14 88 23 97 33 78 79 31 66 66]\n",
      " 13590/50001: episode: 1510, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 62.333 [23.000, 97.000],  loss: 7.471377, mae: 2.129747, mean_q: 4.576509\n",
      "[77 41 30 48 74 82 98 66  0 31]\n",
      " 13599/50001: episode: 1511, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 42.000, mean reward:  4.667 [ 2.000,  8.000], mean action: 52.222 [0.000, 98.000],  loss: 8.499659, mae: 2.164445, mean_q: 4.572484\n",
      "[ 4 13 66 42 63 43 42 31 68  4]\n",
      " 13608/50001: episode: 1512, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward:  4.000, mean reward:  0.444 [-10.000,  5.000], mean action: 41.333 [4.000, 68.000],  loss: 7.834234, mae: 2.185492, mean_q: 4.593503\n",
      "[43 23  9 84  7 50 88 51 51 62]\n",
      " 13617/50001: episode: 1513, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 47.222 [7.000, 88.000],  loss: 5.918853, mae: 2.143930, mean_q: 4.522120\n",
      "[23 34 78 14 98 94 62 34 95 12]\n",
      " 13626/50001: episode: 1514, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 30.000, mean reward:  3.333 [-10.000,  9.000], mean action: 57.889 [12.000, 98.000],  loss: 5.175220, mae: 2.157780, mean_q: 4.603540\n",
      "[86 85 37 96  1 21 48 24 50 61]\n",
      " 13635/50001: episode: 1515, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 40.000, mean reward:  4.444 [ 3.000,  8.000], mean action: 47.000 [1.000, 96.000],  loss: 7.703115, mae: 2.209772, mean_q: 4.699105\n",
      "[ 0 31 88 98  5 17 32 33 42 37]\n",
      " 13644/50001: episode: 1516, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 42.000, mean reward:  4.667 [ 3.000,  9.000], mean action: 42.556 [5.000, 98.000],  loss: 10.607543, mae: 2.262626, mean_q: 4.793334\n",
      "[ 3 79  4 76 69 66 41 21 44 13]\n",
      " 13653/50001: episode: 1517, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 42.000, mean reward:  4.667 [ 3.000, 10.000], mean action: 45.889 [4.000, 79.000],  loss: 8.631644, mae: 2.236299, mean_q: 4.790204\n",
      "[16 13 18 77 99 13 37 21 41 99]\n",
      " 13662/50001: episode: 1518, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward:  6.000, mean reward:  0.667 [-10.000,  6.000], mean action: 46.444 [13.000, 99.000],  loss: 9.750584, mae: 2.138948, mean_q: 4.521324\n",
      "[97 88 48 48 16 28 76 12 66 74]\n",
      " 13671/50001: episode: 1519, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 50.667 [12.000, 88.000],  loss: 7.803474, mae: 2.078521, mean_q: 4.396329\n",
      "[ 4 89 63 33 30 34 48 45 85 48]\n",
      " 13680/50001: episode: 1520, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 21.000, mean reward:  2.333 [-10.000,  6.000], mean action: 52.778 [30.000, 89.000],  loss: 6.677320, mae: 2.084959, mean_q: 4.494289\n",
      "[49 32 83  1 37 13 28 37 76 40]\n",
      " 13689/50001: episode: 1521, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 26.000, mean reward:  2.889 [-10.000,  6.000], mean action: 38.556 [1.000, 83.000],  loss: 6.250517, mae: 2.090506, mean_q: 4.459909\n",
      "[85 21 64 67 64 84 41 44 95 12]\n",
      " 13698/50001: episode: 1522, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 31.000, mean reward:  3.444 [-10.000, 10.000], mean action: 54.667 [12.000, 95.000],  loss: 8.385210, mae: 2.089074, mean_q: 4.443827\n",
      "[79  5 17 14 48 96  8 43 24 52]\n",
      " 13707/50001: episode: 1523, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 34.000, mean reward:  3.778 [ 2.000,  7.000], mean action: 34.111 [5.000, 96.000],  loss: 8.542912, mae: 2.116906, mean_q: 4.490113\n",
      "[14 97 68 83 88 35 39 34 28 28]\n",
      " 13716/50001: episode: 1524, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 24.000, mean reward:  2.667 [-10.000,  8.000], mean action: 55.556 [28.000, 97.000],  loss: 6.490597, mae: 2.081822, mean_q: 4.359498\n",
      "[41 96 23 78  2 80 42  6 80 88]\n",
      " 13725/50001: episode: 1525, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 26.000, mean reward:  2.889 [-10.000,  9.000], mean action: 55.000 [2.000, 96.000],  loss: 8.712470, mae: 2.034961, mean_q: 4.320433\n",
      "[80 89 97 88 14 37 92 37 37 62]\n",
      " 13734/50001: episode: 1526, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward:  7.000, mean reward:  0.778 [-10.000,  6.000], mean action: 61.444 [14.000, 97.000],  loss: 7.958503, mae: 2.069948, mean_q: 4.440382\n",
      "[46 36 40 83 99 32 38  9 41 87]\n",
      " 13743/50001: episode: 1527, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 34.000, mean reward:  3.778 [ 2.000,  6.000], mean action: 51.667 [9.000, 99.000],  loss: 10.005143, mae: 2.110002, mean_q: 4.478509\n",
      "[96  6 42 80 38 75 36  6 34 50]\n",
      " 13752/50001: episode: 1528, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 28.000, mean reward:  3.111 [-10.000, 10.000], mean action: 40.778 [6.000, 80.000],  loss: 9.930132, mae: 2.009658, mean_q: 4.292088\n",
      "[44 57 48 91 33 79  4 79 44 40]\n",
      " 13761/50001: episode: 1529, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward:  8.000, mean reward:  0.889 [-10.000,  6.000], mean action: 52.778 [4.000, 91.000],  loss: 6.312205, mae: 2.005828, mean_q: 4.313647\n",
      "[57 37  7 57 68 78 83 72 61 31]\n",
      " 13770/50001: episode: 1530, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 21.000, mean reward:  2.333 [-10.000,  9.000], mean action: 54.889 [7.000, 83.000],  loss: 7.157779, mae: 2.017126, mean_q: 4.304645\n",
      "[83 95 75  1 36 13 83 56 53 42]\n",
      " 13779/50001: episode: 1531, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 26.000, mean reward:  2.889 [-10.000,  6.000], mean action: 50.444 [1.000, 95.000],  loss: 7.091892, mae: 2.029237, mean_q: 4.402657\n",
      "[96 46 51 49 37 57 51 45 48 93]\n",
      " 13788/50001: episode: 1532, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 53.000 [37.000, 93.000],  loss: 9.284411, mae: 2.065254, mean_q: 4.423935\n",
      "[74 95 13 77 64 77  1 95 27 62]\n",
      " 13797/50001: episode: 1533, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 10.000, mean reward:  1.111 [-10.000,  6.000], mean action: 56.778 [1.000, 95.000],  loss: 7.617011, mae: 2.076183, mean_q: 4.561440\n",
      "[45  5 34 69 42 46 96 92 57 95]\n",
      " 13806/50001: episode: 1534, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 42.000, mean reward:  4.667 [ 3.000, 10.000], mean action: 59.556 [5.000, 96.000],  loss: 8.209505, mae: 2.111938, mean_q: 4.520208\n",
      "[ 3 72 24 95 36 59 67 64 84 12]\n",
      " 13815/50001: episode: 1535, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 41.000, mean reward:  4.556 [ 2.000, 10.000], mean action: 57.000 [12.000, 95.000],  loss: 7.779894, mae: 2.113790, mean_q: 4.526725\n",
      "[26 48 99 61 12 12 44 48 12  2]\n",
      " 13824/50001: episode: 1536, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: -8.000, mean reward: -0.889 [-10.000,  6.000], mean action: 37.556 [2.000, 99.000],  loss: 6.530092, mae: 2.056859, mean_q: 4.401079\n",
      "[80  6 36 28 53 43  1 93 62 93]\n",
      " 13833/50001: episode: 1537, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 46.111 [1.000, 93.000],  loss: 8.280567, mae: 2.165740, mean_q: 4.598524\n",
      "[32 41 73 66 52 34 21 27 94 12]\n",
      " 13842/50001: episode: 1538, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 42.000, mean reward:  4.667 [ 2.000,  9.000], mean action: 46.667 [12.000, 94.000],  loss: 9.662246, mae: 2.099903, mean_q: 4.466388\n",
      "[ 5 95  4 63 54 30 13 90 14 24]\n",
      " 13851/50001: episode: 1539, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 40.000, mean reward:  4.444 [ 3.000,  6.000], mean action: 43.000 [4.000, 95.000],  loss: 8.597950, mae: 2.054435, mean_q: 4.412859\n",
      "[74 26 96 11 34 89 41 49 46 14]\n",
      " 13860/50001: episode: 1540, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 41.000, mean reward:  4.556 [ 2.000,  7.000], mean action: 45.111 [11.000, 96.000],  loss: 8.636486, mae: 2.048050, mean_q: 4.441886\n",
      "[61 66 12 85 99 53  1 27 51 74]\n",
      " 13869/50001: episode: 1541, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 52.000 [1.000, 99.000],  loss: 9.465107, mae: 2.034810, mean_q: 4.333549\n",
      "[20 54 32 14 33 17 14  8 86 69]\n",
      " 13878/50001: episode: 1542, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 20.000, mean reward:  2.222 [-10.000,  5.000], mean action: 36.333 [8.000, 86.000],  loss: 9.170586, mae: 2.005159, mean_q: 4.354183\n",
      "[69 95 73 66  2 67 48 50 58 99]\n",
      " 13887/50001: episode: 1543, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 39.000, mean reward:  4.333 [ 2.000,  6.000], mean action: 62.000 [2.000, 99.000],  loss: 7.462170, mae: 2.015812, mean_q: 4.292280\n",
      "[52 59 30 32 89 49 70 30 55 54]\n",
      " 13896/50001: episode: 1544, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 21.000, mean reward:  2.333 [-10.000,  5.000], mean action: 52.000 [30.000, 89.000],  loss: 6.819687, mae: 2.047852, mean_q: 4.445227\n",
      "[25 50 56 67 65 40 15 42 42 12]\n",
      " 13905/50001: episode: 1545, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 24.000, mean reward:  2.667 [-10.000,  9.000], mean action: 43.222 [12.000, 67.000],  loss: 6.144218, mae: 2.062868, mean_q: 4.337484\n",
      "[50 90 39 32 98 62 53 17 45 11]\n",
      " 13914/50001: episode: 1546, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 32.000, mean reward:  3.556 [ 2.000,  5.000], mean action: 49.667 [11.000, 98.000],  loss: 7.028135, mae: 2.109459, mean_q: 4.500337\n",
      "[23 93 44 32 77 58  9 28 51 74]\n",
      " 13923/50001: episode: 1547, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 36.000, mean reward:  4.000 [ 2.000,  6.000], mean action: 51.778 [9.000, 93.000],  loss: 6.285485, mae: 2.121022, mean_q: 4.504969\n",
      "[25 88 60 86 30 34 28 92 92 32]\n",
      " 13932/50001: episode: 1548, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 60.222 [28.000, 92.000],  loss: 6.250366, mae: 2.116048, mean_q: 4.475139\n",
      "[84 41 67 97 14 42 92 88 83 88]\n",
      " 13941/50001: episode: 1549, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 68.000 [14.000, 97.000],  loss: 7.046620, mae: 2.164958, mean_q: 4.616089\n",
      "[74 99 35  1 90 82 27 95 33 83]\n",
      " 13950/50001: episode: 1550, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 38.000, mean reward:  4.222 [ 3.000,  8.000], mean action: 60.556 [1.000, 99.000],  loss: 6.373312, mae: 2.119442, mean_q: 4.569363\n",
      "[20 44 56 28 13 38 45 83 12  1]\n",
      " 13959/50001: episode: 1551, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 35.556 [1.000, 83.000],  loss: 8.421307, mae: 2.231993, mean_q: 4.686880\n",
      "[96 34 25 48 98 94 41 34 54 97]\n",
      " 13968/50001: episode: 1552, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 58.333 [25.000, 98.000],  loss: 5.794506, mae: 2.184381, mean_q: 4.632133\n",
      "[30 17 47 12 62 57 21 27 78 33]\n",
      " 13977/50001: episode: 1553, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 36.000, mean reward:  4.000 [ 3.000,  5.000], mean action: 39.333 [12.000, 78.000],  loss: 8.562337, mae: 2.199843, mean_q: 4.708953\n",
      "[61 41 17 28 36 53 91 88 34 76]\n",
      " 13986/50001: episode: 1554, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 43.000, mean reward:  4.778 [ 2.000,  9.000], mean action: 51.556 [17.000, 91.000],  loss: 8.029924, mae: 2.186898, mean_q: 4.587637\n",
      "[99 53 27 52 46 34 28 25 66 69]\n",
      " 13995/50001: episode: 1555, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 44.444 [25.000, 69.000],  loss: 6.592936, mae: 2.204566, mean_q: 4.688522\n",
      "[13 95 11 99 48 71 52 30 27 64]\n",
      " 14004/50001: episode: 1556, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 34.000, mean reward:  3.778 [ 2.000,  5.000], mean action: 55.222 [11.000, 99.000],  loss: 8.304066, mae: 2.217072, mean_q: 4.649209\n",
      "[66 53 72 37 28  1 34 59 39 85]\n",
      " 14013/50001: episode: 1557, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 35.000, mean reward:  3.889 [ 2.000,  6.000], mean action: 45.333 [1.000, 85.000],  loss: 5.457184, mae: 2.276572, mean_q: 4.816079\n",
      "[57 12 34 42 20  4 30 94 42 82]\n",
      " 14022/50001: episode: 1558, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 40.000 [4.000, 94.000],  loss: 8.262662, mae: 2.300400, mean_q: 4.860132\n",
      "[93 45 51 23  9 59 88 46  9 42]\n",
      " 14031/50001: episode: 1559, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 41.333 [9.000, 88.000],  loss: 8.549419, mae: 2.284440, mean_q: 4.802115\n",
      "[11 98 97 12 37 37 32 46 99 46]\n",
      " 14040/50001: episode: 1560, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward:  9.000, mean reward:  1.000 [-10.000,  5.000], mean action: 56.000 [12.000, 99.000],  loss: 7.768717, mae: 2.243621, mean_q: 4.793943\n",
      "[ 6 23 52 57 37 95 63 27  2 63]\n",
      " 14049/50001: episode: 1561, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 46.556 [2.000, 95.000],  loss: 7.116073, mae: 2.144082, mean_q: 4.509831\n",
      "[55 23 61 12 25 63 45 78 48 13]\n",
      " 14058/50001: episode: 1562, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 41.000, mean reward:  4.556 [ 2.000,  9.000], mean action: 40.889 [12.000, 78.000],  loss: 8.310111, mae: 2.214762, mean_q: 4.690720\n",
      "[48  5 32 89 51 75 20 20 74  9]\n",
      " 14067/50001: episode: 1563, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 41.667 [5.000, 89.000],  loss: 9.625522, mae: 2.192254, mean_q: 4.628089\n",
      "[61 92 88 11 28 20 10 13 60 37]\n",
      " 14076/50001: episode: 1564, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 44.000, mean reward:  4.889 [ 3.000,  8.000], mean action: 39.889 [10.000, 92.000],  loss: 7.361452, mae: 2.180891, mean_q: 4.642481\n",
      "[27 88 28 37 33 53 38 63 73 48]\n",
      " 14085/50001: episode: 1565, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 38.000, mean reward:  4.222 [ 2.000,  9.000], mean action: 51.222 [28.000, 88.000],  loss: 7.526255, mae: 2.164039, mean_q: 4.632014\n",
      "[28 42 30 62 13  6 50 88 14 82]\n",
      " 14094/50001: episode: 1566, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 40.000, mean reward:  4.444 [ 3.000,  6.000], mean action: 43.000 [6.000, 88.000],  loss: 7.476840, mae: 2.104394, mean_q: 4.418048\n",
      "[89  2 32 28 88 93 74 28 46  5]\n",
      " 14103/50001: episode: 1567, duration: 0.067s, episode steps:   9, steps per second: 133, episode reward: 27.000, mean reward:  3.000 [-10.000,  6.000], mean action: 44.000 [2.000, 93.000],  loss: 9.643696, mae: 2.123638, mean_q: 4.504104\n",
      "[77 76 18  1 28  4 46 94 19 19]\n",
      " 14112/50001: episode: 1568, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 23.000, mean reward:  2.556 [-10.000,  5.000], mean action: 33.889 [1.000, 94.000],  loss: 5.983829, mae: 2.104786, mean_q: 4.398027\n",
      "[32  8 75 95 43 48 88 41 37  4]\n",
      " 14121/50001: episode: 1569, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 43.000, mean reward:  4.778 [ 2.000,  8.000], mean action: 48.778 [4.000, 95.000],  loss: 9.778991, mae: 2.101931, mean_q: 4.446778\n",
      "[91 46 98 34 28 89 95 99 84 41]\n",
      " 14130/50001: episode: 1570, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 68.222 [28.000, 99.000],  loss: 8.508087, mae: 2.052838, mean_q: 4.439627\n",
      "[43 66 40 47 59 50 99 66 89 95]\n",
      " 14139/50001: episode: 1571, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 27.000, mean reward:  3.000 [-10.000, 10.000], mean action: 67.889 [40.000, 99.000],  loss: 9.901506, mae: 2.075553, mean_q: 4.440068\n",
      "[73 10 44 33 12 75 23 95 71 12]\n",
      " 14148/50001: episode: 1572, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 23.000, mean reward:  2.556 [-10.000,  8.000], mean action: 41.667 [10.000, 95.000],  loss: 7.174981, mae: 2.042296, mean_q: 4.387197\n",
      "[ 7  1 88 37 60 34  9  4 84 79]\n",
      " 14157/50001: episode: 1573, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 42.000, mean reward:  4.667 [ 4.000,  6.000], mean action: 44.000 [1.000, 88.000],  loss: 6.909407, mae: 2.036443, mean_q: 4.360357\n",
      "[78 36 60 90 53 75 51 50  1 66]\n",
      " 14166/50001: episode: 1574, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 43.000, mean reward:  4.778 [ 2.000,  7.000], mean action: 53.556 [1.000, 90.000],  loss: 7.851422, mae: 2.032139, mean_q: 4.348800\n",
      "[95 45 32 73 10 72 52 27 71 97]\n",
      " 14175/50001: episode: 1575, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 36.000, mean reward:  4.000 [ 2.000,  9.000], mean action: 53.222 [10.000, 97.000],  loss: 6.764510, mae: 2.100811, mean_q: 4.485741\n",
      "[97 67 90 96 95 46 96 89 14 14]\n",
      " 14184/50001: episode: 1576, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 11.000, mean reward:  1.222 [-10.000,  6.000], mean action: 67.444 [14.000, 96.000],  loss: 10.186722, mae: 2.126810, mean_q: 4.497236\n",
      "[42 27 13 73 64 62 69 21  9 95]\n",
      " 14193/50001: episode: 1577, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 41.000, mean reward:  4.556 [ 2.000, 11.000], mean action: 48.111 [9.000, 95.000],  loss: 8.672912, mae: 2.081681, mean_q: 4.474497\n",
      "[21 34 25 24 86 16 14 77 50 98]\n",
      " 14202/50001: episode: 1578, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 47.111 [14.000, 98.000],  loss: 7.237119, mae: 2.087120, mean_q: 4.478003\n",
      "[45 27 32 47  1 24 19 61 14 42]\n",
      " 14211/50001: episode: 1579, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 40.000, mean reward:  4.444 [ 3.000,  6.000], mean action: 29.667 [1.000, 61.000],  loss: 7.672758, mae: 2.113774, mean_q: 4.483283\n",
      "[54 41 54 27 95 34  2 26 46 15]\n",
      " 14220/50001: episode: 1580, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 26.000, mean reward:  2.889 [-10.000,  6.000], mean action: 37.778 [2.000, 95.000],  loss: 9.177771, mae: 2.136425, mean_q: 4.525621\n",
      "[15 51 90 43 59 37 62 97 82 67]\n",
      " 14229/50001: episode: 1581, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 65.333 [37.000, 97.000],  loss: 8.809019, mae: 2.119601, mean_q: 4.531309\n",
      "[43 89 39 60 98 76 53 23  2  4]\n",
      " 14238/50001: episode: 1582, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 43.000, mean reward:  4.778 [ 2.000,  7.000], mean action: 49.333 [2.000, 98.000],  loss: 6.792973, mae: 2.053544, mean_q: 4.358057\n",
      "[23 95 89 76 83  4 13 89 34 37]\n",
      " 14247/50001: episode: 1583, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 30.000, mean reward:  3.333 [-10.000,  8.000], mean action: 57.778 [4.000, 95.000],  loss: 8.088717, mae: 2.177014, mean_q: 4.656775\n",
      "[ 8 95 91 34 27 13 48 80 94 20]\n",
      " 14256/50001: episode: 1584, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 35.000, mean reward:  3.889 [ 2.000,  5.000], mean action: 55.778 [13.000, 95.000],  loss: 9.908706, mae: 2.197749, mean_q: 4.703475\n",
      "[79 76 28 90 30 12 73 12 98 72]\n",
      " 14265/50001: episode: 1585, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 19.000, mean reward:  2.111 [-10.000,  6.000], mean action: 54.556 [12.000, 98.000],  loss: 8.063302, mae: 2.118266, mean_q: 4.436031\n",
      "[36 27 42 12 39 92  0 99 45 97]\n",
      " 14274/50001: episode: 1586, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 37.000, mean reward:  4.111 [ 2.000,  9.000], mean action: 50.333 [0.000, 99.000],  loss: 5.870964, mae: 2.028624, mean_q: 4.331649\n",
      "[66 53 27 53 32 42 27 57 73 81]\n",
      " 14283/50001: episode: 1587, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward:  6.000, mean reward:  0.667 [-10.000,  5.000], mean action: 49.444 [27.000, 81.000],  loss: 9.116625, mae: 2.038509, mean_q: 4.371960\n",
      "[60 41  4 13 58 57 57  6 53 23]\n",
      " 14292/50001: episode: 1588, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 34.667 [4.000, 58.000],  loss: 9.154100, mae: 2.101447, mean_q: 4.476251\n",
      "[43 31 30 46 53 31 30 21 90 71]\n",
      " 14301/50001: episode: 1589, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward:  8.000, mean reward:  0.889 [-10.000,  6.000], mean action: 44.778 [21.000, 90.000],  loss: 8.313585, mae: 2.114494, mean_q: 4.503206\n",
      "[ 2 28 86 63 80 37  4 79 88 81]\n",
      " 14310/50001: episode: 1590, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 60.667 [4.000, 88.000],  loss: 7.113005, mae: 2.063019, mean_q: 4.398973\n",
      "[64  8 96 59 36 72 83  1 63 50]\n",
      " 14319/50001: episode: 1591, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 40.000, mean reward:  4.444 [ 2.000, 10.000], mean action: 52.000 [1.000, 96.000],  loss: 6.448465, mae: 2.119723, mean_q: 4.483041\n",
      "[48 54 28 93 50 13 83 54 85 12]\n",
      " 14328/50001: episode: 1592, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 27.000, mean reward:  3.000 [-10.000,  9.000], mean action: 52.444 [12.000, 93.000],  loss: 9.042194, mae: 2.182426, mean_q: 4.642158\n",
      "[40 95 35 53 54 52 13 74 31 92]\n",
      " 14337/50001: episode: 1593, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 55.444 [13.000, 95.000],  loss: 7.469891, mae: 2.147982, mean_q: 4.578412\n",
      "[68 87 56 46 97 13 37  4 88 50]\n",
      " 14346/50001: episode: 1594, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 45.000, mean reward:  5.000 [ 3.000,  7.000], mean action: 53.111 [4.000, 97.000],  loss: 7.890527, mae: 2.172038, mean_q: 4.527750\n",
      "[ 2 78 60 84 50 40 92 59 85 48]\n",
      " 14355/50001: episode: 1595, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 38.000, mean reward:  4.222 [ 3.000,  8.000], mean action: 66.222 [40.000, 92.000],  loss: 7.258500, mae: 2.196948, mean_q: 4.700433\n",
      "[20 19 12 56 77 60 74 30 89 74]\n",
      " 14364/50001: episode: 1596, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 23.000, mean reward:  2.556 [-10.000,  5.000], mean action: 54.556 [12.000, 89.000],  loss: 7.000119, mae: 2.207587, mean_q: 4.613338\n",
      "[64  8 53  2 34 41 97  9 10 66]\n",
      " 14373/50001: episode: 1597, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 35.556 [2.000, 97.000],  loss: 7.326644, mae: 2.144232, mean_q: 4.450903\n",
      "[18 48 40 89 76 28 99 48 23 50]\n",
      " 14382/50001: episode: 1598, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 55.667 [23.000, 99.000],  loss: 7.061318, mae: 2.113312, mean_q: 4.452916\n",
      "[26  8 30 28 74 56 98 83  4 83]\n",
      " 14391/50001: episode: 1599, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 51.556 [4.000, 98.000],  loss: 5.620481, mae: 2.183098, mean_q: 4.528444\n",
      "[79 87 39 92  9 88 96 51 95 42]\n",
      " 14400/50001: episode: 1600, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 43.000, mean reward:  4.778 [ 2.000,  9.000], mean action: 66.556 [9.000, 96.000],  loss: 7.654292, mae: 2.169354, mean_q: 4.587847\n",
      "[36  1 65 77 34 46 61 45 19 97]\n",
      " 14409/50001: episode: 1601, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 40.000, mean reward:  4.444 [ 2.000,  9.000], mean action: 49.444 [1.000, 97.000],  loss: 8.137625, mae: 2.195572, mean_q: 4.740356\n",
      "[28 46 25 94 67 57 38 95 99 12]\n",
      " 14418/50001: episode: 1602, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 41.000, mean reward:  4.556 [ 2.000,  9.000], mean action: 59.222 [12.000, 99.000],  loss: 7.550610, mae: 2.203911, mean_q: 4.544302\n",
      "[ 2 95 33 99 97 42 95  2  4 57]\n",
      " 14427/50001: episode: 1603, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward:  9.000, mean reward:  1.000 [-10.000,  5.000], mean action: 58.222 [2.000, 99.000],  loss: 8.139878, mae: 2.131211, mean_q: 4.525712\n",
      "[17 67 68 32 88 13 74 56 31 58]\n",
      " 14436/50001: episode: 1604, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 54.111 [13.000, 88.000],  loss: 7.118805, mae: 2.112210, mean_q: 4.506774\n",
      "[90 89 12 47  2 12 12 18 96 50]\n",
      " 14445/50001: episode: 1605, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward:  8.000, mean reward:  0.889 [-10.000,  8.000], mean action: 37.556 [2.000, 96.000],  loss: 8.137569, mae: 2.126848, mean_q: 4.497053\n",
      "[23 68 88 27 68 34 48 56 95 50]\n",
      " 14454/50001: episode: 1606, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 32.000, mean reward:  3.556 [-10.000,  9.000], mean action: 59.333 [27.000, 95.000],  loss: 6.478608, mae: 2.213132, mean_q: 4.709925\n",
      "[76 45 54 30 89 24 62 90 66 10]\n",
      " 14463/50001: episode: 1607, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 52.222 [10.000, 90.000],  loss: 8.208331, mae: 2.238176, mean_q: 4.640913\n",
      "[14 41 51 75 35 97 88 30 95 88]\n",
      " 14472/50001: episode: 1608, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 28.000, mean reward:  3.111 [-10.000,  9.000], mean action: 66.667 [30.000, 97.000],  loss: 7.704930, mae: 2.209278, mean_q: 4.590483\n",
      "[46 64 72 56  2 98 52  6 53 10]\n",
      " 14481/50001: episode: 1609, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 37.000, mean reward:  4.111 [ 2.000,  5.000], mean action: 45.889 [2.000, 98.000],  loss: 8.232826, mae: 2.162280, mean_q: 4.565663\n",
      "[19 28 61 64 78 57 86 31 52 14]\n",
      " 14490/50001: episode: 1610, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 52.333 [14.000, 86.000],  loss: 6.620989, mae: 2.190306, mean_q: 4.667287\n",
      "[20 82 54 11 13 76 42 73 53 34]\n",
      " 14499/50001: episode: 1611, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 43.000, mean reward:  4.778 [ 3.000, 10.000], mean action: 48.667 [11.000, 82.000],  loss: 9.219713, mae: 2.160166, mean_q: 4.526802\n",
      "[66  9 71 79 48 62 87 97  4 97]\n",
      " 14508/50001: episode: 1612, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 61.556 [4.000, 97.000],  loss: 9.127480, mae: 2.161002, mean_q: 4.633854\n",
      "[10 51 11 60 77 88 20 61 72 88]\n",
      " 14517/50001: episode: 1613, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 16.000, mean reward:  1.778 [-10.000,  5.000], mean action: 58.667 [11.000, 88.000],  loss: 6.224339, mae: 2.174958, mean_q: 4.620014\n",
      "[20 64 99 77 20 57 88 63 37  2]\n",
      " 14526/50001: episode: 1614, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 28.000, mean reward:  3.111 [-10.000,  8.000], mean action: 56.333 [2.000, 99.000],  loss: 9.199694, mae: 2.149182, mean_q: 4.505837\n",
      "[ 2 30 36 39 94 28 80 34 98 54]\n",
      " 14535/50001: episode: 1615, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 38.000, mean reward:  4.222 [ 2.000,  8.000], mean action: 54.778 [28.000, 98.000],  loss: 8.785905, mae: 2.112556, mean_q: 4.506836\n",
      "[10 12 41 41 53 97 34 63  1 31]\n",
      " 14544/50001: episode: 1616, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 31.000, mean reward:  3.444 [-10.000,  7.000], mean action: 41.444 [1.000, 97.000],  loss: 8.061771, mae: 2.219739, mean_q: 4.789899\n",
      "[99  5 32 17 46 10 67 41 62 44]\n",
      " 14553/50001: episode: 1617, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 35.000, mean reward:  3.889 [ 2.000,  5.000], mean action: 36.000 [5.000, 67.000],  loss: 6.754185, mae: 2.116656, mean_q: 4.544096\n",
      "[26 42 41 52 12 38 21 96 12 54]\n",
      " 14562/50001: episode: 1618, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 19.000, mean reward:  2.111 [-10.000,  5.000], mean action: 40.889 [12.000, 96.000],  loss: 7.107328, mae: 2.138421, mean_q: 4.498225\n",
      "[74  1 12 47 33 32 16 95 40 74]\n",
      " 14571/50001: episode: 1619, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 24.000, mean reward:  2.667 [-10.000,  8.000], mean action: 38.889 [1.000, 95.000],  loss: 7.701140, mae: 2.083257, mean_q: 4.436579\n",
      "[58 14 40 17 44 88  0 46 59 57]\n",
      " 14580/50001: episode: 1620, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 40.556 [0.000, 88.000],  loss: 8.541439, mae: 2.256549, mean_q: 4.685457\n",
      "[20 21  2 82 63  0 93 14 52 84]\n",
      " 14589/50001: episode: 1621, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 37.000, mean reward:  4.111 [ 2.000,  5.000], mean action: 45.667 [0.000, 93.000],  loss: 8.813305, mae: 2.160340, mean_q: 4.565931\n",
      "[11 67 63 99 98 42 32 88 28 40]\n",
      " 14598/50001: episode: 1622, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 41.000, mean reward:  4.556 [ 3.000,  6.000], mean action: 61.889 [28.000, 99.000],  loss: 9.078907, mae: 2.191965, mean_q: 4.585014\n",
      "[23 88 20 75 41 64 41 92 81 28]\n",
      " 14607/50001: episode: 1623, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 26.000, mean reward:  2.889 [-10.000,  8.000], mean action: 58.889 [20.000, 92.000],  loss: 7.328285, mae: 2.108440, mean_q: 4.472317\n",
      "[43 33 94 67 28 34 84 45 48 88]\n",
      " 14616/50001: episode: 1624, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 42.000, mean reward:  4.667 [ 2.000,  9.000], mean action: 57.889 [28.000, 94.000],  loss: 5.591254, mae: 2.073425, mean_q: 4.377564\n",
      "[50 43 51  1 84 56 70 57 66  9]\n",
      " 14625/50001: episode: 1625, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 36.000, mean reward:  4.000 [ 2.000,  7.000], mean action: 48.556 [1.000, 84.000],  loss: 8.340838, mae: 2.132035, mean_q: 4.530178\n",
      "[ 9 98 60 53 57 87 15 48 88 12]\n",
      " 14634/50001: episode: 1626, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 43.000, mean reward:  4.778 [ 2.000,  8.000], mean action: 57.556 [12.000, 98.000],  loss: 9.658099, mae: 2.138531, mean_q: 4.596952\n",
      "[95 63 28 12  4 16 28 92 14  9]\n",
      " 14643/50001: episode: 1627, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 21.000, mean reward:  2.333 [-10.000,  5.000], mean action: 29.556 [4.000, 92.000],  loss: 7.555057, mae: 2.090760, mean_q: 4.471075\n",
      "[23 89 99 76 12 28 63 27 12 37]\n",
      " 14652/50001: episode: 1628, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 27.000, mean reward:  3.000 [-10.000,  9.000], mean action: 49.222 [12.000, 99.000],  loss: 8.421938, mae: 2.120849, mean_q: 4.496671\n",
      "[22  5 91 15 63  1 90 95 78 84]\n",
      " 14661/50001: episode: 1629, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 36.000, mean reward:  4.000 [ 2.000,  8.000], mean action: 58.000 [1.000, 95.000],  loss: 7.336920, mae: 2.099021, mean_q: 4.443034\n",
      "[27  2 28 22 54 27 83 21 88 23]\n",
      " 14670/50001: episode: 1630, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 24.000, mean reward:  2.667 [-10.000,  8.000], mean action: 38.667 [2.000, 88.000],  loss: 8.966848, mae: 2.201144, mean_q: 4.605188\n",
      "[17 13 37 87 21 32 88 31 66 12]\n",
      " 14679/50001: episode: 1631, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 44.000, mean reward:  4.889 [ 3.000,  8.000], mean action: 43.000 [12.000, 88.000],  loss: 7.342784, mae: 2.144058, mean_q: 4.465673\n",
      "[93 27 90 12 65 12 79 97 41 89]\n",
      " 14688/50001: episode: 1632, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 56.889 [12.000, 97.000],  loss: 8.629383, mae: 2.117104, mean_q: 4.459860\n",
      "[66 46 85 56  6 99 95 86  2 31]\n",
      " 14697/50001: episode: 1633, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 40.000, mean reward:  4.444 [ 3.000,  8.000], mean action: 56.222 [2.000, 99.000],  loss: 10.018294, mae: 2.095598, mean_q: 4.464592\n",
      "[ 3 28 56 37  8 34 74 42  4  4]\n",
      " 14706/50001: episode: 1634, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 31.889 [4.000, 74.000],  loss: 6.934653, mae: 2.150022, mean_q: 4.476464\n",
      "[68 60 95 34 69 90 59 20 46 66]\n",
      " 14715/50001: episode: 1635, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 59.889 [20.000, 95.000],  loss: 5.912752, mae: 2.097572, mean_q: 4.466846\n",
      "[83 86 50 65 27 13 40 12  5 88]\n",
      " 14724/50001: episode: 1636, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 43.000, mean reward:  4.778 [ 2.000,  8.000], mean action: 42.889 [5.000, 88.000],  loss: 8.316081, mae: 2.203349, mean_q: 4.607174\n",
      "[26 30 87 54 10 63 42 30 84 79]\n",
      " 14733/50001: episode: 1637, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 26.000, mean reward:  2.889 [-10.000,  8.000], mean action: 53.222 [10.000, 87.000],  loss: 7.973103, mae: 2.111576, mean_q: 4.422598\n",
      "[95 44 30 13 63  2 32 31 31 13]\n",
      " 14742/50001: episode: 1638, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward:  8.000, mean reward:  0.889 [-10.000,  5.000], mean action: 28.778 [2.000, 63.000],  loss: 7.179965, mae: 2.145595, mean_q: 4.452700\n",
      "[55 84 87 34  2  9 88 40  2 86]\n",
      " 14751/50001: episode: 1639, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 48.000 [2.000, 88.000],  loss: 7.518003, mae: 2.169345, mean_q: 4.575814\n",
      "[61 40 73 29 62 56 62 31 45 40]\n",
      " 14760/50001: episode: 1640, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward:  5.000, mean reward:  0.556 [-10.000,  6.000], mean action: 48.667 [29.000, 73.000],  loss: 10.392896, mae: 2.146369, mean_q: 4.471996\n",
      "[94 78 50 79 15 13 18 73  4 66]\n",
      " 14769/50001: episode: 1641, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 39.000, mean reward:  4.333 [ 2.000,  8.000], mean action: 44.000 [4.000, 79.000],  loss: 7.093134, mae: 2.139887, mean_q: 4.488036\n",
      "[92 76 32 63 96 96 95  3 99 43]\n",
      " 14778/50001: episode: 1642, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 22.000, mean reward:  2.444 [-10.000,  8.000], mean action: 67.000 [3.000, 99.000],  loss: 6.747528, mae: 2.116213, mean_q: 4.466226\n",
      "[12 50 91 86 46 85 50 29 54 48]\n",
      " 14787/50001: episode: 1643, duration: 0.069s, episode steps:   9, steps per second: 130, episode reward: 23.000, mean reward:  2.556 [-10.000,  9.000], mean action: 59.889 [29.000, 91.000],  loss: 7.862766, mae: 2.140178, mean_q: 4.560544\n",
      "[69 36 96 28 28 28 59 50 54 28]\n",
      " 14796/50001: episode: 1644, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: -5.000, mean reward: -0.556 [-10.000,  7.000], mean action: 45.222 [28.000, 96.000],  loss: 8.074519, mae: 2.219885, mean_q: 4.651453\n",
      "[37 46 52 80 87 32 68  8 45  4]\n",
      " 14805/50001: episode: 1645, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 33.000, mean reward:  3.667 [ 2.000,  7.000], mean action: 46.889 [4.000, 87.000],  loss: 8.466392, mae: 2.193455, mean_q: 4.598536\n",
      "[32 50  4 82 48 62 94 32 42 31]\n",
      " 14814/50001: episode: 1646, duration: 0.073s, episode steps:   9, steps per second: 123, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 49.444 [4.000, 94.000],  loss: 7.411324, mae: 2.104187, mean_q: 4.405406\n",
      "[62 83 47 11 99 53 33 42 93 69]\n",
      " 14823/50001: episode: 1647, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 58.889 [11.000, 99.000],  loss: 9.275282, mae: 2.160731, mean_q: 4.629263\n",
      "[96 93 13 24 27 25 48 12 78 46]\n",
      " 14832/50001: episode: 1648, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 40.667 [12.000, 93.000],  loss: 7.420694, mae: 2.144213, mean_q: 4.548261\n",
      "[78 13 78 77 81 27 34 34 37 38]\n",
      " 14841/50001: episode: 1649, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 10.000, mean reward:  1.111 [-10.000,  7.000], mean action: 46.556 [13.000, 81.000],  loss: 8.781986, mae: 2.181783, mean_q: 4.671556\n",
      "[79 38 67 46 16 59 44 99 42 95]\n",
      " 14850/50001: episode: 1650, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 43.000, mean reward:  4.778 [ 3.000, 12.000], mean action: 56.222 [16.000, 99.000],  loss: 9.701121, mae: 2.195976, mean_q: 4.631070\n",
      "[73 89 79 88 37 23 18 99 14 27]\n",
      " 14859/50001: episode: 1651, duration: 0.080s, episode steps:   9, steps per second: 113, episode reward: 41.000, mean reward:  4.556 [ 3.000,  6.000], mean action: 52.667 [14.000, 99.000],  loss: 6.446560, mae: 2.126817, mean_q: 4.534905\n",
      "[64 97 77 41 28 58 24 88 31 94]\n",
      " 14868/50001: episode: 1652, duration: 0.079s, episode steps:   9, steps per second: 114, episode reward: 41.000, mean reward:  4.556 [ 3.000,  6.000], mean action: 59.778 [24.000, 97.000],  loss: 7.782713, mae: 2.078635, mean_q: 4.361691\n",
      "[46 24 46  4 59 56 96 31 55 50]\n",
      " 14877/50001: episode: 1653, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 25.000, mean reward:  2.778 [-10.000,  9.000], mean action: 46.778 [4.000, 96.000],  loss: 6.819201, mae: 2.023512, mean_q: 4.229809\n",
      "[72 28 23 64 55 56 21 91 52  8]\n",
      " 14886/50001: episode: 1654, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 35.000, mean reward:  3.889 [ 3.000,  6.000], mean action: 44.222 [8.000, 91.000],  loss: 8.101029, mae: 2.089774, mean_q: 4.496347\n",
      "[23 28 16 27 27 77 68 78 88 48]\n",
      " 14895/50001: episode: 1655, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 26.000, mean reward:  2.889 [-10.000,  8.000], mean action: 50.778 [16.000, 88.000],  loss: 7.427903, mae: 2.091327, mean_q: 4.467938\n",
      "[93  0 54 34 34 32 56 69 66 65]\n",
      " 14904/50001: episode: 1656, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 45.556 [0.000, 69.000],  loss: 6.285928, mae: 2.118624, mean_q: 4.433405\n",
      "[93 95 71  2 74 68 76 30 54  7]\n",
      " 14913/50001: episode: 1657, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 31.000, mean reward:  3.444 [ 2.000,  5.000], mean action: 53.000 [2.000, 95.000],  loss: 6.398926, mae: 2.152705, mean_q: 4.614838\n",
      "[ 5 62 50 77 95 32 32 95 24 44]\n",
      " 14922/50001: episode: 1658, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 10.000, mean reward:  1.111 [-10.000,  6.000], mean action: 56.778 [24.000, 95.000],  loss: 7.098269, mae: 2.163799, mean_q: 4.580743\n",
      "[15 50 81 81 94  4 17 28 31 85]\n",
      " 14931/50001: episode: 1659, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 52.333 [4.000, 94.000],  loss: 6.980288, mae: 2.180289, mean_q: 4.550205\n",
      "[82 12 18 75 59 48 13 16 41 42]\n",
      " 14940/50001: episode: 1660, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 38.000, mean reward:  4.222 [ 2.000,  6.000], mean action: 36.000 [12.000, 75.000],  loss: 8.461651, mae: 2.250272, mean_q: 4.753514\n",
      "[22 13 33 13 73 87 59 69 99 12]\n",
      " 14949/50001: episode: 1661, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 22.000, mean reward:  2.444 [-10.000,  8.000], mean action: 50.889 [12.000, 99.000],  loss: 7.230479, mae: 2.203865, mean_q: 4.654077\n",
      "[76 97 67 42 10 25 56 78  4 27]\n",
      " 14958/50001: episode: 1662, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 45.111 [4.000, 97.000],  loss: 8.408751, mae: 2.218413, mean_q: 4.704223\n",
      "[34 12 37  8 53 30 95 64 13 42]\n",
      " 14967/50001: episode: 1663, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 39.333 [8.000, 95.000],  loss: 6.388206, mae: 2.181575, mean_q: 4.599285\n",
      "[56 88 56  1 49 53 88 50 56  4]\n",
      " 14976/50001: episode: 1664, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: -2.000, mean reward: -0.222 [-10.000,  7.000], mean action: 49.444 [1.000, 88.000],  loss: 9.392369, mae: 2.219719, mean_q: 4.691717\n",
      "[82 34 76 77 12 75  3 73 42 31]\n",
      " 14985/50001: episode: 1665, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 37.000, mean reward:  4.111 [ 2.000,  7.000], mean action: 47.000 [3.000, 77.000],  loss: 8.183731, mae: 2.158556, mean_q: 4.523058\n",
      "[98 96 59 11 46 90 42  4 78 28]\n",
      " 14994/50001: episode: 1666, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 40.000, mean reward:  4.444 [ 3.000,  8.000], mean action: 50.444 [4.000, 96.000],  loss: 7.087210, mae: 2.173820, mean_q: 4.540680\n",
      "[97  6 95 96 37 10 20 12 33 40]\n",
      " 15003/50001: episode: 1667, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 36.000, mean reward:  4.000 [ 3.000,  6.000], mean action: 38.778 [6.000, 96.000],  loss: 7.314099, mae: 2.147309, mean_q: 4.508323\n",
      "[71 95  4 59 91 88  1 88 69 97]\n",
      " 15012/50001: episode: 1668, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 65.778 [1.000, 97.000],  loss: 4.454771, mae: 2.167451, mean_q: 4.572628\n",
      "[ 6  4 32 91 32 52 76 42  6 42]\n",
      " 15021/50001: episode: 1669, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: -6.000, mean reward: -0.667 [-10.000,  5.000], mean action: 41.889 [4.000, 91.000],  loss: 8.372635, mae: 2.249048, mean_q: 4.671530\n",
      "[64 76 39 65  2  6 12 63 12 92]\n",
      " 15030/50001: episode: 1670, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward: 22.000, mean reward:  2.444 [-10.000,  7.000], mean action: 40.778 [2.000, 92.000],  loss: 8.048568, mae: 2.225060, mean_q: 4.674519\n",
      "[68 34 74  4 14 40 83 66 79 50]\n",
      " 15039/50001: episode: 1671, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 43.000, mean reward:  4.778 [ 3.000,  8.000], mean action: 49.333 [4.000, 83.000],  loss: 8.945871, mae: 2.221746, mean_q: 4.703605\n",
      "[62 34 24 88 69  4 42 12 98 44]\n",
      " 15048/50001: episode: 1672, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 46.111 [4.000, 98.000],  loss: 7.968706, mae: 2.139257, mean_q: 4.440495\n",
      "[58 78 86 14 38 31 95 99 54 48]\n",
      " 15057/50001: episode: 1673, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 44.000, mean reward:  4.889 [ 3.000,  9.000], mean action: 60.333 [14.000, 99.000],  loss: 7.532024, mae: 2.122730, mean_q: 4.460551\n",
      "[26 14 56 18 70 52 86 53 76 41]\n",
      " 15066/50001: episode: 1674, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 37.000, mean reward:  4.111 [ 3.000,  7.000], mean action: 51.778 [14.000, 86.000],  loss: 7.736985, mae: 2.111052, mean_q: 4.441777\n",
      "[84 31 75 20 99 78 28 89 31 71]\n",
      " 15075/50001: episode: 1675, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 21.000, mean reward:  2.333 [-10.000,  6.000], mean action: 58.000 [20.000, 99.000],  loss: 6.177960, mae: 2.143055, mean_q: 4.509871\n",
      "[76  9 50 97 65 46 46 95 28  7]\n",
      " 15084/50001: episode: 1676, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 49.222 [7.000, 97.000],  loss: 9.341135, mae: 2.141095, mean_q: 4.471931\n",
      "[42 28 87 37 70 12 80 40 56  9]\n",
      " 15093/50001: episode: 1677, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 38.000, mean reward:  4.222 [ 2.000,  6.000], mean action: 46.556 [9.000, 87.000],  loss: 6.951373, mae: 2.126419, mean_q: 4.501588\n",
      "[40 24  8 82 69 68 67 95  9 66]\n",
      " 15102/50001: episode: 1678, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 54.222 [8.000, 95.000],  loss: 7.650510, mae: 2.082071, mean_q: 4.379920\n",
      "[25 48 80 81 52 36  2  4 63 32]\n",
      " 15111/50001: episode: 1679, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 37.000, mean reward:  4.111 [ 2.000,  7.000], mean action: 44.222 [2.000, 81.000],  loss: 7.340442, mae: 2.083706, mean_q: 4.365028\n",
      "[73 91 91 11 28 33 34 79 36 63]\n",
      " 15120/50001: episode: 1680, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 51.778 [11.000, 91.000],  loss: 6.656518, mae: 2.138261, mean_q: 4.464277\n",
      "[37 59 47 79 10 13 87  9  4 40]\n",
      " 15129/50001: episode: 1681, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 36.000, mean reward:  4.000 [ 3.000,  5.000], mean action: 38.667 [4.000, 87.000],  loss: 7.605749, mae: 2.123063, mean_q: 4.390650\n",
      "[50 87 38 92 98 30 88 96 42 93]\n",
      " 15138/50001: episode: 1682, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 41.000, mean reward:  4.556 [ 2.000,  7.000], mean action: 73.778 [30.000, 98.000],  loss: 7.597392, mae: 2.136004, mean_q: 4.445108\n",
      "[72 60  4 39 69 99 51 46 46 52]\n",
      " 15147/50001: episode: 1683, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 51.778 [4.000, 99.000],  loss: 5.777529, mae: 2.182759, mean_q: 4.662816\n",
      "[26 13 20 62 69  1 72 34 14 14]\n",
      " 15156/50001: episode: 1684, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 24.000, mean reward:  2.667 [-10.000,  8.000], mean action: 33.222 [1.000, 72.000],  loss: 6.465496, mae: 2.265239, mean_q: 4.680823\n",
      "[13 31 86 55 54 95 96 32 30 14]\n",
      " 15165/50001: episode: 1685, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 54.778 [14.000, 96.000],  loss: 6.426511, mae: 2.283734, mean_q: 4.732779\n",
      "[35 95 31 44 97 13 79 61 24 82]\n",
      " 15174/50001: episode: 1686, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 40.000, mean reward:  4.444 [ 2.000,  6.000], mean action: 58.444 [13.000, 97.000],  loss: 6.434896, mae: 2.202787, mean_q: 4.556495\n",
      "[51 88 99 40 62 49 51 34 28 62]\n",
      " 15183/50001: episode: 1687, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 10.000, mean reward:  1.111 [-10.000,  8.000], mean action: 57.000 [28.000, 99.000],  loss: 7.593249, mae: 2.268375, mean_q: 4.691448\n",
      "[ 6 53  2 90 64 32 63 24  4  4]\n",
      " 15192/50001: episode: 1688, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 37.333 [2.000, 90.000],  loss: 5.935052, mae: 2.248576, mean_q: 4.743971\n",
      "[86 31 55 74 30 42 33 68  9 27]\n",
      " 15201/50001: episode: 1689, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 41.000 [9.000, 74.000],  loss: 7.046948, mae: 2.276135, mean_q: 4.840576\n",
      "[52 88 32 34 63 90 88 99 90 96]\n",
      " 15210/50001: episode: 1690, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward:  8.000, mean reward:  0.889 [-10.000,  5.000], mean action: 75.556 [32.000, 99.000],  loss: 7.677944, mae: 2.187830, mean_q: 4.600807\n",
      "[98 52 74 20 37 66  1  1 27 26]\n",
      " 15219/50001: episode: 1691, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 21.000, mean reward:  2.333 [-10.000,  6.000], mean action: 33.778 [1.000, 74.000],  loss: 10.564500, mae: 2.118233, mean_q: 4.476369\n",
      "[74 12 56 27 60 93 46 17 97 50]\n",
      " 15228/50001: episode: 1692, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 42.000, mean reward:  4.667 [ 2.000,  9.000], mean action: 50.889 [12.000, 97.000],  loss: 8.749702, mae: 2.152966, mean_q: 4.544596\n",
      "[28 37 88 34 11 39 78  0 25 67]\n",
      " 15237/50001: episode: 1693, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward: 32.000, mean reward:  3.556 [ 2.000,  5.000], mean action: 42.111 [0.000, 88.000],  loss: 6.653031, mae: 2.134278, mean_q: 4.470723\n",
      "[49 88 94 91  2 37 11 66 84 48]\n",
      " 15246/50001: episode: 1694, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 57.889 [2.000, 94.000],  loss: 5.748192, mae: 2.105552, mean_q: 4.530868\n",
      "[28 27 66 12 37 56 99 99 93 98]\n",
      " 15255/50001: episode: 1695, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 65.222 [12.000, 99.000],  loss: 6.958681, mae: 2.048827, mean_q: 4.290070\n",
      "[76 40 75  2 37 25 57 50 97 50]\n",
      " 15264/50001: episode: 1696, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 28.000, mean reward:  3.111 [-10.000,  7.000], mean action: 48.111 [2.000, 97.000],  loss: 10.577765, mae: 2.200420, mean_q: 4.649626\n",
      "[57 50 68 88 11 97 69 30 95 97]\n",
      " 15273/50001: episode: 1697, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 28.000, mean reward:  3.111 [-10.000,  9.000], mean action: 67.222 [11.000, 97.000],  loss: 6.981398, mae: 2.095649, mean_q: 4.393020\n",
      "[77 13  6 37 33 32 24 95  4 90]\n",
      " 15282/50001: episode: 1698, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 37.111 [4.000, 95.000],  loss: 8.664268, mae: 2.082601, mean_q: 4.455911\n",
      "[84  4 24 58 98 50 95 27 31  9]\n",
      " 15291/50001: episode: 1699, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 44.000 [4.000, 98.000],  loss: 9.682708, mae: 2.172984, mean_q: 4.543166\n",
      "[41 31 38 62 92 76 51 68 84 46]\n",
      " 15300/50001: episode: 1700, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 60.889 [31.000, 92.000],  loss: 8.217648, mae: 2.115874, mean_q: 4.449658\n",
      "[85  9 50 48 96 17 31 95 34 66]\n",
      " 15309/50001: episode: 1701, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 46.000, mean reward:  5.111 [ 3.000,  8.000], mean action: 49.556 [9.000, 96.000],  loss: 6.346437, mae: 2.110325, mean_q: 4.459533\n",
      "[ 9 89 89 24 78 14 50 18 24 27]\n",
      " 15318/50001: episode: 1702, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 12.000, mean reward:  1.333 [-10.000,  7.000], mean action: 45.889 [14.000, 89.000],  loss: 7.219050, mae: 2.139697, mean_q: 4.476826\n",
      "[80 86 48 34 99 24 23 10 13 99]\n",
      " 15327/50001: episode: 1703, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 48.444 [10.000, 99.000],  loss: 6.544062, mae: 2.171744, mean_q: 4.546100\n",
      "[ 4  5 66 14 21 14 12 20 27 37]\n",
      " 15336/50001: episode: 1704, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 27.000, mean reward:  3.000 [-10.000,  9.000], mean action: 24.000 [5.000, 66.000],  loss: 8.881142, mae: 2.235789, mean_q: 4.635246\n",
      "[64 79 93 30 69 71 41 48 23 55]\n",
      " 15345/50001: episode: 1705, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 56.556 [23.000, 93.000],  loss: 11.083649, mae: 2.225114, mean_q: 4.654520\n",
      "[19 86 89 68 30 94 41 23 42 45]\n",
      " 15354/50001: episode: 1706, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 37.000, mean reward:  4.111 [ 3.000,  6.000], mean action: 57.556 [23.000, 94.000],  loss: 7.782947, mae: 2.141101, mean_q: 4.499341\n",
      "[92 41 28 62  1 98 20 88 55  9]\n",
      " 15363/50001: episode: 1707, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 44.667 [1.000, 98.000],  loss: 8.815993, mae: 2.048165, mean_q: 4.328398\n",
      "[42 50 56 87 21 14 50 48 40 52]\n",
      " 15372/50001: episode: 1708, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 46.444 [14.000, 87.000],  loss: 7.940974, mae: 2.063794, mean_q: 4.389524\n",
      "[80 53 92 12 14 36 48 30 46 66]\n",
      " 15381/50001: episode: 1709, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 41.000, mean reward:  4.556 [ 2.000,  9.000], mean action: 44.111 [12.000, 92.000],  loss: 8.706501, mae: 2.078475, mean_q: 4.372992\n",
      "[45 23 74 28 46 21  2 92  3  7]\n",
      " 15390/50001: episode: 1710, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 33.000, mean reward:  3.667 [ 2.000,  5.000], mean action: 32.889 [2.000, 92.000],  loss: 7.953897, mae: 2.108976, mean_q: 4.453386\n",
      "[38 76 98 91 28 50 50 97 12 31]\n",
      " 15399/50001: episode: 1711, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 30.000, mean reward:  3.333 [-10.000,  7.000], mean action: 59.222 [12.000, 98.000],  loss: 7.990838, mae: 2.033499, mean_q: 4.376191\n",
      "[41 41 60 28 11 25 88 42 85 69]\n",
      " 15408/50001: episode: 1712, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 49.889 [11.000, 88.000],  loss: 8.012839, mae: 2.097469, mean_q: 4.430447\n",
      "[65 74 56 81 13 92 95 68 86  9]\n",
      " 15417/50001: episode: 1713, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 38.000, mean reward:  4.222 [ 3.000,  8.000], mean action: 63.778 [9.000, 95.000],  loss: 10.242130, mae: 2.099762, mean_q: 4.480335\n",
      "[28 73 20 12 97 24 28 90 48 66]\n",
      " 15426/50001: episode: 1714, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 50.889 [12.000, 97.000],  loss: 7.021212, mae: 2.114974, mean_q: 4.523518\n",
      "[89 61 75 23 56 14 45 12 27 40]\n",
      " 15435/50001: episode: 1715, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 39.222 [12.000, 75.000],  loss: 5.873857, mae: 2.122347, mean_q: 4.422422\n",
      "[41 52 53 95 76 95 78 95  6 28]\n",
      " 15444/50001: episode: 1716, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward:  7.000, mean reward:  0.778 [-10.000,  7.000], mean action: 64.222 [6.000, 95.000],  loss: 8.792602, mae: 2.179471, mean_q: 4.591340\n",
      "[13 67 77  1 48 13 16 84 31 96]\n",
      " 15453/50001: episode: 1717, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 19.000, mean reward:  2.111 [-10.000,  6.000], mean action: 48.111 [1.000, 96.000],  loss: 7.365605, mae: 2.175873, mean_q: 4.592157\n",
      "[69 91 34 98 76 88 95 78 95  9]\n",
      " 15462/50001: episode: 1718, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 73.778 [9.000, 98.000],  loss: 6.488373, mae: 2.221806, mean_q: 4.680845\n",
      "[49 59 67 69 99 77 95 95 82 31]\n",
      " 15471/50001: episode: 1719, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 26.000, mean reward:  2.889 [-10.000,  8.000], mean action: 74.889 [31.000, 99.000],  loss: 5.978792, mae: 2.211120, mean_q: 4.652315\n",
      "[11 95 55 50 68 66 13 84 88 12]\n",
      " 15480/50001: episode: 1720, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 43.000, mean reward:  4.778 [ 2.000,  8.000], mean action: 59.000 [12.000, 95.000],  loss: 9.128001, mae: 2.276108, mean_q: 4.782881\n",
      "[32 60 26 51  4 28  0 20 52 50]\n",
      " 15489/50001: episode: 1721, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 39.000, mean reward:  4.333 [ 2.000,  9.000], mean action: 32.333 [0.000, 60.000],  loss: 7.680788, mae: 2.226022, mean_q: 4.670034\n",
      "[43 34 67 90 99 56 94 90 83 31]\n",
      " 15498/50001: episode: 1722, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 26.000, mean reward:  2.889 [-10.000,  8.000], mean action: 71.556 [31.000, 99.000],  loss: 7.919011, mae: 2.128934, mean_q: 4.552830\n",
      "[76 31 63 99 88 32 67 89 12 14]\n",
      " 15507/50001: episode: 1723, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 55.000 [12.000, 99.000],  loss: 6.361135, mae: 2.164028, mean_q: 4.524146\n",
      "[80 50 31 37 34 41 46 96 98 74]\n",
      " 15516/50001: episode: 1724, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 40.000, mean reward:  4.444 [ 3.000,  6.000], mean action: 56.333 [31.000, 98.000],  loss: 6.282914, mae: 2.212976, mean_q: 4.653080\n",
      "[23 97 37 19 34 98 95 46 41 16]\n",
      " 15525/50001: episode: 1725, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 53.667 [16.000, 98.000],  loss: 7.544664, mae: 2.227050, mean_q: 4.591380\n",
      "[21 40 42  1 98 32 69  4  4 95]\n",
      " 15534/50001: episode: 1726, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 29.000, mean reward:  3.222 [-10.000,  9.000], mean action: 42.778 [1.000, 98.000],  loss: 8.689726, mae: 2.265687, mean_q: 4.771915\n",
      "[62 96 50 66 28 57 18 67 55 50]\n",
      " 15543/50001: episode: 1727, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 20.000, mean reward:  2.222 [-10.000,  5.000], mean action: 54.111 [18.000, 96.000],  loss: 9.229458, mae: 2.218006, mean_q: 4.622923\n",
      "[30 32 97 44 12 13 12 46 13 34]\n",
      " 15552/50001: episode: 1728, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 14.000, mean reward:  1.556 [-10.000,  8.000], mean action: 33.667 [12.000, 97.000],  loss: 6.459479, mae: 2.092856, mean_q: 4.345675\n",
      "[99 78 31 12 94  4 71 66 24 50]\n",
      " 15561/50001: episode: 1729, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 41.000, mean reward:  4.556 [ 2.000,  8.000], mean action: 47.778 [4.000, 94.000],  loss: 7.918465, mae: 2.148105, mean_q: 4.552475\n",
      "[38 14 59 92 91 26 95 66 14 21]\n",
      " 15570/50001: episode: 1730, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 53.111 [14.000, 95.000],  loss: 7.310861, mae: 2.209230, mean_q: 4.618437\n",
      "[52 27 52 89 34 20 48 40  9 42]\n",
      " 15579/50001: episode: 1731, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 27.000, mean reward:  3.000 [-10.000,  6.000], mean action: 40.111 [9.000, 89.000],  loss: 8.855928, mae: 2.180210, mean_q: 4.581518\n",
      "[ 0 44 24 91 18 64 88 95 48 10]\n",
      " 15588/50001: episode: 1732, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 43.000, mean reward:  4.778 [ 2.000,  9.000], mean action: 53.556 [10.000, 95.000],  loss: 7.218017, mae: 2.159718, mean_q: 4.553777\n",
      "[58 95 47  8 69 59 51 37 31 57]\n",
      " 15597/50001: episode: 1733, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 50.444 [8.000, 95.000],  loss: 8.929020, mae: 2.184396, mean_q: 4.578543\n",
      "[ 1 27 77 66  4  1 50 43 97 22]\n",
      " 15606/50001: episode: 1734, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 43.000 [1.000, 97.000],  loss: 8.769121, mae: 2.077882, mean_q: 4.398260\n",
      "[79 69 79 30 76 56 15  4 28 34]\n",
      " 15615/50001: episode: 1735, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 28.000, mean reward:  3.111 [-10.000,  9.000], mean action: 43.444 [4.000, 79.000],  loss: 8.612713, mae: 2.157570, mean_q: 4.477752\n",
      "[86 86 41  2 32 90 28 34 95  4]\n",
      " 15624/50001: episode: 1736, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 33.000, mean reward:  3.667 [-10.000,  8.000], mean action: 45.778 [2.000, 95.000],  loss: 10.067919, mae: 2.131736, mean_q: 4.515146\n",
      "[49 54 94 76 20 86 50 96 97 40]\n",
      " 15633/50001: episode: 1737, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 40.000, mean reward:  4.444 [ 3.000,  8.000], mean action: 68.111 [20.000, 97.000],  loss: 7.959438, mae: 2.080420, mean_q: 4.387585\n",
      "[ 9 95 68 41 28 14 76 18 84 79]\n",
      " 15642/50001: episode: 1738, duration: 0.070s, episode steps:   9, steps per second: 129, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 55.889 [14.000, 95.000],  loss: 7.531226, mae: 2.100460, mean_q: 4.410803\n",
      "[91 63 24 90 91 32  1 58 83 31]\n",
      " 15651/50001: episode: 1739, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 52.556 [1.000, 91.000],  loss: 4.990164, mae: 2.121433, mean_q: 4.440518\n",
      "[ 1 88 68  4 38 99 83 21 51 30]\n",
      " 15660/50001: episode: 1740, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 34.000, mean reward:  3.778 [ 2.000,  6.000], mean action: 53.556 [4.000, 99.000],  loss: 6.921887, mae: 2.193202, mean_q: 4.571118\n",
      "[36 62 52 78 28 58 95  6 61 12]\n",
      " 15669/50001: episode: 1741, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 43.000, mean reward:  4.778 [ 3.000, 10.000], mean action: 50.222 [6.000, 95.000],  loss: 8.095587, mae: 2.217319, mean_q: 4.575530\n",
      "[12 13 36 64 78 79 63  3 96 13]\n",
      " 15678/50001: episode: 1742, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 18.000, mean reward:  2.000 [-10.000,  5.000], mean action: 49.444 [3.000, 96.000],  loss: 7.483073, mae: 2.226750, mean_q: 4.634551\n",
      "[18 88 13  2 79 93 34 34 28 26]\n",
      " 15687/50001: episode: 1743, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 44.111 [2.000, 93.000],  loss: 7.652761, mae: 2.168001, mean_q: 4.524928\n",
      "[91 66  2 13 89 41 34 95 66 57]\n",
      " 15696/50001: episode: 1744, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 30.000, mean reward:  3.333 [-10.000,  7.000], mean action: 51.444 [2.000, 95.000],  loss: 8.627324, mae: 2.153584, mean_q: 4.488016\n",
      "[75 95  4 40 47 13 40 94 19 13]\n",
      " 15705/50001: episode: 1745, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward:  7.000, mean reward:  0.778 [-10.000,  5.000], mean action: 40.556 [4.000, 95.000],  loss: 7.334828, mae: 2.154389, mean_q: 4.505856\n",
      "[38 14 14 48 83 75 89 46 64 83]\n",
      " 15714/50001: episode: 1746, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 10.000, mean reward:  1.111 [-10.000,  6.000], mean action: 57.333 [14.000, 89.000],  loss: 8.385151, mae: 2.099273, mean_q: 4.440114\n",
      "[83 41  4 47 64 28 11 96 31 98]\n",
      " 15723/50001: episode: 1747, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 46.667 [4.000, 98.000],  loss: 5.950149, mae: 2.267042, mean_q: 4.661696\n",
      "[24 91 41 49 75 88  4 59 83 53]\n",
      " 15732/50001: episode: 1748, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 60.333 [4.000, 91.000],  loss: 8.531866, mae: 2.238509, mean_q: 4.679257\n",
      "[58 28 76 54 92 60 99 27  5 28]\n",
      " 15741/50001: episode: 1749, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 23.000, mean reward:  2.556 [-10.000,  5.000], mean action: 52.111 [5.000, 99.000],  loss: 6.939068, mae: 2.220151, mean_q: 4.503358\n",
      "[13 74 53 99 11 14 75 57 28 88]\n",
      " 15750/50001: episode: 1750, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward: 40.000, mean reward:  4.444 [ 3.000,  8.000], mean action: 55.444 [11.000, 99.000],  loss: 8.060211, mae: 2.218477, mean_q: 4.619498\n",
      "[70  4 91  2 99 30  8 14 27 43]\n",
      " 15759/50001: episode: 1751, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 36.000, mean reward:  4.000 [ 3.000,  5.000], mean action: 35.333 [2.000, 99.000],  loss: 7.007599, mae: 2.232741, mean_q: 4.614201\n",
      "[24 41 37 76  3 12 62 31 98 71]\n",
      " 15768/50001: episode: 1752, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 39.000, mean reward:  4.333 [ 2.000,  6.000], mean action: 47.889 [3.000, 98.000],  loss: 6.488653, mae: 2.272256, mean_q: 4.759068\n",
      "[65 34 48 53 30  1  2 28 27 82]\n",
      " 15777/50001: episode: 1753, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 40.000, mean reward:  4.444 [ 3.000,  5.000], mean action: 33.889 [1.000, 82.000],  loss: 5.053743, mae: 2.293441, mean_q: 4.695802\n",
      "[60 15 82 59 92 93 35 78 88 12]\n",
      " 15786/50001: episode: 1754, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 40.000, mean reward:  4.444 [ 2.000,  9.000], mean action: 61.556 [12.000, 93.000],  loss: 9.464248, mae: 2.277547, mean_q: 4.720315\n",
      "[68 27 52 64  2  1 68 24 10 14]\n",
      " 15795/50001: episode: 1755, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 29.111 [1.000, 68.000],  loss: 8.433780, mae: 2.326246, mean_q: 4.840803\n",
      "[98  4 50 37 12 88 95  2 28  2]\n",
      " 15804/50001: episode: 1756, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 27.000, mean reward:  3.000 [-10.000,  6.000], mean action: 35.333 [2.000, 95.000],  loss: 7.247266, mae: 2.228079, mean_q: 4.645573\n",
      "[85 37 27 10 28 48 27 34  8 27]\n",
      " 15813/50001: episode: 1757, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 10.000, mean reward:  1.111 [-10.000,  7.000], mean action: 27.333 [8.000, 48.000],  loss: 7.428226, mae: 2.291553, mean_q: 4.758889\n",
      "[67 96 84 28 88 27 42 54 31 32]\n",
      " 15822/50001: episode: 1758, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 53.556 [27.000, 96.000],  loss: 7.462408, mae: 2.199975, mean_q: 4.581774\n",
      "[97 31 20 52 95  6 59 27 41  9]\n",
      " 15831/50001: episode: 1759, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 37.778 [6.000, 95.000],  loss: 7.333754, mae: 2.251035, mean_q: 4.656099\n",
      "[10 12 92 49 73 81 46 59  2 27]\n",
      " 15840/50001: episode: 1760, duration: 0.071s, episode steps:   9, steps per second: 126, episode reward: 37.000, mean reward:  4.111 [ 2.000,  7.000], mean action: 49.000 [2.000, 92.000],  loss: 7.318431, mae: 2.231068, mean_q: 4.630451\n",
      "[59 27 75 57 10 83 73 53 40 31]\n",
      " 15849/50001: episode: 1761, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 38.000, mean reward:  4.222 [ 2.000,  8.000], mean action: 49.889 [10.000, 83.000],  loss: 7.154921, mae: 2.340237, mean_q: 4.858925\n",
      "[51 83 13 56 92 77 62 41  1 80]\n",
      " 15858/50001: episode: 1762, duration: 0.072s, episode steps:   9, steps per second: 124, episode reward: 37.000, mean reward:  4.111 [ 3.000,  7.000], mean action: 56.111 [1.000, 92.000],  loss: 7.867017, mae: 2.236744, mean_q: 4.620703\n",
      "[37 66  8 77 40 40  2 37 42 12]\n",
      " 15867/50001: episode: 1763, duration: 0.086s, episode steps:   9, steps per second: 105, episode reward: 10.000, mean reward:  1.111 [-10.000,  8.000], mean action: 36.000 [2.000, 77.000],  loss: 5.100444, mae: 2.353854, mean_q: 4.891971\n",
      "[72 49 89 59 37 37 42 95 97 88]\n",
      " 15876/50001: episode: 1764, duration: 0.085s, episode steps:   9, steps per second: 106, episode reward: 30.000, mean reward:  3.333 [-10.000,  8.000], mean action: 65.889 [37.000, 97.000],  loss: 8.994940, mae: 2.331457, mean_q: 4.796085\n",
      "[93 14 28 26 28 42 75 10 21 14]\n",
      " 15885/50001: episode: 1765, duration: 0.087s, episode steps:   9, steps per second: 103, episode reward:  5.000, mean reward:  0.556 [-10.000,  5.000], mean action: 28.667 [10.000, 75.000],  loss: 7.788624, mae: 2.278999, mean_q: 4.694893\n",
      "[81  6 42  2 12 43 13 92 71 55]\n",
      " 15894/50001: episode: 1766, duration: 0.085s, episode steps:   9, steps per second: 105, episode reward: 32.000, mean reward:  3.556 [ 2.000,  6.000], mean action: 37.333 [2.000, 92.000],  loss: 6.616251, mae: 2.213592, mean_q: 4.637028\n",
      "[ 0  6 23 89 10 31 90 34 95 90]\n",
      " 15903/50001: episode: 1767, duration: 0.084s, episode steps:   9, steps per second: 107, episode reward: 32.000, mean reward:  3.556 [-10.000,  9.000], mean action: 52.000 [6.000, 95.000],  loss: 8.086495, mae: 2.213665, mean_q: 4.544293\n",
      "[84 82 28 72 42 62 57 34 54 33]\n",
      " 15912/50001: episode: 1768, duration: 0.078s, episode steps:   9, steps per second: 116, episode reward: 39.000, mean reward:  4.333 [ 2.000,  8.000], mean action: 51.556 [28.000, 82.000],  loss: 6.947043, mae: 2.225934, mean_q: 4.607520\n",
      "[ 6  9 25 12  5  3 15 39 10 97]\n",
      " 15921/50001: episode: 1769, duration: 0.087s, episode steps:   9, steps per second: 104, episode reward: 37.000, mean reward:  4.111 [ 2.000,  9.000], mean action: 23.889 [3.000, 97.000],  loss: 8.052364, mae: 2.208333, mean_q: 4.555388\n",
      "[40 87 72 71 56 56 50 83  5 82]\n",
      " 15930/50001: episode: 1770, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 62.444 [5.000, 87.000],  loss: 7.089399, mae: 2.199586, mean_q: 4.583924\n",
      "[40 30 23 96 10 72 69 88 14 55]\n",
      " 15939/50001: episode: 1771, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 34.000, mean reward:  3.778 [ 2.000,  7.000], mean action: 50.778 [10.000, 96.000],  loss: 9.117274, mae: 2.216491, mean_q: 4.564640\n",
      "[ 5 89 50 45 37 87 50 92 42 59]\n",
      " 15948/50001: episode: 1772, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 21.000, mean reward:  2.333 [-10.000,  6.000], mean action: 61.222 [37.000, 92.000],  loss: 7.751345, mae: 2.159362, mean_q: 4.499631\n",
      "[19 12 41 59 95 17 57 14 27 48]\n",
      " 15957/50001: episode: 1773, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 41.000, mean reward:  4.556 [ 2.000,  7.000], mean action: 41.111 [12.000, 95.000],  loss: 7.934067, mae: 2.169562, mean_q: 4.468561\n",
      "[99 17 50 46 87 87 28 91 66 98]\n",
      " 15966/50001: episode: 1774, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 63.333 [17.000, 98.000],  loss: 6.097273, mae: 2.183907, mean_q: 4.475400\n",
      "[90 67 79 34 40 66 57 68 84 23]\n",
      " 15975/50001: episode: 1775, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 37.000, mean reward:  4.111 [ 3.000,  6.000], mean action: 57.556 [23.000, 84.000],  loss: 5.550638, mae: 2.244679, mean_q: 4.570467\n",
      "[18 89 56 44 74 21 37 96 14 98]\n",
      " 15984/50001: episode: 1776, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 58.778 [14.000, 98.000],  loss: 9.855250, mae: 2.207013, mean_q: 4.558646\n",
      "[42 57 96 12 34 32 14 98 28 20]\n",
      " 15993/50001: episode: 1777, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 43.444 [12.000, 98.000],  loss: 6.630322, mae: 2.130735, mean_q: 4.442980\n",
      "[ 2 27 90 42 76 37 48 82 28 30]\n",
      " 16002/50001: episode: 1778, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 51.111 [27.000, 90.000],  loss: 8.294687, mae: 2.195772, mean_q: 4.604527\n",
      "[91 51 91 79 48 68  1 91 82  4]\n",
      " 16011/50001: episode: 1779, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 11.000, mean reward:  1.222 [-10.000,  7.000], mean action: 57.222 [1.000, 91.000],  loss: 6.621049, mae: 2.204744, mean_q: 4.555182\n",
      "[72 50  7 83 92 69 51 74 88 79]\n",
      " 16020/50001: episode: 1780, duration: 0.068s, episode steps:   9, steps per second: 133, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 65.889 [7.000, 92.000],  loss: 9.732955, mae: 2.209248, mean_q: 4.517100\n",
      "[31 67 62 11 34 49 42 65 20 37]\n",
      " 16029/50001: episode: 1781, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 42.000, mean reward:  4.667 [ 3.000, 10.000], mean action: 43.000 [11.000, 67.000],  loss: 7.522546, mae: 2.244257, mean_q: 4.654699\n",
      "[14 44  5 47 73 53 86 84 13 37]\n",
      " 16038/50001: episode: 1782, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 40.000, mean reward:  4.444 [ 2.000,  9.000], mean action: 49.111 [5.000, 86.000],  loss: 8.168298, mae: 2.202025, mean_q: 4.593229\n",
      "[68 45 97 87 30 40 88 88  2 66]\n",
      " 16047/50001: episode: 1783, duration: 0.069s, episode steps:   9, steps per second: 130, episode reward: 28.000, mean reward:  3.111 [-10.000,  8.000], mean action: 60.333 [2.000, 97.000],  loss: 9.102223, mae: 2.144389, mean_q: 4.569884\n",
      "[73 58 13 99 59 12 90 31 10 71]\n",
      " 16056/50001: episode: 1784, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 34.000, mean reward:  3.778 [ 2.000,  5.000], mean action: 49.222 [10.000, 99.000],  loss: 8.509484, mae: 2.200268, mean_q: 4.552978\n",
      "[18 79 64 34 27 44 48 37 52 37]\n",
      " 16065/50001: episode: 1785, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 46.889 [27.000, 79.000],  loss: 7.077912, mae: 2.093603, mean_q: 4.378138\n",
      "[34  4 98 80  9  4  1  2 82  1]\n",
      " 16074/50001: episode: 1786, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward:  7.000, mean reward:  0.778 [-10.000,  5.000], mean action: 31.222 [1.000, 98.000],  loss: 8.415946, mae: 2.110324, mean_q: 4.431791\n",
      "[79 17 33 43 13 68 75 23 14  2]\n",
      " 16083/50001: episode: 1787, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 38.000, mean reward:  4.222 [ 2.000,  6.000], mean action: 32.000 [2.000, 75.000],  loss: 8.213920, mae: 2.112844, mean_q: 4.438094\n",
      "[90 90 84 93 37 40  4 81 23 66]\n",
      " 16092/50001: episode: 1788, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 57.556 [4.000, 93.000],  loss: 6.439065, mae: 2.164932, mean_q: 4.517736\n",
      "[11  2 13 77 50 87 78 27 10 97]\n",
      " 16101/50001: episode: 1789, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 39.000, mean reward:  4.333 [ 3.000,  8.000], mean action: 49.000 [2.000, 97.000],  loss: 8.242649, mae: 2.227955, mean_q: 4.623957\n",
      "[62 59 10 64 97 90 51 51 53 79]\n",
      " 16110/50001: episode: 1790, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 61.556 [10.000, 97.000],  loss: 8.188310, mae: 2.202537, mean_q: 4.578357\n",
      "[59 82 59 71 33 10 37 78 31 34]\n",
      " 16119/50001: episode: 1791, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 27.000, mean reward:  3.000 [-10.000,  9.000], mean action: 48.333 [10.000, 82.000],  loss: 7.230156, mae: 2.218059, mean_q: 4.543649\n",
      "[38 41 81 37 97 89 95 40 41 88]\n",
      " 16128/50001: episode: 1792, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 30.000, mean reward:  3.333 [-10.000,  7.000], mean action: 67.667 [37.000, 97.000],  loss: 7.805061, mae: 2.279227, mean_q: 4.666449\n",
      "[35 27 32 57 59 10 53 46 88  9]\n",
      " 16137/50001: episode: 1793, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 42.333 [9.000, 88.000],  loss: 6.331932, mae: 2.218874, mean_q: 4.606505\n",
      "[90 14 94 62 65 13 37  2 12  2]\n",
      " 16146/50001: episode: 1794, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 33.444 [2.000, 94.000],  loss: 5.043413, mae: 2.169854, mean_q: 4.473442\n",
      "[83 38 46 34 61 31  4 60 98 12]\n",
      " 16155/50001: episode: 1795, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 46.000, mean reward:  5.111 [ 2.000,  9.000], mean action: 42.667 [4.000, 98.000],  loss: 7.320753, mae: 2.204063, mean_q: 4.622014\n",
      "[26 74  4 41 97 11 96  4 48 97]\n",
      " 16164/50001: episode: 1796, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 10.000, mean reward:  1.111 [-10.000,  7.000], mean action: 52.444 [4.000, 97.000],  loss: 6.844561, mae: 2.302699, mean_q: 4.763843\n",
      "[46 15 89 50 29 88 91 95 61 27]\n",
      " 16173/50001: episode: 1797, duration: 0.079s, episode steps:   9, steps per second: 114, episode reward: 42.000, mean reward:  4.667 [ 2.000,  8.000], mean action: 60.556 [15.000, 95.000],  loss: 7.477516, mae: 2.253040, mean_q: 4.629867\n",
      "[89 26 37 64  2 89 64 67 34 86]\n",
      " 16182/50001: episode: 1798, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward: 11.000, mean reward:  1.222 [-10.000,  9.000], mean action: 52.111 [2.000, 89.000],  loss: 6.676621, mae: 2.274666, mean_q: 4.755443\n",
      "[69  9 53 34 19 14 98 27 54 28]\n",
      " 16191/50001: episode: 1799, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 38.000, mean reward:  4.222 [ 3.000,  7.000], mean action: 37.333 [9.000, 98.000],  loss: 5.291225, mae: 2.266908, mean_q: 4.668077\n",
      "[57 28 84 59 98 17 98  2 84 50]\n",
      " 16200/50001: episode: 1800, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 12.000, mean reward:  1.333 [-10.000,  8.000], mean action: 57.778 [2.000, 98.000],  loss: 6.114259, mae: 2.211113, mean_q: 4.546823\n",
      "[87 34 52 98 20 62 42 74 42 12]\n",
      " 16209/50001: episode: 1801, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 26.000, mean reward:  2.889 [-10.000,  9.000], mean action: 48.444 [12.000, 98.000],  loss: 7.524466, mae: 2.329208, mean_q: 4.849162\n",
      "[71 88 56 33 28 58 69 33 40 64]\n",
      " 16218/50001: episode: 1802, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward: 23.000, mean reward:  2.556 [-10.000,  5.000], mean action: 52.111 [28.000, 88.000],  loss: 6.639360, mae: 2.250725, mean_q: 4.768974\n",
      "[16 31 26 30 71 74 91  8 30 31]\n",
      " 16227/50001: episode: 1803, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward:  4.000, mean reward:  0.444 [-10.000,  4.000], mean action: 43.556 [8.000, 91.000],  loss: 8.860355, mae: 2.252928, mean_q: 4.708225\n",
      "[ 4  1 16 77 95 62 95 64 48 48]\n",
      " 16236/50001: episode: 1804, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward:  9.000, mean reward:  1.000 [-10.000,  6.000], mean action: 56.222 [1.000, 95.000],  loss: 7.816645, mae: 2.199538, mean_q: 4.503523\n",
      "[69 51 77 34 63 58 14 42 94 33]\n",
      " 16245/50001: episode: 1805, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 37.000, mean reward:  4.111 [ 3.000,  6.000], mean action: 51.778 [14.000, 94.000],  loss: 8.152456, mae: 2.115191, mean_q: 4.398105\n",
      "[28 89 60 48 62 91 13 45 82 26]\n",
      " 16254/50001: episode: 1806, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 32.000, mean reward:  3.556 [ 2.000,  6.000], mean action: 57.333 [13.000, 91.000],  loss: 7.663369, mae: 2.145788, mean_q: 4.398508\n",
      "[97 58 38 67 37  1 24 57 14 85]\n",
      " 16263/50001: episode: 1807, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 38.000, mean reward:  4.222 [ 2.000,  6.000], mean action: 42.333 [1.000, 85.000],  loss: 7.028990, mae: 2.118297, mean_q: 4.388228\n",
      "[56 28 18 57  0 63 12 66 10  9]\n",
      " 16272/50001: episode: 1808, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 38.000, mean reward:  4.222 [ 2.000,  6.000], mean action: 29.222 [0.000, 66.000],  loss: 5.672701, mae: 2.179434, mean_q: 4.634787\n",
      "[28 99 56 13 11 13 24 45 99 50]\n",
      " 16281/50001: episode: 1809, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward:  8.000, mean reward:  0.889 [-10.000,  8.000], mean action: 45.556 [11.000, 99.000],  loss: 7.286116, mae: 2.177293, mean_q: 4.536573\n",
      "[65 93 51 57  6 37 10 62 17 37]\n",
      " 16290/50001: episode: 1810, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 19.000, mean reward:  2.111 [-10.000,  6.000], mean action: 41.111 [6.000, 93.000],  loss: 10.016946, mae: 2.172864, mean_q: 4.554726\n",
      "[42 61 12 86 64 50 88 28 82 14]\n",
      " 16299/50001: episode: 1811, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 43.000, mean reward:  4.778 [ 2.000,  7.000], mean action: 53.889 [12.000, 88.000],  loss: 8.009681, mae: 2.216992, mean_q: 4.557734\n",
      "[ 5 74 86 72 27 28 50 99 34 94]\n",
      " 16308/50001: episode: 1812, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 62.667 [27.000, 99.000],  loss: 6.261502, mae: 2.194901, mean_q: 4.601731\n",
      "[83 28 48 55 95 13  4 48 37 49]\n",
      " 16317/50001: episode: 1813, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 41.889 [4.000, 95.000],  loss: 7.205115, mae: 2.256553, mean_q: 4.704987\n",
      "[27 34 41 14  6 77 16 46 51  1]\n",
      " 16326/50001: episode: 1814, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward: 40.000, mean reward:  4.444 [ 3.000,  8.000], mean action: 31.778 [1.000, 77.000],  loss: 5.440710, mae: 2.350471, mean_q: 4.877736\n",
      "[85 53 75  4 46 34 99 21 31 34]\n",
      " 16335/50001: episode: 1815, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 44.111 [4.000, 99.000],  loss: 9.698503, mae: 2.347735, mean_q: 4.832485\n",
      "[69 48 95 28 12 42 88 63 99 10]\n",
      " 16344/50001: episode: 1816, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 37.000, mean reward:  4.111 [ 3.000,  5.000], mean action: 53.889 [10.000, 99.000],  loss: 5.768720, mae: 2.316680, mean_q: 4.853125\n",
      "[41 11 27 97 34 30 95 99  2 24]\n",
      " 16353/50001: episode: 1817, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 46.556 [2.000, 99.000],  loss: 6.281755, mae: 2.305135, mean_q: 4.775210\n",
      "[75 87 54 89 37 56 12 33 88 12]\n",
      " 16362/50001: episode: 1818, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 52.000 [12.000, 89.000],  loss: 8.456367, mae: 2.319152, mean_q: 4.801571\n",
      "[56 95 67 77 12  2 32 33 31 14]\n",
      " 16371/50001: episode: 1819, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 41.000, mean reward:  4.556 [ 3.000,  6.000], mean action: 40.333 [2.000, 95.000],  loss: 7.981820, mae: 2.263176, mean_q: 4.693220\n",
      "[56  2 87 37 67 82  1 91 52 97]\n",
      " 16380/50001: episode: 1820, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 57.333 [1.000, 97.000],  loss: 6.800172, mae: 2.228301, mean_q: 4.646281\n",
      "[95 41 10  8 68 34 79 31 48 32]\n",
      " 16389/50001: episode: 1821, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 40.000, mean reward:  4.444 [ 3.000,  6.000], mean action: 39.000 [8.000, 79.000],  loss: 8.041513, mae: 2.187387, mean_q: 4.513104\n",
      "[64 24 52 75 52 34 52 66 66 12]\n",
      " 16398/50001: episode: 1822, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward:  1.000, mean reward:  0.111 [-10.000,  8.000], mean action: 48.111 [12.000, 75.000],  loss: 6.947628, mae: 2.228652, mean_q: 4.675072\n",
      "[64 74 24 45 28  1 58 85 34 58]\n",
      " 16407/50001: episode: 1823, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 45.222 [1.000, 85.000],  loss: 9.901034, mae: 2.194917, mean_q: 4.537812\n",
      "[ 2 27 16 37 91 60 13 91 99 12]\n",
      " 16416/50001: episode: 1824, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 49.556 [12.000, 99.000],  loss: 7.451958, mae: 2.247391, mean_q: 4.744125\n",
      "[99  1 37 69 33 68 59 27 40 88]\n",
      " 16425/50001: episode: 1825, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 39.000, mean reward:  4.333 [ 3.000,  8.000], mean action: 46.889 [1.000, 88.000],  loss: 6.790608, mae: 2.181525, mean_q: 4.550423\n",
      "[58  5 37 49  6 87 46 95 37 26]\n",
      " 16434/50001: episode: 1826, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 22.000, mean reward:  2.444 [-10.000,  8.000], mean action: 43.111 [5.000, 95.000],  loss: 7.814294, mae: 2.204531, mean_q: 4.678387\n",
      "[34 12 74  2 99 41 20 53  8 64]\n",
      " 16443/50001: episode: 1827, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward: 32.000, mean reward:  3.556 [ 3.000,  5.000], mean action: 41.444 [2.000, 99.000],  loss: 9.713105, mae: 2.194783, mean_q: 4.594718\n",
      "[ 5 46  6 38 20 87 95 78 84 64]\n",
      " 16452/50001: episode: 1828, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 36.000, mean reward:  4.000 [ 2.000,  8.000], mean action: 57.556 [6.000, 95.000],  loss: 8.203293, mae: 2.129264, mean_q: 4.359578\n",
      "[81 91 13 38 44 97 34 13 91 69]\n",
      " 16461/50001: episode: 1829, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward:  9.000, mean reward:  1.000 [-10.000,  7.000], mean action: 54.444 [13.000, 97.000],  loss: 7.347943, mae: 2.123729, mean_q: 4.514073\n",
      "[96 87 50 56 98 34 90 91 19  4]\n",
      " 16470/50001: episode: 1830, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 37.000, mean reward:  4.111 [ 3.000,  6.000], mean action: 58.778 [4.000, 98.000],  loss: 7.915098, mae: 2.138819, mean_q: 4.396859\n",
      "[38 34 41 95 24 38 34  9 95 79]\n",
      " 16479/50001: episode: 1831, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: -2.000, mean reward: -0.222 [-10.000,  6.000], mean action: 49.889 [9.000, 95.000],  loss: 8.496475, mae: 2.149531, mean_q: 4.520034\n",
      "[71  3 19 39 87 99 51 11 12 10]\n",
      " 16488/50001: episode: 1832, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 38.000, mean reward:  4.222 [ 2.000,  8.000], mean action: 36.778 [3.000, 99.000],  loss: 8.326069, mae: 2.163748, mean_q: 4.470108\n",
      "[70 31 73 88 66 37 95  9  2 93]\n",
      " 16497/50001: episode: 1833, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 43.000, mean reward:  4.778 [ 2.000,  7.000], mean action: 54.889 [2.000, 95.000],  loss: 9.715380, mae: 2.196135, mean_q: 4.584592\n",
      "[82 13 86 37 37 56 16 99 86 42]\n",
      " 16506/50001: episode: 1834, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward:  8.000, mean reward:  0.889 [-10.000,  5.000], mean action: 52.444 [13.000, 99.000],  loss: 7.659391, mae: 2.100305, mean_q: 4.399303\n",
      "[34 93 87 62  1 24 97 34 44 79]\n",
      " 16515/50001: episode: 1835, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 57.889 [1.000, 97.000],  loss: 6.962232, mae: 2.148725, mean_q: 4.515692\n",
      "[12 51 73 13 72 91 27 95 88 54]\n",
      " 16524/50001: episode: 1836, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 39.000, mean reward:  4.333 [ 2.000,  8.000], mean action: 62.667 [13.000, 95.000],  loss: 7.635597, mae: 2.247492, mean_q: 4.600456\n",
      "[82  4 28 89 77 95 95 95 78 64]\n",
      " 16533/50001: episode: 1837, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward:  7.000, mean reward:  0.778 [-10.000,  7.000], mean action: 69.444 [4.000, 95.000],  loss: 8.667785, mae: 2.185155, mean_q: 4.462471\n",
      "[98 83 37 26 98 54 34 28 97 50]\n",
      " 16542/50001: episode: 1838, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 30.000, mean reward:  3.333 [-10.000,  8.000], mean action: 56.333 [26.000, 98.000],  loss: 5.695112, mae: 2.115813, mean_q: 4.403553\n",
      "[77  4 50  9 34 69 53 27 64 34]\n",
      " 16551/50001: episode: 1839, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 38.222 [4.000, 69.000],  loss: 6.294649, mae: 2.191245, mean_q: 4.614421\n",
      "[72 95 55 10 36 48 37 14 81 88]\n",
      " 16560/50001: episode: 1840, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 51.556 [10.000, 95.000],  loss: 7.728506, mae: 2.185781, mean_q: 4.525403\n",
      "[70 59 75 51  2 37 98 89 99 34]\n",
      " 16569/50001: episode: 1841, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward: 45.000, mean reward:  5.000 [ 3.000,  9.000], mean action: 60.444 [2.000, 99.000],  loss: 7.796894, mae: 2.204878, mean_q: 4.492874\n",
      "[43 54 30 23 66 57  9 83  9 66]\n",
      " 16578/50001: episode: 1842, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward:  9.000, mean reward:  1.000 [-10.000,  5.000], mean action: 44.111 [9.000, 83.000],  loss: 5.156904, mae: 2.255066, mean_q: 4.622374\n",
      "[76 98 88 70 20 25 44 50 44 66]\n",
      " 16587/50001: episode: 1843, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 26.000, mean reward:  2.889 [-10.000,  9.000], mean action: 56.111 [20.000, 98.000],  loss: 7.724642, mae: 2.324130, mean_q: 4.670383\n",
      "[85 88 34 90 53 74 96 66 61  4]\n",
      " 16596/50001: episode: 1844, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 62.889 [4.000, 96.000],  loss: 7.737924, mae: 2.371027, mean_q: 4.858972\n",
      "[20 50 38 96 41 43 65 34 88  9]\n",
      " 16605/50001: episode: 1845, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 40.000, mean reward:  4.444 [ 2.000,  9.000], mean action: 51.556 [9.000, 96.000],  loss: 7.767292, mae: 2.327137, mean_q: 4.741467\n",
      "[ 7  5 44 67 37 42 46 41 27  1]\n",
      " 16614/50001: episode: 1846, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 43.000, mean reward:  4.778 [ 2.000,  9.000], mean action: 34.444 [1.000, 67.000],  loss: 5.414534, mae: 2.247396, mean_q: 4.504370\n",
      "[10 68 90 89 95  4 48 52 72 14]\n",
      " 16623/50001: episode: 1847, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 38.000, mean reward:  4.222 [ 2.000,  6.000], mean action: 59.111 [4.000, 95.000],  loss: 4.623290, mae: 2.327712, mean_q: 4.685927\n",
      "[80 27 41 47 77 34 77  5 89 23]\n",
      " 16632/50001: episode: 1848, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 46.667 [5.000, 89.000],  loss: 8.618033, mae: 2.385597, mean_q: 4.832662\n",
      "[28 24 20 50 12 71 42 27 93 97]\n",
      " 16641/50001: episode: 1849, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 42.000, mean reward:  4.667 [ 2.000,  8.000], mean action: 48.444 [12.000, 97.000],  loss: 6.015394, mae: 2.268866, mean_q: 4.606858\n",
      "[ 6 90 24 33 32 79 35 74 57 93]\n",
      " 16650/50001: episode: 1850, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 41.000, mean reward:  4.556 [ 3.000,  6.000], mean action: 57.444 [24.000, 93.000],  loss: 8.645423, mae: 2.346491, mean_q: 4.755762\n",
      "[74 59 13 34 50 88 95 31 14 99]\n",
      " 16659/50001: episode: 1851, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 53.667 [13.000, 99.000],  loss: 7.480731, mae: 2.288810, mean_q: 4.666094\n",
      "[52 51 32 56 16 87 48 16 95 74]\n",
      " 16668/50001: episode: 1852, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 27.000, mean reward:  3.000 [-10.000, 10.000], mean action: 52.778 [16.000, 95.000],  loss: 4.897267, mae: 2.291631, mean_q: 4.756153\n",
      "[79 79 67 37 27 56 21 30 79 48]\n",
      " 16677/50001: episode: 1853, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 10.000, mean reward:  1.111 [-10.000,  8.000], mean action: 49.333 [21.000, 79.000],  loss: 5.643456, mae: 2.380692, mean_q: 4.797538\n",
      "[54 30 86 34 48 31  1 31  1 85]\n",
      " 16686/50001: episode: 1854, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 12.000, mean reward:  1.333 [-10.000,  6.000], mean action: 38.556 [1.000, 86.000],  loss: 5.603841, mae: 2.376503, mean_q: 4.854694\n",
      "[ 6 34 60 48 15 78 13  1  1 62]\n",
      " 16695/50001: episode: 1855, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 34.667 [1.000, 78.000],  loss: 7.126004, mae: 2.383901, mean_q: 4.872572\n",
      "[56 88 24 97  1 65 20 79 99 48]\n",
      " 16704/50001: episode: 1856, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 57.889 [1.000, 99.000],  loss: 6.547428, mae: 2.364249, mean_q: 4.759356\n",
      "[81 76 46 40 99  2 42  8 57 23]\n",
      " 16713/50001: episode: 1857, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 43.667 [2.000, 99.000],  loss: 8.281261, mae: 2.397932, mean_q: 4.850811\n",
      "[94 41 99 12 12 60 38 53 17 97]\n",
      " 16722/50001: episode: 1858, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 22.000, mean reward:  2.444 [-10.000,  8.000], mean action: 47.667 [12.000, 99.000],  loss: 8.963225, mae: 2.342027, mean_q: 4.815705\n",
      "[84  5 87 98 80  1 33 53 82 14]\n",
      " 16731/50001: episode: 1859, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 50.333 [1.000, 98.000],  loss: 8.294627, mae: 2.326916, mean_q: 4.783789\n",
      "[41 88 76 28 34 84 99 31 59 87]\n",
      " 16740/50001: episode: 1860, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 37.000, mean reward:  4.111 [ 3.000,  6.000], mean action: 65.111 [28.000, 99.000],  loss: 6.595538, mae: 2.179825, mean_q: 4.475171\n",
      "[79 48 47 14 46 48 17 83  2  4]\n",
      " 16749/50001: episode: 1861, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 34.333 [2.000, 83.000],  loss: 6.765419, mae: 2.160213, mean_q: 4.431124\n",
      "[97 28 79 28 88  6 70 85 41 96]\n",
      " 16758/50001: episode: 1862, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 22.000, mean reward:  2.444 [-10.000,  5.000], mean action: 57.889 [6.000, 96.000],  loss: 7.342731, mae: 2.315923, mean_q: 4.728220\n",
      "[23 59 39 95 13 37 18 47 27 88]\n",
      " 16767/50001: episode: 1863, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 43.000, mean reward:  4.778 [ 2.000,  7.000], mean action: 47.000 [13.000, 95.000],  loss: 6.136444, mae: 2.266749, mean_q: 4.535132\n",
      "[54 27 88 63 66 95 88 84 88  9]\n",
      " 16776/50001: episode: 1864, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 13.000, mean reward:  1.444 [-10.000,  7.000], mean action: 67.556 [9.000, 95.000],  loss: 6.582889, mae: 2.282725, mean_q: 4.574386\n",
      "[51 95 17 97 30 14 50 28 50 13]\n",
      " 16785/50001: episode: 1865, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 43.778 [13.000, 97.000],  loss: 10.618707, mae: 2.298458, mean_q: 4.649333\n",
      "[44 13 88 37 28 63 80 88 13 37]\n",
      " 16794/50001: episode: 1866, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: -7.000, mean reward: -0.778 [-10.000,  5.000], mean action: 49.667 [13.000, 88.000],  loss: 6.149952, mae: 2.251499, mean_q: 4.637943\n",
      "[19 28 49 35 54 17 37  1 42 35]\n",
      " 16803/50001: episode: 1867, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 33.111 [1.000, 54.000],  loss: 8.080410, mae: 2.219192, mean_q: 4.541397\n",
      "[ 9 61 60 12 65 18 55  6 57 79]\n",
      " 16812/50001: episode: 1868, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 36.000, mean reward:  4.000 [ 2.000,  7.000], mean action: 45.889 [6.000, 79.000],  loss: 7.290643, mae: 2.204463, mean_q: 4.537241\n",
      "[16 63 10 92 88 51 95 11 88 10]\n",
      " 16821/50001: episode: 1869, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward:  9.000, mean reward:  1.000 [-10.000,  7.000], mean action: 56.444 [10.000, 95.000],  loss: 7.995808, mae: 2.233946, mean_q: 4.536474\n",
      "[19 31 71 44 30 59 38 95 42 14]\n",
      " 16830/50001: episode: 1870, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 42.000, mean reward:  4.667 [ 2.000,  9.000], mean action: 47.111 [14.000, 95.000],  loss: 5.330190, mae: 2.209610, mean_q: 4.542347\n",
      "[23 34 83 17 27 91 17 96 10 93]\n",
      " 16839/50001: episode: 1871, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 52.000 [10.000, 96.000],  loss: 5.277228, mae: 2.223865, mean_q: 4.625628\n",
      "[59 87 79 29 80 10 13 57  5 31]\n",
      " 16848/50001: episode: 1872, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 43.444 [5.000, 87.000],  loss: 7.099515, mae: 2.187565, mean_q: 4.546772\n",
      "[84 92 90 59 37 57 23 89 40 88]\n",
      " 16857/50001: episode: 1873, duration: 0.106s, episode steps:   9, steps per second:  85, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 63.889 [23.000, 92.000],  loss: 6.974741, mae: 2.256708, mean_q: 4.549122\n",
      "[41 34 27 13 10 70 95 75 64 88]\n",
      " 16866/50001: episode: 1874, duration: 0.078s, episode steps:   9, steps per second: 115, episode reward: 43.000, mean reward:  4.778 [ 2.000,  9.000], mean action: 52.889 [10.000, 95.000],  loss: 6.959693, mae: 2.300038, mean_q: 4.684294\n",
      "[15 13 34 79 32 57 28 52 82 12]\n",
      " 16875/50001: episode: 1875, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 42.000, mean reward:  4.667 [ 4.000,  8.000], mean action: 43.222 [12.000, 82.000],  loss: 6.010202, mae: 2.261008, mean_q: 4.571163\n",
      "[ 2 32  9 54  6  9 69  0 17 50]\n",
      " 16884/50001: episode: 1876, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 21.000, mean reward:  2.333 [-10.000, 10.000], mean action: 27.333 [0.000, 69.000],  loss: 8.201904, mae: 2.244889, mean_q: 4.600504\n",
      "[56 13 61 21 55 18 43 20  2 58]\n",
      " 16893/50001: episode: 1877, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 35.000, mean reward:  3.889 [ 2.000,  7.000], mean action: 32.333 [2.000, 61.000],  loss: 7.852011, mae: 2.205917, mean_q: 4.540941\n",
      "[35 91 52 92 77  6 52 89 56 48]\n",
      " 16902/50001: episode: 1878, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 27.000, mean reward:  3.000 [-10.000,  9.000], mean action: 62.556 [6.000, 92.000],  loss: 7.486023, mae: 2.215974, mean_q: 4.607002\n",
      "[ 0 62 98 12 25 99 41 50  1 83]\n",
      " 16911/50001: episode: 1879, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 52.333 [1.000, 99.000],  loss: 8.398446, mae: 2.208128, mean_q: 4.459429\n",
      "[84 12 50 87 87 13  1 37  9 31]\n",
      " 16920/50001: episode: 1880, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 28.000, mean reward:  3.111 [-10.000,  6.000], mean action: 36.333 [1.000, 87.000],  loss: 8.939121, mae: 2.178026, mean_q: 4.506751\n",
      "[77 51 34 75 92 93 34 95 52 12]\n",
      " 16929/50001: episode: 1881, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 27.000, mean reward:  3.000 [-10.000,  8.000], mean action: 59.778 [12.000, 95.000],  loss: 7.245321, mae: 2.191261, mean_q: 4.547888\n",
      "[91 83 90 95 66 31 88 17 24 20]\n",
      " 16938/50001: episode: 1882, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 40.000, mean reward:  4.444 [ 2.000,  6.000], mean action: 57.111 [17.000, 95.000],  loss: 7.460405, mae: 2.164732, mean_q: 4.467465\n",
      "[17 33 63 11 18 98 64 59 27 10]\n",
      " 16947/50001: episode: 1883, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 42.556 [10.000, 98.000],  loss: 7.891189, mae: 2.126088, mean_q: 4.449226\n",
      "[55  9 35 94 58 79 37 36  2 37]\n",
      " 16956/50001: episode: 1884, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 43.000 [2.000, 94.000],  loss: 7.459228, mae: 2.159977, mean_q: 4.468098\n",
      "[62 93 90 27  1 28 21 28 95 50]\n",
      " 16965/50001: episode: 1885, duration: 0.070s, episode steps:   9, steps per second: 129, episode reward: 31.000, mean reward:  3.444 [-10.000,  9.000], mean action: 48.111 [1.000, 95.000],  loss: 8.462694, mae: 2.143116, mean_q: 4.390236\n",
      "[22 52  8 85 48 57 31 34 41  4]\n",
      " 16974/50001: episode: 1886, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 40.000 [4.000, 85.000],  loss: 7.235482, mae: 2.235956, mean_q: 4.655134\n",
      "[ 2 41 78 15  1 95 96 52 14 95]\n",
      " 16983/50001: episode: 1887, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 54.111 [1.000, 96.000],  loss: 7.443666, mae: 2.189407, mean_q: 4.436555\n",
      "[ 6 49 77 87 80 48  2 37 94 47]\n",
      " 16992/50001: episode: 1888, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 57.889 [2.000, 94.000],  loss: 6.529888, mae: 2.181762, mean_q: 4.466998\n",
      "[31  2 91 46  2 42 97 32 41 38]\n",
      " 17001/50001: episode: 1889, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 21.000, mean reward:  2.333 [-10.000,  5.000], mean action: 43.444 [2.000, 97.000],  loss: 8.503453, mae: 2.235717, mean_q: 4.691065\n",
      "[13 28 55 87 37 86 95 32 23 64]\n",
      " 17010/50001: episode: 1890, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 56.333 [23.000, 95.000],  loss: 7.086336, mae: 2.341455, mean_q: 4.773536\n",
      "[65 34 90 14 54 28 89 21  1 42]\n",
      " 17019/50001: episode: 1891, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 43.000, mean reward:  4.778 [ 3.000,  7.000], mean action: 41.444 [1.000, 90.000],  loss: 7.346518, mae: 2.285010, mean_q: 4.625907\n",
      "[83 95 35 41  2 16 35  1 90 90]\n",
      " 17028/50001: episode: 1892, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 12.000, mean reward:  1.333 [-10.000,  7.000], mean action: 45.000 [1.000, 95.000],  loss: 5.842823, mae: 2.274853, mean_q: 4.601771\n",
      "[34 40 66 21 12 98  1 95 93 88]\n",
      " 17037/50001: episode: 1893, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 44.000, mean reward:  4.889 [ 3.000,  8.000], mean action: 57.111 [1.000, 98.000],  loss: 5.652295, mae: 2.310245, mean_q: 4.677195\n",
      "[10 37 28 42 37 14 13 13 24 41]\n",
      " 17046/50001: episode: 1894, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 10.000, mean reward:  1.111 [-10.000,  5.000], mean action: 27.667 [13.000, 42.000],  loss: 6.390441, mae: 2.418809, mean_q: 4.864044\n",
      "[ 4 14 13 79  6 32 48 93  9 29]\n",
      " 17055/50001: episode: 1895, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 34.000, mean reward:  3.778 [ 2.000,  5.000], mean action: 35.889 [6.000, 93.000],  loss: 6.273969, mae: 2.410956, mean_q: 4.834479\n",
      "[69 74 80 59 37 60  0 25 67 90]\n",
      " 17064/50001: episode: 1896, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 34.000, mean reward:  3.778 [ 2.000,  6.000], mean action: 54.667 [0.000, 90.000],  loss: 7.822567, mae: 2.380472, mean_q: 4.774146\n",
      "[99 13 60 16 59 32 99 31  4 31]\n",
      " 17073/50001: episode: 1897, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward:  8.000, mean reward:  0.889 [-10.000,  5.000], mean action: 38.333 [4.000, 99.000],  loss: 8.142476, mae: 2.350997, mean_q: 4.808751\n",
      "[ 3 13 68 57  2 45 53 86 64 66]\n",
      " 17082/50001: episode: 1898, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 39.000, mean reward:  4.333 [ 2.000,  9.000], mean action: 50.444 [2.000, 86.000],  loss: 6.963564, mae: 2.261449, mean_q: 4.572514\n",
      "[18 24 26 88  9 37 77 34 40  6]\n",
      " 17091/50001: episode: 1899, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 37.889 [6.000, 88.000],  loss: 7.512157, mae: 2.273252, mean_q: 4.643322\n",
      "[22 53 65 84 44 16 52 35 28  8]\n",
      " 17100/50001: episode: 1900, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 36.000, mean reward:  4.000 [ 2.000,  8.000], mean action: 42.778 [8.000, 84.000],  loss: 8.102610, mae: 2.278927, mean_q: 4.610891\n",
      "[91 46 14 48 24 50 10 75 44 98]\n",
      " 17109/50001: episode: 1901, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 45.444 [10.000, 98.000],  loss: 7.220660, mae: 2.305164, mean_q: 4.661598\n",
      "[95 82 41 94  1 75 88 33 52 12]\n",
      " 17118/50001: episode: 1902, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 43.000, mean reward:  4.778 [ 3.000,  9.000], mean action: 53.111 [1.000, 94.000],  loss: 8.368561, mae: 2.258273, mean_q: 4.632833\n",
      "[71 74 16 11 60 30 89 82 76 31]\n",
      " 17127/50001: episode: 1903, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 52.111 [11.000, 89.000],  loss: 6.215754, mae: 2.309680, mean_q: 4.711558\n",
      "[ 5 21 54 32 93 93  0 89 86 12]\n",
      " 17136/50001: episode: 1904, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 26.000, mean reward:  2.889 [-10.000, 11.000], mean action: 53.333 [0.000, 93.000],  loss: 6.290368, mae: 2.309144, mean_q: 4.667313\n",
      "[59 41 61 21 78 53 95 89 21 34]\n",
      " 17145/50001: episode: 1905, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 28.000, mean reward:  3.111 [-10.000, 10.000], mean action: 54.778 [21.000, 95.000],  loss: 9.368726, mae: 2.345446, mean_q: 4.834941\n",
      "[63  6 32 74 58 88 21 62 30 12]\n",
      " 17154/50001: episode: 1906, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 43.000, mean reward:  4.778 [ 3.000, 10.000], mean action: 42.556 [6.000, 88.000],  loss: 8.544524, mae: 2.303993, mean_q: 4.679703\n",
      "[11 95 85 99 36 21 67 74 88 48]\n",
      " 17163/50001: episode: 1907, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 41.000, mean reward:  4.556 [ 2.000,  9.000], mean action: 68.111 [21.000, 99.000],  loss: 5.463169, mae: 2.277513, mean_q: 4.595583\n",
      "[30 99 28 41 28 13 27 27 37 48]\n",
      " 17172/50001: episode: 1908, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 14.000, mean reward:  1.556 [-10.000,  7.000], mean action: 38.667 [13.000, 99.000],  loss: 8.092948, mae: 2.366209, mean_q: 4.772467\n",
      "[ 0 95 92 42 37 63 88 50 66 23]\n",
      " 17181/50001: episode: 1909, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 45.000, mean reward:  5.000 [ 3.000,  7.000], mean action: 61.778 [23.000, 95.000],  loss: 8.271609, mae: 2.277485, mean_q: 4.566687\n",
      "[ 7 99 79 84 95 76 32  6 41 88]\n",
      " 17190/50001: episode: 1910, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 66.667 [6.000, 99.000],  loss: 7.074389, mae: 2.302160, mean_q: 4.727213\n",
      "[48 27 32 42 99 49 62 44 62 31]\n",
      " 17199/50001: episode: 1911, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 20.000, mean reward:  2.222 [-10.000,  7.000], mean action: 49.778 [27.000, 99.000],  loss: 6.643584, mae: 2.281929, mean_q: 4.594071\n",
      "[62 13 52 58 37 16 12 94 37 37]\n",
      " 17208/50001: episode: 1912, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward:  7.000, mean reward:  0.778 [-10.000,  6.000], mean action: 39.556 [12.000, 94.000],  loss: 7.905365, mae: 2.290193, mean_q: 4.679867\n",
      "[49 35 61  0 99 13 95 87 50 31]\n",
      " 17217/50001: episode: 1913, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 41.000, mean reward:  4.556 [ 2.000,  8.000], mean action: 52.333 [0.000, 99.000],  loss: 8.587231, mae: 2.243340, mean_q: 4.554104\n",
      "[73 61 99 12 95 60 57 42 82 12]\n",
      " 17226/50001: episode: 1914, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 57.778 [12.000, 99.000],  loss: 6.878753, mae: 2.249602, mean_q: 4.570947\n",
      "[66 95 69 16 14  9 78  4 82 33]\n",
      " 17235/50001: episode: 1915, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 37.000, mean reward:  4.111 [ 3.000,  6.000], mean action: 44.444 [4.000, 95.000],  loss: 8.752127, mae: 2.169919, mean_q: 4.492298\n",
      "[75 84 55 92 11 66 88 56 21  9]\n",
      " 17244/50001: episode: 1916, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 53.556 [9.000, 92.000],  loss: 6.173127, mae: 2.189253, mean_q: 4.461801\n",
      "[36 67 27 91 20 86 10 64 37 10]\n",
      " 17253/50001: episode: 1917, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 23.000, mean reward:  2.556 [-10.000,  9.000], mean action: 45.778 [10.000, 91.000],  loss: 6.576404, mae: 2.176687, mean_q: 4.417596\n",
      "[39 11 40 89 71  5 93 31 28 41]\n",
      " 17262/50001: episode: 1918, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 45.444 [5.000, 93.000],  loss: 8.446949, mae: 2.244538, mean_q: 4.529012\n",
      "[92 42 42 48 45 48 58 40  9 66]\n",
      " 17271/50001: episode: 1919, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 10.000, mean reward:  1.111 [-10.000,  8.000], mean action: 44.222 [9.000, 66.000],  loss: 8.734930, mae: 2.229405, mean_q: 4.592817\n",
      "[37 95 87 37 81 95 16 35 32 96]\n",
      " 17280/50001: episode: 1920, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward:  4.000, mean reward:  0.444 [-10.000,  5.000], mean action: 63.778 [16.000, 96.000],  loss: 8.824014, mae: 2.164629, mean_q: 4.375659\n",
      "[65 28 41 29 66 78 10 80 23 97]\n",
      " 17289/50001: episode: 1921, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 39.000, mean reward:  4.333 [ 2.000,  8.000], mean action: 50.222 [10.000, 97.000],  loss: 9.892153, mae: 2.195388, mean_q: 4.535137\n",
      "[83 27 59 52 74 54 51  5 55 32]\n",
      " 17298/50001: episode: 1922, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 39.000, mean reward:  4.333 [ 3.000,  8.000], mean action: 45.444 [5.000, 74.000],  loss: 8.614979, mae: 2.241753, mean_q: 4.564201\n",
      "[85  5 58 65 53 24 59 15 89 84]\n",
      " 17307/50001: episode: 1923, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 34.000, mean reward:  3.778 [ 2.000,  5.000], mean action: 50.222 [5.000, 89.000],  loss: 7.022907, mae: 2.140294, mean_q: 4.332964\n",
      "[ 8 89 99 34 96 94 69 95 34 28]\n",
      " 17316/50001: episode: 1924, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 26.000, mean reward:  2.889 [-10.000,  8.000], mean action: 70.889 [28.000, 99.000],  loss: 6.789782, mae: 2.151444, mean_q: 4.346746\n",
      "[59 69 47 13 63 86  1 83 50 12]\n",
      " 17325/50001: episode: 1925, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 44.000, mean reward:  4.889 [ 3.000,  8.000], mean action: 47.111 [1.000, 86.000],  loss: 7.220947, mae: 2.201008, mean_q: 4.454706\n",
      "[ 7 55 79 45 94 56 64 60 23 14]\n",
      " 17334/50001: episode: 1926, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 54.444 [14.000, 94.000],  loss: 5.855046, mae: 2.269161, mean_q: 4.600721\n",
      "[53 58 12 32 74 98 13 95 12 86]\n",
      " 17343/50001: episode: 1927, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 53.333 [12.000, 98.000],  loss: 7.112114, mae: 2.362752, mean_q: 4.745029\n",
      "[65 83 50 27 48  4 33 56 44 21]\n",
      " 17352/50001: episode: 1928, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 36.000, mean reward:  4.000 [ 3.000,  5.000], mean action: 40.667 [4.000, 83.000],  loss: 8.484648, mae: 2.302178, mean_q: 4.710891\n",
      "[ 2 28  8 69 48 34 78 12 84 13]\n",
      " 17361/50001: episode: 1929, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 41.556 [8.000, 84.000],  loss: 5.666642, mae: 2.340104, mean_q: 4.685838\n",
      "[59 28 74 34 66 32 88 67 42 84]\n",
      " 17370/50001: episode: 1930, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 37.000, mean reward:  4.111 [ 3.000,  5.000], mean action: 57.222 [28.000, 88.000],  loss: 6.741767, mae: 2.381248, mean_q: 4.849932\n",
      "[34 48 48 30 53  4 27 21 18 75]\n",
      " 17379/50001: episode: 1931, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 21.000, mean reward:  2.333 [-10.000,  7.000], mean action: 36.000 [4.000, 75.000],  loss: 5.801873, mae: 2.402289, mean_q: 4.731402\n",
      "[97 73 30 50 56 17 37 28  2 34]\n",
      " 17388/50001: episode: 1932, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 43.000, mean reward:  4.778 [ 2.000,  8.000], mean action: 36.333 [2.000, 73.000],  loss: 5.852415, mae: 2.386752, mean_q: 4.783767\n",
      "[13 93 50 99 41 11 95 18 88 48]\n",
      " 17397/50001: episode: 1933, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 60.333 [11.000, 99.000],  loss: 6.883062, mae: 2.386028, mean_q: 4.728563\n",
      "[53 75 41 28 55 52 30 57 99 50]\n",
      " 17406/50001: episode: 1934, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 43.000, mean reward:  4.778 [ 2.000,  9.000], mean action: 54.111 [28.000, 99.000],  loss: 7.908769, mae: 2.345762, mean_q: 4.692081\n",
      "[56 67 30 41 12  2 76 40 90 18]\n",
      " 17415/50001: episode: 1935, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward: 38.000, mean reward:  4.222 [ 3.000,  5.000], mean action: 41.778 [2.000, 90.000],  loss: 8.513695, mae: 2.331120, mean_q: 4.644696\n",
      "[73 49 91 54 37 14 54 52 54 52]\n",
      " 17424/50001: episode: 1936, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: -5.000, mean reward: -0.556 [-10.000,  6.000], mean action: 50.778 [14.000, 91.000],  loss: 7.953137, mae: 2.280428, mean_q: 4.555164\n",
      "[94 61  1 79  4 51 64 76 21 61]\n",
      " 17433/50001: episode: 1937, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 18.000, mean reward:  2.000 [-10.000,  4.000], mean action: 46.444 [1.000, 79.000],  loss: 9.761926, mae: 2.271596, mean_q: 4.567346\n",
      "[86 17 21 50 82 93 11 95 57 12]\n",
      " 17442/50001: episode: 1938, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 47.000, mean reward:  5.222 [ 3.000,  9.000], mean action: 48.667 [11.000, 95.000],  loss: 7.333101, mae: 2.243262, mean_q: 4.501316\n",
      "[56  5 85 14 81 93 52 69 66 12]\n",
      " 17451/50001: episode: 1939, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 40.000, mean reward:  4.444 [ 3.000,  8.000], mean action: 53.000 [5.000, 93.000],  loss: 6.482193, mae: 2.241372, mean_q: 4.508833\n",
      "[19 59 89 28 94 18 33 11 65 32]\n",
      " 17460/50001: episode: 1940, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 37.000, mean reward:  4.111 [ 3.000,  8.000], mean action: 47.667 [11.000, 94.000],  loss: 7.254310, mae: 2.297117, mean_q: 4.652285\n",
      "[94  4 32 56 88 61 16 45  4 41]\n",
      " 17469/50001: episode: 1941, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 20.000, mean reward:  2.222 [-10.000,  6.000], mean action: 38.556 [4.000, 88.000],  loss: 8.798031, mae: 2.287735, mean_q: 4.595008\n",
      "[39 51 64 34 66 95 13 28 67 13]\n",
      " 17478/50001: episode: 1942, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 47.889 [13.000, 95.000],  loss: 7.902765, mae: 2.268724, mean_q: 4.516468\n",
      "[33 95 85 57 63 68 73 43 34 42]\n",
      " 17487/50001: episode: 1943, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 40.000, mean reward:  4.444 [ 2.000, 10.000], mean action: 62.222 [34.000, 95.000],  loss: 6.277430, mae: 2.260167, mean_q: 4.529749\n",
      "[92 36 14 75 16 76  1 67 24 95]\n",
      " 17496/50001: episode: 1944, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 44.000, mean reward:  4.889 [ 2.000, 11.000], mean action: 44.889 [1.000, 95.000],  loss: 8.249804, mae: 2.247422, mean_q: 4.570254\n",
      "[33 94 47 13 50 18 63 69  5  5]\n",
      " 17505/50001: episode: 1945, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward: 23.000, mean reward:  2.556 [-10.000,  5.000], mean action: 40.444 [5.000, 94.000],  loss: 6.635342, mae: 2.214671, mean_q: 4.439783\n",
      "[16 52 60 99  4 35 47 93 37 86]\n",
      " 17514/50001: episode: 1946, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 36.000, mean reward:  4.000 [ 2.000,  7.000], mean action: 57.000 [4.000, 99.000],  loss: 8.307515, mae: 2.236510, mean_q: 4.467503\n",
      "[89 78 41 37 50 99  1 87 53 64]\n",
      " 17523/50001: episode: 1947, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 39.000, mean reward:  4.333 [ 3.000,  5.000], mean action: 56.667 [1.000, 99.000],  loss: 6.893919, mae: 2.252817, mean_q: 4.529991\n",
      "[90 28 24 46 76 46 35 16  1 93]\n",
      " 17532/50001: episode: 1948, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 27.000, mean reward:  3.000 [-10.000,  8.000], mean action: 40.556 [1.000, 93.000],  loss: 7.645749, mae: 2.249700, mean_q: 4.496655\n",
      "[86 46  6 76 44 27 37 65 66 64]\n",
      " 17541/50001: episode: 1949, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 47.889 [6.000, 76.000],  loss: 8.441655, mae: 2.225602, mean_q: 4.481910\n",
      "[ 8 95 54  2 88 41  4 23 33 18]\n",
      " 17550/50001: episode: 1950, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 38.000, mean reward:  4.222 [ 3.000,  5.000], mean action: 39.778 [2.000, 95.000],  loss: 6.131102, mae: 2.197539, mean_q: 4.415323\n",
      "[89 98 27  1 84 53 48 59 64 64]\n",
      " 17559/50001: episode: 1951, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 21.000, mean reward:  2.333 [-10.000,  5.000], mean action: 55.333 [1.000, 98.000],  loss: 6.734591, mae: 2.278847, mean_q: 4.575703\n",
      "[96 72 10 37  8 56 90 88 56 14]\n",
      " 17568/50001: episode: 1952, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 47.889 [8.000, 90.000],  loss: 9.225410, mae: 2.292902, mean_q: 4.621578\n",
      "[89  2 35 47  0 63 59 66 92 97]\n",
      " 17577/50001: episode: 1953, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 51.222 [0.000, 97.000],  loss: 8.155825, mae: 2.257188, mean_q: 4.550757\n",
      "[60 83 47 92 63 18 32 10 68 69]\n",
      " 17586/50001: episode: 1954, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 35.000, mean reward:  3.889 [ 3.000,  5.000], mean action: 53.556 [10.000, 92.000],  loss: 9.853679, mae: 2.243011, mean_q: 4.469591\n",
      "[66 18 29 97 33 50 91 18 21 14]\n",
      " 17595/50001: episode: 1955, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 22.000, mean reward:  2.444 [-10.000,  7.000], mean action: 41.222 [14.000, 97.000],  loss: 5.711806, mae: 2.198370, mean_q: 4.436974\n",
      "[43 74 53 93  2 37  0 42 97 48]\n",
      " 17604/50001: episode: 1956, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 44.000, mean reward:  4.889 [ 2.000,  8.000], mean action: 49.556 [0.000, 97.000],  loss: 7.590294, mae: 2.213248, mean_q: 4.513254\n",
      "[22 91 85 13 12 14 57 79 60 50]\n",
      " 17613/50001: episode: 1957, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 51.222 [12.000, 91.000],  loss: 9.011683, mae: 2.276413, mean_q: 4.637794\n",
      "[94 12 65 12  2 13 58 37 95 62]\n",
      " 17622/50001: episode: 1958, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 39.556 [2.000, 95.000],  loss: 9.989178, mae: 2.296416, mean_q: 4.624241\n",
      "[41 13 46 44 56 57 24  3 95 35]\n",
      " 17631/50001: episode: 1959, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 38.000, mean reward:  4.222 [ 2.000,  9.000], mean action: 41.444 [3.000, 95.000],  loss: 8.193971, mae: 2.245339, mean_q: 4.472747\n",
      "[69 34 13 83 69 74 60  4 11 30]\n",
      " 17640/50001: episode: 1960, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 25.000, mean reward:  2.778 [-10.000,  5.000], mean action: 42.000 [4.000, 83.000],  loss: 8.549170, mae: 2.249612, mean_q: 4.574350\n",
      "[32 74 32 53 52 86 95 78 28 86]\n",
      " 17649/50001: episode: 1961, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 10.000, mean reward:  1.111 [-10.000,  7.000], mean action: 64.889 [28.000, 95.000],  loss: 8.796925, mae: 2.202397, mean_q: 4.407031\n",
      "[46  4 62 12 98 17 96 19 31  5]\n",
      " 17658/50001: episode: 1962, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 36.000, mean reward:  4.000 [ 2.000,  6.000], mean action: 38.222 [4.000, 98.000],  loss: 7.039814, mae: 2.140523, mean_q: 4.368135\n",
      "[16 21 42 52 97 54 74  9  5 12]\n",
      " 17667/50001: episode: 1963, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 43.000, mean reward:  4.778 [ 3.000,  9.000], mean action: 40.667 [5.000, 97.000],  loss: 6.036495, mae: 2.186686, mean_q: 4.535587\n",
      "[96 34  9 50  1 13 41 85 64 24]\n",
      " 17676/50001: episode: 1964, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 35.667 [1.000, 85.000],  loss: 6.925866, mae: 2.255300, mean_q: 4.489157\n",
      "[77 32 23  9 67 49 78  1  8 66]\n",
      " 17685/50001: episode: 1965, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 38.000, mean reward:  4.222 [ 3.000,  8.000], mean action: 37.000 [1.000, 78.000],  loss: 7.030882, mae: 2.263253, mean_q: 4.554617\n",
      "[49 40 13 24 75 52 92 34 79 98]\n",
      " 17694/50001: episode: 1966, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 56.333 [13.000, 98.000],  loss: 7.376741, mae: 2.260009, mean_q: 4.523291\n",
      "[34 50 24 14 69  6 49 57  0 39]\n",
      " 17703/50001: episode: 1967, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 30.000, mean reward:  3.333 [ 2.000,  5.000], mean action: 34.222 [0.000, 69.000],  loss: 7.131092, mae: 2.272057, mean_q: 4.587089\n",
      "[92 34 56 42 31 67 79 14 27 66]\n",
      " 17712/50001: episode: 1968, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 46.222 [14.000, 79.000],  loss: 7.284464, mae: 2.254054, mean_q: 4.495912\n",
      "[77 90 59 24 84 13 24 28 27 13]\n",
      " 17721/50001: episode: 1969, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 10.000, mean reward:  1.111 [-10.000,  6.000], mean action: 40.222 [13.000, 90.000],  loss: 6.862021, mae: 2.290163, mean_q: 4.581118\n",
      "[81 63 12 16 98 90 32  4 70 36]\n",
      " 17730/50001: episode: 1970, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 34.000, mean reward:  3.778 [ 2.000,  5.000], mean action: 46.778 [4.000, 98.000],  loss: 8.164795, mae: 2.247889, mean_q: 4.522440\n",
      "[60 42 10 69 95 32 83  9 40 40]\n",
      " 17739/50001: episode: 1971, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 46.667 [9.000, 95.000],  loss: 8.977579, mae: 2.282016, mean_q: 4.681498\n",
      "[ 4 45 69 84 94 49 87 90  9 49]\n",
      " 17748/50001: episode: 1972, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 19.000, mean reward:  2.111 [-10.000,  6.000], mean action: 64.000 [9.000, 94.000],  loss: 6.336045, mae: 2.242046, mean_q: 4.507360\n",
      "[41 46 24 87 77 34 72 43  5 24]\n",
      " 17757/50001: episode: 1973, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 21.000, mean reward:  2.333 [-10.000,  6.000], mean action: 45.778 [5.000, 87.000],  loss: 7.419900, mae: 2.238917, mean_q: 4.433664\n",
      "[71 57 16 34 64  4 71 64 50 73]\n",
      " 17766/50001: episode: 1974, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 11.000, mean reward:  1.222 [-10.000,  8.000], mean action: 48.111 [4.000, 73.000],  loss: 7.090659, mae: 2.239486, mean_q: 4.569760\n",
      "[45  3 98 66 53 42 94 34 42 21]\n",
      " 17775/50001: episode: 1975, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 50.333 [3.000, 98.000],  loss: 7.195873, mae: 2.259881, mean_q: 4.529628\n",
      "[26 95 40 13  4 48 88 50  6  1]\n",
      " 17784/50001: episode: 1976, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 38.333 [1.000, 95.000],  loss: 5.993533, mae: 2.340963, mean_q: 4.672458\n",
      "[50 30 50 97 95 13 46 48 83  9]\n",
      " 17793/50001: episode: 1977, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 24.000, mean reward:  2.667 [-10.000,  5.000], mean action: 52.333 [9.000, 97.000],  loss: 5.652530, mae: 2.328961, mean_q: 4.649681\n",
      "[15 23 21 65 40 61 41 21 76 90]\n",
      " 17802/50001: episode: 1978, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 22.000, mean reward:  2.444 [-10.000,  8.000], mean action: 48.667 [21.000, 90.000],  loss: 7.502754, mae: 2.368334, mean_q: 4.763081\n",
      "[24 28  6 12 59 31 13 57 79 50]\n",
      " 17811/50001: episode: 1979, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 37.222 [6.000, 79.000],  loss: 5.821102, mae: 2.416721, mean_q: 4.862326\n",
      "[81 54 56 98 93 13 38 13 34 37]\n",
      " 17820/50001: episode: 1980, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 28.000, mean reward:  3.111 [-10.000,  8.000], mean action: 48.444 [13.000, 98.000],  loss: 8.887334, mae: 2.325056, mean_q: 4.639675\n",
      "[97 34 98 12 99 32  4 88 14 86]\n",
      " 17829/50001: episode: 1981, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 36.000, mean reward:  4.000 [ 2.000,  5.000], mean action: 51.889 [4.000, 99.000],  loss: 7.874608, mae: 2.359705, mean_q: 4.803251\n",
      "[40 30 95 11 14 94 46 95 95 37]\n",
      " 17838/50001: episode: 1982, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 10.000, mean reward:  1.111 [-10.000,  7.000], mean action: 57.444 [11.000, 95.000],  loss: 6.828102, mae: 2.318837, mean_q: 4.754050\n",
      "[ 1 53 32 10 94 95 30 74 66  4]\n",
      " 17847/50001: episode: 1983, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 50.889 [4.000, 95.000],  loss: 9.243779, mae: 2.329436, mean_q: 4.722796\n",
      "[85 59 83 40 74 14 90 23 31 42]\n",
      " 17856/50001: episode: 1984, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 41.000, mean reward:  4.556 [ 3.000,  6.000], mean action: 50.667 [14.000, 90.000],  loss: 9.062313, mae: 2.355073, mean_q: 4.742294\n",
      "[27 84 98 69 37 38 10 65 86 98]\n",
      " 17865/50001: episode: 1985, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 19.000, mean reward:  2.111 [-10.000,  6.000], mean action: 65.000 [10.000, 98.000],  loss: 9.218195, mae: 2.279441, mean_q: 4.576549\n",
      "[82 41 50 67 40 48 35 94 37 24]\n",
      " 17874/50001: episode: 1986, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 48.444 [24.000, 94.000],  loss: 7.005200, mae: 2.262805, mean_q: 4.586884\n",
      "[26 78 87 30 59 57 88 23 41 88]\n",
      " 17883/50001: episode: 1987, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 26.000, mean reward:  2.889 [-10.000,  6.000], mean action: 61.222 [23.000, 88.000],  loss: 7.350657, mae: 2.283499, mean_q: 4.699086\n",
      "[71  1 87 65 96 37 23 88 37  1]\n",
      " 17892/50001: episode: 1988, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward:  8.000, mean reward:  0.889 [-10.000,  6.000], mean action: 48.333 [1.000, 96.000],  loss: 9.060498, mae: 2.212636, mean_q: 4.466146\n",
      "[22 27 62  2 13  2 37 42  2 86]\n",
      " 17901/50001: episode: 1989, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward:  7.000, mean reward:  0.778 [-10.000,  6.000], mean action: 30.333 [2.000, 86.000],  loss: 8.905562, mae: 2.248561, mean_q: 4.634713\n",
      "[66 87 86 24 88 53 90 32 23 31]\n",
      " 17910/50001: episode: 1990, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 40.000, mean reward:  4.444 [ 3.000,  6.000], mean action: 57.111 [23.000, 90.000],  loss: 8.743663, mae: 2.111092, mean_q: 4.269255\n",
      "[10 31 91 76 60 66 84 86 59  2]\n",
      " 17919/50001: episode: 1991, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 39.000, mean reward:  4.333 [ 3.000,  8.000], mean action: 61.667 [2.000, 91.000],  loss: 7.602753, mae: 2.144151, mean_q: 4.411492\n",
      "[74 21 31  6 46  4 57 44 61 19]\n",
      " 17928/50001: episode: 1992, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 36.000, mean reward:  4.000 [ 3.000,  5.000], mean action: 32.111 [4.000, 61.000],  loss: 6.600408, mae: 2.223824, mean_q: 4.548997\n",
      "[ 5 34 23 10 60 97 76  3 73 28]\n",
      " 17937/50001: episode: 1993, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 36.000, mean reward:  4.000 [ 2.000,  8.000], mean action: 44.889 [3.000, 97.000],  loss: 10.269772, mae: 2.206771, mean_q: 4.506071\n",
      "[79 28 34 97 51 48 97 42 62 31]\n",
      " 17946/50001: episode: 1994, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 22.000, mean reward:  2.444 [-10.000,  5.000], mean action: 54.444 [28.000, 97.000],  loss: 7.242593, mae: 2.232140, mean_q: 4.468305\n",
      "[52 93 75 60 27 99 62  1 10  9]\n",
      " 17955/50001: episode: 1995, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 48.444 [1.000, 99.000],  loss: 7.075926, mae: 2.201072, mean_q: 4.463092\n",
      "[86 95 49  4 19 23 90  5 56 42]\n",
      " 17964/50001: episode: 1996, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 42.556 [4.000, 95.000],  loss: 7.049725, mae: 2.189931, mean_q: 4.393556\n",
      "[35  4 38 38  0 12 19 64  9 95]\n",
      " 17973/50001: episode: 1997, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 29.000, mean reward:  3.222 [-10.000, 11.000], mean action: 31.000 [0.000, 95.000],  loss: 8.980921, mae: 2.212330, mean_q: 4.518248\n",
      "[33 73 90 95 63  6 49 14 34 27]\n",
      " 17982/50001: episode: 1998, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 43.000, mean reward:  4.778 [ 2.000,  9.000], mean action: 50.111 [6.000, 95.000],  loss: 5.023886, mae: 2.256391, mean_q: 4.619783\n",
      "[25 74 53 24 82 24 95 76 94 87]\n",
      " 17991/50001: episode: 1999, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 67.667 [24.000, 95.000],  loss: 7.666487, mae: 2.266587, mean_q: 4.612577\n",
      "[25 95 41 90 28 42 48 21 84 48]\n",
      " 18000/50001: episode: 2000, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 24.000, mean reward:  2.667 [-10.000,  5.000], mean action: 55.222 [21.000, 95.000],  loss: 8.147846, mae: 2.275754, mean_q: 4.607479\n",
      "[ 8 28 37 28  1 56 10 48 42 67]\n",
      " 18009/50001: episode: 2001, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 23.000, mean reward:  2.556 [-10.000,  5.000], mean action: 35.222 [1.000, 67.000],  loss: 9.626991, mae: 2.309274, mean_q: 4.647199\n",
      "[70  5 28 56 75 30  1 60 14 52]\n",
      " 18018/50001: episode: 2002, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 42.000, mean reward:  4.667 [ 3.000,  6.000], mean action: 35.667 [1.000, 75.000],  loss: 6.516645, mae: 2.307980, mean_q: 4.655973\n",
      "[81 39 28 29  7  2 37 37 23 67]\n",
      " 18027/50001: episode: 2003, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 21.000, mean reward:  2.333 [-10.000,  7.000], mean action: 29.889 [2.000, 67.000],  loss: 8.290313, mae: 2.302232, mean_q: 4.606572\n",
      "[63 54 84 59  2 14 94 90 66 35]\n",
      " 18036/50001: episode: 2004, duration: 0.065s, episode steps:   9, steps per second: 137, episode reward: 37.000, mean reward:  4.111 [ 3.000,  7.000], mean action: 55.333 [2.000, 94.000],  loss: 8.337605, mae: 2.245745, mean_q: 4.577949\n",
      "[72 34 60 12 73 56 62 57 99 31]\n",
      " 18045/50001: episode: 2005, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 53.778 [12.000, 99.000],  loss: 7.697670, mae: 2.279052, mean_q: 4.558369\n",
      "[ 8 93 74 58 46 49 48  0  6 88]\n",
      " 18054/50001: episode: 2006, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 39.000, mean reward:  4.333 [ 2.000,  9.000], mean action: 51.333 [0.000, 93.000],  loss: 6.943927, mae: 2.215920, mean_q: 4.469542\n",
      "[65 34 34 25 59 31 33 83 93 23]\n",
      " 18063/50001: episode: 2007, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 46.111 [23.000, 93.000],  loss: 7.246181, mae: 2.172929, mean_q: 4.468991\n",
      "[38 16 59 28  4 35 58 99 48 37]\n",
      " 18072/50001: episode: 2008, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 44.000, mean reward:  4.889 [ 3.000,  9.000], mean action: 42.667 [4.000, 99.000],  loss: 8.649459, mae: 2.227533, mean_q: 4.515742\n",
      "[69 92 26 41 40 94  1 22 49 34]\n",
      " 18081/50001: episode: 2009, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 41.000, mean reward:  4.556 [ 2.000, 10.000], mean action: 44.333 [1.000, 94.000],  loss: 6.092599, mae: 2.214662, mean_q: 4.458590\n",
      "[61 13 83 15 94  8  3 36 66 13]\n",
      " 18090/50001: episode: 2010, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 19.000, mean reward:  2.111 [-10.000,  8.000], mean action: 36.778 [3.000, 94.000],  loss: 5.122072, mae: 2.357396, mean_q: 4.726398\n",
      "[56 52 50 56 16 82 51 62 95  9]\n",
      " 18099/50001: episode: 2011, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 27.000, mean reward:  3.000 [-10.000, 10.000], mean action: 52.556 [9.000, 95.000],  loss: 7.731432, mae: 2.404486, mean_q: 4.758727\n",
      "[39 28 23 28 14 48 68 90 21 12]\n",
      " 18108/50001: episode: 2012, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 25.000, mean reward:  2.778 [-10.000,  9.000], mean action: 36.889 [12.000, 90.000],  loss: 7.244854, mae: 2.477650, mean_q: 4.933207\n",
      "[40 13 66 77 60 59 38 10 27 63]\n",
      " 18117/50001: episode: 2013, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 33.000, mean reward:  3.667 [ 2.000,  6.000], mean action: 45.889 [10.000, 77.000],  loss: 8.974766, mae: 2.296443, mean_q: 4.648059\n",
      "[41 51 11 34  2 80  1 74 84 47]\n",
      " 18126/50001: episode: 2014, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 38.000, mean reward:  4.222 [ 3.000,  7.000], mean action: 42.667 [1.000, 84.000],  loss: 6.669177, mae: 2.326774, mean_q: 4.693830\n",
      "[26 95 59 78 81 94 95 43 95 48]\n",
      " 18135/50001: episode: 2015, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward:  7.000, mean reward:  0.778 [-10.000,  8.000], mean action: 76.444 [43.000, 95.000],  loss: 5.488328, mae: 2.333650, mean_q: 4.772565\n",
      "[85  1 54 53  1 37 66 96 66 14]\n",
      " 18144/50001: episode: 2016, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 11.000, mean reward:  1.222 [-10.000,  6.000], mean action: 43.111 [1.000, 96.000],  loss: 9.627656, mae: 2.273545, mean_q: 4.637853\n",
      "[60 92 78 42 88 56 42 46 53 12]\n",
      " 18153/50001: episode: 2017, duration: 0.065s, episode steps:   9, steps per second: 137, episode reward: 28.000, mean reward:  3.111 [-10.000,  9.000], mean action: 56.556 [12.000, 92.000],  loss: 7.335064, mae: 2.212520, mean_q: 4.491958\n",
      "[91 41 75 66 99 32 16 62 37 48]\n",
      " 18162/50001: episode: 2018, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 52.889 [16.000, 99.000],  loss: 6.617422, mae: 2.203508, mean_q: 4.532871\n",
      "[25 84  4 86  8 28 77  8 48 89]\n",
      " 18171/50001: episode: 2019, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 48.000 [4.000, 89.000],  loss: 6.947611, mae: 2.255250, mean_q: 4.604007\n",
      "[12 98 97 31 29 14 28 28 67 34]\n",
      " 18180/50001: episode: 2020, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 25.000, mean reward:  2.778 [-10.000,  9.000], mean action: 47.333 [14.000, 98.000],  loss: 6.114634, mae: 2.207283, mean_q: 4.477524\n",
      "[60 97 33 96 69 48 62 23 12  2]\n",
      " 18189/50001: episode: 2021, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 49.111 [2.000, 97.000],  loss: 8.031847, mae: 2.240838, mean_q: 4.496466\n",
      "[15 40 19 18  7 46 67 33 42 48]\n",
      " 18198/50001: episode: 2022, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 40.000, mean reward:  4.444 [ 2.000,  9.000], mean action: 35.556 [7.000, 67.000],  loss: 7.431070, mae: 2.259647, mean_q: 4.617882\n",
      "[30 34 29 16 42 80 73 44 66 88]\n",
      " 18207/50001: episode: 2023, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 41.000, mean reward:  4.556 [ 2.000,  9.000], mean action: 52.444 [16.000, 88.000],  loss: 7.879866, mae: 2.299444, mean_q: 4.626774\n",
      "[57 84 88 98 28 88  0  1 75 79]\n",
      " 18216/50001: episode: 2024, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 60.111 [0.000, 98.000],  loss: 8.431961, mae: 2.259810, mean_q: 4.581930\n",
      "[36 14 29 54 96 60 64 50  2 74]\n",
      " 18225/50001: episode: 2025, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 41.000, mean reward:  4.556 [ 2.000,  8.000], mean action: 49.222 [2.000, 96.000],  loss: 7.151363, mae: 2.217926, mean_q: 4.508352\n",
      "[47 27 59 21 64  4 31 31 89  5]\n",
      " 18234/50001: episode: 2026, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 36.778 [4.000, 89.000],  loss: 6.783350, mae: 2.217347, mean_q: 4.499328\n",
      "[90 28 37 49 62 37 28 50 31 12]\n",
      " 18243/50001: episode: 2027, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 14.000, mean reward:  1.556 [-10.000,  8.000], mean action: 37.111 [12.000, 62.000],  loss: 9.816905, mae: 2.302520, mean_q: 4.683153\n",
      "[91 97 29 98 64 88 93 60 93 52]\n",
      " 18252/50001: episode: 2028, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 74.889 [29.000, 98.000],  loss: 6.132538, mae: 2.305228, mean_q: 4.630985\n",
      "[43  8 57 39 75 74 17 30 78 35]\n",
      " 18261/50001: episode: 2029, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 35.000, mean reward:  3.889 [ 2.000,  6.000], mean action: 45.889 [8.000, 78.000],  loss: 5.606775, mae: 2.216595, mean_q: 4.438592\n",
      "[31 89 37 92 62 93 50 95  5 93]\n",
      " 18270/50001: episode: 2030, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 68.444 [5.000, 95.000],  loss: 6.287663, mae: 2.259750, mean_q: 4.508974\n",
      "[64 14 68 63 85 60 98  2 37 88]\n",
      " 18279/50001: episode: 2031, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 43.000, mean reward:  4.778 [ 3.000,  8.000], mean action: 57.222 [2.000, 98.000],  loss: 5.734386, mae: 2.324363, mean_q: 4.685157\n",
      "[27 27 74 11 77 84 83 93 98  5]\n",
      " 18288/50001: episode: 2032, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 61.333 [5.000, 98.000],  loss: 7.594432, mae: 2.415017, mean_q: 4.739642\n",
      "[74 46 62 62 89 32 32 95 48 98]\n",
      " 18297/50001: episode: 2033, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 17.000, mean reward:  1.889 [-10.000,  8.000], mean action: 62.667 [32.000, 98.000],  loss: 6.231604, mae: 2.339870, mean_q: 4.772924\n",
      "[64 95  1 56 54 10 82 52 56 85]\n",
      " 18306/50001: episode: 2034, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 22.000, mean reward:  2.444 [-10.000,  5.000], mean action: 54.556 [1.000, 95.000],  loss: 9.081448, mae: 2.332053, mean_q: 4.666839\n",
      "[95 13 64 59 61  4 87 93 31 98]\n",
      " 18315/50001: episode: 2035, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 56.667 [4.000, 98.000],  loss: 9.106179, mae: 2.400214, mean_q: 5.043997\n",
      "[88 95 52 92  1 53 94 12 50 92]\n",
      " 18324/50001: episode: 2036, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 60.111 [1.000, 95.000],  loss: 9.115689, mae: 2.375748, mean_q: 4.795270\n",
      "[19 33 12 17 92 93 13 66 78 48]\n",
      " 18333/50001: episode: 2037, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 50.222 [12.000, 93.000],  loss: 8.333261, mae: 2.201509, mean_q: 4.522959\n",
      "[36 14 69 95 40 46 20 88 37 26]\n",
      " 18342/50001: episode: 2038, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 42.000, mean reward:  4.667 [ 2.000,  8.000], mean action: 48.333 [14.000, 95.000],  loss: 7.658973, mae: 2.200438, mean_q: 4.523438\n",
      "[53 84  2 24  4 74 13 27 79 50]\n",
      " 18351/50001: episode: 2039, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 44.000, mean reward:  4.889 [ 3.000,  8.000], mean action: 39.667 [2.000, 84.000],  loss: 8.003724, mae: 2.209152, mean_q: 4.524186\n",
      "[22  4 40 64 54 94 60 34  1 41]\n",
      " 18360/50001: episode: 2040, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 43.556 [1.000, 94.000],  loss: 8.888092, mae: 2.245200, mean_q: 4.650091\n",
      "[98 24 47 13 25 34 25  1 31 24]\n",
      " 18369/50001: episode: 2041, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 12.000, mean reward:  1.333 [-10.000,  6.000], mean action: 24.889 [1.000, 47.000],  loss: 8.621244, mae: 2.289927, mean_q: 4.702983\n",
      "[16 93  4 51 92 97 95 89 91 46]\n",
      " 18378/50001: episode: 2042, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 73.111 [4.000, 97.000],  loss: 5.194312, mae: 2.221372, mean_q: 4.564375\n",
      "[78 46  7 84 78 87 24 42 31 13]\n",
      " 18387/50001: episode: 2043, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 31.000, mean reward:  3.444 [-10.000,  9.000], mean action: 45.778 [7.000, 87.000],  loss: 7.104689, mae: 2.269308, mean_q: 4.659073\n",
      "[46 95  4 11 41 19 89 60 13 30]\n",
      " 18396/50001: episode: 2044, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 40.222 [4.000, 95.000],  loss: 6.959703, mae: 2.291005, mean_q: 4.692513\n",
      "[36 13 51  9 65 17 59 31 68 17]\n",
      " 18405/50001: episode: 2045, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 19.000, mean reward:  2.111 [-10.000,  6.000], mean action: 36.667 [9.000, 68.000],  loss: 6.924799, mae: 2.283628, mean_q: 4.642650\n",
      "[15  2 41 36 12 31 88 34 14 16]\n",
      " 18414/50001: episode: 2046, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 41.000, mean reward:  4.556 [ 2.000,  7.000], mean action: 30.444 [2.000, 88.000],  loss: 6.158782, mae: 2.253248, mean_q: 4.581743\n",
      "[69 14 26 59  1 52 11 59  6 40]\n",
      " 18423/50001: episode: 2047, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 29.778 [1.000, 59.000],  loss: 6.401137, mae: 2.296802, mean_q: 4.671897\n",
      "[85 72 86 66 34 94 88 95 27 14]\n",
      " 18432/50001: episode: 2048, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 44.000, mean reward:  4.889 [ 2.000,  8.000], mean action: 64.000 [14.000, 95.000],  loss: 7.909202, mae: 2.316466, mean_q: 4.687926\n",
      "[83 90 91 14 37 25 66 31 40 98]\n",
      " 18441/50001: episode: 2049, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 43.000, mean reward:  4.778 [ 3.000,  7.000], mean action: 54.667 [14.000, 98.000],  loss: 7.774525, mae: 2.338428, mean_q: 4.703343\n",
      "[80 46  8 18 11 52 64 64 21 54]\n",
      " 18450/50001: episode: 2050, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 21.000, mean reward:  2.333 [-10.000,  5.000], mean action: 37.556 [8.000, 64.000],  loss: 5.776505, mae: 2.279056, mean_q: 4.593636\n",
      "[80 89 97 11 11  1 49 66 13 31]\n",
      " 18459/50001: episode: 2051, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 28.000, mean reward:  3.111 [-10.000,  8.000], mean action: 40.889 [1.000, 97.000],  loss: 8.220749, mae: 2.326069, mean_q: 4.724212\n",
      "[42  4 78 98 10 42 51  1 69 50]\n",
      " 18468/50001: episode: 2052, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 27.000, mean reward:  3.000 [-10.000,  9.000], mean action: 44.778 [1.000, 98.000],  loss: 6.166937, mae: 2.362654, mean_q: 4.726184\n",
      "[43 48 49 86  2 65 62 10 62 36]\n",
      " 18477/50001: episode: 2053, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 17.000, mean reward:  1.889 [-10.000,  5.000], mean action: 46.667 [2.000, 86.000],  loss: 5.752979, mae: 2.430048, mean_q: 4.861230\n",
      "[87 30 31 90 98 28 93 93 27 28]\n",
      " 18486/50001: episode: 2054, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 12.000, mean reward:  1.333 [-10.000,  6.000], mean action: 57.556 [27.000, 98.000],  loss: 6.555119, mae: 2.451782, mean_q: 4.994501\n",
      "[ 1 79 99 15 34 97 95 14 42 87]\n",
      " 18495/50001: episode: 2055, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 62.444 [14.000, 99.000],  loss: 7.967492, mae: 2.408820, mean_q: 4.930014\n",
      "[89 59 32 97 99 84 27 95 82 12]\n",
      " 18504/50001: episode: 2056, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 65.222 [12.000, 99.000],  loss: 6.845593, mae: 2.405584, mean_q: 4.926442\n",
      "[53 34 33 81 24 52 48  9 27 21]\n",
      " 18513/50001: episode: 2057, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 36.556 [9.000, 81.000],  loss: 7.544828, mae: 2.407730, mean_q: 4.849455\n",
      "[63 34 81 45  2 80 78 95 95 10]\n",
      " 18522/50001: episode: 2058, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 24.000, mean reward:  2.667 [-10.000,  9.000], mean action: 57.778 [2.000, 95.000],  loss: 7.319881, mae: 2.365242, mean_q: 4.727381\n",
      "[82 34 28 31 34  4 98 34 54 18]\n",
      " 18531/50001: episode: 2059, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward:  7.000, mean reward:  0.778 [-10.000,  5.000], mean action: 37.222 [4.000, 98.000],  loss: 8.018603, mae: 2.314570, mean_q: 4.609130\n",
      "[90  3 56 89 84 32 88  6 95 21]\n",
      " 18540/50001: episode: 2060, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 40.000, mean reward:  4.444 [ 3.000,  9.000], mean action: 52.667 [3.000, 95.000],  loss: 6.200392, mae: 2.316892, mean_q: 4.711878\n",
      "[ 9 19 34 57 90 12 20  6 21  9]\n",
      " 18549/50001: episode: 2061, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 22.000, mean reward:  2.444 [-10.000,  5.000], mean action: 29.778 [6.000, 90.000],  loss: 4.993146, mae: 2.393045, mean_q: 4.820213\n",
      "[58 88 12 81 98 39 37 27  5 40]\n",
      " 18558/50001: episode: 2062, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 47.444 [5.000, 98.000],  loss: 6.560715, mae: 2.396619, mean_q: 4.833599\n",
      "[51 79 74 77  2 37 92 33 66 48]\n",
      " 18567/50001: episode: 2063, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 56.444 [2.000, 92.000],  loss: 6.592783, mae: 2.303730, mean_q: 4.664833\n",
      "[70 32 42  4  2 93 53 41  6 10]\n",
      " 18576/50001: episode: 2064, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 37.000, mean reward:  4.111 [ 3.000,  5.000], mean action: 31.444 [2.000, 93.000],  loss: 5.831519, mae: 2.338998, mean_q: 4.836622\n",
      "[14 83 77 97 74 57 40 48  4 23]\n",
      " 18585/50001: episode: 2065, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 55.889 [4.000, 97.000],  loss: 6.490438, mae: 2.406620, mean_q: 4.890532\n",
      "[62 40  8 48 31 90 28 11 97  4]\n",
      " 18594/50001: episode: 2066, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 40.000, mean reward:  4.444 [ 3.000,  6.000], mean action: 39.667 [4.000, 97.000],  loss: 9.151375, mae: 2.397598, mean_q: 4.860601\n",
      "[44 95 97 52 66 18 75 97 37 14]\n",
      " 18603/50001: episode: 2067, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 27.000, mean reward:  3.000 [-10.000,  8.000], mean action: 61.222 [14.000, 97.000],  loss: 7.000588, mae: 2.380687, mean_q: 4.810880\n",
      "[30 53 91 88 11 79 37 50 37 38]\n",
      " 18612/50001: episode: 2068, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 53.778 [11.000, 91.000],  loss: 7.663095, mae: 2.304771, mean_q: 4.708357\n",
      "[62 27 79 56 48 57 84 42 17 66]\n",
      " 18621/50001: episode: 2069, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 36.000, mean reward:  4.000 [ 2.000,  8.000], mean action: 52.889 [17.000, 84.000],  loss: 7.731359, mae: 2.318923, mean_q: 4.746210\n",
      "[77 95 64 28 92 31 34 23 99 50]\n",
      " 18630/50001: episode: 2070, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 57.333 [23.000, 99.000],  loss: 7.486938, mae: 2.261290, mean_q: 4.629302\n",
      "[12 24 48 14 25  5 44 63 62 88]\n",
      " 18639/50001: episode: 2071, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 36.000, mean reward:  4.000 [ 2.000,  8.000], mean action: 41.444 [5.000, 88.000],  loss: 6.828587, mae: 2.277995, mean_q: 4.588651\n",
      "[49 42 66 43 61 27 34 13 31 40]\n",
      " 18648/50001: episode: 2072, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 41.000, mean reward:  4.556 [ 2.000,  8.000], mean action: 39.667 [13.000, 66.000],  loss: 7.776487, mae: 2.313385, mean_q: 4.736704\n",
      "[59 72 90 66 62 56 12 94 48 52]\n",
      " 18657/50001: episode: 2073, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 38.000, mean reward:  4.222 [ 2.000,  6.000], mean action: 61.333 [12.000, 94.000],  loss: 8.248874, mae: 2.299966, mean_q: 4.731169\n",
      "[73 34 21 48 99 52 69 21 17 60]\n",
      " 18666/50001: episode: 2074, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 24.000, mean reward:  2.667 [-10.000,  8.000], mean action: 46.778 [17.000, 99.000],  loss: 6.774885, mae: 2.315974, mean_q: 4.697331\n",
      "[65 74 65 66  5 12 50  4 92 48]\n",
      " 18675/50001: episode: 2075, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 29.000, mean reward:  3.222 [-10.000,  7.000], mean action: 46.222 [4.000, 92.000],  loss: 6.759177, mae: 2.269822, mean_q: 4.612852\n",
      "[51 49 50 79 95 34  4 17 32 15]\n",
      " 18684/50001: episode: 2076, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 38.000, mean reward:  4.222 [ 2.000,  6.000], mean action: 41.667 [4.000, 95.000],  loss: 6.366327, mae: 2.361384, mean_q: 4.869479\n",
      "[23 72 25  6 23 24 74 58 95 90]\n",
      " 18693/50001: episode: 2077, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 26.000, mean reward:  2.889 [-10.000, 10.000], mean action: 51.889 [6.000, 95.000],  loss: 8.906019, mae: 2.357371, mean_q: 4.852693\n",
      "[47 61 79 11  1 88 91 92 96 79]\n",
      " 18702/50001: episode: 2078, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 19.000, mean reward:  2.111 [-10.000,  5.000], mean action: 66.444 [1.000, 96.000],  loss: 6.891461, mae: 2.311239, mean_q: 4.742936\n",
      "[66 22 13 94 75 32 13  4 35 41]\n",
      " 18711/50001: episode: 2079, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 19.000, mean reward:  2.111 [-10.000,  5.000], mean action: 36.556 [4.000, 94.000],  loss: 8.773014, mae: 2.251042, mean_q: 4.621466\n",
      "[87 52 10 34 42 58 28 75 24 70]\n",
      " 18720/50001: episode: 2080, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 43.667 [10.000, 75.000],  loss: 7.850458, mae: 2.244051, mean_q: 4.562784\n",
      "[45 80 51 92 67 24  1 32 13 34]\n",
      " 18729/50001: episode: 2081, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 46.000, mean reward:  5.111 [ 3.000,  9.000], mean action: 43.778 [1.000, 92.000],  loss: 7.190818, mae: 2.304053, mean_q: 4.719844\n",
      "[70 79 23 30 97 99 95 99 77 31]\n",
      " 18738/50001: episode: 2082, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 70.000 [23.000, 99.000],  loss: 7.321282, mae: 2.280996, mean_q: 4.654111\n",
      "[57 79 24 87 62 18 35 17 15 93]\n",
      " 18747/50001: episode: 2083, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 35.000, mean reward:  3.889 [ 3.000,  8.000], mean action: 47.778 [15.000, 93.000],  loss: 6.090093, mae: 2.309742, mean_q: 4.654586\n",
      "[82 59  8 41 67 96 90 66 66 36]\n",
      " 18756/50001: episode: 2084, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 19.000, mean reward:  2.111 [-10.000,  6.000], mean action: 58.778 [8.000, 96.000],  loss: 7.258645, mae: 2.315526, mean_q: 4.774577\n",
      "[88 78 66 26 37 10 42  4 34  8]\n",
      " 18765/50001: episode: 2085, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 39.000, mean reward:  4.333 [ 2.000,  8.000], mean action: 33.889 [4.000, 78.000],  loss: 9.468963, mae: 2.369505, mean_q: 4.885330\n",
      "[20 83 10 97 92 32 52 88 90 64]\n",
      " 18774/50001: episode: 2086, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 41.000, mean reward:  4.556 [ 3.000,  6.000], mean action: 67.556 [10.000, 97.000],  loss: 6.046682, mae: 2.402475, mean_q: 4.849032\n",
      "[95  4 32 35 34 83 28 67  2 66]\n",
      " 18783/50001: episode: 2087, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 40.000, mean reward:  4.444 [ 3.000,  8.000], mean action: 39.000 [2.000, 83.000],  loss: 7.520290, mae: 2.351391, mean_q: 4.827662\n",
      "[91 96 54 53  2 68 74 62 93 88]\n",
      " 18792/50001: episode: 2088, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 65.556 [2.000, 96.000],  loss: 9.337661, mae: 2.327178, mean_q: 4.756985\n",
      "[ 7 12 73 96 95 79 59 69 64 97]\n",
      " 18801/50001: episode: 2089, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 71.556 [12.000, 97.000],  loss: 9.244526, mae: 2.239936, mean_q: 4.557783\n",
      "[32 51 28 41 23 95 26 53 28 83]\n",
      " 18810/50001: episode: 2090, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 21.000, mean reward:  2.333 [-10.000,  6.000], mean action: 47.556 [23.000, 95.000],  loss: 7.603288, mae: 2.216826, mean_q: 4.618010\n",
      "[40 16 23  8 30 42 95 36  9 88]\n",
      " 18819/50001: episode: 2091, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 38.556 [8.000, 95.000],  loss: 6.222524, mae: 2.213682, mean_q: 4.507315\n",
      "[70 99 53 98 55 97  2  2  4 95]\n",
      " 18828/50001: episode: 2092, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 29.000, mean reward:  3.222 [-10.000,  9.000], mean action: 56.111 [2.000, 99.000],  loss: 6.144533, mae: 2.270781, mean_q: 4.636136\n",
      "[45 66  2 17 46 79 27 63 94 16]\n",
      " 18837/50001: episode: 2093, duration: 0.078s, episode steps:   9, steps per second: 115, episode reward: 37.000, mean reward:  4.111 [ 3.000,  5.000], mean action: 45.556 [2.000, 94.000],  loss: 9.221566, mae: 2.237021, mean_q: 4.563873\n",
      "[35 46 69 70 14 76 10 95 87 12]\n",
      " 18846/50001: episode: 2094, duration: 0.078s, episode steps:   9, steps per second: 116, episode reward: 41.000, mean reward:  4.556 [ 2.000,  9.000], mean action: 53.222 [10.000, 95.000],  loss: 7.629423, mae: 2.282377, mean_q: 4.685544\n",
      "[48 28 10 33 34 34 50 48 97 48]\n",
      " 18855/50001: episode: 2095, duration: 0.081s, episode steps:   9, steps per second: 111, episode reward: -3.000, mean reward: -0.333 [-10.000,  6.000], mean action: 42.444 [10.000, 97.000],  loss: 8.973075, mae: 2.237750, mean_q: 4.492776\n",
      "[43 51 20 94 44 18 34 50 31 72]\n",
      " 18864/50001: episode: 2096, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 37.000, mean reward:  4.111 [ 1.000,  8.000], mean action: 46.000 [18.000, 94.000],  loss: 9.551992, mae: 2.271612, mean_q: 4.543899\n",
      "[90 92 84 77 96 34 55 27 83 90]\n",
      " 18873/50001: episode: 2097, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 21.000, mean reward:  2.333 [-10.000,  7.000], mean action: 70.889 [27.000, 96.000],  loss: 8.298412, mae: 2.147336, mean_q: 4.446491\n",
      "[31 13 62 53 74 82 95 69 95 31]\n",
      " 18882/50001: episode: 2098, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward:  7.000, mean reward:  0.778 [-10.000,  7.000], mean action: 63.778 [13.000, 95.000],  loss: 8.074977, mae: 2.236833, mean_q: 4.572805\n",
      "[97 14 60 13 34 63  2 89 27 48]\n",
      " 18891/50001: episode: 2099, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 38.889 [2.000, 89.000],  loss: 6.226019, mae: 2.195442, mean_q: 4.509130\n",
      "[11 46 46 76 52  6 72 71  4 93]\n",
      " 18900/50001: episode: 2100, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 51.778 [4.000, 93.000],  loss: 7.094293, mae: 2.255563, mean_q: 4.638548\n",
      "[35 34 25 22 25 73 41  9 44 11]\n",
      " 18909/50001: episode: 2101, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 31.556 [9.000, 73.000],  loss: 7.989274, mae: 2.222725, mean_q: 4.472474\n",
      "[42 37 83 14 21 71 28 74  4 88]\n",
      " 18918/50001: episode: 2102, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 42.000, mean reward:  4.667 [ 2.000,  8.000], mean action: 46.667 [4.000, 88.000],  loss: 5.257102, mae: 2.242823, mean_q: 4.519935\n",
      "[ 0 95 60 95 37 94 50 76 37 23]\n",
      " 18927/50001: episode: 2103, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 11.000, mean reward:  1.222 [-10.000,  6.000], mean action: 63.000 [23.000, 95.000],  loss: 5.585347, mae: 2.303272, mean_q: 4.729983\n",
      "[49 71 28 25 74 18  3 13 60 24]\n",
      " 18936/50001: episode: 2104, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 42.000, mean reward:  4.667 [ 2.000,  8.000], mean action: 35.111 [3.000, 74.000],  loss: 6.624242, mae: 2.292052, mean_q: 4.676711\n",
      "[32 34  6 50 98 13 63 44 28 89]\n",
      " 18945/50001: episode: 2105, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 47.222 [6.000, 98.000],  loss: 7.928507, mae: 2.274238, mean_q: 4.710741\n",
      "[10 93 46 40 48 27  4 60 75 50]\n",
      " 18954/50001: episode: 2106, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 42.000, mean reward:  4.667 [ 3.000,  9.000], mean action: 49.222 [4.000, 93.000],  loss: 7.583628, mae: 2.288013, mean_q: 4.654628\n",
      "[34 74 90 87  2 14  9 23 48 46]\n",
      " 18963/50001: episode: 2107, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 38.000, mean reward:  4.222 [ 3.000,  7.000], mean action: 43.667 [2.000, 90.000],  loss: 8.334766, mae: 2.283924, mean_q: 4.642921\n",
      "[14 89 50 14 23 32 88 30 89 48]\n",
      " 18972/50001: episode: 2108, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 12.000, mean reward:  1.333 [-10.000,  8.000], mean action: 51.444 [14.000, 89.000],  loss: 6.897721, mae: 2.305331, mean_q: 4.794224\n",
      "[23 97 26 50  8 98  4 91 88 14]\n",
      " 18981/50001: episode: 2109, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 52.889 [4.000, 98.000],  loss: 8.329439, mae: 2.249288, mean_q: 4.607477\n",
      "[69 95 61 92 53 51 78 50 66 64]\n",
      " 18990/50001: episode: 2110, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 67.778 [50.000, 95.000],  loss: 7.662004, mae: 2.247618, mean_q: 4.517828\n",
      "[61 60 28 55  1 67 28 24 17 70]\n",
      " 18999/50001: episode: 2111, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 19.000, mean reward:  2.111 [-10.000,  5.000], mean action: 38.889 [1.000, 70.000],  loss: 6.226769, mae: 2.315267, mean_q: 4.768153\n",
      "[88 27 56 48 41 53 88 62  9 64]\n",
      " 19008/50001: episode: 2112, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 22.000, mean reward:  2.444 [-10.000,  5.000], mean action: 49.778 [9.000, 88.000],  loss: 6.403855, mae: 2.228893, mean_q: 4.546163\n",
      "[39 88 97 13 87 27 33 18 95 95]\n",
      " 19017/50001: episode: 2113, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 26.000, mean reward:  2.889 [-10.000, 10.000], mean action: 61.444 [13.000, 97.000],  loss: 8.843175, mae: 2.217031, mean_q: 4.544919\n",
      "[31 98 98 53 59 24 95 13 58 48]\n",
      " 19026/50001: episode: 2114, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 29.000, mean reward:  3.222 [-10.000,  7.000], mean action: 60.667 [13.000, 98.000],  loss: 7.491039, mae: 2.193018, mean_q: 4.516914\n",
      "[78 51 86 11 34 31 82 64 95 34]\n",
      " 19035/50001: episode: 2115, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 28.000, mean reward:  3.111 [-10.000,  9.000], mean action: 54.222 [11.000, 95.000],  loss: 5.631961, mae: 2.305162, mean_q: 4.649806\n",
      "[20 14 94  1 66 28 23 24  4 88]\n",
      " 19044/50001: episode: 2116, duration: 0.071s, episode steps:   9, steps per second: 127, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 38.000 [1.000, 94.000],  loss: 7.384231, mae: 2.291966, mean_q: 4.747675\n",
      "[62 31 13 83 79 57 28 95 24 85]\n",
      " 19053/50001: episode: 2117, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 55.000 [13.000, 95.000],  loss: 7.937147, mae: 2.349985, mean_q: 4.816097\n",
      "[10 69 97 28 66 88 14 14 96 81]\n",
      " 19062/50001: episode: 2118, duration: 0.074s, episode steps:   9, steps per second: 121, episode reward: 20.000, mean reward:  2.222 [-10.000,  5.000], mean action: 61.444 [14.000, 97.000],  loss: 7.800756, mae: 2.230962, mean_q: 4.512441\n",
      "[79 12 50 86 61 25 51 30 31 61]\n",
      " 19071/50001: episode: 2119, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 21.000, mean reward:  2.333 [-10.000,  7.000], mean action: 45.222 [12.000, 86.000],  loss: 9.249429, mae: 2.291182, mean_q: 4.649704\n",
      "[85 63 94  1 37 98 17 49 51 95]\n",
      " 19080/50001: episode: 2120, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 43.000, mean reward:  4.778 [ 2.000, 11.000], mean action: 56.111 [1.000, 98.000],  loss: 7.251086, mae: 2.214270, mean_q: 4.575194\n",
      "[42 27 78 34  1  9 28 77 66 88]\n",
      " 19089/50001: episode: 2121, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 45.000, mean reward:  5.000 [ 3.000,  8.000], mean action: 45.333 [1.000, 88.000],  loss: 8.694689, mae: 2.257791, mean_q: 4.603367\n",
      "[76 89 77 63  2 11 59 38 54 34]\n",
      " 19098/50001: episode: 2122, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 40.000, mean reward:  4.444 [ 2.000, 11.000], mean action: 47.444 [2.000, 89.000],  loss: 9.056240, mae: 2.235171, mean_q: 4.566107\n",
      "[74 62 88 46  9 29 88 61 76 51]\n",
      " 19107/50001: episode: 2123, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 21.000, mean reward:  2.333 [-10.000,  7.000], mean action: 56.667 [9.000, 88.000],  loss: 6.967961, mae: 2.208745, mean_q: 4.517261\n",
      "[80 84 41 90 91 38 50 23 60 34]\n",
      " 19116/50001: episode: 2124, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 44.000, mean reward:  4.889 [ 2.000, 10.000], mean action: 56.778 [23.000, 91.000],  loss: 6.270885, mae: 2.241627, mean_q: 4.624434\n",
      "[20 44 62 14 46 52 48 46 38 87]\n",
      " 19125/50001: episode: 2125, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 19.000, mean reward:  2.111 [-10.000,  5.000], mean action: 48.556 [14.000, 87.000],  loss: 8.733998, mae: 2.242334, mean_q: 4.535588\n",
      "[27 33 41 23 44 91 24 63 93 23]\n",
      " 19134/50001: episode: 2126, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 48.333 [23.000, 93.000],  loss: 9.013820, mae: 2.233475, mean_q: 4.559116\n",
      "[54 57 53 57 55  6 76  4 66 50]\n",
      " 19143/50001: episode: 2127, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 28.000, mean reward:  3.111 [-10.000,  9.000], mean action: 47.111 [4.000, 76.000],  loss: 8.459996, mae: 2.335448, mean_q: 4.735971\n",
      "[ 7 95 86 32 98 50 48 45 24 24]\n",
      " 19152/50001: episode: 2128, duration: 0.080s, episode steps:   9, steps per second: 113, episode reward: 26.000, mean reward:  2.889 [-10.000,  6.000], mean action: 55.778 [24.000, 98.000],  loss: 7.362577, mae: 2.298745, mean_q: 4.743185\n",
      "[74 86  4 33 58 90  1 95 12 41]\n",
      " 19161/50001: episode: 2129, duration: 0.071s, episode steps:   9, steps per second: 127, episode reward: 43.000, mean reward:  4.778 [ 3.000,  8.000], mean action: 46.667 [1.000, 95.000],  loss: 8.897665, mae: 2.276412, mean_q: 4.676338\n",
      "[95  5 24  4 34 41 59 24 54 71]\n",
      " 19170/50001: episode: 2130, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 19.000, mean reward:  2.111 [-10.000,  5.000], mean action: 35.111 [4.000, 71.000],  loss: 7.440786, mae: 2.258798, mean_q: 4.677870\n",
      "[20  4 33  4 15 60 78 97 50 79]\n",
      " 19179/50001: episode: 2131, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 27.000, mean reward:  3.000 [-10.000,  8.000], mean action: 46.667 [4.000, 97.000],  loss: 8.533371, mae: 2.253271, mean_q: 4.578260\n",
      "[23 50 31 56  6 28 50 24 66 12]\n",
      " 19188/50001: episode: 2132, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 28.000, mean reward:  3.111 [-10.000,  7.000], mean action: 35.889 [6.000, 66.000],  loss: 7.406589, mae: 2.235059, mean_q: 4.566874\n",
      "[30 41 42 71 78  3 67 14  9 93]\n",
      " 19197/50001: episode: 2133, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 46.444 [3.000, 93.000],  loss: 5.434529, mae: 2.235793, mean_q: 4.604386\n",
      "[61 95 20 24 69 50 24 28 85 79]\n",
      " 19206/50001: episode: 2134, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 26.000, mean reward:  2.889 [-10.000,  6.000], mean action: 52.667 [20.000, 95.000],  loss: 7.210640, mae: 2.259247, mean_q: 4.628131\n",
      "[29 83 90 95 24 32 62 78 66 48]\n",
      " 19215/50001: episode: 2135, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 44.000, mean reward:  4.889 [ 3.000,  8.000], mean action: 64.222 [24.000, 95.000],  loss: 7.327765, mae: 2.267759, mean_q: 4.629813\n",
      "[81  5 73 95 48 79 63 44 66 93]\n",
      " 19224/50001: episode: 2136, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 62.889 [5.000, 95.000],  loss: 6.120822, mae: 2.270346, mean_q: 4.583673\n",
      "[ 2 44  3  0 58 74  9 95 48 98]\n",
      " 19233/50001: episode: 2137, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 42.000, mean reward:  4.667 [ 2.000,  9.000], mean action: 47.667 [0.000, 98.000],  loss: 6.552397, mae: 2.358447, mean_q: 4.873999\n",
      "[95 13 55 74 32 10 93 52 95 66]\n",
      " 19242/50001: episode: 2138, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 21.000, mean reward:  2.333 [-10.000,  7.000], mean action: 54.444 [10.000, 95.000],  loss: 6.850482, mae: 2.322257, mean_q: 4.793577\n",
      "[17 71 68 98 46 42 95 43 40  1]\n",
      " 19251/50001: episode: 2139, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 44.000, mean reward:  4.889 [ 2.000,  9.000], mean action: 56.000 [1.000, 98.000],  loss: 5.792502, mae: 2.352752, mean_q: 4.773198\n",
      "[14 13 18 16 44 74 11 30 99 34]\n",
      " 19260/50001: episode: 2140, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward: 40.000, mean reward:  4.444 [ 2.000, 10.000], mean action: 37.667 [11.000, 99.000],  loss: 7.403994, mae: 2.354101, mean_q: 4.774591\n",
      "[32 48 15 79 92 33 62 95 27 31]\n",
      " 19269/50001: episode: 2141, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 41.000, mean reward:  4.556 [ 2.000,  9.000], mean action: 53.556 [15.000, 95.000],  loss: 9.586037, mae: 2.306719, mean_q: 4.721299\n",
      "[ 5 14 37 46 26 83 95 27 31 64]\n",
      " 19278/50001: episode: 2142, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 47.000 [14.000, 95.000],  loss: 5.315663, mae: 2.302035, mean_q: 4.756485\n",
      "[77 37  9 40 47 57 90  9  9 64]\n",
      " 19287/50001: episode: 2143, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward:  7.000, mean reward:  0.778 [-10.000,  5.000], mean action: 40.222 [9.000, 90.000],  loss: 4.902352, mae: 2.394330, mean_q: 4.973913\n",
      "[97 28 91 34 59 84 34 31 12  1]\n",
      " 19296/50001: episode: 2144, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 41.556 [1.000, 91.000],  loss: 6.352262, mae: 2.453640, mean_q: 5.043378\n",
      "[24 28 19 41 59 54 10 96 46 56]\n",
      " 19305/50001: episode: 2145, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 37.000, mean reward:  4.111 [ 3.000,  6.000], mean action: 45.444 [10.000, 96.000],  loss: 10.102566, mae: 2.423530, mean_q: 4.922799\n",
      "[93 10 48 35 30 35 87 82 24 37]\n",
      " 19314/50001: episode: 2146, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 27.000, mean reward:  3.000 [-10.000,  9.000], mean action: 43.111 [10.000, 87.000],  loss: 8.492686, mae: 2.347987, mean_q: 4.812447\n",
      "[41 91 83 48 67 34 62 48 27 23]\n",
      " 19323/50001: episode: 2147, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 53.667 [23.000, 91.000],  loss: 6.478164, mae: 2.326202, mean_q: 4.758988\n",
      "[75 87 88 26 76 34 78 23 34 28]\n",
      " 19332/50001: episode: 2148, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 52.667 [23.000, 88.000],  loss: 6.655161, mae: 2.379598, mean_q: 4.889386\n",
      "[88 72 41 88 66 48 79 34 42 52]\n",
      " 19341/50001: episode: 2149, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 58.000 [34.000, 88.000],  loss: 8.136816, mae: 2.362763, mean_q: 4.880066\n",
      "[76 31 46 14 63 46 59 60 53 79]\n",
      " 19350/50001: episode: 2150, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 28.000, mean reward:  3.111 [-10.000,  7.000], mean action: 50.111 [14.000, 79.000],  loss: 8.248015, mae: 2.399880, mean_q: 4.914946\n",
      "[70 86 94 56 95 14 34 27 53 34]\n",
      " 19359/50001: episode: 2151, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 28.000, mean reward:  3.111 [-10.000,  7.000], mean action: 54.778 [14.000, 95.000],  loss: 6.813805, mae: 2.341128, mean_q: 4.793084\n",
      "[95 33 82  4 42  2  9 12 14 94]\n",
      " 19368/50001: episode: 2152, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 32.444 [2.000, 94.000],  loss: 6.564023, mae: 2.429873, mean_q: 4.953834\n",
      "[44  4 36 94 48 93 41 10 95 13]\n",
      " 19377/50001: episode: 2153, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 44.000, mean reward:  4.889 [ 2.000,  9.000], mean action: 48.222 [4.000, 95.000],  loss: 8.054257, mae: 2.354785, mean_q: 4.800486\n",
      "[60 76 97 94 93 56 42  8 32  1]\n",
      " 19386/50001: episode: 2154, duration: 0.069s, episode steps:   9, steps per second: 131, episode reward: 40.000, mean reward:  4.444 [ 3.000,  9.000], mean action: 55.444 [1.000, 97.000],  loss: 6.165125, mae: 2.426218, mean_q: 4.988614\n",
      "[17 64 27 78 62 56 58 94 28 85]\n",
      " 19395/50001: episode: 2155, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 37.000, mean reward:  4.111 [ 3.000,  7.000], mean action: 61.333 [27.000, 94.000],  loss: 7.785528, mae: 2.345550, mean_q: 4.778246\n",
      "[ 0 88 23 25 31 48 48 46 92 50]\n",
      " 19404/50001: episode: 2156, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 27.000, mean reward:  3.000 [-10.000,  8.000], mean action: 50.111 [23.000, 92.000],  loss: 7.536958, mae: 2.324162, mean_q: 4.686119\n",
      "[28 52  4 49  2 50 44 44 10 96]\n",
      " 19413/50001: episode: 2157, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 21.000, mean reward:  2.333 [-10.000,  5.000], mean action: 39.000 [2.000, 96.000],  loss: 7.034283, mae: 2.269386, mean_q: 4.571253\n",
      "[67 83 11 64 53 29  4 52 14 48]\n",
      " 19422/50001: episode: 2158, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 39.778 [4.000, 83.000],  loss: 8.946365, mae: 2.320378, mean_q: 4.679574\n",
      "[61  1 30 10 35 24 89 50 82 51]\n",
      " 19431/50001: episode: 2159, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 41.333 [1.000, 89.000],  loss: 6.175543, mae: 2.308524, mean_q: 4.735919\n",
      "[88  8 63 98 93 93 57 34  2 51]\n",
      " 19440/50001: episode: 2160, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 27.000, mean reward:  3.000 [-10.000,  8.000], mean action: 55.444 [2.000, 98.000],  loss: 7.382103, mae: 2.309654, mean_q: 4.707060\n",
      "[76 27 49 30 23 56 41  3 12 34]\n",
      " 19449/50001: episode: 2161, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 44.000, mean reward:  4.889 [ 3.000, 10.000], mean action: 30.556 [3.000, 56.000],  loss: 9.425333, mae: 2.337530, mean_q: 4.800650\n",
      "[87 62 57 23 98 95 83 16  1 27]\n",
      " 19458/50001: episode: 2162, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 44.000, mean reward:  4.889 [ 3.000,  7.000], mean action: 51.333 [1.000, 98.000],  loss: 5.320467, mae: 2.333743, mean_q: 4.755827\n",
      "[36  9 30 29 26 48 97 41  2 27]\n",
      " 19467/50001: episode: 2163, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 41.000, mean reward:  4.556 [ 2.000,  7.000], mean action: 34.333 [2.000, 97.000],  loss: 7.998745, mae: 2.380988, mean_q: 4.812780\n",
      "[ 4 87 25 38 18 54 52 50 58 39]\n",
      " 19476/50001: episode: 2164, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 36.000, mean reward:  4.000 [ 2.000,  8.000], mean action: 46.778 [18.000, 87.000],  loss: 8.784250, mae: 2.281955, mean_q: 4.618829\n",
      "[50 28 40 41 38 37 67 64 95 14]\n",
      " 19485/50001: episode: 2165, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 41.000, mean reward:  4.556 [ 2.000,  9.000], mean action: 47.111 [14.000, 95.000],  loss: 8.606899, mae: 2.308557, mean_q: 4.769836\n",
      "[27 46 10 95 94 40 16 95 48 48]\n",
      " 19494/50001: episode: 2166, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward:  8.000, mean reward:  0.889 [-10.000,  6.000], mean action: 54.667 [10.000, 95.000],  loss: 7.341566, mae: 2.244844, mean_q: 4.602654\n",
      "[ 8 86 58 34 28 95 95 44 50 48]\n",
      " 19503/50001: episode: 2167, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 30.000, mean reward:  3.333 [-10.000,  7.000], mean action: 59.778 [28.000, 95.000],  loss: 7.424109, mae: 2.172525, mean_q: 4.367338\n",
      "[78 52 28 24 30 99 98 99 51  2]\n",
      " 19512/50001: episode: 2168, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 53.667 [2.000, 99.000],  loss: 7.373896, mae: 2.251453, mean_q: 4.590139\n",
      "[25 11 92 12 58 45 46 96 12 10]\n",
      " 19521/50001: episode: 2169, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 20.000, mean reward:  2.222 [-10.000,  5.000], mean action: 42.444 [10.000, 96.000],  loss: 7.855569, mae: 2.249267, mean_q: 4.700956\n",
      "[51 33 86 46 92 37 78 14  2  1]\n",
      " 19530/50001: episode: 2170, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 43.000, mean reward:  4.778 [ 3.000,  8.000], mean action: 43.222 [1.000, 92.000],  loss: 7.632052, mae: 2.213749, mean_q: 4.541841\n",
      "[64 33 56 15 23  1 12  4 34 99]\n",
      " 19539/50001: episode: 2171, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 30.778 [1.000, 99.000],  loss: 7.094448, mae: 2.238521, mean_q: 4.636316\n",
      "[59 83 90 32 12 10 15 32 95 48]\n",
      " 19548/50001: episode: 2172, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 28.000, mean reward:  3.111 [-10.000,  9.000], mean action: 46.333 [10.000, 95.000],  loss: 7.474400, mae: 2.312515, mean_q: 4.680254\n",
      "[22 13  4 28 95 40 95 89 48 12]\n",
      " 19557/50001: episode: 2173, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 47.111 [4.000, 95.000],  loss: 6.929882, mae: 2.369773, mean_q: 4.752998\n",
      "[53 14 62 38  4 40 95 62 52 52]\n",
      " 19566/50001: episode: 2174, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward:  8.000, mean reward:  0.889 [-10.000,  7.000], mean action: 46.556 [4.000, 95.000],  loss: 6.347491, mae: 2.276668, mean_q: 4.669157\n",
      "[85 16 59 98 32 10 48 33  5 31]\n",
      " 19575/50001: episode: 2175, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 36.889 [5.000, 98.000],  loss: 9.089672, mae: 2.324571, mean_q: 4.735353\n",
      "[50 88 40 15 79 82 79 57 30  8]\n",
      " 19584/50001: episode: 2176, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 21.000, mean reward:  2.333 [-10.000,  5.000], mean action: 53.111 [8.000, 88.000],  loss: 6.247685, mae: 2.370298, mean_q: 4.782282\n",
      "[23 30 53 27 98 60 95 23 99  9]\n",
      " 19593/50001: episode: 2177, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 54.889 [9.000, 99.000],  loss: 7.320059, mae: 2.330974, mean_q: 4.739388\n",
      "[62 83 60 50  2 10 70 56 13 33]\n",
      " 19602/50001: episode: 2178, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 38.000, mean reward:  4.222 [ 2.000,  8.000], mean action: 41.889 [2.000, 83.000],  loss: 7.225291, mae: 2.281083, mean_q: 4.737893\n",
      "[83 45  4 69 88 63 72  1 83 50]\n",
      " 19611/50001: episode: 2179, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 28.000, mean reward:  3.111 [-10.000, 10.000], mean action: 52.778 [1.000, 88.000],  loss: 5.803007, mae: 2.284501, mean_q: 4.599167\n",
      "[ 5 95 13 48 84 51 86 93 98 50]\n",
      " 19620/50001: episode: 2180, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 39.000, mean reward:  4.333 [ 2.000,  8.000], mean action: 68.667 [13.000, 98.000],  loss: 8.350264, mae: 2.314321, mean_q: 4.678369\n",
      "[ 6 36  2 15 24 53  1 24 33 11]\n",
      " 19629/50001: episode: 2181, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 21.000, mean reward:  2.333 [-10.000,  6.000], mean action: 22.111 [1.000, 53.000],  loss: 6.664170, mae: 2.300403, mean_q: 4.742259\n",
      "[87  7 19 34 52 13 95 27 30 10]\n",
      " 19638/50001: episode: 2182, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 42.000, mean reward:  4.667 [ 2.000,  7.000], mean action: 31.889 [7.000, 95.000],  loss: 8.747401, mae: 2.327480, mean_q: 4.732016\n",
      "[34 28  2 13 61 95 27 88 80 71]\n",
      " 19647/50001: episode: 2183, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 34.000, mean reward:  3.778 [ 2.000,  6.000], mean action: 51.667 [2.000, 95.000],  loss: 3.960715, mae: 2.229971, mean_q: 4.547521\n",
      "[47 53 16 98 48 39 56 33 76  4]\n",
      " 19656/50001: episode: 2184, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 36.000, mean reward:  4.000 [ 2.000,  8.000], mean action: 47.000 [4.000, 98.000],  loss: 7.203852, mae: 2.403322, mean_q: 4.797740\n",
      "[ 8 41 45 57 67 36  1 13 71 50]\n",
      " 19665/50001: episode: 2185, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 42.000, mean reward:  4.667 [ 2.000,  9.000], mean action: 42.333 [1.000, 71.000],  loss: 6.759022, mae: 2.412916, mean_q: 4.829110\n",
      "[59 14 32 50 68 37 57  9 62 92]\n",
      " 19674/50001: episode: 2186, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 37.000, mean reward:  4.111 [ 3.000,  6.000], mean action: 46.778 [9.000, 92.000],  loss: 5.080690, mae: 2.482095, mean_q: 4.906705\n",
      "[25 95 87 97 15 99 50 62 97 48]\n",
      " 19683/50001: episode: 2187, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 26.000, mean reward:  2.889 [-10.000,  8.000], mean action: 72.222 [15.000, 99.000],  loss: 7.571912, mae: 2.455786, mean_q: 4.887763\n",
      "[59 34  2 91 74 96 51 41 89 69]\n",
      " 19692/50001: episode: 2188, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 36.000, mean reward:  4.000 [ 3.000,  5.000], mean action: 60.778 [2.000, 96.000],  loss: 7.059365, mae: 2.490335, mean_q: 5.037764\n",
      "[64 30 35 17 31 13 34 99 67 92]\n",
      " 19701/50001: episode: 2189, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 46.444 [13.000, 99.000],  loss: 8.123142, mae: 2.463049, mean_q: 4.930520\n",
      "[ 0 14 13 95 26 27 10 46 52 12]\n",
      " 19710/50001: episode: 2190, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 43.000, mean reward:  4.778 [ 2.000,  9.000], mean action: 32.778 [10.000, 95.000],  loss: 8.413292, mae: 2.397860, mean_q: 4.846747\n",
      "[32 20 56 88 97 48 42 52 12 82]\n",
      " 19719/50001: episode: 2191, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 55.222 [12.000, 97.000],  loss: 6.924018, mae: 2.278119, mean_q: 4.639104\n",
      "[35 69 90 64  1 13  1 30  1 89]\n",
      " 19728/50001: episode: 2192, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 10.000, mean reward:  1.111 [-10.000,  6.000], mean action: 39.778 [1.000, 90.000],  loss: 7.342664, mae: 2.325147, mean_q: 4.651225\n",
      "[15 28 32 77 50 88 95 69 64 37]\n",
      " 19737/50001: episode: 2193, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 44.000, mean reward:  4.889 [ 3.000,  8.000], mean action: 60.000 [28.000, 95.000],  loss: 8.545124, mae: 2.328220, mean_q: 4.756667\n",
      "[29 31 77  1 28 66 95 97 84 77]\n",
      " 19746/50001: episode: 2194, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 61.778 [1.000, 97.000],  loss: 8.895512, mae: 2.261667, mean_q: 4.605852\n",
      "[88 27 34 57 95 34 60 57 27  4]\n",
      " 19755/50001: episode: 2195, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: -4.000, mean reward: -0.444 [-10.000,  6.000], mean action: 43.889 [4.000, 95.000],  loss: 7.155303, mae: 2.271972, mean_q: 4.599073\n",
      "[83 12 15 50 85 95 34 84 97 12]\n",
      " 19764/50001: episode: 2196, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 28.000, mean reward:  3.111 [-10.000,  7.000], mean action: 53.778 [12.000, 97.000],  loss: 6.200676, mae: 2.209174, mean_q: 4.404688\n",
      "[34 52 76 14 16 68 48 21 88 79]\n",
      " 19773/50001: episode: 2197, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 38.000, mean reward:  4.222 [ 3.000,  7.000], mean action: 51.333 [14.000, 88.000],  loss: 9.163780, mae: 2.299764, mean_q: 4.795855\n",
      "[14 41 94 27 74 25 60 86 87 87]\n",
      " 19782/50001: episode: 2198, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 21.000, mean reward:  2.333 [-10.000,  6.000], mean action: 64.556 [25.000, 94.000],  loss: 10.768272, mae: 2.201374, mean_q: 4.488710\n",
      "[57 67 23 33  5 53 37 18 88 50]\n",
      " 19791/50001: episode: 2199, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 44.000, mean reward:  4.889 [ 3.000,  9.000], mean action: 41.556 [5.000, 88.000],  loss: 6.204020, mae: 2.277066, mean_q: 4.629980\n",
      "[61 61 17 63 17  4 90 98 79 62]\n",
      " 19800/50001: episode: 2200, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 15.000, mean reward:  1.667 [-10.000,  6.000], mean action: 54.556 [4.000, 98.000],  loss: 6.309889, mae: 2.233844, mean_q: 4.591862\n",
      "[19 26 59 65 74 27 98 23 84  4]\n",
      " 19809/50001: episode: 2201, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 51.111 [4.000, 98.000],  loss: 6.266985, mae: 2.266021, mean_q: 4.622608\n",
      "[15 88 84 95 11 66 50 84 31  1]\n",
      " 19818/50001: episode: 2202, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 32.000, mean reward:  3.556 [-10.000,  8.000], mean action: 56.667 [1.000, 95.000],  loss: 7.339144, mae: 2.308822, mean_q: 4.664896\n",
      "[75 34 89 88 37 66 14 44 66 46]\n",
      " 19827/50001: episode: 2203, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 53.778 [14.000, 89.000],  loss: 7.038713, mae: 2.345845, mean_q: 4.834116\n",
      "[55  2 46 68 34 10 51 34 98 12]\n",
      " 19836/50001: episode: 2204, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 27.000, mean reward:  3.000 [-10.000,  8.000], mean action: 39.444 [2.000, 98.000],  loss: 6.974551, mae: 2.295637, mean_q: 4.612958\n",
      "[36 96 13 47 11 78 74 89 34 48]\n",
      " 19845/50001: episode: 2205, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 43.000, mean reward:  4.778 [ 3.000,  9.000], mean action: 54.444 [11.000, 96.000],  loss: 5.745492, mae: 2.396077, mean_q: 4.853634\n",
      "[75 11 44 53 95 86 41 80  2  2]\n",
      " 19854/50001: episode: 2206, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 46.000 [2.000, 95.000],  loss: 7.925334, mae: 2.376773, mean_q: 4.809726\n",
      "[91 10 52  9 48 55 16 69 92 10]\n",
      " 19863/50001: episode: 2207, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 17.000, mean reward:  1.889 [-10.000,  4.000], mean action: 40.111 [9.000, 92.000],  loss: 7.840420, mae: 2.347662, mean_q: 4.607578\n",
      "[29 89  5 99 89 66 11 50 79 47]\n",
      " 19872/50001: episode: 2208, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 28.000, mean reward:  3.111 [-10.000,  7.000], mean action: 59.444 [5.000, 99.000],  loss: 8.361172, mae: 2.311193, mean_q: 4.745358\n",
      "[ 7 17  0 68 25 18 99 95 24 32]\n",
      " 19881/50001: episode: 2209, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 43.000, mean reward:  4.778 [ 2.000,  9.000], mean action: 42.000 [0.000, 99.000],  loss: 8.527519, mae: 2.313086, mean_q: 4.663714\n",
      "[98 45  4 82 46 63 13 62 52 37]\n",
      " 19890/50001: episode: 2210, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 42.000, mean reward:  4.667 [ 2.000,  9.000], mean action: 44.889 [4.000, 82.000],  loss: 5.769575, mae: 2.243354, mean_q: 4.537519\n",
      "[92 53 18 49 46 60 69 91 66 69]\n",
      " 19899/50001: episode: 2211, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 57.889 [18.000, 91.000],  loss: 8.661499, mae: 2.254170, mean_q: 4.581528\n",
      "[48 49 79 50 13 66 79 90 93 28]\n",
      " 19908/50001: episode: 2212, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 26.000, mean reward:  2.889 [-10.000,  6.000], mean action: 60.778 [13.000, 93.000],  loss: 7.341234, mae: 2.234004, mean_q: 4.483955\n",
      "[48 84  4 59 23 60 78 24 14 80]\n",
      " 19917/50001: episode: 2213, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 33.000, mean reward:  3.667 [ 2.000,  5.000], mean action: 47.333 [4.000, 84.000],  loss: 6.771809, mae: 2.252732, mean_q: 4.428066\n",
      "[98 48 75 63 78 57 82 78 82 81]\n",
      " 19926/50001: episode: 2214, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward:  6.000, mean reward:  0.667 [-10.000,  5.000], mean action: 71.556 [48.000, 82.000],  loss: 6.574441, mae: 2.298033, mean_q: 4.686357\n",
      "[57 69 97 69 72 99 88  1 69 83]\n",
      " 19935/50001: episode: 2215, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward:  9.000, mean reward:  1.000 [-10.000,  6.000], mean action: 71.889 [1.000, 99.000],  loss: 9.422803, mae: 2.320165, mean_q: 4.698278\n",
      "[64 36  8 64 30 61 34  8 62 33]\n",
      " 19944/50001: episode: 2216, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 10.000, mean reward:  1.111 [-10.000,  8.000], mean action: 37.333 [8.000, 64.000],  loss: 8.568450, mae: 2.330018, mean_q: 4.673844\n",
      "[26 32 16 12 13 63 95 88  8 83]\n",
      " 19953/50001: episode: 2217, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 45.556 [8.000, 95.000],  loss: 8.535447, mae: 2.241359, mean_q: 4.505092\n",
      "[88 42 23 64 68 27 62 31 27 88]\n",
      " 19962/50001: episode: 2218, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward:  6.000, mean reward:  0.667 [-10.000,  6.000], mean action: 48.000 [23.000, 88.000],  loss: 7.479211, mae: 2.208457, mean_q: 4.422541\n",
      "[82  2 65 75 28 46 75  2 55 14]\n",
      " 19971/50001: episode: 2219, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward:  9.000, mean reward:  1.000 [-10.000,  6.000], mean action: 40.222 [2.000, 75.000],  loss: 8.830532, mae: 2.254488, mean_q: 4.706234\n",
      "[28 95 59 38 98  2 13 67 13 49]\n",
      " 19980/50001: episode: 2220, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 48.222 [2.000, 98.000],  loss: 8.807717, mae: 2.196532, mean_q: 4.531882\n",
      "[13 89 74 95  2 66 50 50 24 48]\n",
      " 19989/50001: episode: 2221, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 55.333 [2.000, 95.000],  loss: 6.803422, mae: 2.248233, mean_q: 4.584092\n",
      "[20 95 58 33 50 79 13 20 34 97]\n",
      " 19998/50001: episode: 2222, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 31.000, mean reward:  3.444 [-10.000,  8.000], mean action: 53.222 [13.000, 97.000],  loss: 7.396076, mae: 2.235425, mean_q: 4.587674\n",
      "[55 95 51 72 31 88 34 76 72 27]\n",
      " 20007/50001: episode: 2223, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 60.667 [27.000, 95.000],  loss: 7.288121, mae: 2.290376, mean_q: 4.627259\n",
      "[41 31 78 66 32 88 10 95 64 66]\n",
      " 20016/50001: episode: 2224, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 26.000, mean reward:  2.889 [-10.000,  8.000], mean action: 58.889 [10.000, 95.000],  loss: 8.092999, mae: 2.319668, mean_q: 4.593595\n",
      "[24 68 24 23 48 92 87 44 31 92]\n",
      " 20025/50001: episode: 2225, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward:  5.000, mean reward:  0.556 [-10.000,  6.000], mean action: 56.556 [23.000, 92.000],  loss: 5.057247, mae: 2.229977, mean_q: 4.548502\n",
      "[82 28 49 48 66 25 35 28 86 39]\n",
      " 20034/50001: episode: 2226, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 18.000, mean reward:  2.000 [-10.000,  4.000], mean action: 44.889 [25.000, 86.000],  loss: 6.357895, mae: 2.252952, mean_q: 4.519236\n",
      "[44 18 93 86 67 37 95 31 88 67]\n",
      " 20043/50001: episode: 2227, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 30.000, mean reward:  3.333 [-10.000,  7.000], mean action: 64.667 [18.000, 95.000],  loss: 7.075717, mae: 2.366372, mean_q: 4.736445\n",
      "[58 78 33 95 37 76 89 95 55 98]\n",
      " 20052/50001: episode: 2228, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 72.889 [33.000, 98.000],  loss: 7.770121, mae: 2.307552, mean_q: 4.659660\n",
      "[22 88 24 10 74 46 73 62 27  5]\n",
      " 20061/50001: episode: 2229, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 45.444 [5.000, 88.000],  loss: 8.660048, mae: 2.325814, mean_q: 4.704216\n",
      "[85 98 59 57 34 63 98 10  5 61]\n",
      " 20070/50001: episode: 2230, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 53.889 [5.000, 98.000],  loss: 7.044456, mae: 2.365228, mean_q: 4.684690\n",
      "[11 94 87 98  2 62 50 76 29 14]\n",
      " 20079/50001: episode: 2231, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 38.000, mean reward:  4.222 [ 3.000,  7.000], mean action: 56.889 [2.000, 98.000],  loss: 7.160154, mae: 2.312621, mean_q: 4.616709\n",
      "[84 34 37 13 31 13 92 62 41 24]\n",
      " 20088/50001: episode: 2232, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 38.556 [13.000, 92.000],  loss: 8.355393, mae: 2.362892, mean_q: 4.783756\n",
      "[41 32 74 80 11 46 16  8 12 39]\n",
      " 20097/50001: episode: 2233, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 35.000, mean reward:  3.889 [ 2.000,  8.000], mean action: 35.333 [8.000, 80.000],  loss: 9.257266, mae: 2.376611, mean_q: 4.740155\n",
      "[78  4 21 75 99  4 36 27 40 44]\n",
      " 20106/50001: episode: 2234, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 38.889 [4.000, 99.000],  loss: 7.237817, mae: 2.317103, mean_q: 4.727159\n",
      "[10 68 79 72 34 24 92 90 27 86]\n",
      " 20115/50001: episode: 2235, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 37.000, mean reward:  4.111 [ 2.000,  6.000], mean action: 63.556 [24.000, 92.000],  loss: 7.456168, mae: 2.366697, mean_q: 4.760511\n",
      "[42 89 60 68 56 57 84 31  6  0]\n",
      " 20124/50001: episode: 2236, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 35.000, mean reward:  3.889 [ 3.000,  6.000], mean action: 50.111 [0.000, 89.000],  loss: 6.148007, mae: 2.320416, mean_q: 4.658720\n",
      "[47 45 86 42 40 31 95  2 24  1]\n",
      " 20133/50001: episode: 2237, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 43.000, mean reward:  4.778 [ 2.000,  8.000], mean action: 40.667 [1.000, 95.000],  loss: 6.981177, mae: 2.366443, mean_q: 4.732620\n",
      "[71 74 97 66 37 14 68 66 40 88]\n",
      " 20142/50001: episode: 2238, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 61.111 [14.000, 97.000],  loss: 7.371953, mae: 2.429804, mean_q: 4.812281\n",
      "[37 93  8 50 60  1 98 66 24 31]\n",
      " 20151/50001: episode: 2239, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 41.000, mean reward:  4.556 [ 3.000,  6.000], mean action: 47.889 [1.000, 98.000],  loss: 6.770926, mae: 2.405604, mean_q: 4.922224\n",
      "[13  1 19 59 28 82 88 51 50 24]\n",
      " 20160/50001: episode: 2240, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 44.667 [1.000, 88.000],  loss: 5.727715, mae: 2.344734, mean_q: 4.712694\n",
      "[24 13 40 39  6 46 40 74 75 37]\n",
      " 20169/50001: episode: 2241, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 28.000, mean reward:  3.111 [-10.000,  9.000], mean action: 41.111 [6.000, 75.000],  loss: 9.917544, mae: 2.353243, mean_q: 4.723197\n",
      "[47 46 41  2 37 24 94 96 77 92]\n",
      " 20178/50001: episode: 2242, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 34.000, mean reward:  3.778 [ 3.000,  5.000], mean action: 56.556 [2.000, 96.000],  loss: 9.705701, mae: 2.333324, mean_q: 4.755095\n",
      "[26 88 24 30 50 95 96 28 95 33]\n",
      " 20187/50001: episode: 2243, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 59.889 [24.000, 96.000],  loss: 10.976608, mae: 2.246252, mean_q: 4.562815\n",
      "[72 44  0 30 25 37 27 34 37 13]\n",
      " 20196/50001: episode: 2244, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 29.000, mean reward:  3.222 [-10.000,  8.000], mean action: 27.444 [0.000, 44.000],  loss: 7.240080, mae: 2.158707, mean_q: 4.316963\n",
      "[99 44 37 13 64  4  5 31 54 93]\n",
      " 20205/50001: episode: 2245, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 37.000, mean reward:  4.111 [ 2.000,  6.000], mean action: 38.333 [4.000, 93.000],  loss: 7.413828, mae: 2.142164, mean_q: 4.379000\n",
      "[59 13 31 60 19 37 89 41 40  9]\n",
      " 20214/50001: episode: 2246, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 37.667 [9.000, 89.000],  loss: 4.965602, mae: 2.149026, mean_q: 4.433229\n",
      "[54 27 64 92 24 46 20 65 89 12]\n",
      " 20223/50001: episode: 2247, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 42.000, mean reward:  4.667 [ 3.000, 10.000], mean action: 48.778 [12.000, 92.000],  loss: 6.541865, mae: 2.206535, mean_q: 4.501562\n",
      "[88 87  6 37 12 50 33 98 59 48]\n",
      " 20232/50001: episode: 2248, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 47.778 [6.000, 98.000],  loss: 7.118292, mae: 2.212586, mean_q: 4.464801\n",
      "[75 27 50 85 93 13 53 27 41 28]\n",
      " 20241/50001: episode: 2249, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 46.333 [13.000, 93.000],  loss: 5.074331, mae: 2.302652, mean_q: 4.746071\n",
      "[92  1 51 55 32 32 30 16 27 52]\n",
      " 20250/50001: episode: 2250, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 32.889 [1.000, 55.000],  loss: 8.610574, mae: 2.372605, mean_q: 4.708337\n",
      "[45  8 37 37 54 82 13 33 69 50]\n",
      " 20259/50001: episode: 2251, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 27.000, mean reward:  3.000 [-10.000,  9.000], mean action: 42.556 [8.000, 82.000],  loss: 9.839905, mae: 2.340867, mean_q: 4.721436\n",
      "[38 57 59 84 44 88 62 27  4  6]\n",
      " 20268/50001: episode: 2252, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 47.889 [4.000, 88.000],  loss: 8.666534, mae: 2.175394, mean_q: 4.474335\n",
      "[63 73  6 78 27 46 13 59 31 41]\n",
      " 20277/50001: episode: 2253, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 41.556 [6.000, 78.000],  loss: 9.281396, mae: 2.154641, mean_q: 4.343859\n",
      "[34  6 87 53 13 80 41 67 20 62]\n",
      " 20286/50001: episode: 2254, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 34.000, mean reward:  3.778 [ 3.000,  5.000], mean action: 47.667 [6.000, 87.000],  loss: 7.643129, mae: 2.188897, mean_q: 4.474339\n",
      "[ 6 75 55 92 32 98 23 78 13 88]\n",
      " 20295/50001: episode: 2255, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 42.000, mean reward:  4.667 [ 2.000,  8.000], mean action: 61.556 [13.000, 98.000],  loss: 6.335983, mae: 2.282416, mean_q: 4.611925\n",
      "[67 28 86 51  8 66 37  1 63 37]\n",
      " 20304/50001: episode: 2256, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 41.889 [1.000, 86.000],  loss: 6.502869, mae: 2.270050, mean_q: 4.579954\n",
      "[42  1 30 68 88 34 22  1 40 84]\n",
      " 20313/50001: episode: 2257, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 40.889 [1.000, 88.000],  loss: 7.132734, mae: 2.369932, mean_q: 4.752619\n",
      "[78  9 76 34 89 82 86 36 66 31]\n",
      " 20322/50001: episode: 2258, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 41.000, mean reward:  4.556 [ 2.000,  8.000], mean action: 56.556 [9.000, 89.000],  loss: 6.408404, mae: 2.322806, mean_q: 4.694079\n",
      "[29  6 37 65 44 42 41 34 59 88]\n",
      " 20331/50001: episode: 2259, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 42.000, mean reward:  4.667 [ 2.000,  8.000], mean action: 46.222 [6.000, 88.000],  loss: 7.285738, mae: 2.397381, mean_q: 4.836103\n",
      "[21 89 20  4 56 65 49 46 63 31]\n",
      " 20340/50001: episode: 2260, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 47.000 [4.000, 89.000],  loss: 7.219537, mae: 2.330439, mean_q: 4.662736\n",
      "[36  1 10 34 53 53 73 89 12 12]\n",
      " 20349/50001: episode: 2261, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 13.000, mean reward:  1.444 [-10.000,  8.000], mean action: 37.444 [1.000, 89.000],  loss: 8.621372, mae: 2.388556, mean_q: 4.858543\n",
      "[84 35  4 74 10 88 51 23 86 87]\n",
      " 20358/50001: episode: 2262, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 34.000, mean reward:  3.778 [ 3.000,  5.000], mean action: 50.889 [4.000, 88.000],  loss: 7.788877, mae: 2.290877, mean_q: 4.643147\n",
      "[28 35 49 37 35 27 34 83 81 88]\n",
      " 20367/50001: episode: 2263, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 28.000, mean reward:  3.111 [-10.000,  8.000], mean action: 52.111 [27.000, 88.000],  loss: 7.176397, mae: 2.269500, mean_q: 4.569369\n",
      "[94 34 31 97 72 95 83 92 44 12]\n",
      " 20376/50001: episode: 2264, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 42.000, mean reward:  4.667 [ 2.000,  9.000], mean action: 62.222 [12.000, 97.000],  loss: 6.727051, mae: 2.266942, mean_q: 4.638420\n",
      "[36 52  9 87  2 28 81 86 86 12]\n",
      " 20385/50001: episode: 2265, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 26.000, mean reward:  2.889 [-10.000, 10.000], mean action: 49.222 [2.000, 87.000],  loss: 6.640879, mae: 2.329197, mean_q: 4.713818\n",
      "[85  1 62 88 37 65 13 95 27 12]\n",
      " 20394/50001: episode: 2266, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 45.000, mean reward:  5.000 [ 2.000,  8.000], mean action: 44.444 [1.000, 95.000],  loss: 8.872280, mae: 2.316380, mean_q: 4.668854\n",
      "[89 40 77 63 94 33 42  4 40 12]\n",
      " 20403/50001: episode: 2267, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 45.000 [4.000, 94.000],  loss: 7.755005, mae: 2.240035, mean_q: 4.517492\n",
      "[24 50 24 50 98 82 45 51 68 33]\n",
      " 20412/50001: episode: 2268, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward:  5.000, mean reward:  0.556 [-10.000,  5.000], mean action: 55.667 [24.000, 98.000],  loss: 8.027598, mae: 2.196860, mean_q: 4.455984\n",
      "[94 76 37 79 17 10 13 88 78 50]\n",
      " 20421/50001: episode: 2269, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 42.000, mean reward:  4.667 [ 3.000,  9.000], mean action: 49.778 [10.000, 88.000],  loss: 7.265881, mae: 2.256986, mean_q: 4.585083\n",
      "[58 64 50 50  1  2 88 84 80 12]\n",
      " 20430/50001: episode: 2270, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 24.000, mean reward:  2.667 [-10.000,  8.000], mean action: 47.889 [1.000, 88.000],  loss: 8.701476, mae: 2.341158, mean_q: 4.706454\n",
      "[52 37 40 66 91 46 52 92  8 57]\n",
      " 20439/50001: episode: 2271, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 22.000, mean reward:  2.444 [-10.000,  7.000], mean action: 54.333 [8.000, 92.000],  loss: 7.884199, mae: 2.229619, mean_q: 4.553706\n",
      "[70  2 24 47 94 30  1  1 60  1]\n",
      " 20448/50001: episode: 2272, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 11.000, mean reward:  1.222 [-10.000,  6.000], mean action: 28.889 [1.000, 94.000],  loss: 7.552486, mae: 2.238279, mean_q: 4.562409\n",
      "[47 66 34 47 76 34 74 90 31 66]\n",
      " 20457/50001: episode: 2273, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: -6.000, mean reward: -0.667 [-10.000,  6.000], mean action: 57.556 [31.000, 90.000],  loss: 5.904501, mae: 2.257007, mean_q: 4.567279\n",
      "[97 46 59 90  1  2 77 95 82 48]\n",
      " 20466/50001: episode: 2274, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 44.000, mean reward:  4.889 [ 3.000,  8.000], mean action: 55.556 [1.000, 95.000],  loss: 10.411987, mae: 2.278737, mean_q: 4.645789\n",
      "[85 59 13 92 28 60 95 53 45  8]\n",
      " 20475/50001: episode: 2275, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 36.000, mean reward:  4.000 [ 2.000,  7.000], mean action: 50.333 [8.000, 95.000],  loss: 4.847487, mae: 2.294779, mean_q: 4.576151\n",
      "[40 95 85 34 36 79 78 39 53 13]\n",
      " 20484/50001: episode: 2276, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 39.000, mean reward:  4.333 [ 2.000,  9.000], mean action: 56.889 [13.000, 95.000],  loss: 5.319838, mae: 2.326495, mean_q: 4.649507\n",
      "[39 95 86 34 85 50 28 50 37 12]\n",
      " 20493/50001: episode: 2277, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 31.000, mean reward:  3.444 [-10.000,  7.000], mean action: 53.000 [12.000, 95.000],  loss: 5.030553, mae: 2.402283, mean_q: 4.774248\n",
      "[70  0 26 32 88 44 95 98 85 31]\n",
      " 20502/50001: episode: 2278, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward: 42.000, mean reward:  4.667 [ 2.000,  8.000], mean action: 55.444 [0.000, 98.000],  loss: 8.271615, mae: 2.405830, mean_q: 4.790267\n",
      "[32 31 41 68 84 91 51 34 38 12]\n",
      " 20511/50001: episode: 2279, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 41.000, mean reward:  4.556 [ 2.000,  9.000], mean action: 50.000 [12.000, 91.000],  loss: 8.162045, mae: 2.403205, mean_q: 4.869502\n",
      "[43 60 15 19 78 24 83 37 78 35]\n",
      " 20520/50001: episode: 2280, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 22.000, mean reward:  2.444 [-10.000,  8.000], mean action: 47.667 [15.000, 83.000],  loss: 7.996348, mae: 2.303763, mean_q: 4.659466\n",
      "[90 74 47 57 59 43 95 69 13 27]\n",
      " 20529/50001: episode: 2281, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 53.778 [13.000, 95.000],  loss: 5.618077, mae: 2.295060, mean_q: 4.730571\n",
      "[52 60  8 38 52 40 51 26 24 66]\n",
      " 20538/50001: episode: 2282, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 24.000, mean reward:  2.667 [-10.000,  8.000], mean action: 40.556 [8.000, 66.000],  loss: 8.266548, mae: 2.462363, mean_q: 4.942262\n",
      "[88  8 59 16 62 84 34  6  1 95]\n",
      " 20547/50001: episode: 2283, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 45.000, mean reward:  5.000 [ 3.000, 11.000], mean action: 40.556 [1.000, 95.000],  loss: 9.651834, mae: 2.378396, mean_q: 4.809912\n",
      "[ 8 66 23 60 29 30 79 50 54 28]\n",
      " 20556/50001: episode: 2284, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 46.556 [23.000, 79.000],  loss: 8.062522, mae: 2.247427, mean_q: 4.452808\n",
      "[68 56 46  4 31 33 28 64 33  3]\n",
      " 20565/50001: episode: 2285, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 20.000, mean reward:  2.222 [-10.000,  5.000], mean action: 33.111 [3.000, 64.000],  loss: 7.402305, mae: 2.260203, mean_q: 4.630253\n",
      "[20 50 61 34 95 34 59 83 48 42]\n",
      " 20574/50001: episode: 2286, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 56.222 [34.000, 95.000],  loss: 8.166103, mae: 2.261194, mean_q: 4.592535\n",
      "[96 98 50 18 94 41 75 66 84 14]\n",
      " 20583/50001: episode: 2287, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 60.000 [14.000, 98.000],  loss: 7.070986, mae: 2.279092, mean_q: 4.612020\n",
      "[35 55 31  9 90 66 67 43 57 12]\n",
      " 20592/50001: episode: 2288, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 40.000, mean reward:  4.444 [ 2.000,  9.000], mean action: 47.778 [9.000, 90.000],  loss: 6.052321, mae: 2.286090, mean_q: 4.656397\n",
      "[62 51 89 93 10  1 88 63 75 81]\n",
      " 20601/50001: episode: 2289, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 61.222 [1.000, 93.000],  loss: 6.565262, mae: 2.285783, mean_q: 4.560421\n",
      "[13 32 59 70 80 60 70 54 88 12]\n",
      " 20610/50001: episode: 2290, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 28.000, mean reward:  3.111 [-10.000, 10.000], mean action: 58.333 [12.000, 88.000],  loss: 8.288474, mae: 2.368430, mean_q: 4.804203\n",
      "[88 95 37 73 34  9  2 64 90 50]\n",
      " 20619/50001: episode: 2291, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 41.000, mean reward:  4.556 [ 2.000,  8.000], mean action: 50.444 [2.000, 95.000],  loss: 6.223545, mae: 2.355587, mean_q: 4.700630\n",
      "[14 44 10 31 56 98 41 37 95 33]\n",
      " 20628/50001: episode: 2292, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 41.000, mean reward:  4.556 [ 2.000,  8.000], mean action: 49.444 [10.000, 98.000],  loss: 10.613552, mae: 2.324286, mean_q: 4.659642\n",
      "[35 27 34 49 16 95  1 69 95 12]\n",
      " 20637/50001: episode: 2293, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 29.000, mean reward:  3.222 [-10.000,  8.000], mean action: 44.222 [1.000, 95.000],  loss: 5.324443, mae: 2.364511, mean_q: 4.725063\n",
      "[39 66 55 50 41 50 88 66 21 88]\n",
      " 20646/50001: episode: 2294, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: -7.000, mean reward: -0.778 [-10.000,  6.000], mean action: 58.333 [21.000, 88.000],  loss: 6.113470, mae: 2.280199, mean_q: 4.553000\n",
      "[80 95 94 57 93 68 13 23 45  1]\n",
      " 20655/50001: episode: 2295, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 39.000, mean reward:  4.333 [ 2.000,  8.000], mean action: 54.333 [1.000, 95.000],  loss: 8.096457, mae: 2.326352, mean_q: 4.650841\n",
      "[51 23 63 12 98  5 46 58 50 41]\n",
      " 20664/50001: episode: 2296, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 44.000 [5.000, 98.000],  loss: 7.704173, mae: 2.331385, mean_q: 4.678767\n",
      "[80 27 40 66 97  3 30 12 86 79]\n",
      " 20673/50001: episode: 2297, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 48.889 [3.000, 97.000],  loss: 8.969397, mae: 2.314552, mean_q: 4.630399\n",
      "[22  4 43  9 11 90 37 28 97 31]\n",
      " 20682/50001: episode: 2298, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 43.000, mean reward:  4.778 [ 2.000,  7.000], mean action: 38.889 [4.000, 97.000],  loss: 7.009509, mae: 2.298631, mean_q: 4.590442\n",
      "[ 0 83 74 32 42 42 59 37 66 50]\n",
      " 20691/50001: episode: 2299, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 31.000, mean reward:  3.444 [-10.000,  9.000], mean action: 53.889 [32.000, 83.000],  loss: 9.887500, mae: 2.275327, mean_q: 4.516240\n",
      "[35 95  4 48 16 40 62 24 77 80]\n",
      " 20700/50001: episode: 2300, duration: 0.085s, episode steps:   9, steps per second: 106, episode reward: 36.000, mean reward:  4.000 [ 3.000,  6.000], mean action: 49.556 [4.000, 95.000],  loss: 7.582487, mae: 2.223191, mean_q: 4.494730\n",
      "[71 80 41 68  2 97 95 52 62 84]\n",
      " 20709/50001: episode: 2301, duration: 0.086s, episode steps:   9, steps per second: 105, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 64.556 [2.000, 97.000],  loss: 7.732946, mae: 2.270864, mean_q: 4.569228\n",
      "[16 58 47 46 88 17 12 97 31 49]\n",
      " 20718/50001: episode: 2302, duration: 0.086s, episode steps:   9, steps per second: 105, episode reward: 39.000, mean reward:  4.333 [ 2.000,  6.000], mean action: 49.444 [12.000, 97.000],  loss: 6.716613, mae: 2.231625, mean_q: 4.434595\n",
      "[57 41 94 95 37 75 95 40  2 40]\n",
      " 20727/50001: episode: 2303, duration: 0.069s, episode steps:   9, steps per second: 130, episode reward: 11.000, mean reward:  1.222 [-10.000,  6.000], mean action: 57.667 [2.000, 95.000],  loss: 6.718015, mae: 2.342087, mean_q: 4.643140\n",
      "[ 4 37 86 58 92 76 30 30 12  8]\n",
      " 20736/50001: episode: 2304, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 23.000, mean reward:  2.556 [-10.000,  9.000], mean action: 47.667 [8.000, 92.000],  loss: 5.501973, mae: 2.357030, mean_q: 4.802120\n",
      "[38 79 24  3 95 48 11 28 74 90]\n",
      " 20745/50001: episode: 2305, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 42.000, mean reward:  4.667 [ 3.000,  6.000], mean action: 50.222 [3.000, 95.000],  loss: 6.198737, mae: 2.383229, mean_q: 4.760435\n",
      "[74 95 56 28 53 60 21 88 28 31]\n",
      " 20754/50001: episode: 2306, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 51.111 [21.000, 95.000],  loss: 6.086205, mae: 2.434710, mean_q: 4.914999\n",
      "[84 68 62 69 50 58 13 73 82 48]\n",
      " 20763/50001: episode: 2307, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 58.111 [13.000, 82.000],  loss: 6.513464, mae: 2.469820, mean_q: 4.940735\n",
      "[31 64 28 92  6 88 16 88 40 20]\n",
      " 20772/50001: episode: 2308, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 20.000, mean reward:  2.222 [-10.000,  5.000], mean action: 49.111 [6.000, 92.000],  loss: 7.904771, mae: 2.513078, mean_q: 4.915417\n",
      "[64 93  8 30 47 78 83 78 24 50]\n",
      " 20781/50001: episode: 2309, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 27.000, mean reward:  3.000 [-10.000, 10.000], mean action: 54.556 [8.000, 93.000],  loss: 5.559770, mae: 2.419488, mean_q: 4.885065\n",
      "[93  1 50 68 34 26 92  5 14 28]\n",
      " 20790/50001: episode: 2310, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 35.333 [1.000, 92.000],  loss: 5.444046, mae: 2.449214, mean_q: 4.923817\n",
      "[91 41 48 94 52 20 73 66 66  9]\n",
      " 20799/50001: episode: 2311, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 52.111 [9.000, 94.000],  loss: 6.719774, mae: 2.414720, mean_q: 4.808983\n",
      "[16 95 86  2 64 23 13 95 53 50]\n",
      " 20808/50001: episode: 2312, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 27.000, mean reward:  3.000 [-10.000,  8.000], mean action: 53.444 [2.000, 95.000],  loss: 6.478751, mae: 2.428648, mean_q: 4.857285\n",
      "[89 38 52 46 39  9  9 44 98 90]\n",
      " 20817/50001: episode: 2313, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 47.222 [9.000, 98.000],  loss: 8.875479, mae: 2.434512, mean_q: 4.834857\n",
      "[21 11 16 89 48 88 34 40 64 50]\n",
      " 20826/50001: episode: 2314, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 43.000, mean reward:  4.778 [ 3.000,  8.000], mean action: 48.889 [11.000, 89.000],  loss: 8.656119, mae: 2.364864, mean_q: 4.739964\n",
      "[73 63 30 24 41  1 15 56 82 96]\n",
      " 20835/50001: episode: 2315, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 45.333 [1.000, 96.000],  loss: 8.468593, mae: 2.386716, mean_q: 4.781213\n",
      "[63 63 28  8 28 93 95 34 52 95]\n",
      " 20844/50001: episode: 2316, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: -1.000, mean reward: -0.111 [-10.000,  7.000], mean action: 55.111 [8.000, 95.000],  loss: 6.950987, mae: 2.300217, mean_q: 4.612056\n",
      "[72 88 30 98 53 95 52 34  1 59]\n",
      " 20853/50001: episode: 2317, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 46.000, mean reward:  5.111 [ 4.000,  7.000], mean action: 56.667 [1.000, 98.000],  loss: 10.183605, mae: 2.247109, mean_q: 4.491480\n",
      "[91 43 13 32 97 91 59 36  1 12]\n",
      " 20862/50001: episode: 2318, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 27.000, mean reward:  3.000 [-10.000,  9.000], mean action: 42.667 [1.000, 97.000],  loss: 6.776754, mae: 2.233464, mean_q: 4.542518\n",
      "[ 6 94 31 92 34 13 92 56 13 14]\n",
      " 20871/50001: episode: 2319, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 10.000, mean reward:  1.111 [-10.000,  6.000], mean action: 48.778 [13.000, 94.000],  loss: 7.033852, mae: 2.255614, mean_q: 4.634438\n",
      "[25 95 93 88 34 14 48 38 97 24]\n",
      " 20880/50001: episode: 2320, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 41.000, mean reward:  4.556 [ 2.000,  6.000], mean action: 59.000 [14.000, 97.000],  loss: 5.390341, mae: 2.259923, mean_q: 4.605422\n",
      "[53 27 18 18 53 54 73 46 24 43]\n",
      " 20889/50001: episode: 2321, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward:  7.000, mean reward:  0.778 [-10.000,  7.000], mean action: 39.556 [18.000, 73.000],  loss: 7.642817, mae: 2.284152, mean_q: 4.564089\n",
      "[92 79 94 14 75 44 13  1 37 63]\n",
      " 20898/50001: episode: 2322, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 46.667 [1.000, 94.000],  loss: 8.124685, mae: 2.398138, mean_q: 4.830561\n",
      "[75 54 42 28 26 99 13 76 34 43]\n",
      " 20907/50001: episode: 2323, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 38.000, mean reward:  4.222 [ 2.000,  9.000], mean action: 46.111 [13.000, 99.000],  loss: 5.617299, mae: 2.285956, mean_q: 4.641614\n",
      "[31 24 14 84 88 69 27 59 88 12]\n",
      " 20916/50001: episode: 2324, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 51.667 [12.000, 88.000],  loss: 7.241167, mae: 2.334277, mean_q: 4.749675\n",
      "[48 95 19 40  2 41 95 94 67 51]\n",
      " 20925/50001: episode: 2325, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 20.000, mean reward:  2.222 [-10.000,  5.000], mean action: 56.000 [2.000, 95.000],  loss: 5.742619, mae: 2.402077, mean_q: 4.904272\n",
      "[93 83 30 27  1  8 53 43 45 62]\n",
      " 20934/50001: episode: 2326, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 33.000, mean reward:  3.667 [ 2.000,  6.000], mean action: 39.111 [1.000, 83.000],  loss: 8.413177, mae: 2.383354, mean_q: 4.714026\n",
      "[14 83 53 73 37 24 62 78 93 50]\n",
      " 20943/50001: episode: 2327, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 42.000, mean reward:  4.667 [ 2.000, 10.000], mean action: 61.444 [24.000, 93.000],  loss: 8.367164, mae: 2.378079, mean_q: 4.688206\n",
      "[35 30 29 64 95 33 83  3 41 88]\n",
      " 20952/50001: episode: 2328, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 38.000, mean reward:  4.222 [ 2.000,  9.000], mean action: 51.778 [3.000, 95.000],  loss: 5.543104, mae: 2.378744, mean_q: 4.837145\n",
      "[86 83 72 56 53 88 88  2 31  9]\n",
      " 20961/50001: episode: 2329, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 26.000, mean reward:  2.889 [-10.000,  6.000], mean action: 53.556 [2.000, 88.000],  loss: 5.296527, mae: 2.423395, mean_q: 4.789149\n",
      "[48 51 69 56 51 57 89 31  2 46]\n",
      " 20970/50001: episode: 2330, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 50.222 [2.000, 89.000],  loss: 5.262259, mae: 2.414153, mean_q: 4.815214\n",
      "[48 76 24  4 24 77 28 98 99 30]\n",
      " 20979/50001: episode: 2331, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 51.111 [4.000, 99.000],  loss: 7.661634, mae: 2.469431, mean_q: 4.986652\n",
      "[62 41 13 52 98 32  1 41 95 33]\n",
      " 20988/50001: episode: 2332, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 31.000, mean reward:  3.444 [-10.000,  9.000], mean action: 45.111 [1.000, 98.000],  loss: 7.165113, mae: 2.488674, mean_q: 5.039639\n",
      "[96 13 20 65 38 75 41 62 99 88]\n",
      " 20997/50001: episode: 2333, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 55.667 [13.000, 99.000],  loss: 8.028044, mae: 2.405089, mean_q: 4.804834\n",
      "[74 37  4 64 23 68 59 50  5 89]\n",
      " 21006/50001: episode: 2334, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 37.000, mean reward:  4.111 [ 3.000,  7.000], mean action: 44.333 [4.000, 89.000],  loss: 6.296769, mae: 2.363176, mean_q: 4.782810\n",
      "[19 67 51 10 97 82 34 58  5 27]\n",
      " 21015/50001: episode: 2335, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 47.889 [5.000, 97.000],  loss: 7.198465, mae: 2.291080, mean_q: 4.598985\n",
      "[18 63 29 51 98 96 32 51 98 50]\n",
      " 21024/50001: episode: 2336, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 12.000, mean reward:  1.333 [-10.000,  9.000], mean action: 63.111 [29.000, 98.000],  loss: 8.783471, mae: 2.335063, mean_q: 4.715912\n",
      "[82 32 90 11 37 23 58 77 79 48]\n",
      " 21033/50001: episode: 2337, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 50.556 [11.000, 90.000],  loss: 7.326029, mae: 2.330536, mean_q: 4.712361\n",
      "[50 86 41 36 85 99 67 78 53 12]\n",
      " 21042/50001: episode: 2338, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 39.000, mean reward:  4.333 [ 2.000, 10.000], mean action: 61.889 [12.000, 99.000],  loss: 7.689695, mae: 2.320341, mean_q: 4.713946\n",
      "[72  5 13 32 19 17 88 23 93  5]\n",
      " 21051/50001: episode: 2339, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 32.778 [5.000, 93.000],  loss: 5.635790, mae: 2.255293, mean_q: 4.500805\n",
      "[42 24 62 76 95 37 11 80 42 12]\n",
      " 21060/50001: episode: 2340, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 27.000, mean reward:  3.000 [-10.000,  9.000], mean action: 48.778 [11.000, 95.000],  loss: 8.682005, mae: 2.314305, mean_q: 4.667202\n",
      "[18 85 60 59 27 35 13 18 81 89]\n",
      " 21069/50001: episode: 2341, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 51.889 [13.000, 89.000],  loss: 6.797951, mae: 2.337969, mean_q: 4.585616\n",
      "[29 71 19 37  5 20 26 99  4 89]\n",
      " 21078/50001: episode: 2342, duration: 0.070s, episode steps:   9, steps per second: 129, episode reward: 35.000, mean reward:  3.889 [ 2.000,  6.000], mean action: 41.111 [4.000, 99.000],  loss: 7.908054, mae: 2.378167, mean_q: 4.756093\n",
      "[67 52  9 30 97 58 96 94 48 99]\n",
      " 21087/50001: episode: 2343, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 64.778 [9.000, 99.000],  loss: 7.556344, mae: 2.288423, mean_q: 4.615693\n",
      "[18 95 41 56 23 98 44 51 62  1]\n",
      " 21096/50001: episode: 2344, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 52.333 [1.000, 98.000],  loss: 10.543048, mae: 2.308737, mean_q: 4.697012\n",
      "[41  6 60 63 34 74 89 78 14 89]\n",
      " 21105/50001: episode: 2345, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 56.333 [6.000, 89.000],  loss: 8.943893, mae: 2.230580, mean_q: 4.500934\n",
      "[45 73 74 79  2 66 28 88 12 39]\n",
      " 21114/50001: episode: 2346, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 41.000, mean reward:  4.556 [ 2.000,  7.000], mean action: 51.222 [2.000, 88.000],  loss: 6.647626, mae: 2.224472, mean_q: 4.475829\n",
      "[76 95 81 79 85 23  2 95  5 98]\n",
      " 21123/50001: episode: 2347, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 62.556 [2.000, 98.000],  loss: 10.214388, mae: 2.268100, mean_q: 4.558849\n",
      "[24  2 26 34 86 46 40  8 88 74]\n",
      " 21132/50001: episode: 2348, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 44.889 [2.000, 88.000],  loss: 8.934097, mae: 2.287393, mean_q: 4.586373\n",
      "[45 34 13 50 31 15 46 69 41 76]\n",
      " 21141/50001: episode: 2349, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 37.000, mean reward:  4.111 [ 2.000,  5.000], mean action: 41.667 [13.000, 76.000],  loss: 6.132989, mae: 2.316464, mean_q: 4.618744\n",
      "[34 34 35 37 12 48  1 48 64 64]\n",
      " 21150/50001: episode: 2350, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: -7.000, mean reward: -0.778 [-10.000,  5.000], mean action: 38.111 [1.000, 64.000],  loss: 6.928494, mae: 2.335565, mean_q: 4.653877\n",
      "[26 95 41 48 13 97 88 34  9 10]\n",
      " 21159/50001: episode: 2351, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 43.000, mean reward:  4.778 [ 4.000,  7.000], mean action: 48.333 [9.000, 97.000],  loss: 8.084000, mae: 2.263636, mean_q: 4.520730\n",
      "[79  5 76 11 95 60 89 95 66 84]\n",
      " 21168/50001: episode: 2352, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 64.556 [5.000, 95.000],  loss: 7.510263, mae: 2.303741, mean_q: 4.616602\n",
      "[13 50 77  1 27 76 38 95 49 79]\n",
      " 21177/50001: episode: 2353, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 39.000, mean reward:  4.333 [ 2.000,  8.000], mean action: 54.667 [1.000, 95.000],  loss: 8.967915, mae: 2.313205, mean_q: 4.703260\n",
      "[ 4 50 62 25 37 50 74 31 59 68]\n",
      " 21186/50001: episode: 2354, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 18.000, mean reward:  2.000 [-10.000,  5.000], mean action: 50.667 [25.000, 74.000],  loss: 9.641737, mae: 2.362932, mean_q: 4.753456\n",
      "[85 94 56  6 58 58 24 97 27 74]\n",
      " 21195/50001: episode: 2355, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 26.000, mean reward:  2.889 [-10.000,  6.000], mean action: 54.889 [6.000, 97.000],  loss: 7.861765, mae: 2.334742, mean_q: 4.607917\n",
      "[52 61 98 46 24 14 97  1  4 18]\n",
      " 21204/50001: episode: 2356, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 39.000, mean reward:  4.333 [ 2.000,  6.000], mean action: 40.333 [1.000, 98.000],  loss: 8.391218, mae: 2.286059, mean_q: 4.506135\n",
      "[46 95 48 13 63 10  5 44  5 99]\n",
      " 21213/50001: episode: 2357, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 20.000, mean reward:  2.222 [-10.000,  5.000], mean action: 42.444 [5.000, 99.000],  loss: 6.843298, mae: 2.228925, mean_q: 4.438900\n",
      "[80 27 84 62 66  8 87 12 27 12]\n",
      " 21222/50001: episode: 2358, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward:  8.000, mean reward:  0.889 [-10.000,  7.000], mean action: 42.778 [8.000, 87.000],  loss: 4.712379, mae: 2.257746, mean_q: 4.592203\n",
      "[11 50 79 66 34 88 23  6 38 46]\n",
      " 21231/50001: episode: 2359, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 36.000, mean reward:  4.000 [ 2.000,  6.000], mean action: 47.778 [6.000, 88.000],  loss: 6.567961, mae: 2.232292, mean_q: 4.470461\n",
      "[12 37 85 10 14 89 49 67 42 95]\n",
      " 21240/50001: episode: 2360, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 42.000, mean reward:  4.667 [ 3.000, 11.000], mean action: 54.222 [10.000, 95.000],  loss: 6.771982, mae: 2.331226, mean_q: 4.760078\n",
      "[70 69 38 75 75 31  1 66 93 12]\n",
      " 21249/50001: episode: 2361, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 32.000, mean reward:  3.556 [-10.000,  9.000], mean action: 51.111 [1.000, 93.000],  loss: 5.898481, mae: 2.358011, mean_q: 4.714121\n",
      "[27 41 46  9 34  2 88 96 24 98]\n",
      " 21258/50001: episode: 2362, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 42.000, mean reward:  4.667 [ 3.000,  6.000], mean action: 48.667 [2.000, 98.000],  loss: 8.437833, mae: 2.378741, mean_q: 4.830466\n",
      "[44 34 67 51 66 34 82 23 87 35]\n",
      " 21267/50001: episode: 2363, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 22.000, mean reward:  2.444 [-10.000,  5.000], mean action: 53.222 [23.000, 87.000],  loss: 8.622135, mae: 2.331012, mean_q: 4.753971\n",
      "[95 90 94 50 53 42 66 94 85 95]\n",
      " 21276/50001: episode: 2364, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward:  7.000, mean reward:  0.778 [-10.000,  5.000], mean action: 74.333 [42.000, 95.000],  loss: 8.623651, mae: 2.209361, mean_q: 4.458951\n",
      "[85  7 54 49 37 12 75  4 69 21]\n",
      " 21285/50001: episode: 2365, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 38.000, mean reward:  4.222 [ 2.000,  6.000], mean action: 36.444 [4.000, 75.000],  loss: 6.783823, mae: 2.296225, mean_q: 4.580934\n",
      "[53  1 32 94 88 60 34  1 72 69]\n",
      " 21294/50001: episode: 2366, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 50.111 [1.000, 94.000],  loss: 6.930240, mae: 2.297275, mean_q: 4.667528\n",
      "[80 58 88  1 49 37 40 56 34  4]\n",
      " 21303/50001: episode: 2367, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 40.778 [1.000, 88.000],  loss: 3.788872, mae: 2.378840, mean_q: 4.739676\n",
      "[37 84 25 90  2 42 88 57 93 33]\n",
      " 21312/50001: episode: 2368, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 37.000, mean reward:  4.111 [ 2.000,  6.000], mean action: 57.111 [2.000, 93.000],  loss: 8.837693, mae: 2.420916, mean_q: 4.814281\n",
      "[81 31 34 18 60  9 84 31 37  5]\n",
      " 21321/50001: episode: 2369, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 26.000, mean reward:  2.889 [-10.000,  8.000], mean action: 34.333 [5.000, 84.000],  loss: 7.308708, mae: 2.370977, mean_q: 4.767688\n",
      "[65 34 97 81 37 60 79 66 88 79]\n",
      " 21330/50001: episode: 2370, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 28.000, mean reward:  3.111 [-10.000,  6.000], mean action: 69.000 [34.000, 97.000],  loss: 8.300202, mae: 2.403791, mean_q: 4.800558\n",
      "[22  8 85 20 23 78 16 24 13 59]\n",
      " 21339/50001: episode: 2371, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 38.000, mean reward:  4.222 [ 3.000,  8.000], mean action: 36.222 [8.000, 85.000],  loss: 7.037478, mae: 2.277300, mean_q: 4.552066\n",
      "[72 12 37  2 17 97 13 70 10 99]\n",
      " 21348/50001: episode: 2372, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 39.000, mean reward:  4.333 [ 2.000,  6.000], mean action: 39.667 [2.000, 99.000],  loss: 6.869387, mae: 2.254451, mean_q: 4.473015\n",
      "[15  3 28 84 78 32  9 46 44  5]\n",
      " 21357/50001: episode: 2373, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 36.556 [3.000, 84.000],  loss: 7.082329, mae: 2.278328, mean_q: 4.557840\n",
      "[91 41 69 34 59 91 27 68 84 48]\n",
      " 21366/50001: episode: 2374, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 57.889 [27.000, 91.000],  loss: 8.284547, mae: 2.357208, mean_q: 4.755806\n",
      "[83 14 78 16 77 94 46 11 53 87]\n",
      " 21375/50001: episode: 2375, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 34.000, mean reward:  3.778 [ 3.000,  5.000], mean action: 52.889 [11.000, 94.000],  loss: 7.508184, mae: 2.294318, mean_q: 4.649959\n",
      "[36 67 11 73 34 63 32 45 26 14]\n",
      " 21384/50001: episode: 2376, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 39.000, mean reward:  4.333 [ 2.000,  8.000], mean action: 40.556 [11.000, 73.000],  loss: 7.428358, mae: 2.273130, mean_q: 4.620912\n",
      "[88 67 63 28 93 97 95 97 12 48]\n",
      " 21393/50001: episode: 2377, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 31.000, mean reward:  3.444 [-10.000,  7.000], mean action: 66.667 [12.000, 97.000],  loss: 8.280851, mae: 2.298700, mean_q: 4.561728\n",
      "[15 89 37 99 28 79 52 14 76 74]\n",
      " 21402/50001: episode: 2378, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 41.000, mean reward:  4.556 [ 3.000,  6.000], mean action: 60.889 [14.000, 99.000],  loss: 6.627435, mae: 2.330712, mean_q: 4.694629\n",
      "[29 98 87 10 24 21 10 57  4 43]\n",
      " 21411/50001: episode: 2379, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 39.333 [4.000, 98.000],  loss: 9.646450, mae: 2.232149, mean_q: 4.480574\n",
      "[12 14 30 55 30 80 34 46 99 12]\n",
      " 21420/50001: episode: 2380, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward:  8.000, mean reward:  0.889 [-10.000,  7.000], mean action: 44.444 [12.000, 99.000],  loss: 7.323143, mae: 2.312178, mean_q: 4.618518\n",
      "[16  5 34 37 37 48 34 93 79 40]\n",
      " 21429/50001: episode: 2381, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 11.000, mean reward:  1.222 [-10.000,  5.000], mean action: 45.222 [5.000, 93.000],  loss: 9.411896, mae: 2.401324, mean_q: 4.759305\n",
      "[38 41 94 53 37 14 96 59 31  4]\n",
      " 21438/50001: episode: 2382, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 47.667 [4.000, 96.000],  loss: 8.019783, mae: 2.325372, mean_q: 4.675713\n",
      "[35 41 72 10 37 88  1  1 43 73]\n",
      " 21447/50001: episode: 2383, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 40.667 [1.000, 88.000],  loss: 7.066363, mae: 2.355437, mean_q: 4.672606\n",
      "[28  5 34 24 11 44 99 27 74 23]\n",
      " 21456/50001: episode: 2384, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 36.000, mean reward:  4.000 [ 2.000,  5.000], mean action: 37.889 [5.000, 99.000],  loss: 5.602222, mae: 2.321216, mean_q: 4.593634\n",
      "[64 84 51 41 79 13 57 50 50 32]\n",
      " 21465/50001: episode: 2385, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 27.000, mean reward:  3.000 [-10.000,  6.000], mean action: 50.778 [13.000, 84.000],  loss: 6.305668, mae: 2.389667, mean_q: 4.746999\n",
      "[17 52 35 81 32 86  2  1 12 11]\n",
      " 21474/50001: episode: 2386, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 34.667 [1.000, 86.000],  loss: 8.628799, mae: 2.450965, mean_q: 4.910561\n",
      "[70 52 55 10  4 69 10 11 28 74]\n",
      " 21483/50001: episode: 2387, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 34.778 [4.000, 74.000],  loss: 8.901487, mae: 2.346114, mean_q: 4.720369\n",
      "[13 51 97 92 80  8  8 14 18 48]\n",
      " 21492/50001: episode: 2388, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 21.000, mean reward:  2.333 [-10.000,  7.000], mean action: 46.222 [8.000, 97.000],  loss: 5.724937, mae: 2.342935, mean_q: 4.597834\n",
      "[20 19 56  9 81 46 28 56 32 98]\n",
      " 21501/50001: episode: 2389, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 26.000, mean reward:  2.889 [-10.000,  6.000], mean action: 47.222 [9.000, 98.000],  loss: 6.724917, mae: 2.352509, mean_q: 4.597044\n",
      "[53 62  9 17 78 96  1 95 79 97]\n",
      " 21510/50001: episode: 2390, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 43.000, mean reward:  4.778 [ 3.000,  9.000], mean action: 59.333 [1.000, 97.000],  loss: 7.122931, mae: 2.384634, mean_q: 4.748024\n",
      "[ 8 33 48 51 24 33 88 95 97 55]\n",
      " 21519/50001: episode: 2391, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 26.000, mean reward:  2.889 [-10.000,  8.000], mean action: 58.222 [24.000, 97.000],  loss: 7.136106, mae: 2.345428, mean_q: 4.588499\n",
      "[57 54 90 28 95 27 78 37 34 89]\n",
      " 21528/50001: episode: 2392, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 44.000, mean reward:  4.889 [ 3.000,  8.000], mean action: 59.111 [27.000, 95.000],  loss: 7.472326, mae: 2.413786, mean_q: 4.708160\n",
      "[70 34 87 10 81 57 67  1 94 50]\n",
      " 21537/50001: episode: 2393, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 43.000, mean reward:  4.778 [ 3.000,  9.000], mean action: 53.444 [1.000, 94.000],  loss: 9.836720, mae: 2.373538, mean_q: 4.673363\n",
      "[94 12 67 13 67 42 91 95 98 12]\n",
      " 21546/50001: episode: 2394, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 13.000, mean reward:  1.444 [-10.000,  8.000], mean action: 55.222 [12.000, 98.000],  loss: 5.650386, mae: 2.298987, mean_q: 4.617125\n",
      "[82 96 97 53  1  4  4 98 78 75]\n",
      " 21555/50001: episode: 2395, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 56.222 [1.000, 98.000],  loss: 4.416879, mae: 2.324708, mean_q: 4.589828\n",
      "[15 28 37 38 37 77 52 95 12 49]\n",
      " 21564/50001: episode: 2396, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 26.000, mean reward:  2.889 [-10.000,  8.000], mean action: 47.222 [12.000, 95.000],  loss: 7.272287, mae: 2.371790, mean_q: 4.681828\n",
      "[ 5 24 28 48  2 99 69 12 37 62]\n",
      " 21573/50001: episode: 2397, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 38.000, mean reward:  4.222 [ 3.000,  7.000], mean action: 42.333 [2.000, 99.000],  loss: 7.962071, mae: 2.356919, mean_q: 4.685523\n",
      "[85 50 10 88 92 27 87 89 48 12]\n",
      " 21582/50001: episode: 2398, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 55.889 [10.000, 92.000],  loss: 4.191607, mae: 2.359285, mean_q: 4.688532\n",
      "[86 37 68 37  9 97 50 36  1 78]\n",
      " 21591/50001: episode: 2399, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 45.889 [1.000, 97.000],  loss: 5.604605, mae: 2.434249, mean_q: 4.818423\n",
      "[34 82 69 13  4 40 99  1 78 74]\n",
      " 21600/50001: episode: 2400, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 36.000, mean reward:  4.000 [ 3.000,  6.000], mean action: 51.111 [1.000, 99.000],  loss: 6.681706, mae: 2.528499, mean_q: 5.107215\n",
      "[83 46 48 54 39 37 11 46  5 37]\n",
      " 21609/50001: episode: 2401, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 10.000, mean reward:  1.111 [-10.000,  7.000], mean action: 35.889 [5.000, 54.000],  loss: 6.102118, mae: 2.473315, mean_q: 4.856962\n",
      "[ 6 83 25  4 83 89 89 34 27 98]\n",
      " 21618/50001: episode: 2402, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 14.000, mean reward:  1.556 [-10.000,  8.000], mean action: 59.111 [4.000, 98.000],  loss: 9.010076, mae: 2.478905, mean_q: 4.946558\n",
      "[80 69 40 65 13 66 49  4 94 33]\n",
      " 21627/50001: episode: 2403, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 35.000, mean reward:  3.889 [ 2.000,  6.000], mean action: 48.111 [4.000, 94.000],  loss: 7.206312, mae: 2.394805, mean_q: 4.814142\n",
      "[50  1 65 42 26 34 41 99 64 12]\n",
      " 21636/50001: episode: 2404, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 41.000, mean reward:  4.556 [ 2.000,  9.000], mean action: 42.667 [1.000, 99.000],  loss: 6.838264, mae: 2.397024, mean_q: 4.799717\n",
      "[70 28 44 86 56 86 51 96 31 27]\n",
      " 21645/50001: episode: 2405, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 56.111 [27.000, 96.000],  loss: 6.854397, mae: 2.317718, mean_q: 4.649312\n",
      "[22 96 41  4 39 44 47 24 34 23]\n",
      " 21654/50001: episode: 2406, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 41.000, mean reward:  4.556 [ 2.000,  8.000], mean action: 39.111 [4.000, 96.000],  loss: 7.251513, mae: 2.352237, mean_q: 4.704256\n",
      "[59  5 41 37  0 76 95 82 78 95]\n",
      " 21663/50001: episode: 2407, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 22.000, mean reward:  2.444 [-10.000,  7.000], mean action: 56.556 [0.000, 95.000],  loss: 7.465509, mae: 2.415332, mean_q: 4.823354\n",
      "[59 74 19 82 13 86 33 97 82 35]\n",
      " 21672/50001: episode: 2408, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 57.889 [13.000, 97.000],  loss: 5.372221, mae: 2.325840, mean_q: 4.708168\n",
      "[66 74 94 34 93 27 89 40 14 14]\n",
      " 21681/50001: episode: 2409, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 53.222 [14.000, 94.000],  loss: 7.079671, mae: 2.337789, mean_q: 4.700094\n",
      "[59 41 66 97 12 69 88 66  3 31]\n",
      " 21690/50001: episode: 2410, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 52.556 [3.000, 97.000],  loss: 7.247974, mae: 2.385396, mean_q: 4.737108\n",
      "[20 78 34 42 94 79  6 42  3 98]\n",
      " 21699/50001: episode: 2411, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 52.889 [3.000, 98.000],  loss: 8.263844, mae: 2.371733, mean_q: 4.776187\n",
      "[69 69  4 79  1 27 47 66 12 31]\n",
      " 21708/50001: episode: 2412, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 28.000, mean reward:  3.111 [-10.000,  6.000], mean action: 37.333 [1.000, 79.000],  loss: 7.173990, mae: 2.408057, mean_q: 4.895020\n",
      "[52 31 84  1 52 74 57 40  5 12]\n",
      " 21717/50001: episode: 2413, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 28.000, mean reward:  3.111 [-10.000,  8.000], mean action: 39.556 [1.000, 84.000],  loss: 6.908317, mae: 2.395915, mean_q: 4.815900\n",
      "[48 30 95 50 69 34 95 17 14 71]\n",
      " 21726/50001: episode: 2414, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 20.000, mean reward:  2.222 [-10.000,  6.000], mean action: 52.778 [14.000, 95.000],  loss: 5.765253, mae: 2.378053, mean_q: 4.825763\n",
      "[98 30 66 46 37 25 13 55 14 21]\n",
      " 21735/50001: episode: 2415, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 37.000, mean reward:  4.111 [ 2.000,  6.000], mean action: 34.111 [13.000, 66.000],  loss: 9.389254, mae: 2.421466, mean_q: 4.787152\n",
      "[68 12 83 53 18 89 28 99  2  1]\n",
      " 21744/50001: episode: 2416, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 42.778 [1.000, 99.000],  loss: 6.474684, mae: 2.433441, mean_q: 4.842847\n",
      "[56 95 63 62 53 14 67 60  4 41]\n",
      " 21753/50001: episode: 2417, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 51.000 [4.000, 95.000],  loss: 6.952849, mae: 2.310606, mean_q: 4.654759\n",
      "[74 89 86 95 85 89 95 97 27 40]\n",
      " 21762/50001: episode: 2418, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 12.000, mean reward:  1.333 [-10.000,  6.000], mean action: 78.111 [27.000, 97.000],  loss: 7.066946, mae: 2.351525, mean_q: 4.729873\n",
      "[94 42 53 72 63 60 13 24 89 69]\n",
      " 21771/50001: episode: 2419, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 53.889 [13.000, 89.000],  loss: 5.900801, mae: 2.408822, mean_q: 4.775818\n",
      "[12  9 67 65 32 74 95 32  9 40]\n",
      " 21780/50001: episode: 2420, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 10.000, mean reward:  1.111 [-10.000,  8.000], mean action: 47.000 [9.000, 95.000],  loss: 7.853381, mae: 2.420605, mean_q: 4.889080\n",
      "[91  5 32 69 75  1 67 42 82 66]\n",
      " 21789/50001: episode: 2421, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 40.000, mean reward:  4.444 [ 3.000,  8.000], mean action: 48.778 [1.000, 82.000],  loss: 6.632950, mae: 2.494906, mean_q: 4.894699\n",
      "[66 89 15 28 30 40 79 82 85 52]\n",
      " 21798/50001: episode: 2422, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 38.000, mean reward:  4.222 [ 2.000,  6.000], mean action: 55.556 [15.000, 89.000],  loss: 7.027792, mae: 2.383447, mean_q: 4.827950\n",
      "[ 7 12 23 69 93 79 63 50 12 66]\n",
      " 21807/50001: episode: 2423, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 51.889 [12.000, 93.000],  loss: 8.053603, mae: 2.418731, mean_q: 4.895254\n",
      "[46 28 55 79 37  2 34 31 31 27]\n",
      " 21816/50001: episode: 2424, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 26.000, mean reward:  2.889 [-10.000,  6.000], mean action: 36.000 [2.000, 79.000],  loss: 8.863523, mae: 2.362177, mean_q: 4.858918\n",
      "[21 76 86 88 98 57 88 78 37 14]\n",
      " 21825/50001: episode: 2425, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 29.000, mean reward:  3.222 [-10.000,  8.000], mean action: 69.111 [14.000, 98.000],  loss: 9.745378, mae: 2.303443, mean_q: 4.609025\n",
      "[90 92 30 53 57 46 52  4 95 50]\n",
      " 21834/50001: episode: 2426, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 46.000, mean reward:  5.111 [ 3.000,  9.000], mean action: 53.222 [4.000, 95.000],  loss: 7.770994, mae: 2.264509, mean_q: 4.557476\n",
      "[27 53 32 76 10 62  1 78 93 79]\n",
      " 21843/50001: episode: 2427, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 53.778 [1.000, 93.000],  loss: 7.558065, mae: 2.243000, mean_q: 4.602866\n",
      "[80 33 61  4 81 14 13 54 40 36]\n",
      " 21852/50001: episode: 2428, duration: 0.066s, episode steps:   9, steps per second: 135, episode reward: 34.000, mean reward:  3.778 [ 2.000,  6.000], mean action: 37.333 [4.000, 81.000],  loss: 10.747660, mae: 2.211863, mean_q: 4.461752\n",
      "[72 12 46 43 12 74 96 31 88 20]\n",
      " 21861/50001: episode: 2429, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 46.889 [12.000, 96.000],  loss: 6.548603, mae: 2.242925, mean_q: 4.477537\n",
      "[22 27 74 56 10 84  1 88 15 27]\n",
      " 21870/50001: episode: 2430, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 20.000, mean reward:  2.222 [-10.000,  7.000], mean action: 42.444 [1.000, 88.000],  loss: 7.948864, mae: 2.230925, mean_q: 4.538816\n",
      "[30  1 74 31 97 62  8 11 37 31]\n",
      " 21879/50001: episode: 2431, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 24.000, mean reward:  2.667 [-10.000,  8.000], mean action: 39.111 [1.000, 97.000],  loss: 5.940166, mae: 2.376835, mean_q: 4.724572\n",
      "[47 34 13 59 54 91 37 16 86 46]\n",
      " 21888/50001: episode: 2432, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 37.000, mean reward:  4.111 [ 3.000,  7.000], mean action: 48.444 [13.000, 91.000],  loss: 9.215337, mae: 2.309495, mean_q: 4.744291\n",
      "[87 71 20 13 55 63 34 67 79 68]\n",
      " 21897/50001: episode: 2433, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 37.000, mean reward:  4.111 [ 2.000,  7.000], mean action: 52.222 [13.000, 79.000],  loss: 6.768312, mae: 2.395009, mean_q: 4.775928\n",
      "[92 79 37 41 39 82 14 42 57 96]\n",
      " 21906/50001: episode: 2434, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 39.000, mean reward:  4.333 [ 2.000,  6.000], mean action: 54.111 [14.000, 96.000],  loss: 7.491677, mae: 2.324008, mean_q: 4.600485\n",
      "[38 14 99 56 65 37 88 69 50 14]\n",
      " 21915/50001: episode: 2435, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 54.667 [14.000, 99.000],  loss: 6.877029, mae: 2.422541, mean_q: 4.780946\n",
      "[43 97 19 24  2 30 24 33 53 48]\n",
      " 21924/50001: episode: 2436, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 26.000, mean reward:  2.889 [-10.000,  8.000], mean action: 36.667 [2.000, 97.000],  loss: 7.658501, mae: 2.386413, mean_q: 4.755731\n",
      "[ 5 10 60 44 88 74  4 21  2 94]\n",
      " 21933/50001: episode: 2437, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 37.000, mean reward:  4.111 [ 3.000,  6.000], mean action: 44.111 [2.000, 94.000],  loss: 6.227990, mae: 2.377519, mean_q: 4.722770\n",
      "[18 86 77 23 17 52 95 34 44 21]\n",
      " 21942/50001: episode: 2438, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 40.000, mean reward:  4.444 [ 3.000,  8.000], mean action: 49.889 [17.000, 95.000],  loss: 8.057073, mae: 2.397248, mean_q: 4.787972\n",
      "[45 50 75 62 99 89 80 37 12 31]\n",
      " 21951/50001: episode: 2439, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 43.000, mean reward:  4.778 [ 3.000,  8.000], mean action: 59.444 [12.000, 99.000],  loss: 6.708984, mae: 2.330252, mean_q: 4.715825\n",
      "[59 13 34 95 53 46 37  2 11 66]\n",
      " 21960/50001: episode: 2440, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 39.667 [2.000, 95.000],  loss: 6.545546, mae: 2.350941, mean_q: 4.709490\n",
      "[50 62 88 28 48  9 60 13 60 95]\n",
      " 21969/50001: episode: 2441, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 28.000, mean reward:  3.111 [-10.000,  9.000], mean action: 51.444 [9.000, 95.000],  loss: 5.428124, mae: 2.416259, mean_q: 4.856721\n",
      "[ 8 79 20 58  1 78  6 10 29 13]\n",
      " 21978/50001: episode: 2442, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 40.000, mean reward:  4.444 [ 3.000, 10.000], mean action: 32.667 [1.000, 79.000],  loss: 8.623906, mae: 2.421974, mean_q: 4.886892\n",
      "[59  1 50 70 75 82 98  9 46 84]\n",
      " 21987/50001: episode: 2443, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 39.000, mean reward:  4.333 [ 2.000,  6.000], mean action: 57.222 [1.000, 98.000],  loss: 6.424284, mae: 2.439350, mean_q: 4.839257\n",
      "[99 73 75 90 54  5 74 50 23 48]\n",
      " 21996/50001: episode: 2444, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 54.667 [5.000, 90.000],  loss: 4.787800, mae: 2.462674, mean_q: 4.919556\n",
      "[25 60 12 11  2 24 82 32 98 10]\n",
      " 22005/50001: episode: 2445, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 36.778 [2.000, 98.000],  loss: 8.143888, mae: 2.448517, mean_q: 4.832902\n",
      "[ 2 76 39 97 13 52 29 10 11 21]\n",
      " 22014/50001: episode: 2446, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 32.000, mean reward:  3.556 [ 2.000,  5.000], mean action: 38.667 [10.000, 97.000],  loss: 9.139900, mae: 2.398833, mean_q: 4.740739\n",
      "[20 93 95 79 37 83 50 62 97 88]\n",
      " 22023/50001: episode: 2447, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 43.000, mean reward:  4.778 [ 3.000,  7.000], mean action: 76.000 [37.000, 97.000],  loss: 6.681773, mae: 2.338579, mean_q: 4.681963\n",
      "[95 23 98 53 78 12 12 97 28 40]\n",
      " 22032/50001: episode: 2448, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 49.000 [12.000, 98.000],  loss: 9.068161, mae: 2.338323, mean_q: 4.663195\n",
      "[95 20 89  1 50 60  3 30  4 57]\n",
      " 22041/50001: episode: 2449, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward: 37.000, mean reward:  4.111 [ 2.000,  6.000], mean action: 34.889 [1.000, 89.000],  loss: 6.912416, mae: 2.290409, mean_q: 4.564817\n",
      "[17 50 41 59 92  4 99 14 93 93]\n",
      " 22050/50001: episode: 2450, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 24.000, mean reward:  2.667 [-10.000,  5.000], mean action: 60.556 [4.000, 99.000],  loss: 8.327883, mae: 2.300914, mean_q: 4.544109\n",
      "[14 34 17 30 60 66 34  8 13 95]\n",
      " 22059/50001: episode: 2451, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 30.000, mean reward:  3.333 [-10.000,  9.000], mean action: 39.667 [8.000, 95.000],  loss: 6.017729, mae: 2.305554, mean_q: 4.676224\n",
      "[46 43 20 20 12 12 50 48 42 44]\n",
      " 22068/50001: episode: 2452, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward:  9.000, mean reward:  1.000 [-10.000,  6.000], mean action: 32.333 [12.000, 50.000],  loss: 7.148901, mae: 2.308085, mean_q: 4.642177\n",
      "[87 40 64  1 15 63 30 87 64 39]\n",
      " 22077/50001: episode: 2453, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward:  4.000, mean reward:  0.444 [-10.000,  5.000], mean action: 44.778 [1.000, 87.000],  loss: 8.346943, mae: 2.385148, mean_q: 4.812040\n",
      "[ 8 95 68 44 91 42 56 85  5 27]\n",
      " 22086/50001: episode: 2454, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 36.000, mean reward:  4.000 [ 2.000,  6.000], mean action: 57.000 [5.000, 95.000],  loss: 8.060546, mae: 2.356658, mean_q: 4.766464\n",
      "[51 36  4 42 74 60 95  5  3 23]\n",
      " 22095/50001: episode: 2455, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 38.000 [3.000, 95.000],  loss: 7.348584, mae: 2.356498, mean_q: 4.711269\n",
      "[29 10 37 37 97 21 78 93 50 98]\n",
      " 22104/50001: episode: 2456, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 27.000, mean reward:  3.000 [-10.000,  8.000], mean action: 57.889 [10.000, 98.000],  loss: 8.250929, mae: 2.364845, mean_q: 4.786873\n",
      "[76 95 28 48 82 21  1 76 84 50]\n",
      " 22113/50001: episode: 2457, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 29.000, mean reward:  3.222 [-10.000,  9.000], mean action: 53.889 [1.000, 95.000],  loss: 7.500135, mae: 2.298568, mean_q: 4.588712\n",
      "[82  2 42 84 70 68 78 34 37 31]\n",
      " 22122/50001: episode: 2458, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 41.000, mean reward:  4.556 [ 2.000,  8.000], mean action: 49.556 [2.000, 84.000],  loss: 9.374210, mae: 2.320311, mean_q: 4.622532\n",
      "[39  5 87 44 68 82 95 95 28 55]\n",
      " 22131/50001: episode: 2459, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 24.000, mean reward:  2.667 [-10.000,  8.000], mean action: 62.111 [5.000, 95.000],  loss: 9.189032, mae: 2.300797, mean_q: 4.576598\n",
      "[95 83 34 87 20 14 52 63 93 23]\n",
      " 22140/50001: episode: 2460, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 36.000, mean reward:  4.000 [ 2.000,  6.000], mean action: 52.111 [14.000, 93.000],  loss: 8.710099, mae: 2.263278, mean_q: 4.577183\n",
      "[28  1 85 27 31 11 66 92 90 14]\n",
      " 22149/50001: episode: 2461, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 46.333 [1.000, 92.000],  loss: 4.739044, mae: 2.262552, mean_q: 4.638000\n",
      "[45 27 31 79 99 16 95 50 32 14]\n",
      " 22158/50001: episode: 2462, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 43.000, mean reward:  4.778 [ 3.000,  7.000], mean action: 49.222 [14.000, 99.000],  loss: 7.399036, mae: 2.231868, mean_q: 4.477451\n",
      "[51 95  4 60 35 30 48 28 37 34]\n",
      " 22167/50001: episode: 2463, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 45.000, mean reward:  5.000 [ 2.000,  8.000], mean action: 41.222 [4.000, 95.000],  loss: 9.236202, mae: 2.322260, mean_q: 4.621826\n",
      "[60 82 19 28  2 31 47 94 10 27]\n",
      " 22176/50001: episode: 2464, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 37.000, mean reward:  4.111 [ 3.000,  7.000], mean action: 37.778 [2.000, 94.000],  loss: 7.929185, mae: 2.301857, mean_q: 4.517588\n",
      "[ 3 86 59 42 33  4 27 32 70 33]\n",
      " 22185/50001: episode: 2465, duration: 0.068s, episode steps:   9, steps per second: 133, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 42.889 [4.000, 86.000],  loss: 11.581905, mae: 2.300199, mean_q: 4.624382\n",
      "[30 76  2 79 40 32  7 41 50 50]\n",
      " 22194/50001: episode: 2466, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 41.889 [2.000, 79.000],  loss: 5.479259, mae: 2.265571, mean_q: 4.492725\n",
      "[35 95 37 60 82 10 48 12  4 78]\n",
      " 22203/50001: episode: 2467, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 39.000, mean reward:  4.333 [ 2.000,  6.000], mean action: 47.333 [4.000, 95.000],  loss: 8.690545, mae: 2.217358, mean_q: 4.503563\n",
      "[29 95 67  5 80 31 21 64 99 50]\n",
      " 22212/50001: episode: 2468, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 40.000, mean reward:  4.444 [ 3.000,  9.000], mean action: 56.889 [5.000, 99.000],  loss: 6.927595, mae: 2.294703, mean_q: 4.590307\n",
      "[ 1 64 40 73 63 90  1  1 10  1]\n",
      " 22221/50001: episode: 2469, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: -10.000, mean reward: -1.111 [-10.000,  4.000], mean action: 38.111 [1.000, 90.000],  loss: 5.815956, mae: 2.301736, mean_q: 4.634848\n",
      "[89 50 41 10 53 27 14 89  5 13]\n",
      " 22230/50001: episode: 2470, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 28.000, mean reward:  3.111 [-10.000,  9.000], mean action: 33.556 [5.000, 89.000],  loss: 7.755091, mae: 2.306119, mean_q: 4.593870\n",
      "[17 50 31 12 15 93 28 91 50 48]\n",
      " 22239/50001: episode: 2471, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 46.444 [12.000, 93.000],  loss: 7.307261, mae: 2.361285, mean_q: 4.684474\n",
      "[46 84 97 66 24 96 13 88 42 32]\n",
      " 22248/50001: episode: 2472, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 41.000, mean reward:  4.556 [ 3.000,  6.000], mean action: 60.222 [13.000, 97.000],  loss: 5.689346, mae: 2.419343, mean_q: 4.825747\n",
      "[18 83 97 63 12 89 67 51 13 31]\n",
      " 22257/50001: episode: 2473, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 56.222 [12.000, 97.000],  loss: 8.984677, mae: 2.370926, mean_q: 4.676371\n",
      "[83 78  8 88 97 66 42 53 89 97]\n",
      " 22266/50001: episode: 2474, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 24.000, mean reward:  2.667 [-10.000,  5.000], mean action: 68.667 [8.000, 97.000],  loss: 8.683035, mae: 2.350001, mean_q: 4.693843\n",
      "[71 89  2 47 90 49 73 12 12 74]\n",
      " 22275/50001: episode: 2475, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 49.778 [2.000, 90.000],  loss: 4.512265, mae: 2.351154, mean_q: 4.658332\n",
      "[37 62 32 57 77 32  8 34 62 98]\n",
      " 22284/50001: episode: 2476, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 11.000, mean reward:  1.222 [-10.000,  8.000], mean action: 51.333 [8.000, 98.000],  loss: 7.434907, mae: 2.401086, mean_q: 4.754782\n",
      "[97 96 32 68 77 67  1 49 37 54]\n",
      " 22293/50001: episode: 2477, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 37.000, mean reward:  4.111 [ 3.000,  8.000], mean action: 53.444 [1.000, 96.000],  loss: 8.109621, mae: 2.409289, mean_q: 4.792036\n",
      "[84 30 30 18 54 60 76 95 57 84]\n",
      " 22302/50001: episode: 2478, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 11.000, mean reward:  1.222 [-10.000,  8.000], mean action: 56.000 [18.000, 95.000],  loss: 6.814999, mae: 2.393541, mean_q: 4.742224\n",
      "[42 11 46 88  6 30  7 48 46 81]\n",
      " 22311/50001: episode: 2479, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 40.333 [6.000, 88.000],  loss: 6.000289, mae: 2.478751, mean_q: 4.886791\n",
      "[65 30 98 68 31 37 39 14  1 17]\n",
      " 22320/50001: episode: 2480, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 37.000, mean reward:  4.111 [ 2.000,  7.000], mean action: 37.222 [1.000, 98.000],  loss: 7.580850, mae: 2.456571, mean_q: 4.830361\n",
      "[64 76 49 92 98 50  2 34 78 50]\n",
      " 22329/50001: episode: 2481, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 58.778 [2.000, 98.000],  loss: 8.345380, mae: 2.354660, mean_q: 4.631230\n",
      "[21  0 34 99 37 94 32 96  8 12]\n",
      " 22338/50001: episode: 2482, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 45.778 [0.000, 99.000],  loss: 8.153139, mae: 2.387280, mean_q: 4.733166\n",
      "[16  6 48 34 80 59 34 76 12 89]\n",
      " 22347/50001: episode: 2483, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 48.667 [6.000, 89.000],  loss: 6.554455, mae: 2.428866, mean_q: 4.767030\n",
      "[26 10 67 40 16 10 33 17 97 79]\n",
      " 22356/50001: episode: 2484, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 41.000 [10.000, 97.000],  loss: 6.656256, mae: 2.417305, mean_q: 4.734997\n",
      "[85 51 98  4 66  4 49 53 48 74]\n",
      " 22365/50001: episode: 2485, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 49.667 [4.000, 98.000],  loss: 5.669621, mae: 2.367911, mean_q: 4.831120\n",
      "[97 37 50 90 95 18 48 60 82 98]\n",
      " 22374/50001: episode: 2486, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 40.000, mean reward:  4.444 [ 3.000,  6.000], mean action: 64.222 [18.000, 98.000],  loss: 6.407572, mae: 2.459936, mean_q: 4.835029\n",
      "[10  9 76 68 62 34  1 95 79 23]\n",
      " 22383/50001: episode: 2487, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 43.000, mean reward:  4.778 [ 3.000,  8.000], mean action: 49.667 [1.000, 95.000],  loss: 8.054016, mae: 2.461761, mean_q: 4.885539\n",
      "[16 27 68 11 69 37  2 82 94 74]\n",
      " 22392/50001: episode: 2488, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 51.556 [2.000, 94.000],  loss: 7.743095, mae: 2.410305, mean_q: 4.795053\n",
      "[70  1 54 36 41 88 89 95 29 31]\n",
      " 22401/50001: episode: 2489, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 43.000, mean reward:  4.778 [ 2.000,  9.000], mean action: 51.556 [1.000, 95.000],  loss: 6.506895, mae: 2.481757, mean_q: 4.972060\n",
      "[59 95 69 14 11 40 47 81 25 28]\n",
      " 22410/50001: episode: 2490, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 37.000, mean reward:  4.111 [ 3.000,  7.000], mean action: 45.556 [11.000, 95.000],  loss: 6.252223, mae: 2.400885, mean_q: 4.784259\n",
      "[ 5 87  8 60 97 17 74 67 50 75]\n",
      " 22419/50001: episode: 2491, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 59.444 [8.000, 97.000],  loss: 7.022418, mae: 2.369734, mean_q: 4.636516\n",
      "[ 2 87 47 43  4 24 98 89 13 60]\n",
      " 22428/50001: episode: 2492, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 42.000, mean reward:  4.667 [ 2.000,  8.000], mean action: 51.667 [4.000, 98.000],  loss: 4.842538, mae: 2.384030, mean_q: 4.752933\n",
      "[86 12 88 37 34  2 12 97 27 64]\n",
      " 22437/50001: episode: 2493, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 26.000, mean reward:  2.889 [-10.000,  5.000], mean action: 41.444 [2.000, 97.000],  loss: 9.784539, mae: 2.405134, mean_q: 4.920821\n",
      "[30 99 60 77 37 96 88 55 31 37]\n",
      " 22446/50001: episode: 2494, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 64.444 [31.000, 99.000],  loss: 6.085759, mae: 2.435214, mean_q: 4.843925\n",
      "[18 15 93 42 23 88 48 96 12 27]\n",
      " 22455/50001: episode: 2495, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 41.000, mean reward:  4.556 [ 2.000,  7.000], mean action: 49.333 [12.000, 96.000],  loss: 6.704533, mae: 2.410239, mean_q: 4.773356\n",
      "[24 50 30 68 59 63 42  6  1 92]\n",
      " 22464/50001: episode: 2496, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 38.000, mean reward:  4.222 [ 3.000,  8.000], mean action: 45.667 [1.000, 92.000],  loss: 8.644094, mae: 2.428674, mean_q: 4.803428\n",
      "[20 75 79 79 14 41 74 52 99 50]\n",
      " 22473/50001: episode: 2497, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 28.000, mean reward:  3.111 [-10.000,  9.000], mean action: 62.556 [14.000, 99.000],  loss: 10.195626, mae: 2.409775, mean_q: 4.692463\n",
      "[69 42 14 35 58  2 34  0 58  6]\n",
      " 22482/50001: episode: 2498, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 27.667 [0.000, 58.000],  loss: 4.211208, mae: 2.269632, mean_q: 4.579995\n",
      "[18 38  1 11 13 71 95 50 74 35]\n",
      " 22491/50001: episode: 2499, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 43.000, mean reward:  4.778 [ 2.000,  8.000], mean action: 43.111 [1.000, 95.000],  loss: 5.210584, mae: 2.362678, mean_q: 4.699083\n",
      "[89 74 98 79 66 20 46 34 66 90]\n",
      " 22500/50001: episode: 2500, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 63.667 [20.000, 98.000],  loss: 8.010586, mae: 2.373601, mean_q: 4.708948\n",
      "[89 13  9 87 53  0 88  2 10  2]\n",
      " 22509/50001: episode: 2501, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 29.333 [0.000, 88.000],  loss: 6.627700, mae: 2.366619, mean_q: 4.682091\n",
      "[27  6  6 62  6 69 34 95 97 12]\n",
      " 22518/50001: episode: 2502, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 20.000, mean reward:  2.222 [-10.000,  8.000], mean action: 43.000 [6.000, 97.000],  loss: 6.925581, mae: 2.346347, mean_q: 4.709183\n",
      "[33 92 30 17 62 14 66 88 50 44]\n",
      " 22527/50001: episode: 2503, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 43.000, mean reward:  4.778 [ 3.000,  8.000], mean action: 51.444 [14.000, 92.000],  loss: 6.519679, mae: 2.344519, mean_q: 4.631337\n",
      "[40 60 89 34 75 43 33 56 28 28]\n",
      " 22536/50001: episode: 2504, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 49.556 [28.000, 89.000],  loss: 9.559484, mae: 2.388999, mean_q: 4.730680\n",
      "[79 53 32 93 59 93 10  1 95 33]\n",
      " 22545/50001: episode: 2505, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 25.000, mean reward:  2.778 [-10.000,  9.000], mean action: 52.111 [1.000, 95.000],  loss: 6.627176, mae: 2.371717, mean_q: 4.702188\n",
      "[77 67 66 34 66 41 31 30 90 50]\n",
      " 22554/50001: episode: 2506, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 30.000, mean reward:  3.333 [-10.000,  9.000], mean action: 52.778 [30.000, 90.000],  loss: 10.472139, mae: 2.306481, mean_q: 4.664582\n",
      "[71 41 88 13 63 49 72  5 99 50]\n",
      " 22563/50001: episode: 2507, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 41.000, mean reward:  4.556 [ 2.000, 10.000], mean action: 53.333 [5.000, 99.000],  loss: 9.968881, mae: 2.364809, mean_q: 4.611724\n",
      "[45 89 76  0 24 95 62 46 46 92]\n",
      " 22572/50001: episode: 2508, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 58.889 [0.000, 95.000],  loss: 8.443747, mae: 2.317155, mean_q: 4.580446\n",
      "[80 88 66 63 40 89 67 13 43 96]\n",
      " 22581/50001: episode: 2509, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 36.000, mean reward:  4.000 [ 2.000,  7.000], mean action: 62.778 [13.000, 96.000],  loss: 8.261155, mae: 2.273028, mean_q: 4.512764\n",
      "[15 52 24 46 23 32 69 74 64 44]\n",
      " 22590/50001: episode: 2510, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 37.000, mean reward:  4.111 [ 3.000,  5.000], mean action: 47.556 [23.000, 74.000],  loss: 6.764978, mae: 2.266762, mean_q: 4.500481\n",
      "[86 78 97 32 95 42 85 31 37  4]\n",
      " 22599/50001: episode: 2511, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 43.000, mean reward:  4.778 [ 3.000,  8.000], mean action: 55.667 [4.000, 97.000],  loss: 7.856349, mae: 2.299018, mean_q: 4.513018\n",
      "[ 6 99 32 74 28 60 20 33 79 51]\n",
      " 22608/50001: episode: 2512, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 52.889 [20.000, 99.000],  loss: 6.101605, mae: 2.316748, mean_q: 4.649140\n",
      "[53 28  7 52 75 67 95 45 34 53]\n",
      " 22617/50001: episode: 2513, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 26.000, mean reward:  2.889 [-10.000,  9.000], mean action: 50.667 [7.000, 95.000],  loss: 8.451779, mae: 2.328973, mean_q: 4.627697\n",
      "[63 12 38 65 95 96 95 50 28 42]\n",
      " 22626/50001: episode: 2514, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 57.889 [12.000, 96.000],  loss: 8.419563, mae: 2.273564, mean_q: 4.576567\n",
      "[45 30 23 24 12 62 21 11 10 68]\n",
      " 22635/50001: episode: 2515, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward: 35.000, mean reward:  3.889 [ 3.000,  5.000], mean action: 29.000 [10.000, 68.000],  loss: 9.755696, mae: 2.271468, mean_q: 4.522711\n",
      "[35  9  1  7 48 84 13 95 99 74]\n",
      " 22644/50001: episode: 2516, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 39.000, mean reward:  4.333 [ 2.000,  8.000], mean action: 47.778 [1.000, 99.000],  loss: 6.318276, mae: 2.226732, mean_q: 4.532149\n",
      "[24 95 71  4 13  1 68 14 78 88]\n",
      " 22653/50001: episode: 2517, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 39.000, mean reward:  4.333 [ 2.000,  8.000], mean action: 48.000 [1.000, 95.000],  loss: 8.881600, mae: 2.241990, mean_q: 4.458399\n",
      "[61 50 41 91 80 88 94 60 27 78]\n",
      " 22662/50001: episode: 2518, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 67.667 [27.000, 94.000],  loss: 5.729387, mae: 2.257234, mean_q: 4.541586\n",
      "[56 95 98  9 42 57  1 88 90 18]\n",
      " 22671/50001: episode: 2519, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 41.000, mean reward:  4.556 [ 3.000,  6.000], mean action: 55.333 [1.000, 98.000],  loss: 8.664930, mae: 2.327252, mean_q: 4.631213\n",
      "[95 44 82 93 92  5 79 66 14 82]\n",
      " 22680/50001: episode: 2520, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 61.889 [5.000, 93.000],  loss: 6.589995, mae: 2.362345, mean_q: 4.680341\n",
      "[56 95 11 46 74 44 95 82 48 44]\n",
      " 22689/50001: episode: 2521, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 11.000, mean reward:  1.222 [-10.000,  7.000], mean action: 59.889 [11.000, 95.000],  loss: 7.924682, mae: 2.397302, mean_q: 4.791976\n",
      "[77 74 60 34 40 88 34 53 68 60]\n",
      " 22698/50001: episode: 2522, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward:  9.000, mean reward:  1.000 [-10.000,  6.000], mean action: 56.778 [34.000, 88.000],  loss: 6.939519, mae: 2.323524, mean_q: 4.593532\n",
      "[ 7  6 54  2 48 84  1  9 52 95]\n",
      " 22707/50001: episode: 2523, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 46.000, mean reward:  5.111 [ 3.000, 11.000], mean action: 39.000 [1.000, 95.000],  loss: 6.553755, mae: 2.375603, mean_q: 4.791788\n",
      "[94 95 80  4 52 23 88 82 54 10]\n",
      " 22716/50001: episode: 2524, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 36.000, mean reward:  4.000 [ 2.000,  5.000], mean action: 54.222 [4.000, 95.000],  loss: 5.562308, mae: 2.464442, mean_q: 4.885413\n",
      "[20 28 59 96 95 23 32 66 88 31]\n",
      " 22725/50001: episode: 2525, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 41.000, mean reward:  4.556 [ 3.000,  6.000], mean action: 57.556 [23.000, 96.000],  loss: 7.716485, mae: 2.417058, mean_q: 4.829251\n",
      "[65 42 86 67 59 73 51 96 37 89]\n",
      " 22734/50001: episode: 2526, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 39.000, mean reward:  4.333 [ 3.000,  9.000], mean action: 66.667 [37.000, 96.000],  loss: 7.611857, mae: 2.467534, mean_q: 4.852701\n",
      "[83 12 18 34 54 59  4 41 56 33]\n",
      " 22743/50001: episode: 2527, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward: 38.000, mean reward:  4.222 [ 2.000,  6.000], mean action: 34.556 [4.000, 59.000],  loss: 6.005975, mae: 2.434922, mean_q: 4.862094\n",
      "[40 95  4 90 77 79 95 52 41 34]\n",
      " 22752/50001: episode: 2528, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 27.000, mean reward:  3.000 [-10.000,  9.000], mean action: 63.000 [4.000, 95.000],  loss: 9.121626, mae: 2.424516, mean_q: 4.826481\n",
      "[27 67  1 81 47 56 27 96 89 50]\n",
      " 22761/50001: episode: 2529, duration: 0.069s, episode steps:   9, steps per second: 130, episode reward: 26.000, mean reward:  2.889 [-10.000,  9.000], mean action: 57.111 [1.000, 96.000],  loss: 6.094820, mae: 2.368443, mean_q: 4.712733\n",
      "[77 26 66 29  9 28 11 72 66 48]\n",
      " 22770/50001: episode: 2530, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 21.000, mean reward:  2.333 [-10.000,  8.000], mean action: 39.444 [9.000, 72.000],  loss: 7.622366, mae: 2.461542, mean_q: 4.823041\n",
      "[45 64 55 99 28 79 50  1  1 63]\n",
      " 22779/50001: episode: 2531, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 27.000, mean reward:  3.000 [-10.000,  6.000], mean action: 48.889 [1.000, 99.000],  loss: 7.608753, mae: 2.422310, mean_q: 4.765643\n",
      "[51 13  2 72 98  3 88 95 96 66]\n",
      " 22788/50001: episode: 2532, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 42.000, mean reward:  4.667 [ 2.000,  8.000], mean action: 59.222 [2.000, 98.000],  loss: 7.935735, mae: 2.366716, mean_q: 4.656596\n",
      "[81 83 31 37  6 90 21 46  1 68]\n",
      " 22797/50001: episode: 2533, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 40.000, mean reward:  4.444 [ 3.000,  8.000], mean action: 42.556 [1.000, 90.000],  loss: 8.015300, mae: 2.337827, mean_q: 4.650328\n",
      "[93 60 50 76  4 62 64 46 81 50]\n",
      " 22806/50001: episode: 2534, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 24.000, mean reward:  2.667 [-10.000,  5.000], mean action: 54.778 [4.000, 81.000],  loss: 8.968876, mae: 2.365565, mean_q: 4.737109\n",
      "[95 83 11 41  6 94 34 74 53  1]\n",
      " 22815/50001: episode: 2535, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 41.000, mean reward:  4.556 [ 2.000,  8.000], mean action: 44.111 [1.000, 94.000],  loss: 6.260004, mae: 2.405568, mean_q: 4.864347\n",
      "[30 30 25 60 61 18 64 14 75  4]\n",
      " 22824/50001: episode: 2536, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 39.000 [4.000, 75.000],  loss: 8.096346, mae: 2.330182, mean_q: 4.705257\n",
      "[28 49 38  0 31 66 70 93 50 50]\n",
      " 22833/50001: episode: 2537, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 26.000, mean reward:  2.889 [-10.000,  9.000], mean action: 49.667 [0.000, 93.000],  loss: 9.430478, mae: 2.359742, mean_q: 4.676983\n",
      "[27 27 91 11 79 31 37 73 32 53]\n",
      " 22842/50001: episode: 2538, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 26.000, mean reward:  2.889 [-10.000,  6.000], mean action: 48.222 [11.000, 91.000],  loss: 6.242857, mae: 2.329041, mean_q: 4.628600\n",
      "[22 68 64 69 78 67 88 56  5 14]\n",
      " 22851/50001: episode: 2539, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 56.556 [5.000, 88.000],  loss: 6.316588, mae: 2.326894, mean_q: 4.547874\n",
      "[52 38 94  2  1 79 50 30 42 99]\n",
      " 22860/50001: episode: 2540, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 42.000, mean reward:  4.667 [ 3.000,  6.000], mean action: 48.333 [1.000, 99.000],  loss: 5.629737, mae: 2.424719, mean_q: 4.861504\n",
      "[27 27 12 88 74 58 83 74 31  9]\n",
      " 22869/50001: episode: 2541, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward:  9.000, mean reward:  1.000 [-10.000,  6.000], mean action: 50.667 [9.000, 88.000],  loss: 9.804638, mae: 2.466252, mean_q: 4.913351\n",
      "[98 40 40 50 74 96 95 86 50  5]\n",
      " 22878/50001: episode: 2542, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward:  9.000, mean reward:  1.000 [-10.000,  7.000], mean action: 59.556 [5.000, 96.000],  loss: 8.540560, mae: 2.354735, mean_q: 4.698689\n",
      "[12 32 56 96 62 66 95 92 55  7]\n",
      " 22887/50001: episode: 2543, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 31.000, mean reward:  3.444 [ 1.000,  7.000], mean action: 62.333 [7.000, 96.000],  loss: 10.344006, mae: 2.327615, mean_q: 4.648819\n",
      "[13 42 20 71 96 76 51  5  3 11]\n",
      " 22896/50001: episode: 2544, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 32.000, mean reward:  3.556 [ 2.000,  5.000], mean action: 41.667 [3.000, 96.000],  loss: 8.244526, mae: 2.238195, mean_q: 4.485114\n",
      "[ 8 18 98 73 97 82 95 29  5 67]\n",
      " 22905/50001: episode: 2545, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 38.000, mean reward:  4.222 [ 2.000,  8.000], mean action: 62.667 [5.000, 98.000],  loss: 6.480173, mae: 2.217884, mean_q: 4.474653\n",
      "[16 27 41  4 53 76  4 28 95 63]\n",
      " 22914/50001: episode: 2546, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 27.000, mean reward:  3.000 [-10.000,  8.000], mean action: 43.444 [4.000, 95.000],  loss: 7.693873, mae: 2.200542, mean_q: 4.395452\n",
      "[18 50 13 20 46  4 77 42 30 50]\n",
      " 22923/50001: episode: 2547, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 36.889 [4.000, 77.000],  loss: 6.271886, mae: 2.198438, mean_q: 4.438851\n",
      "[85 89 34 12  4 90 24 79 95 75]\n",
      " 22932/50001: episode: 2548, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 55.778 [4.000, 95.000],  loss: 6.925153, mae: 2.216204, mean_q: 4.528331\n",
      "[18  9  4 36  2 64 92 13 50 39]\n",
      " 22941/50001: episode: 2549, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 34.333 [2.000, 92.000],  loss: 6.362741, mae: 2.291804, mean_q: 4.632563\n",
      "[63 84 24 52  2 29 90 26  5 98]\n",
      " 22950/50001: episode: 2550, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 37.000, mean reward:  4.111 [ 2.000,  7.000], mean action: 45.556 [2.000, 98.000],  loss: 10.032932, mae: 2.354157, mean_q: 4.670484\n",
      "[86 97 98 69  2 37 69 99 21 37]\n",
      " 22959/50001: episode: 2551, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 10.000, mean reward:  1.111 [-10.000,  6.000], mean action: 58.778 [2.000, 99.000],  loss: 7.235919, mae: 2.352697, mean_q: 4.648917\n",
      "[51  5 56 28 34 76 27 79 40  9]\n",
      " 22968/50001: episode: 2552, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 39.333 [5.000, 79.000],  loss: 8.405886, mae: 2.261899, mean_q: 4.466140\n",
      "[83 60 10 95 80 13  2 19 27  3]\n",
      " 22977/50001: episode: 2553, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 36.000, mean reward:  4.000 [ 2.000,  6.000], mean action: 34.333 [2.000, 95.000],  loss: 6.729228, mae: 2.261197, mean_q: 4.493025\n",
      "[20 95 50 99 32 77 44 14 13 90]\n",
      " 22986/50001: episode: 2554, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 57.111 [13.000, 99.000],  loss: 6.885883, mae: 2.273973, mean_q: 4.502077\n",
      "[18 33 29 89  6 36 48 49 50 55]\n",
      " 22995/50001: episode: 2555, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 38.000, mean reward:  4.222 [ 2.000,  9.000], mean action: 43.889 [6.000, 89.000],  loss: 7.635389, mae: 2.302462, mean_q: 4.609470\n",
      "[68 28 94 20 68  2 59 34 23 56]\n",
      " 23004/50001: episode: 2556, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 42.667 [2.000, 94.000],  loss: 6.791014, mae: 2.257618, mean_q: 4.492473\n",
      "[99 46 64 96 77 45 88 31 27 93]\n",
      " 23013/50001: episode: 2557, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 63.000 [27.000, 96.000],  loss: 7.344852, mae: 2.360018, mean_q: 4.635825\n",
      "[68 66 37 95 37 57  2 95 32 62]\n",
      " 23022/50001: episode: 2558, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward:  9.000, mean reward:  1.000 [-10.000,  5.000], mean action: 53.667 [2.000, 95.000],  loss: 7.986554, mae: 2.334329, mean_q: 4.631015\n",
      "[49 27 14 69 56 85 95 62  9 24]\n",
      " 23031/50001: episode: 2559, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 40.000, mean reward:  4.444 [ 3.000,  8.000], mean action: 49.000 [9.000, 95.000],  loss: 5.699936, mae: 2.406595, mean_q: 4.756967\n",
      "[18 28 27 14 80 35 11 32 93 18]\n",
      " 23040/50001: episode: 2560, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 37.556 [11.000, 93.000],  loss: 9.052732, mae: 2.428416, mean_q: 4.847990\n",
      "[51 50 21 49 13 58 66 95 96 31]\n",
      " 23049/50001: episode: 2561, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 53.222 [13.000, 96.000],  loss: 7.755190, mae: 2.475597, mean_q: 4.924472\n",
      "[51 95 63 56  4 82 41 95 46 42]\n",
      " 23058/50001: episode: 2562, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 23.000, mean reward:  2.556 [-10.000,  5.000], mean action: 58.222 [4.000, 95.000],  loss: 4.710840, mae: 2.426829, mean_q: 4.799469\n",
      "[32 89 22  4 77 31 60 37 74 88]\n",
      " 23067/50001: episode: 2563, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 53.556 [4.000, 89.000],  loss: 5.868332, mae: 2.513780, mean_q: 4.878024\n",
      "[85 46 63 48  2 94 11 93 27  2]\n",
      " 23076/50001: episode: 2564, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 42.889 [2.000, 94.000],  loss: 5.774314, mae: 2.502018, mean_q: 4.880352\n",
      "[52 51 85 37 31 69 78 27 66 50]\n",
      " 23085/50001: episode: 2565, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 42.000, mean reward:  4.667 [ 3.000,  9.000], mean action: 54.889 [27.000, 85.000],  loss: 6.441198, mae: 2.491771, mean_q: 4.965465\n",
      "[46 86 60 99 30 56 79 99 27 27]\n",
      " 23094/50001: episode: 2566, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward:  8.000, mean reward:  0.889 [-10.000,  5.000], mean action: 62.556 [27.000, 99.000],  loss: 9.164024, mae: 2.445105, mean_q: 4.916999\n",
      "[ 8 33 55 46  1 93 42 42 41 97]\n",
      " 23103/50001: episode: 2567, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 27.000, mean reward:  3.000 [-10.000,  8.000], mean action: 50.000 [1.000, 97.000],  loss: 10.425975, mae: 2.460774, mean_q: 4.863256\n",
      "[34 28 25 49 95 56 73 12 78 90]\n",
      " 23112/50001: episode: 2568, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 35.000, mean reward:  3.889 [ 2.000,  6.000], mean action: 56.222 [12.000, 95.000],  loss: 6.108704, mae: 2.371878, mean_q: 4.765711\n",
      "[58 32 98 34 37 84  1 94 12 87]\n",
      " 23121/50001: episode: 2569, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 37.000, mean reward:  4.111 [ 2.000,  7.000], mean action: 53.222 [1.000, 98.000],  loss: 6.222410, mae: 2.356013, mean_q: 4.714186\n",
      "[95 74 10 45 88 49 13 28 69 50]\n",
      " 23130/50001: episode: 2570, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 47.333 [10.000, 88.000],  loss: 6.192465, mae: 2.472208, mean_q: 4.834931\n",
      "[80 30  4 63 98 50 95 89 24 12]\n",
      " 23139/50001: episode: 2571, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 46.000, mean reward:  5.111 [ 3.000,  9.000], mean action: 51.667 [4.000, 98.000],  loss: 6.297390, mae: 2.441636, mean_q: 4.891401\n",
      "[78  0 98 77 66 82 87 64 37 97]\n",
      " 23148/50001: episode: 2572, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 44.000, mean reward:  4.889 [ 2.000,  9.000], mean action: 67.556 [0.000, 98.000],  loss: 6.736689, mae: 2.555337, mean_q: 5.053179\n",
      "[86  0 87 19  1 16 59 84 66 88]\n",
      " 23157/50001: episode: 2573, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 46.667 [0.000, 88.000],  loss: 6.174577, mae: 2.527459, mean_q: 5.013377\n",
      "[97 60 44 51 73 95 95 79 66 13]\n",
      " 23166/50001: episode: 2574, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 27.000, mean reward:  3.000 [-10.000,  8.000], mean action: 64.000 [13.000, 95.000],  loss: 6.121677, mae: 2.458480, mean_q: 4.838264\n",
      "[30  4 34 34 69  5 88 78 50 71]\n",
      " 23175/50001: episode: 2575, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 48.111 [4.000, 88.000],  loss: 7.561240, mae: 2.414547, mean_q: 4.770210\n",
      "[39 41  2 66 21 98 27  9 80 46]\n",
      " 23184/50001: episode: 2576, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 43.333 [2.000, 98.000],  loss: 5.477829, mae: 2.396260, mean_q: 4.751218\n",
      "[ 5 84 77 94 27 74 92 98 66 88]\n",
      " 23193/50001: episode: 2577, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 41.000, mean reward:  4.556 [ 2.000,  8.000], mean action: 77.778 [27.000, 98.000],  loss: 7.473963, mae: 2.504044, mean_q: 4.926378\n",
      "[ 4 63 31 99 37 62 46 51 24 30]\n",
      " 23202/50001: episode: 2578, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 49.222 [24.000, 99.000],  loss: 5.374066, mae: 2.443337, mean_q: 4.845795\n",
      "[48 31 37 87 93 46 87 99  4 82]\n",
      " 23211/50001: episode: 2579, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 62.889 [4.000, 99.000],  loss: 8.155737, mae: 2.449776, mean_q: 4.827628\n",
      "[ 4 97 78  4 23 95 50 69 13 42]\n",
      " 23220/50001: episode: 2580, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 28.000, mean reward:  3.111 [-10.000,  7.000], mean action: 52.333 [4.000, 97.000],  loss: 9.153195, mae: 2.468064, mean_q: 4.800680\n",
      "[89 37 30 49 91 42 86 45 31 64]\n",
      " 23229/50001: episode: 2581, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 38.000, mean reward:  4.222 [ 3.000,  7.000], mean action: 52.778 [30.000, 91.000],  loss: 7.996544, mae: 2.446332, mean_q: 4.769007\n",
      "[97  9 54 63 31 93 95 95  9  2]\n",
      " 23238/50001: episode: 2582, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 11.000, mean reward:  1.222 [-10.000,  7.000], mean action: 50.111 [2.000, 95.000],  loss: 8.627899, mae: 2.266872, mean_q: 4.524197\n",
      "[95 71 40 40 13 34 97  4 50 32]\n",
      " 23247/50001: episode: 2583, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 27.000, mean reward:  3.000 [-10.000,  6.000], mean action: 42.333 [4.000, 97.000],  loss: 7.439718, mae: 2.384427, mean_q: 4.677050\n",
      "[18 26 41 74 68 75 88 30 10 28]\n",
      " 23256/50001: episode: 2584, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 42.000, mean reward:  4.667 [ 2.000,  8.000], mean action: 48.889 [10.000, 88.000],  loss: 6.469786, mae: 2.347054, mean_q: 4.626880\n",
      "[89  1 33 28 93 75 95  2 66 12]\n",
      " 23265/50001: episode: 2585, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 45.000, mean reward:  5.000 [ 3.000,  8.000], mean action: 45.000 [1.000, 95.000],  loss: 8.927903, mae: 2.370228, mean_q: 4.667846\n",
      "[ 7 46 11  4 99 50 11 84 46 94]\n",
      " 23274/50001: episode: 2586, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 10.000, mean reward:  1.111 [-10.000,  6.000], mean action: 49.444 [4.000, 99.000],  loss: 7.370339, mae: 2.316870, mean_q: 4.507010\n",
      "[46 97 20 24 76  2 79 60 41 50]\n",
      " 23283/50001: episode: 2587, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 43.000, mean reward:  4.778 [ 3.000,  9.000], mean action: 49.889 [2.000, 97.000],  loss: 5.646124, mae: 2.344177, mean_q: 4.689875\n",
      "[93  1 34 17 34 32 98 54  8 18]\n",
      " 23292/50001: episode: 2588, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 21.000, mean reward:  2.333 [-10.000,  5.000], mean action: 32.889 [1.000, 98.000],  loss: 6.271609, mae: 2.381049, mean_q: 4.712240\n",
      "[36 97  2 19 97 27 51 97 88 49]\n",
      " 23301/50001: episode: 2589, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward:  9.000, mean reward:  1.000 [-10.000,  6.000], mean action: 58.556 [2.000, 97.000],  loss: 8.390653, mae: 2.376942, mean_q: 4.716520\n",
      "[97  5 62 67 34 34 37 27 12  2]\n",
      " 23310/50001: episode: 2590, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 28.000, mean reward:  3.111 [-10.000,  7.000], mean action: 31.111 [2.000, 67.000],  loss: 3.715106, mae: 2.354037, mean_q: 4.662047\n",
      "[79 22 92 18 66 23 59 31 50 59]\n",
      " 23319/50001: episode: 2591, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 23.000, mean reward:  2.556 [-10.000,  8.000], mean action: 46.667 [18.000, 92.000],  loss: 7.167246, mae: 2.423337, mean_q: 4.821333\n",
      "[20 79 37 90 37 44 98 10 32 48]\n",
      " 23328/50001: episode: 2592, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 52.778 [10.000, 98.000],  loss: 8.993691, mae: 2.377178, mean_q: 4.721364\n",
      "[48 27 90 78 79 95 52 79  9 14]\n",
      " 23337/50001: episode: 2593, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 26.000, mean reward:  2.889 [-10.000,  6.000], mean action: 58.111 [9.000, 95.000],  loss: 9.102917, mae: 2.364828, mean_q: 4.681067\n",
      "[98 34 31 88 75  1 48 27 86 50]\n",
      " 23346/50001: episode: 2594, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 44.000, mean reward:  4.889 [ 3.000,  9.000], mean action: 48.889 [1.000, 88.000],  loss: 7.350973, mae: 2.418535, mean_q: 4.751004\n",
      "[92 27 86 96  1 88 74 95 14 31]\n",
      " 23355/50001: episode: 2595, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 43.000, mean reward:  4.778 [ 3.000,  8.000], mean action: 56.889 [1.000, 96.000],  loss: 7.991508, mae: 2.316800, mean_q: 4.606694\n",
      "[14  0 62 21 98 84 52 97 84  9]\n",
      " 23364/50001: episode: 2596, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 56.333 [0.000, 98.000],  loss: 7.328452, mae: 2.357307, mean_q: 4.669640\n",
      "[21 63 38 52 37 23 22 50 94 50]\n",
      " 23373/50001: episode: 2597, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 47.667 [22.000, 94.000],  loss: 8.720860, mae: 2.396741, mean_q: 4.730212\n",
      "[ 4  4 48 70 48 48 31 40 88 12]\n",
      " 23382/50001: episode: 2598, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: -2.000, mean reward: -0.222 [-10.000,  7.000], mean action: 43.222 [4.000, 88.000],  loss: 5.542768, mae: 2.410368, mean_q: 4.830679\n",
      "[31 13 30  4 11  4 44 96 67 50]\n",
      " 23391/50001: episode: 2599, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 23.000, mean reward:  2.556 [-10.000,  9.000], mean action: 35.444 [4.000, 96.000],  loss: 7.400627, mae: 2.373631, mean_q: 4.697649\n",
      "[98 98 23 88 90 85 95 91 66 14]\n",
      " 23400/50001: episode: 2600, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 29.000, mean reward:  3.222 [-10.000,  7.000], mean action: 72.222 [14.000, 98.000],  loss: 8.710109, mae: 2.377084, mean_q: 4.765294\n",
      "[93 27  4 25 41 34 83 43 23 39]\n",
      " 23409/50001: episode: 2601, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 33.000, mean reward:  3.667 [ 2.000,  6.000], mean action: 35.444 [4.000, 83.000],  loss: 5.458626, mae: 2.399160, mean_q: 4.757386\n",
      "[27 60 41 95 48 50 96 32  2 41]\n",
      " 23418/50001: episode: 2602, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 25.000, mean reward:  2.778 [-10.000,  5.000], mean action: 51.667 [2.000, 96.000],  loss: 9.621501, mae: 2.388173, mean_q: 4.677881\n",
      "[76 33 88  9 66 13 63 71 18 50]\n",
      " 23427/50001: episode: 2603, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 41.000, mean reward:  4.556 [ 2.000, 10.000], mean action: 45.667 [9.000, 88.000],  loss: 9.016616, mae: 2.416642, mean_q: 4.801817\n",
      "[65 12 94 34 27 63  8 24  1 74]\n",
      " 23436/50001: episode: 2604, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 43.000, mean reward:  4.778 [ 3.000,  7.000], mean action: 37.444 [1.000, 94.000],  loss: 8.277688, mae: 2.378009, mean_q: 4.680565\n",
      "[22 69 86 34 57 95 59 60 28 13]\n",
      " 23445/50001: episode: 2605, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 44.000, mean reward:  4.889 [ 3.000,  8.000], mean action: 55.667 [13.000, 95.000],  loss: 8.152423, mae: 2.316014, mean_q: 4.647470\n",
      "[94  4 28 37 53 30 59 82 66 12]\n",
      " 23454/50001: episode: 2606, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 43.000, mean reward:  4.778 [ 3.000,  8.000], mean action: 41.222 [4.000, 82.000],  loss: 7.278097, mae: 2.314765, mean_q: 4.615903\n",
      "[88 40 82 66  1 76 53 88 66 28]\n",
      " 23463/50001: episode: 2607, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward:  8.000, mean reward:  0.889 [-10.000,  7.000], mean action: 55.556 [1.000, 88.000],  loss: 5.568530, mae: 2.266395, mean_q: 4.555671\n",
      "[35 52 56 75 14  1 99 90 28 77]\n",
      " 23472/50001: episode: 2608, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 54.667 [1.000, 99.000],  loss: 7.366152, mae: 2.294310, mean_q: 4.514364\n",
      "[14 52 69 38 98 53 90 54  4 27]\n",
      " 23481/50001: episode: 2609, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 53.889 [4.000, 98.000],  loss: 6.158795, mae: 2.401988, mean_q: 4.732268\n",
      "[50 27  1 46 13 47 62 83 10 73]\n",
      " 23490/50001: episode: 2610, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 33.000, mean reward:  3.667 [ 2.000,  5.000], mean action: 40.222 [1.000, 83.000],  loss: 9.230548, mae: 2.370576, mean_q: 4.709185\n",
      "[77  4 67  9 30 34 52 12 99 79]\n",
      " 23499/50001: episode: 2611, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 43.000, mean reward:  4.778 [ 3.000,  7.000], mean action: 42.889 [4.000, 99.000],  loss: 9.103906, mae: 2.348204, mean_q: 4.646701\n",
      "[ 4 40  8 98 96  2 52 37 14 14]\n",
      " 23508/50001: episode: 2612, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 40.111 [2.000, 98.000],  loss: 7.985394, mae: 2.366908, mean_q: 4.766945\n",
      "[16 28 97 66 37 66  2 98 34 40]\n",
      " 23517/50001: episode: 2613, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 28.000, mean reward:  3.111 [-10.000,  7.000], mean action: 52.000 [2.000, 98.000],  loss: 5.574275, mae: 2.315283, mean_q: 4.591107\n",
      "[ 5 21 47 93 37 33  9 46 46 88]\n",
      " 23526/50001: episode: 2614, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 27.000, mean reward:  3.000 [-10.000,  8.000], mean action: 46.667 [9.000, 93.000],  loss: 6.283523, mae: 2.311626, mean_q: 4.616256\n",
      "[25 83 97 45 30 48 52 11 50 48]\n",
      " 23535/50001: episode: 2615, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 51.556 [11.000, 97.000],  loss: 5.736944, mae: 2.333603, mean_q: 4.601541\n",
      "[61 83 25 75 23  9 62 98 82 23]\n",
      " 23544/50001: episode: 2616, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 53.333 [9.000, 98.000],  loss: 7.906718, mae: 2.360795, mean_q: 4.750322\n",
      "[29 52 11 27 97 52 88 98 31 93]\n",
      " 23553/50001: episode: 2617, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward: 28.000, mean reward:  3.111 [-10.000,  6.000], mean action: 61.000 [11.000, 98.000],  loss: 8.079844, mae: 2.401739, mean_q: 4.757414\n",
      "[42 64 38 74  1 42 50 50  5 42]\n",
      " 23562/50001: episode: 2618, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: -3.000, mean reward: -0.333 [-10.000,  7.000], mean action: 40.667 [1.000, 74.000],  loss: 7.839961, mae: 2.361009, mean_q: 4.684762\n",
      "[82  5 19 41 34 35 75 89 98 50]\n",
      " 23571/50001: episode: 2619, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 44.000, mean reward:  4.889 [ 3.000,  9.000], mean action: 49.556 [5.000, 98.000],  loss: 5.434083, mae: 2.335231, mean_q: 4.634419\n",
      "[76  2 65 92 66 26 66 37 40 66]\n",
      " 23580/50001: episode: 2620, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward:  8.000, mean reward:  0.889 [-10.000,  7.000], mean action: 51.111 [2.000, 92.000],  loss: 4.742062, mae: 2.450849, mean_q: 4.873430\n",
      "[30 95 59 60 74 75  1 12  1 79]\n",
      " 23589/50001: episode: 2621, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 28.000, mean reward:  3.111 [-10.000,  6.000], mean action: 50.667 [1.000, 95.000],  loss: 8.953121, mae: 2.499655, mean_q: 4.917402\n",
      "[ 4 51 98 48 12 34 66 77 90 57]\n",
      " 23598/50001: episode: 2622, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 37.000, mean reward:  4.111 [ 2.000,  6.000], mean action: 59.222 [12.000, 98.000],  loss: 6.502655, mae: 2.463006, mean_q: 4.805914\n",
      "[65  1 93 53  3 53  4 49  1 23]\n",
      " 23607/50001: episode: 2623, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward:  9.000, mean reward:  1.000 [-10.000,  6.000], mean action: 31.111 [1.000, 93.000],  loss: 7.310755, mae: 2.451410, mean_q: 4.804836\n",
      "[67 97 78 66 81 31 95 56 13 53]\n",
      " 23616/50001: episode: 2624, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 63.333 [13.000, 97.000],  loss: 10.155097, mae: 2.461894, mean_q: 4.899184\n",
      "[57 13 83  4 14 39 90  2 64 97]\n",
      " 23625/50001: episode: 2625, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 38.000, mean reward:  4.222 [ 2.000,  8.000], mean action: 45.111 [2.000, 97.000],  loss: 6.835638, mae: 2.473165, mean_q: 4.818178\n",
      "[56 87 98 79 37  4 93 13 84  6]\n",
      " 23634/50001: episode: 2626, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 55.667 [4.000, 98.000],  loss: 7.947509, mae: 2.435988, mean_q: 4.817259\n",
      "[35 89 40 10 78 98 83 34 62 22]\n",
      " 23643/50001: episode: 2627, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 40.000, mean reward:  4.444 [ 3.000,  8.000], mean action: 57.333 [10.000, 98.000],  loss: 6.758679, mae: 2.419689, mean_q: 4.749949\n",
      "[16 98 54 40 60 75 95 51 28 34]\n",
      " 23652/50001: episode: 2628, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 46.000, mean reward:  5.111 [ 3.000,  9.000], mean action: 59.444 [28.000, 98.000],  loss: 6.256944, mae: 2.439558, mean_q: 4.764585\n",
      "[91 98 99 34 81  1 99  4  6 24]\n",
      " 23661/50001: episode: 2629, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 49.556 [1.000, 99.000],  loss: 7.382692, mae: 2.396484, mean_q: 4.736681\n",
      "[10 50 24 53 62 21 55 49 28 30]\n",
      " 23670/50001: episode: 2630, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 37.000, mean reward:  4.111 [ 3.000,  7.000], mean action: 41.333 [21.000, 62.000],  loss: 7.596434, mae: 2.386005, mean_q: 4.686319\n",
      "[26 86 99  1 95 98 83 88 89 51]\n",
      " 23679/50001: episode: 2631, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 76.667 [1.000, 99.000],  loss: 6.578485, mae: 2.277639, mean_q: 4.548010\n",
      "[45 15 47 85 65 23  1 21 92 50]\n",
      " 23688/50001: episode: 2632, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 42.000, mean reward:  4.667 [ 2.000, 10.000], mean action: 44.333 [1.000, 92.000],  loss: 8.417907, mae: 2.385446, mean_q: 4.696416\n",
      "[36 74  2  9 46 30 75 56  5 71]\n",
      " 23697/50001: episode: 2633, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 40.889 [2.000, 75.000],  loss: 5.706153, mae: 2.309985, mean_q: 4.499558\n",
      "[80 13 32 34 99 88 13 53 53 74]\n",
      " 23706/50001: episode: 2634, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward:  9.000, mean reward:  1.000 [-10.000,  5.000], mean action: 51.000 [13.000, 99.000],  loss: 8.379769, mae: 2.335542, mean_q: 4.628981\n",
      "[17 61 34 17 49 31 42 57 44 52]\n",
      " 23715/50001: episode: 2635, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 26.000, mean reward:  2.889 [-10.000,  6.000], mean action: 43.000 [17.000, 61.000],  loss: 7.325587, mae: 2.377367, mean_q: 4.621889\n",
      "[59 12 18 24 88 84 59 99 31 43]\n",
      " 23724/50001: episode: 2636, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 20.000, mean reward:  2.222 [-10.000,  6.000], mean action: 50.889 [12.000, 99.000],  loss: 8.439129, mae: 2.336485, mean_q: 4.548622\n",
      "[40 41 37 69  9 84 40 96 30 28]\n",
      " 23733/50001: episode: 2637, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 48.222 [9.000, 96.000],  loss: 7.812691, mae: 2.320696, mean_q: 4.583074\n",
      "[ 5 82 33 86 37 51 88  2 46 20]\n",
      " 23742/50001: episode: 2638, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 41.000, mean reward:  4.556 [ 3.000,  6.000], mean action: 49.444 [2.000, 88.000],  loss: 8.093275, mae: 2.341217, mean_q: 4.555650\n",
      "[19  8 32 70 91 60 30 10 92 12]\n",
      " 23751/50001: episode: 2639, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 41.000, mean reward:  4.556 [ 2.000,  9.000], mean action: 45.000 [8.000, 92.000],  loss: 6.108286, mae: 2.300981, mean_q: 4.544523\n",
      "[34 85 32 90 10 40  1 62 32 66]\n",
      " 23760/50001: episode: 2640, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 46.444 [1.000, 90.000],  loss: 8.525702, mae: 2.398575, mean_q: 4.709017\n",
      "[70 95 73 77 93 21 57 60 88 46]\n",
      " 23769/50001: episode: 2641, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 43.000, mean reward:  4.778 [ 2.000,  8.000], mean action: 67.778 [21.000, 95.000],  loss: 4.886250, mae: 2.461505, mean_q: 4.821456\n",
      "[22  9 54 35 28 64 96 66 28 93]\n",
      " 23778/50001: episode: 2642, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 52.556 [9.000, 96.000],  loss: 6.519730, mae: 2.481212, mean_q: 4.826787\n",
      "[55 11 35  6 75 97 34 95 97 66]\n",
      " 23787/50001: episode: 2643, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 31.000, mean reward:  3.444 [-10.000,  8.000], mean action: 57.333 [6.000, 97.000],  loss: 7.314911, mae: 2.516952, mean_q: 4.916158\n",
      "[56  4 85 69 61 37 78 95 82 14]\n",
      " 23796/50001: episode: 2644, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 58.333 [4.000, 95.000],  loss: 8.928174, mae: 2.504286, mean_q: 4.901115\n",
      "[ 3 95 32 31 90  6 65 23 31 98]\n",
      " 23805/50001: episode: 2645, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 52.333 [6.000, 98.000],  loss: 4.896145, mae: 2.426057, mean_q: 4.812295\n",
      "[33 41 36 35  1 41 75 13 48 84]\n",
      " 23814/50001: episode: 2646, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 28.000, mean reward:  3.111 [-10.000,  8.000], mean action: 41.556 [1.000, 84.000],  loss: 8.406686, mae: 2.466206, mean_q: 4.841613\n",
      "[99 74 24 82 86  4 63 86 37 21]\n",
      " 23823/50001: episode: 2647, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 24.000, mean reward:  2.667 [-10.000,  8.000], mean action: 53.000 [4.000, 86.000],  loss: 10.571361, mae: 2.406336, mean_q: 4.694335\n",
      "[94 34 27 34 47 34 10 93 98 71]\n",
      " 23832/50001: episode: 2648, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward:  7.000, mean reward:  0.778 [-10.000,  5.000], mean action: 49.778 [10.000, 98.000],  loss: 8.100597, mae: 2.325059, mean_q: 4.592257\n",
      "[77 30 59 27 41 43 11 11 37 87]\n",
      " 23841/50001: episode: 2649, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 22.000, mean reward:  2.444 [-10.000,  9.000], mean action: 38.444 [11.000, 87.000],  loss: 8.191255, mae: 2.340447, mean_q: 4.732068\n",
      "[81 95 75  4 24 44 20 88 60 36]\n",
      " 23850/50001: episode: 2650, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 37.000, mean reward:  4.111 [ 2.000,  7.000], mean action: 49.556 [4.000, 95.000],  loss: 6.379311, mae: 2.315503, mean_q: 4.535159\n",
      "[33 13 39 14 78 80 13 26 72  6]\n",
      " 23859/50001: episode: 2651, duration: 0.069s, episode steps:   9, steps per second: 130, episode reward: 16.000, mean reward:  1.778 [-10.000,  6.000], mean action: 37.889 [6.000, 80.000],  loss: 7.555513, mae: 2.338729, mean_q: 4.539828\n",
      "[91 24 48 43 99 68 88 66 93 63]\n",
      " 23868/50001: episode: 2652, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 40.000, mean reward:  4.444 [ 2.000,  6.000], mean action: 65.778 [24.000, 99.000],  loss: 5.969820, mae: 2.341594, mean_q: 4.514651\n",
      "[92 31 90 30 98 42 98 31  9 28]\n",
      " 23877/50001: episode: 2653, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 13.000, mean reward:  1.444 [-10.000,  7.000], mean action: 50.778 [9.000, 98.000],  loss: 8.883278, mae: 2.328080, mean_q: 4.616995\n",
      "[52 41 59 12 85 60 95 99 13 99]\n",
      " 23886/50001: episode: 2654, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 28.000, mean reward:  3.111 [-10.000,  7.000], mean action: 62.556 [12.000, 99.000],  loss: 5.840658, mae: 2.366741, mean_q: 4.615228\n",
      "[13 53 79 61 37 88  2 74  2 28]\n",
      " 23895/50001: episode: 2655, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 47.111 [2.000, 88.000],  loss: 6.832863, mae: 2.390238, mean_q: 4.728293\n",
      "[58 13 48 58 23 94 13  6 66 34]\n",
      " 23904/50001: episode: 2656, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 12.000, mean reward:  1.333 [-10.000,  9.000], mean action: 39.444 [6.000, 94.000],  loss: 11.144499, mae: 2.412499, mean_q: 4.701956\n",
      "[33  5 84 28  1 14 31 56 59 28]\n",
      " 23913/50001: episode: 2657, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 23.000, mean reward:  2.556 [-10.000,  5.000], mean action: 34.000 [1.000, 84.000],  loss: 7.203364, mae: 2.343845, mean_q: 4.592636\n",
      "[47 14 71 12 27 59  6 11  1  4]\n",
      " 23922/50001: episode: 2658, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 36.000, mean reward:  4.000 [ 2.000,  7.000], mean action: 22.778 [1.000, 71.000],  loss: 6.443487, mae: 2.325484, mean_q: 4.512713\n",
      "[ 1 84 93 94 11 75 20 56 66 88]\n",
      " 23931/50001: episode: 2659, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 65.222 [11.000, 94.000],  loss: 6.352454, mae: 2.364316, mean_q: 4.632237\n",
      "[66 31 92 66 92 46 86  2 46  4]\n",
      " 23940/50001: episode: 2660, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: -2.000, mean reward: -0.222 [-10.000,  7.000], mean action: 51.667 [2.000, 92.000],  loss: 6.192891, mae: 2.386172, mean_q: 4.650712\n",
      "[13  6 93 13 99 93 52 14 31 27]\n",
      " 23949/50001: episode: 2661, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward:  9.000, mean reward:  1.000 [-10.000,  6.000], mean action: 47.556 [6.000, 99.000],  loss: 7.992251, mae: 2.421874, mean_q: 4.659997\n",
      "[94 31  0 51 37 41 83 27  5 59]\n",
      " 23958/50001: episode: 2662, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 38.000, mean reward:  4.222 [ 2.000,  6.000], mean action: 37.111 [0.000, 83.000],  loss: 5.900558, mae: 2.421926, mean_q: 4.741551\n",
      "[62 52 98 49 63 85 67 96 31 88]\n",
      " 23967/50001: episode: 2663, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 40.000, mean reward:  4.444 [ 3.000,  8.000], mean action: 69.889 [31.000, 98.000],  loss: 8.786029, mae: 2.409330, mean_q: 4.698740\n",
      "[12 44 34 19 94 31  7 17 97 49]\n",
      " 23976/50001: episode: 2664, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 32.000, mean reward:  3.556 [ 1.000,  7.000], mean action: 43.556 [7.000, 97.000],  loss: 7.489912, mae: 2.374335, mean_q: 4.668378\n",
      "[80 92  8 53 78 71  2 41  6 54]\n",
      " 23985/50001: episode: 2665, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 34.000, mean reward:  3.778 [ 2.000,  6.000], mean action: 45.000 [2.000, 92.000],  loss: 6.189455, mae: 2.358734, mean_q: 4.630309\n",
      "[86  9 58 60 18 19 46 54  9 97]\n",
      " 23994/50001: episode: 2666, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 25.000, mean reward:  2.778 [-10.000,  9.000], mean action: 41.111 [9.000, 97.000],  loss: 6.274442, mae: 2.402740, mean_q: 4.690387\n",
      "[24 52 69 95 48 68  8 38 30 32]\n",
      " 24003/50001: episode: 2667, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 37.000, mean reward:  4.111 [ 2.000,  8.000], mean action: 48.889 [8.000, 95.000],  loss: 6.308991, mae: 2.394989, mean_q: 4.686831\n",
      "[ 1 72 79 99 42 79 74 41 27 66]\n",
      " 24012/50001: episode: 2668, duration: 0.069s, episode steps:   9, steps per second: 130, episode reward: 26.000, mean reward:  2.889 [-10.000,  8.000], mean action: 64.333 [27.000, 99.000],  loss: 6.207343, mae: 2.396223, mean_q: 4.611718\n",
      "[91 48 89 12 79 27 40 31 31 90]\n",
      " 24021/50001: episode: 2669, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 49.667 [12.000, 90.000],  loss: 8.271018, mae: 2.380673, mean_q: 4.586620\n",
      "[41 99  4 37 41  7 89 12 31 88]\n",
      " 24030/50001: episode: 2670, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 28.000, mean reward:  3.111 [-10.000,  7.000], mean action: 45.333 [4.000, 99.000],  loss: 7.786744, mae: 2.459624, mean_q: 4.745907\n",
      "[97 24 66 42  6 95 37 93 34  2]\n",
      " 24039/50001: episode: 2671, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 43.000, mean reward:  4.778 [ 3.000,  8.000], mean action: 44.333 [2.000, 95.000],  loss: 6.926021, mae: 2.509872, mean_q: 4.961446\n",
      "[44  5 32 48 92 93 28 17 67 68]\n",
      " 24048/50001: episode: 2672, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 33.000, mean reward:  3.667 [ 2.000,  5.000], mean action: 50.000 [5.000, 93.000],  loss: 6.570011, mae: 2.548462, mean_q: 4.961844\n",
      "[90  0 56 31 83 46 49 37 70 97]\n",
      " 24057/50001: episode: 2673, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 42.000, mean reward:  4.667 [ 2.000,  9.000], mean action: 52.111 [0.000, 97.000],  loss: 6.110022, mae: 2.480566, mean_q: 4.808244\n",
      "[45 22 95 44 68 49 95  1 27 66]\n",
      " 24066/50001: episode: 2674, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 27.000, mean reward:  3.000 [-10.000,  8.000], mean action: 51.889 [1.000, 95.000],  loss: 6.658227, mae: 2.548823, mean_q: 4.896189\n",
      "[39 53 96 87 47 63  2 38 46 20]\n",
      " 24075/50001: episode: 2675, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 36.000, mean reward:  4.000 [ 2.000,  6.000], mean action: 50.222 [2.000, 96.000],  loss: 7.027342, mae: 2.524172, mean_q: 4.925800\n",
      "[15 43 80 31 27 44 29 79 15 12]\n",
      " 24084/50001: episode: 2676, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 26.000, mean reward:  2.889 [-10.000,  9.000], mean action: 40.000 [12.000, 80.000],  loss: 8.203074, mae: 2.456966, mean_q: 4.733998\n",
      "[93 59 41 43 30 49 90 42 28 14]\n",
      " 24093/50001: episode: 2677, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 43.000, mean reward:  4.778 [ 2.000,  8.000], mean action: 44.000 [14.000, 90.000],  loss: 7.771780, mae: 2.471806, mean_q: 4.789991\n",
      "[47 67 28 32  4 82 76 11 45 96]\n",
      " 24102/50001: episode: 2678, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 32.000, mean reward:  3.556 [ 2.000,  4.000], mean action: 49.000 [4.000, 96.000],  loss: 8.147176, mae: 2.385226, mean_q: 4.616885\n",
      "[94 35 78 27 94  3  3 13  5 76]\n",
      " 24111/50001: episode: 2679, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 11.000, mean reward:  1.222 [-10.000,  8.000], mean action: 37.111 [3.000, 94.000],  loss: 6.995579, mae: 2.312941, mean_q: 4.541612\n",
      "[14 95 41 98 27 32  0 65 19 38]\n",
      " 24120/50001: episode: 2680, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 32.000, mean reward:  3.556 [ 2.000,  5.000], mean action: 46.111 [0.000, 98.000],  loss: 7.314421, mae: 2.302595, mean_q: 4.542467\n",
      "[65 92 80 33 95 42 81 23 23 12]\n",
      " 24129/50001: episode: 2681, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 53.444 [12.000, 95.000],  loss: 5.289263, mae: 2.383375, mean_q: 4.631083\n",
      "[44  1 85 12 66 84 95 93 50 44]\n",
      " 24138/50001: episode: 2682, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 58.889 [1.000, 95.000],  loss: 7.482462, mae: 2.481475, mean_q: 4.806162\n",
      "[93 11 79 26 10 37 98  9 44 55]\n",
      " 24147/50001: episode: 2683, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 34.000, mean reward:  3.778 [ 2.000,  6.000], mean action: 41.000 [9.000, 98.000],  loss: 6.175729, mae: 2.507578, mean_q: 4.856241\n",
      "[19 34 38 21 14 84 51 96 93 34]\n",
      " 24156/50001: episode: 2684, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 20.000, mean reward:  2.222 [-10.000,  6.000], mean action: 51.667 [14.000, 96.000],  loss: 6.158312, mae: 2.480823, mean_q: 4.859953\n",
      "[ 8  1 57 63 13 44 79 11 32 64]\n",
      " 24165/50001: episode: 2685, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 37.000, mean reward:  4.111 [ 2.000,  6.000], mean action: 40.444 [1.000, 79.000],  loss: 5.590560, mae: 2.471633, mean_q: 4.837424\n",
      "[88 95 87  4 12 49 47 48  1 51]\n",
      " 24174/50001: episode: 2686, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 40.000, mean reward:  4.444 [ 3.000,  6.000], mean action: 43.778 [1.000, 95.000],  loss: 10.330843, mae: 2.507118, mean_q: 4.828405\n",
      "[60 16 49 80 88 25 11  7 21 93]\n",
      " 24183/50001: episode: 2687, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 34.000, mean reward:  3.778 [ 2.000,  7.000], mean action: 43.333 [7.000, 93.000],  loss: 5.426136, mae: 2.518527, mean_q: 4.901113\n",
      "[34 27  2 27  9 24  7 76 37 48]\n",
      " 24192/50001: episode: 2688, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 27.000, mean reward:  3.000 [-10.000,  8.000], mean action: 28.556 [2.000, 76.000],  loss: 7.330783, mae: 2.480668, mean_q: 4.744782\n",
      "[66  7 77 34 60 42 52  1  4 88]\n",
      " 24201/50001: episode: 2689, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 43.000, mean reward:  4.778 [ 2.000,  7.000], mean action: 40.556 [1.000, 88.000],  loss: 6.395755, mae: 2.428067, mean_q: 4.686006\n",
      "[76 69  9  6 90 33 49 90 98 48]\n",
      " 24210/50001: episode: 2690, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 26.000, mean reward:  2.889 [-10.000,  9.000], mean action: 54.667 [6.000, 98.000],  loss: 9.002317, mae: 2.454441, mean_q: 4.747911\n",
      "[ 0 13 26  4 68 24 50 34 92 29]\n",
      " 24219/50001: episode: 2691, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 37.000, mean reward:  4.111 [ 2.000,  7.000], mean action: 37.778 [4.000, 92.000],  loss: 4.673095, mae: 2.370610, mean_q: 4.710109\n",
      "[10 90 63 57 98 74 93  8  5 16]\n",
      " 24228/50001: episode: 2692, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 37.000, mean reward:  4.111 [ 3.000,  5.000], mean action: 56.000 [5.000, 98.000],  loss: 5.689044, mae: 2.351786, mean_q: 4.611697\n",
      "[47 74 28  2 63 66 86 57 23  8]\n",
      " 24237/50001: episode: 2693, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 35.000, mean reward:  3.889 [ 3.000,  5.000], mean action: 45.222 [2.000, 86.000],  loss: 5.877741, mae: 2.388479, mean_q: 4.695950\n",
      "[34 95 90 84 53  1 90 82 82 14]\n",
      " 24246/50001: episode: 2694, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward:  8.000, mean reward:  0.889 [-10.000,  6.000], mean action: 65.667 [1.000, 95.000],  loss: 7.716532, mae: 2.438098, mean_q: 4.692644\n",
      "[39 79 79 77 51 41 42 54 88 48]\n",
      " 24255/50001: episode: 2695, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 29.000, mean reward:  3.222 [-10.000,  9.000], mean action: 62.111 [41.000, 88.000],  loss: 7.123746, mae: 2.440430, mean_q: 4.686991\n",
      "[71  4 48 41 13 87 93 33 41 66]\n",
      " 24264/50001: episode: 2696, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 25.000, mean reward:  2.778 [-10.000,  9.000], mean action: 47.333 [4.000, 93.000],  loss: 6.118757, mae: 2.495343, mean_q: 4.962479\n",
      "[ 4 37 34 62 37 80 37 73 41 14]\n",
      " 24273/50001: episode: 2697, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward:  8.000, mean reward:  0.889 [-10.000,  7.000], mean action: 46.111 [14.000, 80.000],  loss: 6.060883, mae: 2.566360, mean_q: 4.885981\n",
      "[54 14 75 57 50 31 95 31 34 14]\n",
      " 24282/50001: episode: 2698, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 14.000, mean reward:  1.556 [-10.000,  7.000], mean action: 44.556 [14.000, 95.000],  loss: 6.760118, mae: 2.484530, mean_q: 4.820568\n",
      "[57 86 11 58 30 42 47 98  5 31]\n",
      " 24291/50001: episode: 2699, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 43.000, mean reward:  4.778 [ 3.000,  7.000], mean action: 45.333 [5.000, 98.000],  loss: 8.152511, mae: 2.571677, mean_q: 4.994896\n",
      "[ 4 24 28 41 74 48 23 48 64 15]\n",
      " 24300/50001: episode: 2700, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 21.000, mean reward:  2.333 [-10.000,  5.000], mean action: 40.556 [15.000, 74.000],  loss: 7.407118, mae: 2.483690, mean_q: 4.792126\n",
      "[34 14 69 66 44 76 60 97 40  2]\n",
      " 24309/50001: episode: 2701, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 52.000 [2.000, 97.000],  loss: 7.821545, mae: 2.414929, mean_q: 4.711734\n",
      "[59 84 97 42 37 88 98 52 41  2]\n",
      " 24318/50001: episode: 2702, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 40.000, mean reward:  4.444 [ 3.000,  6.000], mean action: 60.111 [2.000, 98.000],  loss: 5.183602, mae: 2.353729, mean_q: 4.630606\n",
      "[84 14  1 82 88 62 86 13 89 50]\n",
      " 24327/50001: episode: 2703, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 53.889 [1.000, 89.000],  loss: 6.645668, mae: 2.339946, mean_q: 4.584805\n",
      "[57 14 80 83 34 88 75 50  1 42]\n",
      " 24336/50001: episode: 2704, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 43.000, mean reward:  4.778 [ 2.000,  7.000], mean action: 51.889 [1.000, 88.000],  loss: 9.041399, mae: 2.418101, mean_q: 4.726431\n",
      "[69 30 63 53 63 10 79 56 31 12]\n",
      " 24345/50001: episode: 2705, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 27.000, mean reward:  3.000 [-10.000,  8.000], mean action: 44.111 [10.000, 79.000],  loss: 7.138953, mae: 2.450971, mean_q: 4.789584\n",
      "[24 14 96 89 93 46 75 66 24 26]\n",
      " 24354/50001: episode: 2706, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 58.778 [14.000, 96.000],  loss: 6.733644, mae: 2.522275, mean_q: 4.953241\n",
      "[50 26 32 40 16 86 48 99 50 88]\n",
      " 24363/50001: episode: 2707, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 21.000, mean reward:  2.333 [-10.000,  7.000], mean action: 53.889 [16.000, 99.000],  loss: 7.045729, mae: 2.464110, mean_q: 4.843266\n",
      "[84 28 37  4 34 94 95 50  9 63]\n",
      " 24372/50001: episode: 2708, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 46.000 [4.000, 95.000],  loss: 7.898130, mae: 2.414825, mean_q: 4.769807\n",
      "[92 97 88 59 95 40  1 28 60 47]\n",
      " 24381/50001: episode: 2709, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 44.000, mean reward:  4.889 [ 3.000,  7.000], mean action: 57.222 [1.000, 97.000],  loss: 7.602512, mae: 2.436028, mean_q: 4.769300\n",
      "[34 60 34 63 15 88 45 17 51 63]\n",
      " 24390/50001: episode: 2710, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward:  5.000, mean reward:  0.556 [-10.000,  6.000], mean action: 48.444 [15.000, 88.000],  loss: 8.017490, mae: 2.387482, mean_q: 4.725050\n",
      "[69 51 41 77 31 33 68 28 34  8]\n",
      " 24399/50001: episode: 2711, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 38.000, mean reward:  4.222 [ 3.000,  8.000], mean action: 41.222 [8.000, 77.000],  loss: 8.424953, mae: 2.421831, mean_q: 4.707464\n",
      "[73 51 17  2 29 90 44  3 80 11]\n",
      " 24408/50001: episode: 2712, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 33.000, mean reward:  3.667 [ 2.000,  6.000], mean action: 36.333 [2.000, 90.000],  loss: 6.011446, mae: 2.374181, mean_q: 4.662194\n",
      "[ 9 17 45 16 20 61 89 49  1 31]\n",
      " 24417/50001: episode: 2713, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 40.000, mean reward:  4.444 [ 3.000,  8.000], mean action: 36.556 [1.000, 89.000],  loss: 7.876626, mae: 2.415120, mean_q: 4.700388\n",
      "[33 23 13 79  2 50 88 95 79 24]\n",
      " 24426/50001: episode: 2714, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 50.333 [2.000, 95.000],  loss: 6.905375, mae: 2.424279, mean_q: 4.789145\n",
      "[24 46 76 99 95 97 88 99 23 27]\n",
      " 24435/50001: episode: 2715, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 28.000, mean reward:  3.111 [-10.000,  6.000], mean action: 72.222 [23.000, 99.000],  loss: 8.923389, mae: 2.405942, mean_q: 4.790537\n",
      "[94 53 32 97 92 89 24 97 99 97]\n",
      " 24444/50001: episode: 2716, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward:  8.000, mean reward:  0.889 [-10.000,  5.000], mean action: 75.556 [24.000, 99.000],  loss: 9.472389, mae: 2.400379, mean_q: 4.708430\n",
      "[76 63 58 74  2  2 88 82 10 52]\n",
      " 24453/50001: episode: 2717, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 23.000, mean reward:  2.556 [-10.000,  5.000], mean action: 47.889 [2.000, 88.000],  loss: 7.090284, mae: 2.385175, mean_q: 4.682346\n",
      "[78 27 31 93 59 60 11 11 11 83]\n",
      " 24462/50001: episode: 2718, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward:  8.000, mean reward:  0.889 [-10.000,  5.000], mean action: 42.889 [11.000, 93.000],  loss: 7.417544, mae: 2.407962, mean_q: 4.752973\n",
      "[84 60 94 94 74  4 29 12 12  4]\n",
      " 24471/50001: episode: 2719, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: -5.000, mean reward: -0.556 [-10.000,  7.000], mean action: 42.556 [4.000, 94.000],  loss: 8.307695, mae: 2.350757, mean_q: 4.697277\n",
      "[21 50 85 99 95 13 53 66 24 14]\n",
      " 24480/50001: episode: 2720, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 40.000, mean reward:  4.444 [ 3.000,  6.000], mean action: 55.444 [13.000, 99.000],  loss: 6.397696, mae: 2.322348, mean_q: 4.594132\n",
      "[79 99 51 99 53 85 91 99 27 52]\n",
      " 24489/50001: episode: 2721, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward:  6.000, mean reward:  0.667 [-10.000,  5.000], mean action: 72.889 [27.000, 99.000],  loss: 7.726179, mae: 2.344832, mean_q: 4.511066\n",
      "[83 46 40 31 53 99 78 99 93 23]\n",
      " 24498/50001: episode: 2722, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 62.444 [23.000, 99.000],  loss: 8.518279, mae: 2.431002, mean_q: 4.738914\n",
      "[94  2 32 69 27 96 50 69 93  7]\n",
      " 24507/50001: episode: 2723, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 20.000, mean reward:  2.222 [-10.000,  6.000], mean action: 49.444 [2.000, 96.000],  loss: 9.312431, mae: 2.417874, mean_q: 4.748393\n",
      "[17 34 53 85 55 71  1 46  1 10]\n",
      " 24516/50001: episode: 2724, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 39.556 [1.000, 85.000],  loss: 6.834421, mae: 2.309027, mean_q: 4.568460\n",
      "[20  1 29 85 80 46 30 40 44  1]\n",
      " 24525/50001: episode: 2725, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 21.000, mean reward:  2.333 [-10.000,  5.000], mean action: 39.556 [1.000, 85.000],  loss: 6.525761, mae: 2.359034, mean_q: 4.629414\n",
      "[85 57 66 34 34 31 68 99 23 89]\n",
      " 24534/50001: episode: 2726, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 21.000, mean reward:  2.333 [-10.000,  6.000], mean action: 55.667 [23.000, 99.000],  loss: 6.591362, mae: 2.427445, mean_q: 4.744081\n",
      "[33 82 53 74 95 74 11 72 46 37]\n",
      " 24543/50001: episode: 2727, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 27.000, mean reward:  3.000 [-10.000,  9.000], mean action: 60.444 [11.000, 95.000],  loss: 5.296995, mae: 2.397952, mean_q: 4.717779\n",
      "[63 84  6 85 79 85 76 24 95  9]\n",
      " 24552/50001: episode: 2728, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 29.000, mean reward:  3.222 [-10.000,  9.000], mean action: 60.333 [6.000, 95.000],  loss: 6.706754, mae: 2.357507, mean_q: 4.618387\n",
      "[10 33 82 42  1  2 85 99 41 75]\n",
      " 24561/50001: episode: 2729, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 51.111 [1.000, 99.000],  loss: 7.791162, mae: 2.359336, mean_q: 4.618673\n",
      "[41 82 28 79 93 14 48 86 50 50]\n",
      " 24570/50001: episode: 2730, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 58.889 [14.000, 93.000],  loss: 9.048998, mae: 2.431214, mean_q: 4.822492\n",
      "[ 1 50 46 42 33 40 45  9 32  2]\n",
      " 24579/50001: episode: 2731, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 33.222 [2.000, 50.000],  loss: 7.721052, mae: 2.360922, mean_q: 4.668460\n",
      "[47 88 52 79 13 45 74 60 34 50]\n",
      " 24588/50001: episode: 2732, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 44.000, mean reward:  4.889 [ 2.000,  8.000], mean action: 55.000 [13.000, 88.000],  loss: 6.937287, mae: 2.360640, mean_q: 4.619552\n",
      "[97  9 41 34 34 53 27 87 14 75]\n",
      " 24597/50001: episode: 2733, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 41.556 [9.000, 87.000],  loss: 8.189673, mae: 2.401193, mean_q: 4.683170\n",
      "[76 67  1 20 66 41 17  1 60 27]\n",
      " 24606/50001: episode: 2734, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 33.333 [1.000, 67.000],  loss: 7.973374, mae: 2.453322, mean_q: 4.789605\n",
      "[ 0 34 89 66 89 82 95 97 27 28]\n",
      " 24615/50001: episode: 2735, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 32.000, mean reward:  3.556 [-10.000,  7.000], mean action: 67.444 [27.000, 97.000],  loss: 8.904939, mae: 2.429643, mean_q: 4.788044\n",
      "[28 89 75 10  1 98 79 48 28 60]\n",
      " 24624/50001: episode: 2736, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 26.000, mean reward:  2.889 [-10.000,  6.000], mean action: 54.222 [1.000, 98.000],  loss: 6.269363, mae: 2.325810, mean_q: 4.503092\n",
      "[22 11 25 64 50 81 43 84 55 48]\n",
      " 24633/50001: episode: 2737, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward: 35.000, mean reward:  3.889 [ 3.000,  8.000], mean action: 51.222 [11.000, 84.000],  loss: 8.559059, mae: 2.350871, mean_q: 4.611388\n",
      "[77 95 11  1  2 41 76 13 21 72]\n",
      " 24642/50001: episode: 2738, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward: 35.000, mean reward:  3.889 [ 2.000,  7.000], mean action: 36.889 [1.000, 95.000],  loss: 6.927628, mae: 2.398471, mean_q: 4.662348\n",
      "[66  1 25 19 34 32 74 52 84 43]\n",
      " 24651/50001: episode: 2739, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 34.000, mean reward:  3.778 [ 2.000,  6.000], mean action: 40.444 [1.000, 84.000],  loss: 6.194565, mae: 2.375431, mean_q: 4.613188\n",
      "[61 95  6 37 60  1 53 82 61 50]\n",
      " 24660/50001: episode: 2740, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 30.000, mean reward:  3.333 [-10.000,  9.000], mean action: 49.444 [1.000, 95.000],  loss: 7.030631, mae: 2.371319, mean_q: 4.583876\n",
      "[57 13 56  8  2 28 27 63 79 73]\n",
      " 24669/50001: episode: 2741, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 35.000, mean reward:  3.889 [ 2.000,  6.000], mean action: 38.778 [2.000, 79.000],  loss: 7.770050, mae: 2.390001, mean_q: 4.622393\n",
      "[69 32 50 94 92 50 67 27 37  9]\n",
      " 24678/50001: episode: 2742, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 24.000, mean reward:  2.667 [-10.000,  8.000], mean action: 50.889 [9.000, 94.000],  loss: 6.846280, mae: 2.371642, mean_q: 4.656565\n",
      "[11 53 79 66 89 77 32 33  4 10]\n",
      " 24687/50001: episode: 2743, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 37.000, mean reward:  4.111 [ 3.000,  6.000], mean action: 49.222 [4.000, 89.000],  loss: 7.580538, mae: 2.346656, mean_q: 4.612098\n",
      "[54 84 88 38 31 56 96 97 27 28]\n",
      " 24696/50001: episode: 2744, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 43.000, mean reward:  4.778 [ 2.000,  8.000], mean action: 60.556 [27.000, 97.000],  loss: 6.451520, mae: 2.307121, mean_q: 4.520316\n",
      "[75 88 88 99 34 34 42 21 66 85]\n",
      " 24705/50001: episode: 2745, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward: 10.000, mean reward:  1.111 [-10.000,  7.000], mean action: 61.889 [21.000, 99.000],  loss: 5.539553, mae: 2.394919, mean_q: 4.651612\n",
      "[ 0 84 51 10 79 34 46 14 92 97]\n",
      " 24714/50001: episode: 2746, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 56.333 [10.000, 97.000],  loss: 5.307470, mae: 2.442442, mean_q: 4.707089\n",
      "[40 46 99  9  9 75 14 28 30 20]\n",
      " 24723/50001: episode: 2747, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 28.000, mean reward:  3.111 [-10.000,  6.000], mean action: 36.667 [9.000, 99.000],  loss: 9.528064, mae: 2.461108, mean_q: 4.778340\n",
      "[66 98 75  4 88 46 88 53 78 50]\n",
      " 24732/50001: episode: 2748, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 26.000, mean reward:  2.889 [-10.000,  9.000], mean action: 64.444 [4.000, 98.000],  loss: 5.424809, mae: 2.316439, mean_q: 4.497805\n",
      "[ 4 17 85 20 28 95 98 52  2  1]\n",
      " 24741/50001: episode: 2749, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 43.000, mean reward:  4.778 [ 3.000,  8.000], mean action: 44.222 [1.000, 98.000],  loss: 8.428981, mae: 2.409840, mean_q: 4.666965\n",
      "[15 78  4 71  7 35 74 17 98 50]\n",
      " 24750/50001: episode: 2750, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 40.000, mean reward:  4.444 [ 2.000, 10.000], mean action: 48.222 [4.000, 98.000],  loss: 8.528004, mae: 2.420671, mean_q: 4.718333\n",
      "[74 33 93  4 53 87 57 27 37 78]\n",
      " 24759/50001: episode: 2751, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 39.000, mean reward:  4.333 [ 3.000,  8.000], mean action: 52.111 [4.000, 93.000],  loss: 8.047189, mae: 2.345380, mean_q: 4.555110\n",
      "[38 60 11 82 84 86 95 95 98 79]\n",
      " 24768/50001: episode: 2752, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 27.000, mean reward:  3.000 [-10.000,  8.000], mean action: 76.667 [11.000, 98.000],  loss: 9.462567, mae: 2.341944, mean_q: 4.606215\n",
      "[51 31 59 28  5 94 93 56  9 19]\n",
      " 24777/50001: episode: 2753, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 36.000, mean reward:  4.000 [ 3.000,  5.000], mean action: 43.778 [5.000, 94.000],  loss: 8.083358, mae: 2.373727, mean_q: 4.662383\n",
      "[83 93 15 89 58  6 98 68 50 51]\n",
      " 24786/50001: episode: 2754, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 41.000, mean reward:  4.556 [ 2.000,  8.000], mean action: 58.667 [6.000, 98.000],  loss: 7.724648, mae: 2.343506, mean_q: 4.601675\n",
      "[ 7 28 59 66  6  8  1  1  1 13]\n",
      " 24795/50001: episode: 2755, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 12.000, mean reward:  1.333 [-10.000,  8.000], mean action: 20.333 [1.000, 66.000],  loss: 8.795532, mae: 2.370576, mean_q: 4.702202\n",
      "[22 42 92 64 44 93 10 87 82 14]\n",
      " 24804/50001: episode: 2756, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 38.000, mean reward:  4.222 [ 3.000,  7.000], mean action: 58.667 [10.000, 93.000],  loss: 7.904012, mae: 2.318851, mean_q: 4.600718\n",
      "[ 3 34 65 46 40 98 42 74 85 79]\n",
      " 24813/50001: episode: 2757, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 41.000, mean reward:  4.556 [ 2.000,  8.000], mean action: 62.556 [34.000, 98.000],  loss: 10.530964, mae: 2.326841, mean_q: 4.535036\n",
      "[75 83 90 93 88 34  1  1  2 60]\n",
      " 24822/50001: episode: 2758, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 28.000, mean reward:  3.111 [-10.000,  6.000], mean action: 50.222 [1.000, 93.000],  loss: 6.600452, mae: 2.297866, mean_q: 4.550484\n",
      "[15 89 13 79 92 42 46  1 95 79]\n",
      " 24831/50001: episode: 2759, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 30.000, mean reward:  3.333 [-10.000,  9.000], mean action: 59.556 [1.000, 95.000],  loss: 8.155950, mae: 2.297629, mean_q: 4.470665\n",
      "[55 65  0  9 63 13 14 21 40 97]\n",
      " 24840/50001: episode: 2760, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 35.778 [0.000, 97.000],  loss: 7.088968, mae: 2.350772, mean_q: 4.638631\n",
      "[71 62 51 69 58 40 48  1 70 23]\n",
      " 24849/50001: episode: 2761, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 36.000, mean reward:  4.000 [ 2.000,  6.000], mean action: 46.889 [1.000, 70.000],  loss: 7.521407, mae: 2.335370, mean_q: 4.514753\n",
      "[70 24 88 92 81 25 34 12 97 12]\n",
      " 24858/50001: episode: 2762, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 28.000, mean reward:  3.111 [-10.000,  7.000], mean action: 51.667 [12.000, 97.000],  loss: 6.068127, mae: 2.375950, mean_q: 4.679281\n",
      "[38 33 90 40 28 98 46 71 27  9]\n",
      " 24867/50001: episode: 2763, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 41.000, mean reward:  4.556 [ 3.000,  6.000], mean action: 49.111 [9.000, 98.000],  loss: 5.782286, mae: 2.369466, mean_q: 4.619802\n",
      "[31 76 73  5 94 93 35 33  5 37]\n",
      " 24876/50001: episode: 2764, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 24.000, mean reward:  2.667 [-10.000, 10.000], mean action: 50.111 [5.000, 94.000],  loss: 5.962121, mae: 2.430210, mean_q: 4.695905\n",
      "[95  1 94 88 74 27 85 14 27 28]\n",
      " 24885/50001: episode: 2765, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 22.000, mean reward:  2.444 [-10.000,  7.000], mean action: 48.667 [1.000, 94.000],  loss: 8.751644, mae: 2.406281, mean_q: 4.599826\n",
      "[93 88 13 30 95  2 65 69 31 50]\n",
      " 24894/50001: episode: 2766, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 49.222 [2.000, 95.000],  loss: 5.779078, mae: 2.396840, mean_q: 4.606211\n",
      "[19 46  0 13 30  8 62 67 24 52]\n",
      " 24903/50001: episode: 2767, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward: 38.000, mean reward:  4.222 [ 2.000,  6.000], mean action: 33.556 [0.000, 67.000],  loss: 6.858380, mae: 2.415250, mean_q: 4.648428\n",
      "[14 41 91 95  2 34 88 74 86 73]\n",
      " 24912/50001: episode: 2768, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 64.889 [2.000, 95.000],  loss: 10.194118, mae: 2.381181, mean_q: 4.641675\n",
      "[69 86 23 34 26 44 34 71 76 75]\n",
      " 24921/50001: episode: 2769, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 21.000, mean reward:  2.333 [-10.000,  8.000], mean action: 52.111 [23.000, 86.000],  loss: 9.332379, mae: 2.365289, mean_q: 4.716039\n",
      "[60 72 28 48 34 92 67 52 21  5]\n",
      " 24930/50001: episode: 2770, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 35.000, mean reward:  3.889 [ 2.000,  6.000], mean action: 46.556 [5.000, 92.000],  loss: 5.609800, mae: 2.343705, mean_q: 4.631900\n",
      "[72 68 16 63 97 95 52 52 88 50]\n",
      " 24939/50001: episode: 2771, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 31.000, mean reward:  3.444 [-10.000,  9.000], mean action: 64.556 [16.000, 97.000],  loss: 6.762259, mae: 2.367281, mean_q: 4.679205\n",
      "[38 12 85 66 70 46 95 14 30 32]\n",
      " 24948/50001: episode: 2772, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 44.000, mean reward:  4.889 [ 3.000,  8.000], mean action: 50.000 [12.000, 95.000],  loss: 9.002437, mae: 2.384571, mean_q: 4.708454\n",
      "[58 40 52 12 83 73  2 24 23 55]\n",
      " 24957/50001: episode: 2773, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 33.000, mean reward:  3.667 [ 2.000,  5.000], mean action: 40.444 [2.000, 83.000],  loss: 6.874177, mae: 2.329303, mean_q: 4.590300\n",
      "[75 83 28 59 97 31 90 44 14 57]\n",
      " 24966/50001: episode: 2774, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 55.889 [14.000, 97.000],  loss: 6.378341, mae: 2.344229, mean_q: 4.531101\n",
      "[98 82 99 69  2  1 48  4 77  6]\n",
      " 24975/50001: episode: 2775, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 35.000, mean reward:  3.889 [ 3.000,  5.000], mean action: 43.111 [1.000, 99.000],  loss: 6.924717, mae: 2.327011, mean_q: 4.596730\n",
      "[65 87 41 40 98 32 45  2 62 97]\n",
      " 24984/50001: episode: 2776, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 56.000 [2.000, 98.000],  loss: 7.197671, mae: 2.436759, mean_q: 4.745388\n",
      "[40 84 64 37 34 13 74 63 34 95]\n",
      " 24993/50001: episode: 2777, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 29.000, mean reward:  3.222 [-10.000,  9.000], mean action: 55.333 [13.000, 95.000],  loss: 6.596595, mae: 2.500168, mean_q: 4.812086\n",
      "[47  2 11 72 34 88 66  4 24 10]\n",
      " 25002/50001: episode: 2778, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 39.000, mean reward:  4.333 [ 2.000,  6.000], mean action: 34.556 [2.000, 88.000],  loss: 7.217027, mae: 2.509605, mean_q: 4.875652\n",
      "[10 91 39 42 75 67 13 23 14 95]\n",
      " 25011/50001: episode: 2779, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 44.000, mean reward:  4.889 [ 2.000, 10.000], mean action: 51.000 [13.000, 95.000],  loss: 7.338186, mae: 2.502525, mean_q: 4.890154\n",
      "[38 41 90 31 23 49 81 86 85 45]\n",
      " 25020/50001: episode: 2780, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 35.000, mean reward:  3.889 [ 3.000,  5.000], mean action: 59.000 [23.000, 90.000],  loss: 7.980746, mae: 2.501451, mean_q: 4.821682\n",
      "[19 97 90  4 37 32 75 90 85 62]\n",
      " 25029/50001: episode: 2781, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 22.000, mean reward:  2.444 [-10.000,  5.000], mean action: 63.556 [4.000, 97.000],  loss: 5.951964, mae: 2.421053, mean_q: 4.735479\n",
      "[31 87 47 90 37 37 88  4 66 56]\n",
      " 25038/50001: episode: 2782, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 24.000, mean reward:  2.667 [-10.000,  5.000], mean action: 56.889 [4.000, 90.000],  loss: 6.693857, mae: 2.441258, mean_q: 4.828341\n",
      "[26 49 24 30 30 63 98  4 97 23]\n",
      " 25047/50001: episode: 2783, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 30.000, mean reward:  3.333 [-10.000,  7.000], mean action: 46.444 [4.000, 98.000],  loss: 5.032564, mae: 2.460313, mean_q: 4.828310\n",
      "[24 68 90 48 19 95 95 88  4 42]\n",
      " 25056/50001: episode: 2784, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 61.000 [4.000, 95.000],  loss: 7.165485, mae: 2.464146, mean_q: 4.809461\n",
      "[90 50 90 40 74 95 34 95 37 86]\n",
      " 25065/50001: episode: 2785, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 10.000, mean reward:  1.111 [-10.000,  6.000], mean action: 66.778 [34.000, 95.000],  loss: 7.458041, mae: 2.449838, mean_q: 4.742315\n",
      "[95 14 98 66  1 92 74 21 19 14]\n",
      " 25074/50001: episode: 2786, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 18.000, mean reward:  2.000 [-10.000,  4.000], mean action: 44.333 [1.000, 98.000],  loss: 7.513569, mae: 2.385015, mean_q: 4.666648\n",
      "[46 95 41 46 60  1 32 41  5 12]\n",
      " 25083/50001: episode: 2787, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 16.000, mean reward:  1.778 [-10.000,  8.000], mean action: 37.000 [1.000, 95.000],  loss: 7.476848, mae: 2.368869, mean_q: 4.603907\n",
      "[68 92 65 49 89 14 60 60 49 50]\n",
      " 25092/50001: episode: 2788, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 13.000, mean reward:  1.444 [-10.000, 10.000], mean action: 58.667 [14.000, 92.000],  loss: 8.092604, mae: 2.399714, mean_q: 4.714897\n",
      "[79 83 48 55 98 68 78  2 28 98]\n",
      " 25101/50001: episode: 2789, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 62.000 [2.000, 98.000],  loss: 8.303342, mae: 2.392502, mean_q: 4.671327\n",
      "[76 23 37 97 87 88 13 45 93 34]\n",
      " 25110/50001: episode: 2790, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 42.000, mean reward:  4.667 [ 2.000,  9.000], mean action: 57.444 [13.000, 97.000],  loss: 7.271044, mae: 2.445202, mean_q: 4.764609\n",
      "[74 82 76 76 34 75 35 60 40 28]\n",
      " 25119/50001: episode: 2791, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 28.000, mean reward:  3.111 [-10.000,  7.000], mean action: 56.222 [28.000, 82.000],  loss: 4.990290, mae: 2.415336, mean_q: 4.770143\n",
      "[23 79  4 31 89 21 46 23 57 67]\n",
      " 25128/50001: episode: 2792, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 46.333 [4.000, 89.000],  loss: 10.238737, mae: 2.497989, mean_q: 4.862512\n",
      "[63 24  6 48 29 68 91 81 54 50]\n",
      " 25137/50001: episode: 2793, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward: 38.000, mean reward:  4.222 [ 2.000, 10.000], mean action: 50.111 [6.000, 91.000],  loss: 7.032959, mae: 2.422624, mean_q: 4.736912\n",
      "[ 5 16  9 93 13 62 53 78 37 55]\n",
      " 25146/50001: episode: 2794, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 36.000, mean reward:  4.000 [ 3.000,  8.000], mean action: 46.222 [9.000, 93.000],  loss: 5.998267, mae: 2.385131, mean_q: 4.665161\n",
      "[21 98 19 79  4 46 98 34 41 14]\n",
      " 25155/50001: episode: 2795, duration: 0.070s, episode steps:   9, steps per second: 128, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 48.111 [4.000, 98.000],  loss: 7.993488, mae: 2.412738, mean_q: 4.738131\n",
      "[17 89 59 90 14 80 74  4 33 62]\n",
      " 25164/50001: episode: 2796, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 37.000, mean reward:  4.111 [ 3.000,  6.000], mean action: 56.111 [4.000, 90.000],  loss: 4.933695, mae: 2.361083, mean_q: 4.638596\n",
      "[13 95 43 79 46 41 75 48 34 48]\n",
      " 25173/50001: episode: 2797, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 27.000, mean reward:  3.000 [-10.000,  8.000], mean action: 56.556 [34.000, 95.000],  loss: 8.344194, mae: 2.403787, mean_q: 4.711286\n",
      "[29 24  4 32 32  1 96 57 14 55]\n",
      " 25182/50001: episode: 2798, duration: 0.069s, episode steps:   9, steps per second: 130, episode reward: 22.000, mean reward:  2.444 [-10.000,  5.000], mean action: 35.000 [1.000, 96.000],  loss: 6.506736, mae: 2.399744, mean_q: 4.726500\n",
      "[ 3 52 35 66 56  4 52 41 98 75]\n",
      " 25191/50001: episode: 2799, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 53.222 [4.000, 98.000],  loss: 6.663062, mae: 2.415011, mean_q: 4.665345\n",
      "[49  5 32  2 82  5 49  9 60 13]\n",
      " 25200/50001: episode: 2800, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 15.000, mean reward:  1.667 [-10.000,  9.000], mean action: 28.556 [2.000, 82.000],  loss: 6.696146, mae: 2.392556, mean_q: 4.693678\n",
      "[33 97 71 43 28 74 10 76 82 92]\n",
      " 25209/50001: episode: 2801, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 35.000, mean reward:  3.889 [ 2.000,  5.000], mean action: 63.667 [10.000, 97.000],  loss: 7.731900, mae: 2.424371, mean_q: 4.737786\n",
      "[31 27 37 19 56 89 31 87 46 79]\n",
      " 25218/50001: episode: 2802, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 52.333 [19.000, 89.000],  loss: 7.406949, mae: 2.408748, mean_q: 4.750937\n",
      "[ 4 48 47 82 98 13 48 54 48 75]\n",
      " 25227/50001: episode: 2803, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward:  8.000, mean reward:  0.889 [-10.000,  6.000], mean action: 57.000 [13.000, 98.000],  loss: 7.098852, mae: 2.346939, mean_q: 4.610807\n",
      "[25 88 53 34 95 38 18 44 90 50]\n",
      " 25236/50001: episode: 2804, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 41.000, mean reward:  4.556 [ 2.000,  9.000], mean action: 56.667 [18.000, 95.000],  loss: 7.269384, mae: 2.346732, mean_q: 4.608041\n",
      "[32 59 25 34  8 87 74 14 86 50]\n",
      " 25245/50001: episode: 2805, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 39.000, mean reward:  4.333 [ 3.000,  9.000], mean action: 48.556 [8.000, 87.000],  loss: 5.198314, mae: 2.418657, mean_q: 4.775429\n",
      "[93 48 97 48 66  4 37 31 82  4]\n",
      " 25254/50001: episode: 2806, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 10.000, mean reward:  1.111 [-10.000,  6.000], mean action: 46.333 [4.000, 97.000],  loss: 8.526533, mae: 2.373162, mean_q: 4.664640\n",
      "[42 12 48 95 50 44 63 89  2 48]\n",
      " 25263/50001: episode: 2807, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 50.111 [2.000, 95.000],  loss: 6.962969, mae: 2.376272, mean_q: 4.656908\n",
      "[ 7  5 20 53 32 82 95 50 12 98]\n",
      " 25272/50001: episode: 2808, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 47.000, mean reward:  5.222 [ 3.000,  8.000], mean action: 49.667 [5.000, 98.000],  loss: 9.011802, mae: 2.404201, mean_q: 4.763476\n",
      "[40 63 92 32 89 90 96 37  2 66]\n",
      " 25281/50001: episode: 2809, duration: 0.069s, episode steps:   9, steps per second: 130, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 63.000 [2.000, 96.000],  loss: 7.697764, mae: 2.281355, mean_q: 4.426887\n",
      "[76 51 13 69 88 50 13 74 41 46]\n",
      " 25290/50001: episode: 2810, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 49.444 [13.000, 88.000],  loss: 8.010332, mae: 2.325535, mean_q: 4.481985\n",
      "[84 95  1 75 82 95 88 46 27 44]\n",
      " 25299/50001: episode: 2811, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 27.000, mean reward:  3.000 [-10.000,  6.000], mean action: 61.444 [1.000, 95.000],  loss: 8.576339, mae: 2.337930, mean_q: 4.586458\n",
      "[47 37  9 50 88 74 44 95 51 31]\n",
      " 25308/50001: episode: 2812, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 38.000, mean reward:  4.222 [ 2.000,  8.000], mean action: 53.222 [9.000, 95.000],  loss: 9.615014, mae: 2.363796, mean_q: 4.596198\n",
      "[85  2 96  9  1  1 63 81 20 92]\n",
      " 25317/50001: episode: 2813, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 22.000, mean reward:  2.444 [-10.000,  5.000], mean action: 40.556 [1.000, 96.000],  loss: 5.426977, mae: 2.287978, mean_q: 4.585407\n",
      "[69 27 48  4 23 85 94 27 97 23]\n",
      " 25326/50001: episode: 2814, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward:  6.000, mean reward:  0.667 [-10.000,  7.000], mean action: 47.556 [4.000, 97.000],  loss: 8.809691, mae: 2.395712, mean_q: 4.676797\n",
      "[29 89 79 55 98 14 95  4 10 92]\n",
      " 25335/50001: episode: 2815, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 59.556 [4.000, 98.000],  loss: 7.006256, mae: 2.325448, mean_q: 4.527656\n",
      "[93 64 83 10 57 93 95 99 34 38]\n",
      " 25344/50001: episode: 2816, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 24.000, mean reward:  2.667 [-10.000,  8.000], mean action: 63.667 [10.000, 99.000],  loss: 7.087049, mae: 2.350833, mean_q: 4.654146\n",
      "[90  7 98 93 34 46 24 64 27 88]\n",
      " 25353/50001: episode: 2817, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 45.000, mean reward:  5.000 [ 2.000,  9.000], mean action: 53.444 [7.000, 98.000],  loss: 9.531955, mae: 2.365366, mean_q: 4.662601\n",
      "[61 60 37 18 30 90 34 79 94 78]\n",
      " 25362/50001: episode: 2818, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 57.778 [18.000, 94.000],  loss: 7.908410, mae: 2.376450, mean_q: 4.625916\n",
      "[ 2 75 12 77 41 86 73 91 48 14]\n",
      " 25371/50001: episode: 2819, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 38.000, mean reward:  4.222 [ 2.000,  8.000], mean action: 57.444 [12.000, 91.000],  loss: 6.815116, mae: 2.431717, mean_q: 4.710023\n",
      "[93 93 26 68 90 90 95 95 98 50]\n",
      " 25380/50001: episode: 2820, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: -1.000, mean reward: -0.111 [-10.000,  8.000], mean action: 78.333 [26.000, 98.000],  loss: 8.113486, mae: 2.361045, mean_q: 4.637078\n",
      "[59 14 54 34 34 94 13 68 11 82]\n",
      " 25389/50001: episode: 2821, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 21.000, mean reward:  2.333 [-10.000,  6.000], mean action: 44.889 [11.000, 94.000],  loss: 5.549950, mae: 2.394939, mean_q: 4.625133\n",
      "[33 67 35 53 32 50 64  4 68 48]\n",
      " 25398/50001: episode: 2822, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 46.778 [4.000, 68.000],  loss: 8.132077, mae: 2.443265, mean_q: 4.752801\n",
      "[33 95 87 23 43 98  8 74 95 52]\n",
      " 25407/50001: episode: 2823, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 21.000, mean reward:  2.333 [-10.000,  5.000], mean action: 63.889 [8.000, 98.000],  loss: 5.136488, mae: 2.425760, mean_q: 4.763189\n",
      "[59 10 20 60 37 34 74 34  9 41]\n",
      " 25416/50001: episode: 2824, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 35.444 [9.000, 74.000],  loss: 8.463982, mae: 2.394588, mean_q: 4.644025\n",
      "[ 8 74 24 68  4 55 69 77  9 21]\n",
      " 25425/50001: episode: 2825, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 34.000, mean reward:  3.778 [ 3.000,  5.000], mean action: 44.556 [4.000, 77.000],  loss: 8.241892, mae: 2.437612, mean_q: 4.822113\n",
      "[ 5 37  2 58 76 62 34 95 31 48]\n",
      " 25434/50001: episode: 2826, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 45.000, mean reward:  5.000 [ 3.000,  8.000], mean action: 49.222 [2.000, 95.000],  loss: 7.873856, mae: 2.376078, mean_q: 4.637018\n",
      "[84 86 89  4  4 34 47 79 57 79]\n",
      " 25443/50001: episode: 2827, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 12.000, mean reward:  1.333 [-10.000,  6.000], mean action: 53.222 [4.000, 89.000],  loss: 10.056048, mae: 2.346615, mean_q: 4.626353\n",
      "[45 28 16 87 31 51 91 24 28 23]\n",
      " 25452/50001: episode: 2828, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 42.111 [16.000, 91.000],  loss: 7.890556, mae: 2.340277, mean_q: 4.580815\n",
      "[39 67 64 54 66 41 57 26 62  4]\n",
      " 25461/50001: episode: 2829, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 39.000, mean reward:  4.333 [ 2.000,  8.000], mean action: 49.000 [4.000, 67.000],  loss: 7.747123, mae: 2.314767, mean_q: 4.571554\n",
      "[43 89 79 46 41 77 47 99 28 66]\n",
      " 25470/50001: episode: 2830, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 45.000, mean reward:  5.000 [ 3.000,  8.000], mean action: 63.556 [28.000, 99.000],  loss: 7.256841, mae: 2.286758, mean_q: 4.447003\n",
      "[ 2 93 60 15 63 95 20 89 98 50]\n",
      " 25479/50001: episode: 2831, duration: 0.069s, episode steps:   9, steps per second: 130, episode reward: 42.000, mean reward:  4.667 [ 2.000,  9.000], mean action: 64.778 [15.000, 98.000],  loss: 7.340911, mae: 2.329750, mean_q: 4.586342\n",
      "[44 89  9 82  2 79 16 14 74 96]\n",
      " 25488/50001: episode: 2832, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 38.000, mean reward:  4.222 [ 3.000,  5.000], mean action: 51.222 [2.000, 96.000],  loss: 7.154847, mae: 2.344067, mean_q: 4.582707\n",
      "[34 37 63 25 98  9 34 31 50 90]\n",
      " 25497/50001: episode: 2833, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 48.556 [9.000, 98.000],  loss: 7.902937, mae: 2.375607, mean_q: 4.648423\n",
      "[66 11 75 72 75 57 95  9 12 97]\n",
      " 25506/50001: episode: 2834, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 29.000, mean reward:  3.222 [-10.000,  8.000], mean action: 55.889 [9.000, 97.000],  loss: 7.657280, mae: 2.344257, mean_q: 4.547035\n",
      "[58 31 46 82 71 42 88 62 48 64]\n",
      " 25515/50001: episode: 2835, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 59.333 [31.000, 88.000],  loss: 5.407706, mae: 2.380412, mean_q: 4.722789\n",
      "[18 96 90 69 88 90 20 34 53 79]\n",
      " 25524/50001: episode: 2836, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 27.000, mean reward:  3.000 [-10.000,  8.000], mean action: 68.778 [20.000, 96.000],  loss: 8.317917, mae: 2.358773, mean_q: 4.584870\n",
      "[10 94 97 24 93 50 96 13 34 34]\n",
      " 25533/50001: episode: 2837, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 28.000, mean reward:  3.111 [-10.000,  8.000], mean action: 59.444 [13.000, 97.000],  loss: 8.055973, mae: 2.353136, mean_q: 4.577575\n",
      "[10 13 12 41 73 34 26 46 17 31]\n",
      " 25542/50001: episode: 2838, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 36.000, mean reward:  4.000 [ 2.000,  7.000], mean action: 32.556 [12.000, 73.000],  loss: 8.407712, mae: 2.381370, mean_q: 4.595209\n",
      "[30 13 53 34 52 88 51 95 59  8]\n",
      " 25551/50001: episode: 2839, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward: 37.000, mean reward:  4.111 [ 3.000,  7.000], mean action: 50.333 [8.000, 95.000],  loss: 6.641686, mae: 2.392105, mean_q: 4.667707\n",
      "[72 61 80 16 63 85 45 63 46 36]\n",
      " 25560/50001: episode: 2840, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 17.000, mean reward:  1.889 [-10.000,  7.000], mean action: 55.000 [16.000, 85.000],  loss: 9.713468, mae: 2.381564, mean_q: 4.611812\n",
      "[32 38 59 49 94 48 88 23 88 75]\n",
      " 25569/50001: episode: 2841, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 62.444 [23.000, 94.000],  loss: 6.865658, mae: 2.365680, mean_q: 4.626486\n",
      "[10 34 24 91 45 50 74 14 61 28]\n",
      " 25578/50001: episode: 2842, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 46.778 [14.000, 91.000],  loss: 7.265423, mae: 2.329079, mean_q: 4.544022\n",
      "[63  4 77 93 81 37 74 19 31  2]\n",
      " 25587/50001: episode: 2843, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 46.444 [2.000, 93.000],  loss: 7.800157, mae: 2.384624, mean_q: 4.672975\n",
      "[69  0 37 35 34 56 33 67 93 12]\n",
      " 25596/50001: episode: 2844, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 42.000, mean reward:  4.667 [ 2.000,  9.000], mean action: 40.778 [0.000, 93.000],  loss: 6.366117, mae: 2.371869, mean_q: 4.538139\n",
      "[48 34 53 15 15 99 74 95  9 84]\n",
      " 25605/50001: episode: 2845, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 26.000, mean reward:  2.889 [-10.000,  8.000], mean action: 53.111 [9.000, 99.000],  loss: 8.201229, mae: 2.324197, mean_q: 4.602482\n",
      "[94 23 31  8 67 57 20 37 28 27]\n",
      " 25614/50001: episode: 2846, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 33.111 [8.000, 67.000],  loss: 7.361788, mae: 2.328244, mean_q: 4.622706\n",
      "[50 95 11 40 98 44 60 57 66 71]\n",
      " 25623/50001: episode: 2847, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 36.000, mean reward:  4.000 [ 2.000,  7.000], mean action: 60.222 [11.000, 98.000],  loss: 6.704035, mae: 2.344445, mean_q: 4.500479\n",
      "[70 31 34  1 48  3 50 67 73 53]\n",
      " 25632/50001: episode: 2848, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 37.000, mean reward:  4.111 [ 2.000,  6.000], mean action: 40.000 [1.000, 73.000],  loss: 7.712492, mae: 2.362827, mean_q: 4.602786\n",
      "[20 46 21 31 17 10 30  2 93 97]\n",
      " 25641/50001: episode: 2849, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 38.556 [2.000, 97.000],  loss: 5.244068, mae: 2.343669, mean_q: 4.558574\n",
      "[36 69 60 37  2 80 13 50 62  8]\n",
      " 25650/50001: episode: 2850, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 37.000, mean reward:  4.111 [ 3.000,  6.000], mean action: 42.333 [2.000, 80.000],  loss: 7.572586, mae: 2.426975, mean_q: 4.744185\n",
      "[92 67  2 46 92 38 99 42 24 50]\n",
      " 25659/50001: episode: 2851, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward: 28.000, mean reward:  3.111 [-10.000,  9.000], mean action: 51.111 [2.000, 99.000],  loss: 5.879487, mae: 2.459207, mean_q: 4.790252\n",
      "[97 51 48 34  1 64 10 50 97 50]\n",
      " 25668/50001: episode: 2852, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward:  7.000, mean reward:  0.778 [-10.000,  6.000], mean action: 45.000 [1.000, 97.000],  loss: 7.274545, mae: 2.533200, mean_q: 4.939613\n",
      "[67 46  1 93 37  6 99 37 50 12]\n",
      " 25677/50001: episode: 2853, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 30.000, mean reward:  3.333 [-10.000,  8.000], mean action: 42.333 [1.000, 99.000],  loss: 9.217013, mae: 2.454898, mean_q: 4.752498\n",
      "[40 79  2 97  2  1 74 44 31 41]\n",
      " 25686/50001: episode: 2854, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 41.222 [1.000, 97.000],  loss: 7.609110, mae: 2.402638, mean_q: 4.719220\n",
      "[78 84 80 66 88 42  4  1 75 74]\n",
      " 25695/50001: episode: 2855, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 42.000, mean reward:  4.667 [ 3.000,  6.000], mean action: 57.111 [1.000, 88.000],  loss: 6.687338, mae: 2.382649, mean_q: 4.685999\n",
      "[17 13 32 29 73 71  1 13 21 88]\n",
      " 25704/50001: episode: 2856, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 22.000, mean reward:  2.444 [-10.000,  8.000], mean action: 37.889 [1.000, 88.000],  loss: 7.956167, mae: 2.395588, mean_q: 4.728549\n",
      "[74 22 41 12 88 24 37 41 66 46]\n",
      " 25713/50001: episode: 2857, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 31.000, mean reward:  3.444 [-10.000,  7.000], mean action: 41.889 [12.000, 88.000],  loss: 8.351871, mae: 2.339334, mean_q: 4.614449\n",
      "[ 5 96 13 34 90 42 24 61 60 50]\n",
      " 25722/50001: episode: 2858, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 44.000, mean reward:  4.889 [ 3.000,  9.000], mean action: 52.222 [13.000, 96.000],  loss: 6.078182, mae: 2.325735, mean_q: 4.521962\n",
      "[ 6 17 69 93 12 10 59 42 79 24]\n",
      " 25731/50001: episode: 2859, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 45.000 [10.000, 93.000],  loss: 5.027034, mae: 2.409310, mean_q: 4.732834\n",
      "[ 4 82 90 49 13 78 50 21 79 48]\n",
      " 25740/50001: episode: 2860, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 56.667 [13.000, 90.000],  loss: 7.246170, mae: 2.490415, mean_q: 4.822584\n",
      "[32 28 26  8 27 42 88 14 21 93]\n",
      " 25749/50001: episode: 2861, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 37.000, mean reward:  4.111 [ 2.000,  6.000], mean action: 38.556 [8.000, 93.000],  loss: 7.374563, mae: 2.547969, mean_q: 4.898221\n",
      "[42 51 98 53 56  1 34 68 88 99]\n",
      " 25758/50001: episode: 2862, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 60.889 [1.000, 99.000],  loss: 7.313249, mae: 2.531609, mean_q: 4.871889\n",
      "[ 8 95 31 92 74  2 69 40 18 88]\n",
      " 25767/50001: episode: 2863, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 38.000, mean reward:  4.222 [ 3.000,  8.000], mean action: 56.556 [2.000, 95.000],  loss: 6.819807, mae: 2.522171, mean_q: 4.879189\n",
      "[67 41 98 46  1 59 79 42 97 88]\n",
      " 25776/50001: episode: 2864, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 44.000, mean reward:  4.889 [ 4.000,  7.000], mean action: 61.222 [1.000, 98.000],  loss: 6.308694, mae: 2.454026, mean_q: 4.700911\n",
      "[74 59  1 41 12  1 66 98 89 69]\n",
      " 25785/50001: episode: 2865, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 24.000, mean reward:  2.667 [-10.000,  5.000], mean action: 48.444 [1.000, 98.000],  loss: 7.800120, mae: 2.440842, mean_q: 4.688854\n",
      "[78 97 78 98 37 62 81 95  9 37]\n",
      " 25794/50001: episode: 2866, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 14.000, mean reward:  1.556 [-10.000,  8.000], mean action: 66.000 [9.000, 98.000],  loss: 9.626275, mae: 2.457489, mean_q: 4.809682\n",
      "[33 50 72 86 24 67 50 34 32  9]\n",
      " 25803/50001: episode: 2867, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 47.111 [9.000, 86.000],  loss: 7.005832, mae: 2.363339, mean_q: 4.669371\n",
      "[62 31 37 53 97 34 13 17 79 15]\n",
      " 25812/50001: episode: 2868, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 36.000, mean reward:  4.000 [ 2.000,  6.000], mean action: 41.778 [13.000, 97.000],  loss: 9.457031, mae: 2.321893, mean_q: 4.451169\n",
      "[37 95 82 84 50 20 48 86 85 48]\n",
      " 25821/50001: episode: 2869, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 20.000, mean reward:  2.222 [-10.000,  5.000], mean action: 66.444 [20.000, 95.000],  loss: 7.763649, mae: 2.341149, mean_q: 4.508820\n",
      "[25 13 35 52 50 21  7 11  1 64]\n",
      " 25830/50001: episode: 2870, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 37.000, mean reward:  4.111 [ 2.000,  7.000], mean action: 28.222 [1.000, 64.000],  loss: 6.652087, mae: 2.390367, mean_q: 4.596493\n",
      "[37 74 47 87 72 50 62 42 69 33]\n",
      " 25839/50001: episode: 2871, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 35.000, mean reward:  3.889 [ 2.000,  6.000], mean action: 59.556 [33.000, 87.000],  loss: 10.387950, mae: 2.371286, mean_q: 4.641835\n",
      "[41 78 40 68 12 76 10 21 73 51]\n",
      " 25848/50001: episode: 2872, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 36.000, mean reward:  4.000 [ 3.000,  7.000], mean action: 47.667 [10.000, 78.000],  loss: 6.920352, mae: 2.243457, mean_q: 4.324574\n",
      "[58 30 25 77 20 74 96 51 37 27]\n",
      " 25857/50001: episode: 2873, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 40.000, mean reward:  4.444 [ 3.000,  8.000], mean action: 48.556 [20.000, 96.000],  loss: 8.976036, mae: 2.298965, mean_q: 4.538237\n",
      "[64 30 44 63 31 42 74  5 93 75]\n",
      " 25866/50001: episode: 2874, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 43.000, mean reward:  4.778 [ 3.000,  8.000], mean action: 50.778 [5.000, 93.000],  loss: 7.536568, mae: 2.292045, mean_q: 4.452978\n",
      "[84  8 94 98 41 46 46 34 12 12]\n",
      " 25875/50001: episode: 2875, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 15.000, mean reward:  1.667 [-10.000,  8.000], mean action: 43.444 [8.000, 98.000],  loss: 6.371359, mae: 2.300090, mean_q: 4.503336\n",
      "[84 95  2 61 10 47 61 37 32 79]\n",
      " 25884/50001: episode: 2876, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 29.000, mean reward:  3.222 [-10.000,  7.000], mean action: 47.111 [2.000, 95.000],  loss: 8.231384, mae: 2.290287, mean_q: 4.483783\n",
      "[69 97 53 42 25 74 74 37 21 87]\n",
      " 25893/50001: episode: 2877, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 21.000, mean reward:  2.333 [-10.000,  7.000], mean action: 56.667 [21.000, 97.000],  loss: 5.386461, mae: 2.277805, mean_q: 4.384693\n",
      "[14 37 86 76 94 80 73  9 88 46]\n",
      " 25902/50001: episode: 2878, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 39.000, mean reward:  4.333 [ 3.000,  8.000], mean action: 65.444 [9.000, 94.000],  loss: 6.815543, mae: 2.380888, mean_q: 4.601223\n",
      "[26 68 94 66 95 60 78 93 28 39]\n",
      " 25911/50001: episode: 2879, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 69.000 [28.000, 95.000],  loss: 6.994852, mae: 2.445617, mean_q: 4.812968\n",
      "[72 34 41 95 97 52 45 89 14 23]\n",
      " 25920/50001: episode: 2880, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 41.000, mean reward:  4.556 [ 2.000,  6.000], mean action: 54.444 [14.000, 97.000],  loss: 8.839956, mae: 2.365817, mean_q: 4.573308\n",
      "[98  5 87 87 84 37 76 36 44 81]\n",
      " 25929/50001: episode: 2881, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 19.000, mean reward:  2.111 [-10.000,  6.000], mean action: 59.667 [5.000, 87.000],  loss: 8.276093, mae: 2.423280, mean_q: 4.738137\n",
      "[61  5 83 88 66 93 13 95 88 97]\n",
      " 25938/50001: episode: 2882, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 29.000, mean reward:  3.222 [-10.000,  8.000], mean action: 69.778 [5.000, 97.000],  loss: 6.749075, mae: 2.377720, mean_q: 4.607481\n",
      "[60 41 87 12 13 14 54 65 88 88]\n",
      " 25947/50001: episode: 2883, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 51.333 [12.000, 88.000],  loss: 7.680648, mae: 2.400842, mean_q: 4.638489\n",
      "[81 28 28 60 34 46 86 37 37 88]\n",
      " 25956/50001: episode: 2884, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 14.000, mean reward:  1.556 [-10.000,  7.000], mean action: 49.333 [28.000, 88.000],  loss: 8.264347, mae: 2.365561, mean_q: 4.658140\n",
      "[95 55 37 68 46 13 62  1  2  2]\n",
      " 25965/50001: episode: 2885, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 31.778 [1.000, 68.000],  loss: 7.359073, mae: 2.379376, mean_q: 4.623143\n",
      "[49  8 50 77 88 37 94 28 31 81]\n",
      " 25974/50001: episode: 2886, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 40.000, mean reward:  4.444 [ 3.000,  6.000], mean action: 54.889 [8.000, 94.000],  loss: 6.796135, mae: 2.359557, mean_q: 4.563901\n",
      "[81  5  8 88 48 88 80 51 89 12]\n",
      " 25983/50001: episode: 2887, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 25.000, mean reward:  2.778 [-10.000,  9.000], mean action: 52.111 [5.000, 89.000],  loss: 6.029316, mae: 2.414108, mean_q: 4.700698\n",
      "[53 13 52 46  9 67 50 44  9 42]\n",
      " 25992/50001: episode: 2888, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 36.889 [9.000, 67.000],  loss: 7.872812, mae: 2.375664, mean_q: 4.683093\n",
      "[82 60 12 64 89 37  4 34 34 54]\n",
      " 26001/50001: episode: 2889, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 43.111 [4.000, 89.000],  loss: 5.985497, mae: 2.376416, mean_q: 4.646626\n",
      "[11 95  3  3 51 87 32 13 34 13]\n",
      " 26010/50001: episode: 2890, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 14.000, mean reward:  1.556 [-10.000,  8.000], mean action: 36.778 [3.000, 95.000],  loss: 8.314711, mae: 2.358663, mean_q: 4.563278\n",
      "[44 67 28 66 67 90 37 80 94 12]\n",
      " 26019/50001: episode: 2891, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 28.000, mean reward:  3.111 [-10.000,  9.000], mean action: 60.111 [12.000, 94.000],  loss: 4.836305, mae: 2.442827, mean_q: 4.665358\n",
      "[54  1 95  4 28 36 15 69 24 18]\n",
      " 26028/50001: episode: 2892, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 35.000, mean reward:  3.889 [ 2.000,  6.000], mean action: 32.222 [1.000, 95.000],  loss: 6.588924, mae: 2.401457, mean_q: 4.743743\n",
      "[23 41 66 54 89 95 27 83 12 66]\n",
      " 26037/50001: episode: 2893, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 27.000, mean reward:  3.000 [-10.000,  8.000], mean action: 59.222 [12.000, 95.000],  loss: 8.397485, mae: 2.413688, mean_q: 4.631565\n",
      "[ 0 51  5  4 68 99 91  2 16 76]\n",
      " 26046/50001: episode: 2894, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 36.000, mean reward:  4.000 [ 3.000,  6.000], mean action: 45.778 [2.000, 99.000],  loss: 7.432729, mae: 2.421351, mean_q: 4.696866\n",
      "[84 32 97 34 27 23 66 40 31 48]\n",
      " 26055/50001: episode: 2895, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 42.000, mean reward:  4.667 [ 4.000,  7.000], mean action: 44.222 [23.000, 97.000],  loss: 8.563679, mae: 2.417101, mean_q: 4.633126\n",
      "[25  2 30  4 30 82 63 97 66 79]\n",
      " 26064/50001: episode: 2896, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 29.000, mean reward:  3.222 [-10.000,  7.000], mean action: 50.333 [2.000, 97.000],  loss: 8.283578, mae: 2.463735, mean_q: 4.738326\n",
      "[ 3  1 23 20 63 46 21 60 97 14]\n",
      " 26073/50001: episode: 2897, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 38.333 [1.000, 97.000],  loss: 8.332820, mae: 2.374768, mean_q: 4.608623\n",
      "[30 13 56 55 60 34 23 93 17 31]\n",
      " 26082/50001: episode: 2898, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 37.000, mean reward:  4.111 [ 2.000,  6.000], mean action: 42.444 [13.000, 93.000],  loss: 7.553209, mae: 2.422637, mean_q: 4.656771\n",
      "[ 6 13  0 20 35 60 13 69 47 52]\n",
      " 26091/50001: episode: 2899, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 34.333 [0.000, 69.000],  loss: 7.395048, mae: 2.342715, mean_q: 4.574955\n",
      "[57 62 20 90 72 96 51 95 27 94]\n",
      " 26100/50001: episode: 2900, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 40.000, mean reward:  4.444 [ 2.000,  9.000], mean action: 67.444 [20.000, 96.000],  loss: 7.692686, mae: 2.384712, mean_q: 4.565008\n",
      "[25 30 83 27 95 96 46 95  8  8]\n",
      " 26109/50001: episode: 2901, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward:  7.000, mean reward:  0.778 [-10.000,  6.000], mean action: 54.222 [8.000, 96.000],  loss: 8.077919, mae: 2.337225, mean_q: 4.549640\n",
      "[21 40 56 27 49 29 40 14 23  2]\n",
      " 26118/50001: episode: 2902, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 31.111 [2.000, 56.000],  loss: 7.087214, mae: 2.331044, mean_q: 4.522902\n",
      "[68 85 31 53 80  1 50 63 94 50]\n",
      " 26127/50001: episode: 2903, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 56.333 [1.000, 94.000],  loss: 7.060760, mae: 2.315924, mean_q: 4.439201\n",
      "[27 45 55 82 30 79 21 34 50 67]\n",
      " 26136/50001: episode: 2904, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 41.000, mean reward:  4.556 [ 2.000,  8.000], mean action: 51.444 [21.000, 82.000],  loss: 5.982822, mae: 2.368809, mean_q: 4.593993\n",
      "[93 60 82 93 69 32 28 93 24 24]\n",
      " 26145/50001: episode: 2905, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: -4.000, mean reward: -0.444 [-10.000,  5.000], mean action: 56.111 [24.000, 93.000],  loss: 5.838125, mae: 2.363508, mean_q: 4.500417\n",
      "[58 37 86 13 70 39 68 42 43 79]\n",
      " 26154/50001: episode: 2906, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 36.000, mean reward:  4.000 [ 2.000,  9.000], mean action: 53.000 [13.000, 86.000],  loss: 5.764677, mae: 2.425411, mean_q: 4.607874\n",
      "[16 63 80 62 66 94 13 67 27 98]\n",
      " 26163/50001: episode: 2907, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 63.333 [13.000, 98.000],  loss: 9.127018, mae: 2.506821, mean_q: 4.805351\n",
      "[75  9 31 83 50 49 28 34 37 13]\n",
      " 26172/50001: episode: 2908, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 44.000, mean reward:  4.889 [ 3.000,  7.000], mean action: 37.111 [9.000, 83.000],  loss: 7.162600, mae: 2.457387, mean_q: 4.671541\n",
      "[77 95 99 14 66 60 21 24 89 47]\n",
      " 26181/50001: episode: 2909, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 57.222 [14.000, 99.000],  loss: 9.803676, mae: 2.478008, mean_q: 4.718798\n",
      "[17 95 13 60 40 46 35 47 98 50]\n",
      " 26190/50001: episode: 2910, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 53.778 [13.000, 98.000],  loss: 6.602448, mae: 2.405812, mean_q: 4.677407\n",
      "[98 44 46 67 50 85 64  3 95 50]\n",
      " 26199/50001: episode: 2911, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 24.000, mean reward:  2.667 [-10.000, 10.000], mean action: 56.000 [3.000, 95.000],  loss: 6.323008, mae: 2.443179, mean_q: 4.723774\n",
      "[ 4 14 73 57 24 41 37 13 83 23]\n",
      " 26208/50001: episode: 2912, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 38.000, mean reward:  4.222 [ 2.000,  6.000], mean action: 40.556 [13.000, 83.000],  loss: 6.688366, mae: 2.485621, mean_q: 4.799327\n",
      "[88 28 60 93 93 32 16 64 93 90]\n",
      " 26217/50001: episode: 2913, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward:  8.000, mean reward:  0.889 [-10.000,  6.000], mean action: 63.222 [16.000, 93.000],  loss: 7.029387, mae: 2.566010, mean_q: 4.921568\n",
      "[59 15 32 46 34 50 14 67 37 52]\n",
      " 26226/50001: episode: 2914, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 41.000, mean reward:  4.556 [ 2.000,  7.000], mean action: 38.556 [14.000, 67.000],  loss: 7.299039, mae: 2.504227, mean_q: 4.800677\n",
      "[45 50 71 79  2 37 46 44 88  6]\n",
      " 26235/50001: episode: 2915, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 41.000, mean reward:  4.556 [ 2.000,  7.000], mean action: 47.000 [2.000, 88.000],  loss: 7.148437, mae: 2.488980, mean_q: 4.751599\n",
      "[37  5 17 40 67 97  6 41 41 59]\n",
      " 26244/50001: episode: 2916, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 19.000, mean reward:  2.111 [-10.000,  5.000], mean action: 41.444 [5.000, 97.000],  loss: 6.776249, mae: 2.492415, mean_q: 4.847998\n",
      "[53 93 90 67 12 95 21  9 89 79]\n",
      " 26253/50001: episode: 2917, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 61.667 [9.000, 95.000],  loss: 6.153347, mae: 2.487370, mean_q: 4.824707\n",
      "[ 2 30  4 90  4 99 28 51 69 97]\n",
      " 26262/50001: episode: 2918, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 52.444 [4.000, 99.000],  loss: 7.164442, mae: 2.423119, mean_q: 4.670664\n",
      "[56 49 79 37 75 50  1 34 28 33]\n",
      " 26271/50001: episode: 2919, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 42.889 [1.000, 79.000],  loss: 7.103276, mae: 2.371025, mean_q: 4.648764\n",
      "[99 74 88 98 59 32 23 18 62 10]\n",
      " 26280/50001: episode: 2920, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 34.000, mean reward:  3.778 [ 3.000,  5.000], mean action: 51.556 [10.000, 98.000],  loss: 6.797543, mae: 2.398634, mean_q: 4.697352\n",
      "[16 72 47 76 64 27 57 23 76 48]\n",
      " 26289/50001: episode: 2921, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 54.444 [23.000, 76.000],  loss: 7.006007, mae: 2.452864, mean_q: 4.646681\n",
      "[53  6 94 50 97 51 63 74 26 89]\n",
      " 26298/50001: episode: 2922, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 37.000, mean reward:  4.111 [ 2.000,  6.000], mean action: 61.111 [6.000, 97.000],  loss: 9.225626, mae: 2.454702, mean_q: 4.666231\n",
      "[23 76 84 15 93 37 32 86 82 50]\n",
      " 26307/50001: episode: 2923, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 41.000, mean reward:  4.556 [ 2.000,  9.000], mean action: 61.667 [15.000, 93.000],  loss: 6.381270, mae: 2.346709, mean_q: 4.526345\n",
      "[33 24 73 95 89  1 21 12 93 50]\n",
      " 26316/50001: episode: 2924, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 45.000, mean reward:  5.000 [ 2.000,  9.000], mean action: 50.889 [1.000, 95.000],  loss: 8.383792, mae: 2.329982, mean_q: 4.457143\n",
      "[19 59 90 34 37 29 34  9 82 48]\n",
      " 26325/50001: episode: 2925, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 46.889 [9.000, 90.000],  loss: 5.960713, mae: 2.346023, mean_q: 4.620123\n",
      "[95 87 37 13  1 80 37 50 40 93]\n",
      " 26334/50001: episode: 2926, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 22.000, mean reward:  2.444 [-10.000,  5.000], mean action: 48.667 [1.000, 93.000],  loss: 8.181965, mae: 2.375938, mean_q: 4.695341\n",
      "[29 30 72 98  9 41 44 48 97 10]\n",
      " 26343/50001: episode: 2927, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 42.000, mean reward:  4.667 [ 2.000,  8.000], mean action: 49.889 [9.000, 98.000],  loss: 7.845044, mae: 2.405429, mean_q: 4.648916\n",
      "[10 74 94 37 83 48 27 99 71 55]\n",
      " 26352/50001: episode: 2928, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 33.000, mean reward:  3.667 [ 2.000,  5.000], mean action: 65.333 [27.000, 99.000],  loss: 3.866180, mae: 2.392039, mean_q: 4.596871\n",
      "[ 2 36 57 28  8 98 95 48 78 20]\n",
      " 26361/50001: episode: 2929, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 52.000 [8.000, 98.000],  loss: 8.136686, mae: 2.502377, mean_q: 4.785723\n",
      "[85 74 90  4 12 13 34 12 49  1]\n",
      " 26370/50001: episode: 2930, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 32.111 [1.000, 90.000],  loss: 6.565090, mae: 2.521574, mean_q: 4.870277\n",
      "[26 28 51 59 39 88 13 62 14  2]\n",
      " 26379/50001: episode: 2931, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 40.000, mean reward:  4.444 [ 2.000,  6.000], mean action: 39.556 [2.000, 88.000],  loss: 6.272262, mae: 2.487465, mean_q: 4.758403\n",
      "[85 83 31 60 80 32 37  2 49 45]\n",
      " 26388/50001: episode: 2932, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 35.000, mean reward:  3.889 [ 3.000,  6.000], mean action: 46.556 [2.000, 83.000],  loss: 4.523009, mae: 2.491569, mean_q: 4.762939\n",
      "[78 67 58 34 31 21 39 60 52 28]\n",
      " 26397/50001: episode: 2933, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 42.000, mean reward:  4.667 [ 2.000,  8.000], mean action: 43.333 [21.000, 67.000],  loss: 5.874819, mae: 2.462382, mean_q: 4.748747\n",
      "[45 13  3 29 11  1 11 44 32 31]\n",
      " 26406/50001: episode: 2934, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 19.444 [1.000, 44.000],  loss: 10.298744, mae: 2.455020, mean_q: 4.763264\n",
      "[30  8 96 82 10 12 12 64 88 55]\n",
      " 26415/50001: episode: 2935, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 23.000, mean reward:  2.556 [-10.000,  8.000], mean action: 47.444 [8.000, 96.000],  loss: 8.906007, mae: 2.464888, mean_q: 4.796635\n",
      "[57  2 98 12  2 70 10 93 13 34]\n",
      " 26424/50001: episode: 2936, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 30.000, mean reward:  3.333 [-10.000,  9.000], mean action: 37.111 [2.000, 98.000],  loss: 8.732370, mae: 2.401115, mean_q: 4.687058\n",
      "[80 43 50 57 69 40 78 88 14 14]\n",
      " 26433/50001: episode: 2937, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 50.333 [14.000, 88.000],  loss: 5.592614, mae: 2.376431, mean_q: 4.637258\n",
      "[61 23 89 27 56 32 93 38 40 34]\n",
      " 26442/50001: episode: 2938, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 43.000, mean reward:  4.778 [ 2.000, 10.000], mean action: 48.000 [23.000, 93.000],  loss: 9.812450, mae: 2.365623, mean_q: 4.622274\n",
      "[85 75 32 31 97  9 54 27 17 30]\n",
      " 26451/50001: episode: 2939, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 35.000, mean reward:  3.889 [ 2.000,  6.000], mean action: 41.333 [9.000, 97.000],  loss: 6.066290, mae: 2.395238, mean_q: 4.686882\n",
      "[37  2 82  1 37 97 92 12 23 12]\n",
      " 26460/50001: episode: 2940, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward:  8.000, mean reward:  0.889 [-10.000,  6.000], mean action: 39.778 [1.000, 97.000],  loss: 5.437053, mae: 2.392281, mean_q: 4.636298\n",
      "[94 78 97 56 86 34 57 43 32  1]\n",
      " 26469/50001: episode: 2941, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 43.000, mean reward:  4.778 [ 3.000,  9.000], mean action: 53.778 [1.000, 97.000],  loss: 7.314742, mae: 2.396685, mean_q: 4.651988\n",
      "[20 14  1 30 11 48 48 13 39 63]\n",
      " 26478/50001: episode: 2942, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 29.667 [1.000, 63.000],  loss: 8.524216, mae: 2.443499, mean_q: 4.688355\n",
      "[81 24 94 14 63 10 15 66 46 83]\n",
      " 26487/50001: episode: 2943, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 36.000, mean reward:  4.000 [ 2.000,  6.000], mean action: 46.111 [10.000, 94.000],  loss: 6.948005, mae: 2.433595, mean_q: 4.757974\n",
      "[30 20 55 41 20  4 31 19 46 59]\n",
      " 26496/50001: episode: 2944, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 32.778 [4.000, 59.000],  loss: 9.355533, mae: 2.468793, mean_q: 4.793936\n",
      "[39 34  7 81 93 93 88 11 40 50]\n",
      " 26505/50001: episode: 2945, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 29.000, mean reward:  3.222 [-10.000,  9.000], mean action: 55.222 [7.000, 93.000],  loss: 7.411918, mae: 2.358482, mean_q: 4.551126\n",
      "[62 41 10 45  1  8 89 18 23 30]\n",
      " 26514/50001: episode: 2946, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 29.444 [1.000, 89.000],  loss: 7.827473, mae: 2.393325, mean_q: 4.711205\n",
      "[37 41  1 37  9 76 57 70 40 42]\n",
      " 26523/50001: episode: 2947, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 21.000, mean reward:  2.333 [-10.000,  7.000], mean action: 41.444 [1.000, 76.000],  loss: 6.150659, mae: 2.351026, mean_q: 4.535636\n",
      "[51 84 19 97 42 32 75 70 98 42]\n",
      " 26532/50001: episode: 2948, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 22.000, mean reward:  2.444 [-10.000,  7.000], mean action: 62.111 [19.000, 98.000],  loss: 7.780138, mae: 2.354476, mean_q: 4.600438\n",
      "[32 34 53 14  9 99 52 51 14 68]\n",
      " 26541/50001: episode: 2949, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 19.000, mean reward:  2.111 [-10.000,  5.000], mean action: 43.778 [9.000, 99.000],  loss: 7.466117, mae: 2.371475, mean_q: 4.617014\n",
      "[60 70 50 12 85 20 27 37 50 47]\n",
      " 26550/50001: episode: 2950, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 44.222 [12.000, 85.000],  loss: 7.563553, mae: 2.310423, mean_q: 4.457004\n",
      "[14 30 27 58 12 89 41  6 28 80]\n",
      " 26559/50001: episode: 2951, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 37.000, mean reward:  4.111 [ 3.000,  7.000], mean action: 41.222 [6.000, 89.000],  loss: 5.209880, mae: 2.456580, mean_q: 4.674165\n",
      "[63 60 48 31 93 32 95  5 69 50]\n",
      " 26568/50001: episode: 2952, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 43.000, mean reward:  4.778 [ 4.000,  8.000], mean action: 53.667 [5.000, 95.000],  loss: 6.100336, mae: 2.397817, mean_q: 4.732111\n",
      "[25 28 93 41 21 27 12 18 97 12]\n",
      " 26577/50001: episode: 2953, duration: 0.076s, episode steps:   9, steps per second: 118, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 38.778 [12.000, 97.000],  loss: 7.581455, mae: 2.459708, mean_q: 4.719393\n",
      "[39 88 10 98 79  1 48 42 32 88]\n",
      " 26586/50001: episode: 2954, duration: 0.076s, episode steps:   9, steps per second: 118, episode reward: 28.000, mean reward:  3.111 [-10.000,  6.000], mean action: 54.000 [1.000, 98.000],  loss: 9.403089, mae: 2.445314, mean_q: 4.697219\n",
      "[84 51 40 16 27 93 95 37 88 41]\n",
      " 26595/50001: episode: 2955, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 54.222 [16.000, 95.000],  loss: 6.371124, mae: 2.459920, mean_q: 4.768196\n",
      "[42 53 11 13 27 97 24  6 23 67]\n",
      " 26604/50001: episode: 2956, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 37.000, mean reward:  4.111 [ 3.000,  5.000], mean action: 35.667 [6.000, 97.000],  loss: 6.840889, mae: 2.449521, mean_q: 4.756979\n",
      "[ 1 34  2 12 34 66 74 24 40 75]\n",
      " 26613/50001: episode: 2957, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 40.111 [2.000, 75.000],  loss: 9.008816, mae: 2.414132, mean_q: 4.636049\n",
      "[10 12 34 86 82 46 57 14 13 28]\n",
      " 26622/50001: episode: 2958, duration: 0.071s, episode steps:   9, steps per second: 127, episode reward: 44.000, mean reward:  4.889 [ 3.000,  7.000], mean action: 41.333 [12.000, 86.000],  loss: 7.285975, mae: 2.473312, mean_q: 4.828558\n",
      "[38 95  4 95 48 79 30 80  9  6]\n",
      " 26631/50001: episode: 2959, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 23.000, mean reward:  2.556 [-10.000,  5.000], mean action: 49.556 [4.000, 95.000],  loss: 8.542862, mae: 2.370482, mean_q: 4.591573\n",
      "[86 56 32 64 24 21 73 40 10 10]\n",
      " 26640/50001: episode: 2960, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 21.000, mean reward:  2.333 [-10.000,  6.000], mean action: 36.667 [10.000, 73.000],  loss: 9.441957, mae: 2.385806, mean_q: 4.606511\n",
      "[78 78 63 93 37 52 23 48  3 40]\n",
      " 26649/50001: episode: 2961, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 27.000, mean reward:  3.000 [-10.000,  6.000], mean action: 48.556 [3.000, 93.000],  loss: 6.742912, mae: 2.334858, mean_q: 4.506774\n",
      "[ 9 46 79 36 24 79 14 63 34 14]\n",
      " 26658/50001: episode: 2962, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 12.000, mean reward:  1.333 [-10.000,  8.000], mean action: 43.222 [14.000, 79.000],  loss: 6.850618, mae: 2.356564, mean_q: 4.550332\n",
      "[33 34 49 12 58 80 27 37  7 95]\n",
      " 26667/50001: episode: 2963, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 42.000, mean reward:  4.667 [ 1.000, 11.000], mean action: 44.333 [7.000, 95.000],  loss: 7.304280, mae: 2.343217, mean_q: 4.625109\n",
      "[38 41 53 16  1 26 95 46 32 62]\n",
      " 26676/50001: episode: 2964, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 42.000, mean reward:  4.667 [ 2.000,  8.000], mean action: 41.333 [1.000, 95.000],  loss: 9.486960, mae: 2.349285, mean_q: 4.559500\n",
      "[26 21 31 60 10 75  8 13 34 30]\n",
      " 26685/50001: episode: 2965, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 31.333 [8.000, 75.000],  loss: 6.473118, mae: 2.276757, mean_q: 4.530083\n",
      "[63 71 93 37 95 97 34 99 30 27]\n",
      " 26694/50001: episode: 2966, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 41.000, mean reward:  4.556 [ 2.000,  6.000], mean action: 64.778 [27.000, 99.000],  loss: 5.627088, mae: 2.309594, mean_q: 4.446161\n",
      "[87 76 28 93 80 42 20 80 37 12]\n",
      " 26703/50001: episode: 2967, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 29.000, mean reward:  3.222 [-10.000,  9.000], mean action: 52.000 [12.000, 93.000],  loss: 8.362939, mae: 2.382440, mean_q: 4.597464\n",
      "[36 83 84 28 99  8 34 18 59 24]\n",
      " 26712/50001: episode: 2968, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 48.556 [8.000, 99.000],  loss: 6.503706, mae: 2.416701, mean_q: 4.730548\n",
      "[98  5 37 52 98 59 35 95 66 97]\n",
      " 26721/50001: episode: 2969, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 28.000, mean reward:  3.111 [-10.000,  8.000], mean action: 60.444 [5.000, 98.000],  loss: 7.550756, mae: 2.518410, mean_q: 4.794562\n",
      "[76 12 96 69 80 57 46 64 13  9]\n",
      " 26730/50001: episode: 2970, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 39.000, mean reward:  4.333 [ 3.000,  8.000], mean action: 49.556 [9.000, 96.000],  loss: 10.138632, mae: 2.498160, mean_q: 4.785500\n",
      "[62 42  4 34 57 93 34 16 46 48]\n",
      " 26739/50001: episode: 2971, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 26.000, mean reward:  2.889 [-10.000,  8.000], mean action: 41.556 [4.000, 93.000],  loss: 8.835590, mae: 2.399808, mean_q: 4.652698\n",
      "[19 34 47 86 49 76 62 40  1 42]\n",
      " 26748/50001: episode: 2972, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 48.556 [1.000, 86.000],  loss: 7.246498, mae: 2.405562, mean_q: 4.583392\n",
      "[41 89 28 54 14 12 81 33 92 74]\n",
      " 26757/50001: episode: 2973, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 36.000, mean reward:  4.000 [ 3.000,  6.000], mean action: 53.000 [12.000, 92.000],  loss: 5.642687, mae: 2.399637, mean_q: 4.536928\n",
      "[13 46 79 50 35  4 93 57 48 77]\n",
      " 26766/50001: episode: 2974, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 37.000, mean reward:  4.111 [ 3.000,  6.000], mean action: 54.333 [4.000, 93.000],  loss: 6.787799, mae: 2.343393, mean_q: 4.468564\n",
      "[21 75  2  2 96 97 86 74 57 48]\n",
      " 26775/50001: episode: 2975, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 27.000, mean reward:  3.000 [-10.000,  8.000], mean action: 59.667 [2.000, 97.000],  loss: 11.552895, mae: 2.406008, mean_q: 4.622416\n",
      "[39 34 31 42 69 75 28 95 50 12]\n",
      " 26784/50001: episode: 2976, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 46.000, mean reward:  5.111 [ 3.000,  8.000], mean action: 48.444 [12.000, 95.000],  loss: 9.310719, mae: 2.350246, mean_q: 4.554044\n",
      "[57 23 59 20 75 59  1 53 98 12]\n",
      " 26793/50001: episode: 2977, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 27.000, mean reward:  3.000 [-10.000,  8.000], mean action: 44.444 [1.000, 98.000],  loss: 6.373842, mae: 2.301054, mean_q: 4.508892\n",
      "[23 13 20  6 73 34 27 12 79 51]\n",
      " 26802/50001: episode: 2978, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 35.000 [6.000, 79.000],  loss: 7.788423, mae: 2.258644, mean_q: 4.386893\n",
      "[45 50 96 76 30 37 91 14 28 40]\n",
      " 26811/50001: episode: 2979, duration: 0.077s, episode steps:   9, steps per second: 116, episode reward: 41.000, mean reward:  4.556 [ 3.000,  6.000], mean action: 51.333 [14.000, 96.000],  loss: 8.795299, mae: 2.301604, mean_q: 4.442629\n",
      "[51 17 28 27 79  8 86 50 93 79]\n",
      " 26820/50001: episode: 2980, duration: 0.072s, episode steps:   9, steps per second: 125, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 51.889 [8.000, 93.000],  loss: 6.699450, mae: 2.330285, mean_q: 4.530072\n",
      "[19  5 26 85 75 49 13 24 51 46]\n",
      " 26829/50001: episode: 2981, duration: 0.067s, episode steps:   9, steps per second: 133, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 41.556 [5.000, 85.000],  loss: 9.474894, mae: 2.406060, mean_q: 4.639523\n",
      "[93 53 27 47 95 91 95 97 57 97]\n",
      " 26838/50001: episode: 2982, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 10.000, mean reward:  1.111 [-10.000,  6.000], mean action: 73.222 [27.000, 97.000],  loss: 7.232700, mae: 2.340935, mean_q: 4.540033\n",
      "[20 37 28 80 33 95 33 83 88 64]\n",
      " 26847/50001: episode: 2983, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 60.111 [28.000, 95.000],  loss: 6.662400, mae: 2.343437, mean_q: 4.584550\n",
      "[73 12 14 83 98  1 94  5 98 46]\n",
      " 26856/50001: episode: 2984, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 50.111 [1.000, 98.000],  loss: 5.785174, mae: 2.390653, mean_q: 4.659392\n",
      "[46 27 88 80 93 57 86 34 86 31]\n",
      " 26865/50001: episode: 2985, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 26.000, mean reward:  2.889 [-10.000,  8.000], mean action: 64.667 [27.000, 93.000],  loss: 5.579494, mae: 2.477198, mean_q: 4.726542\n",
      "[85 82 76 30 63 94 95 97  4 98]\n",
      " 26874/50001: episode: 2986, duration: 0.075s, episode steps:   9, steps per second: 119, episode reward: 44.000, mean reward:  4.889 [ 3.000,  8.000], mean action: 71.000 [4.000, 98.000],  loss: 8.382253, mae: 2.522777, mean_q: 4.923164\n",
      "[60 53 18  9 49 48  1 75 64 12]\n",
      " 26883/50001: episode: 2987, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 36.556 [1.000, 75.000],  loss: 8.093882, mae: 2.479436, mean_q: 4.788517\n",
      "[56 82 84 99 28  4 56 28 56 46]\n",
      " 26892/50001: episode: 2988, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: -5.000, mean reward: -0.556 [-10.000,  6.000], mean action: 53.667 [4.000, 99.000],  loss: 5.819952, mae: 2.507218, mean_q: 4.866816\n",
      "[67 37 88 11 64  4  5 66 82 74]\n",
      " 26901/50001: episode: 2989, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 39.000, mean reward:  4.333 [ 3.000,  5.000], mean action: 47.889 [4.000, 88.000],  loss: 6.565388, mae: 2.563926, mean_q: 4.924203\n",
      "[ 5 20 12 88 92 46 88 50 71 12]\n",
      " 26910/50001: episode: 2990, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward:  8.000, mean reward:  0.889 [-10.000,  7.000], mean action: 53.222 [12.000, 92.000],  loss: 6.448016, mae: 2.519918, mean_q: 4.871280\n",
      "[60 60 40  6 46 12 40 48 93 32]\n",
      " 26919/50001: episode: 2991, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 11.000, mean reward:  1.222 [-10.000,  6.000], mean action: 41.889 [6.000, 93.000],  loss: 7.028251, mae: 2.537185, mean_q: 4.913724\n",
      "[ 2 61 38 31 20 57 47 31 60 37]\n",
      " 26928/50001: episode: 2992, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 28.000, mean reward:  3.111 [-10.000,  8.000], mean action: 42.444 [20.000, 61.000],  loss: 9.405999, mae: 2.498821, mean_q: 4.910676\n",
      "[91 49 66 16 93 53  2 91 97 66]\n",
      " 26937/50001: episode: 2993, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward:  9.000, mean reward:  1.000 [-10.000,  7.000], mean action: 59.222 [2.000, 97.000],  loss: 7.182933, mae: 2.393889, mean_q: 4.640582\n",
      "[18 11 35 62 76  3 32 32 89 79]\n",
      " 26946/50001: episode: 2994, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 46.556 [3.000, 89.000],  loss: 7.624853, mae: 2.408273, mean_q: 4.657795\n",
      "[27 27 32 76  2 93 68 93  9 14]\n",
      " 26955/50001: episode: 2995, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 10.000, mean reward:  1.111 [-10.000,  6.000], mean action: 46.000 [2.000, 93.000],  loss: 7.577198, mae: 2.355126, mean_q: 4.597315\n",
      "[63 53 12 66 40 59 12 98 13 23]\n",
      " 26964/50001: episode: 2996, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 41.778 [12.000, 98.000],  loss: 11.216973, mae: 2.324147, mean_q: 4.436270\n",
      "[88 12 50 46 48 84 52 98 34 77]\n",
      " 26973/50001: episode: 2997, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 39.000, mean reward:  4.333 [ 3.000,  8.000], mean action: 55.667 [12.000, 98.000],  loss: 6.950651, mae: 2.306302, mean_q: 4.456795\n",
      "[55 93 63 12 50 65  0 37 78 75]\n",
      " 26982/50001: episode: 2998, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 39.000, mean reward:  4.333 [ 2.000,  8.000], mean action: 52.556 [0.000, 93.000],  loss: 8.998684, mae: 2.293017, mean_q: 4.427942\n",
      "[67 36 25 95 98 69 34 21 95 73]\n",
      " 26991/50001: episode: 2999, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 22.000, mean reward:  2.444 [-10.000,  7.000], mean action: 60.667 [21.000, 98.000],  loss: 7.878158, mae: 2.279440, mean_q: 4.436327\n",
      "[87 28 88 69 86  5 57 68  9 48]\n",
      " 27000/50001: episode: 3000, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 39.000, mean reward:  4.333 [ 3.000,  8.000], mean action: 50.889 [5.000, 88.000],  loss: 7.398323, mae: 2.363208, mean_q: 4.559085\n",
      "[69 31 74 62 44 13 48 68 74 37]\n",
      " 27009/50001: episode: 3001, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 50.111 [13.000, 74.000],  loss: 6.987775, mae: 2.337182, mean_q: 4.487435\n",
      "[76 62 80 94  0 41 95  1 57 65]\n",
      " 27018/50001: episode: 3002, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 55.000 [0.000, 95.000],  loss: 4.375802, mae: 2.380615, mean_q: 4.603428\n",
      "[98 12 96 59 87  6 87 84 13 98]\n",
      " 27027/50001: episode: 3003, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward:  8.000, mean reward:  0.889 [-10.000,  8.000], mean action: 60.222 [6.000, 98.000],  loss: 9.680166, mae: 2.492814, mean_q: 4.773176\n",
      "[65  4  3 75 42 74 16 31 12 20]\n",
      " 27036/50001: episode: 3004, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 30.778 [3.000, 75.000],  loss: 7.816723, mae: 2.395154, mean_q: 4.691066\n",
      "[27 60 80  8  0 34 66  9 88 25]\n",
      " 27045/50001: episode: 3005, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 36.000, mean reward:  4.000 [ 2.000,  7.000], mean action: 41.111 [0.000, 88.000],  loss: 7.284917, mae: 2.364724, mean_q: 4.643148\n",
      "[58 11 46 67 62 57  5  9 11 34]\n",
      " 27054/50001: episode: 3006, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 28.000, mean reward:  3.111 [-10.000,  9.000], mean action: 33.556 [5.000, 67.000],  loss: 8.983105, mae: 2.377317, mean_q: 4.639505\n",
      "[71 97 58 50 34 95  8 30 94 48]\n",
      " 27063/50001: episode: 3007, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 41.000, mean reward:  4.556 [ 2.000,  7.000], mean action: 57.111 [8.000, 97.000],  loss: 7.674235, mae: 2.392993, mean_q: 4.548785\n",
      "[ 6 31 80  7 32 60 61 24  2 59]\n",
      " 27072/50001: episode: 3008, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 39.000, mean reward:  4.333 [ 2.000,  6.000], mean action: 39.556 [2.000, 80.000],  loss: 8.775577, mae: 2.306900, mean_q: 4.430126\n",
      "[ 9 46  2 76 55 32 64 34 62  5]\n",
      " 27081/50001: episode: 3009, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 41.778 [2.000, 76.000],  loss: 7.791042, mae: 2.265344, mean_q: 4.426199\n",
      "[24 34 75 23 98 30 81 90 67 56]\n",
      " 27090/50001: episode: 3010, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 35.000, mean reward:  3.889 [ 3.000,  6.000], mean action: 61.556 [23.000, 98.000],  loss: 7.631644, mae: 2.286738, mean_q: 4.408925\n",
      "[40  5 53  1 67 37 85 50 13 54]\n",
      " 27099/50001: episode: 3011, duration: 0.076s, episode steps:   9, steps per second: 118, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 40.556 [1.000, 85.000],  loss: 9.100864, mae: 2.303321, mean_q: 4.502690\n",
      "[ 8 42 51 41 21 52 79 48 34 34]\n",
      " 27108/50001: episode: 3012, duration: 0.071s, episode steps:   9, steps per second: 128, episode reward: 27.000, mean reward:  3.000 [-10.000,  8.000], mean action: 44.667 [21.000, 79.000],  loss: 7.994196, mae: 2.291241, mean_q: 4.476122\n",
      "[79 64 52 99 29 96 94 44 97 50]\n",
      " 27117/50001: episode: 3013, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 39.000, mean reward:  4.333 [ 2.000, 10.000], mean action: 69.444 [29.000, 99.000],  loss: 9.614011, mae: 2.337433, mean_q: 4.534359\n",
      "[79 16 18 10  4  2 88 55 40 42]\n",
      " 27126/50001: episode: 3014, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 37.000, mean reward:  4.111 [ 2.000,  6.000], mean action: 30.556 [2.000, 88.000],  loss: 10.108528, mae: 2.234829, mean_q: 4.327776\n",
      "[84 49 62 49 16 14 60 28 93 12]\n",
      " 27135/50001: episode: 3015, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 32.000, mean reward:  3.556 [-10.000,  9.000], mean action: 42.556 [12.000, 93.000],  loss: 8.941876, mae: 2.239424, mean_q: 4.404116\n",
      "[84 97 58 16 52 11 13  4 34 33]\n",
      " 27144/50001: episode: 3016, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 40.000, mean reward:  4.444 [ 3.000,  8.000], mean action: 35.333 [4.000, 97.000],  loss: 7.888885, mae: 2.245450, mean_q: 4.357978\n",
      "[58 42 26 47 76 57 13 41 64  9]\n",
      " 27153/50001: episode: 3017, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 39.000, mean reward:  4.333 [ 2.000,  6.000], mean action: 41.667 [9.000, 76.000],  loss: 6.655097, mae: 2.306778, mean_q: 4.441818\n",
      "[16  4 83  1 95  5  8 57 95 44]\n",
      " 27162/50001: episode: 3018, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 21.000, mean reward:  2.333 [-10.000,  6.000], mean action: 43.556 [1.000, 95.000],  loss: 8.485030, mae: 2.331687, mean_q: 4.585964\n",
      "[66 28 24 48 67 34 84 71  4 50]\n",
      " 27171/50001: episode: 3019, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 45.556 [4.000, 84.000],  loss: 8.765335, mae: 2.296692, mean_q: 4.452209\n",
      "[75 30 97 63 63 28  4 27  8 85]\n",
      " 27180/50001: episode: 3020, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 22.000, mean reward:  2.444 [-10.000,  5.000], mean action: 45.000 [4.000, 97.000],  loss: 5.812239, mae: 2.322225, mean_q: 4.511460\n",
      "[94 95 37 41  2 89 20 42 97 79]\n",
      " 27189/50001: episode: 3021, duration: 0.070s, episode steps:   9, steps per second: 129, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 55.778 [2.000, 97.000],  loss: 8.529688, mae: 2.411321, mean_q: 4.618871\n",
      "[15 62 59 28 89 91  1 51 59  5]\n",
      " 27198/50001: episode: 3022, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 26.000, mean reward:  2.889 [-10.000,  6.000], mean action: 49.444 [1.000, 91.000],  loss: 7.966589, mae: 2.312838, mean_q: 4.439878\n",
      "[46 11 73 28 93 42 74 74 66  5]\n",
      " 27207/50001: episode: 3023, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 51.778 [5.000, 93.000],  loss: 8.956009, mae: 2.352143, mean_q: 4.537538\n",
      "[13 29 37 46  2  4 54  9 85 56]\n",
      " 27216/50001: episode: 3024, duration: 0.068s, episode steps:   9, steps per second: 133, episode reward: 35.000, mean reward:  3.889 [ 2.000,  5.000], mean action: 35.778 [2.000, 85.000],  loss: 7.345352, mae: 2.387804, mean_q: 4.612687\n",
      "[ 0 72 98 38 79 24 47 88 89 50]\n",
      " 27225/50001: episode: 3025, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 46.000, mean reward:  5.111 [ 2.000,  9.000], mean action: 65.000 [24.000, 98.000],  loss: 7.385920, mae: 2.391966, mean_q: 4.625163\n",
      "[88 12 40 52 19 88 89 21 74  5]\n",
      " 27234/50001: episode: 3026, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 21.000, mean reward:  2.333 [-10.000,  6.000], mean action: 44.444 [5.000, 89.000],  loss: 9.624596, mae: 2.365830, mean_q: 4.643334\n",
      "[38 41 90 55 95 57  2 42 85 48]\n",
      " 27243/50001: episode: 3027, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 43.000, mean reward:  4.778 [ 3.000,  8.000], mean action: 57.222 [2.000, 95.000],  loss: 6.513979, mae: 2.353969, mean_q: 4.593795\n",
      "[53 28 60 62 14 13 98 48 50 24]\n",
      " 27252/50001: episode: 3028, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 43.000, mean reward:  4.778 [ 3.000,  7.000], mean action: 44.111 [13.000, 98.000],  loss: 8.320835, mae: 2.345540, mean_q: 4.597383\n",
      "[ 9 40 34 73 32 90 13 91  8 12]\n",
      " 27261/50001: episode: 3029, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 39.000, mean reward:  4.333 [ 2.000,  8.000], mean action: 43.667 [8.000, 91.000],  loss: 9.629386, mae: 2.325974, mean_q: 4.488277\n",
      "[53 34 60 24 62 88  8  1 13  1]\n",
      " 27270/50001: episode: 3030, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 32.333 [1.000, 88.000],  loss: 7.618094, mae: 2.318652, mean_q: 4.507555\n",
      "[53 52 31  0 83 41 28 11 98 87]\n",
      " 27279/50001: episode: 3031, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 36.000, mean reward:  4.000 [ 2.000,  6.000], mean action: 47.889 [0.000, 98.000],  loss: 9.707446, mae: 2.268195, mean_q: 4.408174\n",
      "[22 74 17 49 48 78 33 28 28 10]\n",
      " 27288/50001: episode: 3032, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 40.556 [10.000, 78.000],  loss: 7.816640, mae: 2.273148, mean_q: 4.391472\n",
      "[22 60 47 49 28 59  2 64 37 97]\n",
      " 27297/50001: episode: 3033, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 44.000, mean reward:  4.889 [ 3.000,  8.000], mean action: 49.222 [2.000, 97.000],  loss: 8.489069, mae: 2.245786, mean_q: 4.371620\n",
      "[44  2 77 91 96 79 95 46  2 13]\n",
      " 27306/50001: episode: 3034, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 28.000, mean reward:  3.111 [-10.000,  8.000], mean action: 55.667 [2.000, 96.000],  loss: 7.106172, mae: 2.284373, mean_q: 4.417354\n",
      "[ 3 83  4 57 15 99 11 62 46 37]\n",
      " 27315/50001: episode: 3035, duration: 0.070s, episode steps:   9, steps per second: 128, episode reward: 42.000, mean reward:  4.667 [ 2.000, 10.000], mean action: 46.000 [4.000, 99.000],  loss: 6.784050, mae: 2.288253, mean_q: 4.442709\n",
      "[20 93 12 30  1 68 50 83 24 14]\n",
      " 27324/50001: episode: 3036, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 40.000, mean reward:  4.444 [ 3.000,  6.000], mean action: 41.667 [1.000, 93.000],  loss: 6.889165, mae: 2.326267, mean_q: 4.424084\n",
      "[84 28 63 54 34 88 10 76  3 98]\n",
      " 27333/50001: episode: 3037, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 37.000, mean reward:  4.111 [ 2.000,  7.000], mean action: 50.444 [3.000, 98.000],  loss: 6.264301, mae: 2.355687, mean_q: 4.600655\n",
      "[60 90 98 28 39 59 37 95 27 23]\n",
      " 27342/50001: episode: 3038, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 42.000, mean reward:  4.667 [ 2.000,  8.000], mean action: 55.111 [23.000, 98.000],  loss: 7.742087, mae: 2.457899, mean_q: 4.782937\n",
      "[13 60 58 60 74 15 32 84 27 52]\n",
      " 27351/50001: episode: 3039, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 21.000, mean reward:  2.333 [-10.000,  6.000], mean action: 51.333 [15.000, 84.000],  loss: 5.202251, mae: 2.395339, mean_q: 4.656047\n",
      "[76 10 32 32 98 59 49 51 64 12]\n",
      " 27360/50001: episode: 3040, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 26.000, mean reward:  2.889 [-10.000,  9.000], mean action: 45.222 [10.000, 98.000],  loss: 7.523438, mae: 2.462130, mean_q: 4.721312\n",
      "[10 98 58 48  0 55 98 75 14  4]\n",
      " 27369/50001: episode: 3041, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 50.000 [0.000, 98.000],  loss: 7.779526, mae: 2.487124, mean_q: 4.791563\n",
      "[74 13 39 70 88 37 82  8 37  5]\n",
      " 27378/50001: episode: 3042, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 42.111 [5.000, 88.000],  loss: 5.465096, mae: 2.471822, mean_q: 4.824737\n",
      "[15 58 81 41 95 82 33 17  2 54]\n",
      " 27387/50001: episode: 3043, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 38.000, mean reward:  4.222 [ 3.000,  7.000], mean action: 51.444 [2.000, 95.000],  loss: 6.999521, mae: 2.476518, mean_q: 4.745343\n",
      "[79  2 11 62 50 68 74 92 66 48]\n",
      " 27396/50001: episode: 3044, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 52.556 [2.000, 92.000],  loss: 5.996303, mae: 2.443437, mean_q: 4.709477\n",
      "[56 14 18 83 94 25 13 74 16 54]\n",
      " 27405/50001: episode: 3045, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 35.000, mean reward:  3.889 [ 2.000,  7.000], mean action: 43.444 [13.000, 94.000],  loss: 7.707469, mae: 2.484876, mean_q: 4.846821\n",
      "[81 95 61 77 85 54  1 88 24 82]\n",
      " 27414/50001: episode: 3046, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 63.000 [1.000, 95.000],  loss: 8.038667, mae: 2.465695, mean_q: 4.833464\n",
      "[68  8  2 24 89 27 34 62 50 74]\n",
      " 27423/50001: episode: 3047, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 41.111 [2.000, 89.000],  loss: 6.668477, mae: 2.428110, mean_q: 4.726864\n",
      "[17 95 99 44 75 18 57 37 37 27]\n",
      " 27432/50001: episode: 3048, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 26.000, mean reward:  2.889 [-10.000,  8.000], mean action: 54.333 [18.000, 99.000],  loss: 8.260751, mae: 2.443711, mean_q: 4.680739\n",
      "[68 57 11 27 79  6 79 28  6 24]\n",
      " 27441/50001: episode: 3049, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 10.000, mean reward:  1.111 [-10.000,  6.000], mean action: 35.222 [6.000, 79.000],  loss: 7.621100, mae: 2.379485, mean_q: 4.566142\n",
      "[25 32 28 48 58 82 13 95 94 50]\n",
      " 27450/50001: episode: 3050, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 44.000, mean reward:  4.889 [ 3.000,  8.000], mean action: 55.556 [13.000, 95.000],  loss: 5.740955, mae: 2.370405, mean_q: 4.511600\n",
      "[41 39  5 41 74 13 97  4 98 48]\n",
      " 27459/50001: episode: 3051, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 29.000, mean reward:  3.222 [-10.000,  8.000], mean action: 46.556 [4.000, 98.000],  loss: 7.088199, mae: 2.468412, mean_q: 4.852598\n",
      "[46 92 31 23 48 35 62 82 37 37]\n",
      " 27468/50001: episode: 3052, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 49.667 [23.000, 92.000],  loss: 6.362703, mae: 2.460924, mean_q: 4.697259\n",
      "[46 66 28 37 29  1 61 90 14 41]\n",
      " 27477/50001: episode: 3053, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 39.000, mean reward:  4.333 [ 2.000,  6.000], mean action: 40.778 [1.000, 90.000],  loss: 9.460735, mae: 2.473611, mean_q: 4.684996\n",
      "[84 48 86 66 83 95 13 72 59 23]\n",
      " 27486/50001: episode: 3054, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 60.556 [13.000, 95.000],  loss: 6.042996, mae: 2.590607, mean_q: 4.889184\n",
      "[75 40  4 68 86 75 52 41 66 34]\n",
      " 27495/50001: episode: 3055, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 29.000, mean reward:  3.222 [-10.000, 10.000], mean action: 51.778 [4.000, 86.000],  loss: 7.014728, mae: 2.529628, mean_q: 4.890071\n",
      "[42 64 55 34 92 95 34 28 89 50]\n",
      " 27504/50001: episode: 3056, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 29.000, mean reward:  3.222 [-10.000,  8.000], mean action: 60.111 [28.000, 95.000],  loss: 6.658849, mae: 2.468348, mean_q: 4.709784\n",
      "[16 43 62 34 88  9 56 50 89 18]\n",
      " 27513/50001: episode: 3057, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 49.889 [9.000, 89.000],  loss: 9.025578, mae: 2.514081, mean_q: 4.848570\n",
      "[33 13 93 74 15 13 97 85 75 81]\n",
      " 27522/50001: episode: 3058, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 21.000, mean reward:  2.333 [-10.000,  6.000], mean action: 60.667 [13.000, 97.000],  loss: 6.825640, mae: 2.445881, mean_q: 4.754106\n",
      "[75 13 28 48 48 69 84  2  9 50]\n",
      " 27531/50001: episode: 3059, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 24.000, mean reward:  2.667 [-10.000,  8.000], mean action: 39.000 [2.000, 84.000],  loss: 7.094684, mae: 2.461987, mean_q: 4.659993\n",
      "[85  5 47 75 96 87 74 13 13 50]\n",
      " 27540/50001: episode: 3060, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 26.000, mean reward:  2.889 [-10.000,  8.000], mean action: 51.111 [5.000, 96.000],  loss: 6.369157, mae: 2.424952, mean_q: 4.711844\n",
      "[24 95 25 42 94 58 83 53 12 53]\n",
      " 27549/50001: episode: 3061, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 24.000, mean reward:  2.667 [-10.000,  8.000], mean action: 57.222 [12.000, 95.000],  loss: 7.769619, mae: 2.461102, mean_q: 4.749346\n",
      "[53 13 37  4 11 48 11 95 87 79]\n",
      " 27558/50001: episode: 3062, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 42.778 [4.000, 95.000],  loss: 6.966271, mae: 2.465006, mean_q: 4.699325\n",
      "[ 1 95  9 21 92 18 75 82 77 32]\n",
      " 27567/50001: episode: 3063, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 40.000, mean reward:  4.444 [ 3.000,  8.000], mean action: 55.667 [9.000, 95.000],  loss: 9.429451, mae: 2.420622, mean_q: 4.704521\n",
      "[91 54 34  4 34 60 95 88 31 50]\n",
      " 27576/50001: episode: 3064, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 31.000, mean reward:  3.444 [-10.000,  7.000], mean action: 50.000 [4.000, 95.000],  loss: 7.533140, mae: 2.330631, mean_q: 4.504784\n",
      "[98 79 98 59 86 78 87 11 41 60]\n",
      " 27585/50001: episode: 3065, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 22.000, mean reward:  2.444 [-10.000,  7.000], mean action: 66.556 [11.000, 98.000],  loss: 9.172822, mae: 2.298320, mean_q: 4.511420\n",
      "[81 14 74 23 97 44 15 53 62 93]\n",
      " 27594/50001: episode: 3066, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 35.000, mean reward:  3.889 [ 2.000,  7.000], mean action: 52.778 [14.000, 97.000],  loss: 6.945777, mae: 2.340940, mean_q: 4.475645\n",
      "[61 51 76 93 77 94  1 31 67 45]\n",
      " 27603/50001: episode: 3067, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 35.000, mean reward:  3.889 [ 3.000,  6.000], mean action: 59.444 [1.000, 94.000],  loss: 8.163194, mae: 2.338273, mean_q: 4.541492\n",
      "[25 66 62 66 98 13 32 68 61 13]\n",
      " 27612/50001: episode: 3068, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward:  8.000, mean reward:  0.889 [-10.000,  6.000], mean action: 53.222 [13.000, 98.000],  loss: 8.985824, mae: 2.301383, mean_q: 4.462595\n",
      "[80 38  2 55 11 34 25 88 66 28]\n",
      " 27621/50001: episode: 3069, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 44.000, mean reward:  4.889 [ 3.000,  7.000], mean action: 38.556 [2.000, 88.000],  loss: 5.560771, mae: 2.335041, mean_q: 4.496607\n",
      "[80 64 98  1 37 75 95 68 13  1]\n",
      " 27630/50001: episode: 3070, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 50.222 [1.000, 98.000],  loss: 8.316916, mae: 2.293026, mean_q: 4.476836\n",
      "[64 28 88 27 89 56 76 67 83 97]\n",
      " 27639/50001: episode: 3071, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 40.000, mean reward:  4.444 [ 3.000,  9.000], mean action: 67.889 [27.000, 97.000],  loss: 9.062886, mae: 2.405131, mean_q: 4.610209\n",
      "[58 87 82 92 17 45 50 32 14 37]\n",
      " 27648/50001: episode: 3072, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 44.000, mean reward:  4.889 [ 3.000,  9.000], mean action: 50.667 [14.000, 92.000],  loss: 6.711070, mae: 2.414495, mean_q: 4.638794\n",
      "[85 60 48 39 20 34 95 28 24 52]\n",
      " 27657/50001: episode: 3073, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 42.000, mean reward:  4.667 [ 2.000,  7.000], mean action: 44.444 [20.000, 95.000],  loss: 7.330878, mae: 2.337762, mean_q: 4.579153\n",
      "[60 34 11 38 77  6 67 41 23 98]\n",
      " 27666/50001: episode: 3074, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 43.889 [6.000, 98.000],  loss: 7.109743, mae: 2.349432, mean_q: 4.571656\n",
      "[93 60 32  4 62 31 88  4 48 27]\n",
      " 27675/50001: episode: 3075, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 26.000, mean reward:  2.889 [-10.000,  6.000], mean action: 39.556 [4.000, 88.000],  loss: 5.998927, mae: 2.335041, mean_q: 4.567647\n",
      "[32 52 72 82 25 30 10 75  5 88]\n",
      " 27684/50001: episode: 3076, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 42.000, mean reward:  4.667 [ 2.000,  9.000], mean action: 48.778 [5.000, 88.000],  loss: 6.519848, mae: 2.402957, mean_q: 4.691672\n",
      "[89 11 75 16 10 72 59 44 50 31]\n",
      " 27693/50001: episode: 3077, duration: 0.073s, episode steps:   9, steps per second: 123, episode reward: 38.000, mean reward:  4.222 [ 2.000,  9.000], mean action: 40.889 [10.000, 75.000],  loss: 9.010613, mae: 2.437563, mean_q: 4.780291\n",
      "[64 89 48 34  2  2 97 41 13  2]\n",
      " 27702/50001: episode: 3078, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 12.000, mean reward:  1.333 [-10.000,  7.000], mean action: 36.444 [2.000, 97.000],  loss: 3.994792, mae: 2.424226, mean_q: 4.663478\n",
      "[48 88 79 90 40 28 37 49 34 32]\n",
      " 27711/50001: episode: 3079, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 43.000, mean reward:  4.778 [ 3.000,  8.000], mean action: 53.000 [28.000, 90.000],  loss: 8.756367, mae: 2.375460, mean_q: 4.567147\n",
      "[45 79 85  1 14 42 24 24 22 88]\n",
      " 27720/50001: episode: 3080, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 28.000, mean reward:  3.111 [-10.000,  8.000], mean action: 42.111 [1.000, 88.000],  loss: 8.430060, mae: 2.384423, mean_q: 4.669563\n",
      "[91 14  2 65  8 64 11 59 88 81]\n",
      " 27729/50001: episode: 3081, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 36.000, mean reward:  4.000 [ 2.000,  8.000], mean action: 43.556 [2.000, 88.000],  loss: 6.249085, mae: 2.401441, mean_q: 4.581335\n",
      "[48 67 96  1 56 27 66 13 39 32]\n",
      " 27738/50001: episode: 3082, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 44.111 [1.000, 96.000],  loss: 10.046823, mae: 2.429827, mean_q: 4.702628\n",
      "[59  1 37 15 21 53 81 74 66 50]\n",
      " 27747/50001: episode: 3083, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 42.000, mean reward:  4.667 [ 2.000,  9.000], mean action: 44.222 [1.000, 81.000],  loss: 8.459064, mae: 2.389799, mean_q: 4.550406\n",
      "[64 67 66 50 32 31 47 15 66 69]\n",
      " 27756/50001: episode: 3084, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 22.000, mean reward:  2.444 [-10.000,  5.000], mean action: 49.222 [15.000, 69.000],  loss: 5.961959, mae: 2.371350, mean_q: 4.614023\n",
      "[92 95 40 30 18 13 56 27 94 47]\n",
      " 27765/50001: episode: 3085, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 46.667 [13.000, 95.000],  loss: 7.698669, mae: 2.362433, mean_q: 4.575254\n",
      "[30 83 28 87 13 61 64 78 48 24]\n",
      " 27774/50001: episode: 3086, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 39.000, mean reward:  4.333 [ 2.000,  8.000], mean action: 54.000 [13.000, 87.000],  loss: 7.607798, mae: 2.406029, mean_q: 4.598707\n",
      "[ 4 21 76 99 56 46 10 46 24 66]\n",
      " 27783/50001: episode: 3087, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 28.000, mean reward:  3.111 [-10.000,  9.000], mean action: 49.333 [10.000, 99.000],  loss: 4.193490, mae: 2.397107, mean_q: 4.637068\n",
      "[92 30 54 74 92 94 33 45  1  0]\n",
      " 27792/50001: episode: 3088, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 22.000, mean reward:  2.444 [-10.000,  8.000], mean action: 47.000 [0.000, 94.000],  loss: 7.856784, mae: 2.491040, mean_q: 4.752651\n",
      "[ 1 28 79 33 72 27 51 57 76 50]\n",
      " 27801/50001: episode: 3089, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 38.000, mean reward:  4.222 [ 2.000,  9.000], mean action: 52.556 [27.000, 79.000],  loss: 6.425898, mae: 2.509357, mean_q: 4.802075\n",
      "[73 73 66 56 95 63  2  1 42 95]\n",
      " 27810/50001: episode: 3090, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 13.000, mean reward:  1.444 [-10.000,  7.000], mean action: 54.778 [1.000, 95.000],  loss: 6.034809, mae: 2.469839, mean_q: 4.716303\n",
      "[28 27  3 10 15 90  9 59 41 39]\n",
      " 27819/50001: episode: 3091, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 34.000, mean reward:  3.778 [ 2.000,  6.000], mean action: 32.556 [3.000, 90.000],  loss: 7.943259, mae: 2.458219, mean_q: 4.713562\n",
      "[35 41 32 93 17 84 86  1 37 50]\n",
      " 27828/50001: episode: 3092, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 45.000, mean reward:  5.000 [ 3.000,  8.000], mean action: 49.000 [1.000, 93.000],  loss: 8.618498, mae: 2.498052, mean_q: 4.880872\n",
      "[ 3  1 28 59 97 46  1 11 97  5]\n",
      " 27837/50001: episode: 3093, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward:  9.000, mean reward:  1.000 [-10.000,  5.000], mean action: 38.333 [1.000, 97.000],  loss: 8.809491, mae: 2.432621, mean_q: 4.691439\n",
      "[49 95 66 69 95  1 90 56 24 53]\n",
      " 27846/50001: episode: 3094, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward: 23.000, mean reward:  2.556 [-10.000,  5.000], mean action: 61.000 [1.000, 95.000],  loss: 8.625157, mae: 2.386445, mean_q: 4.635146\n",
      "[32 22 93 94 28 69  2 23 97 50]\n",
      " 27855/50001: episode: 3095, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 40.000, mean reward:  4.444 [ 3.000,  8.000], mean action: 53.111 [2.000, 97.000],  loss: 5.938358, mae: 2.308972, mean_q: 4.474554\n",
      "[48 28 51 25 84 18 37  1 95  9]\n",
      " 27864/50001: episode: 3096, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 42.000, mean reward:  4.667 [ 2.000,  9.000], mean action: 38.667 [1.000, 95.000],  loss: 5.976653, mae: 2.294734, mean_q: 4.474301\n",
      "[91 63 89 88 46 20 13 31 44 53]\n",
      " 27873/50001: episode: 3097, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 49.667 [13.000, 89.000],  loss: 8.510418, mae: 2.400923, mean_q: 4.582365\n",
      "[32 67  7 77 93  1 34 17 98 51]\n",
      " 27882/50001: episode: 3098, duration: 0.065s, episode steps:   9, steps per second: 137, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 49.444 [1.000, 98.000],  loss: 8.330816, mae: 2.403041, mean_q: 4.651209\n",
      "[55  5 49 37 74 77 52 91 86 12]\n",
      " 27891/50001: episode: 3099, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 40.000, mean reward:  4.444 [ 3.000, 10.000], mean action: 53.667 [5.000, 91.000],  loss: 6.100156, mae: 2.459666, mean_q: 4.701903\n",
      "[30 27 87 74 44 75 34  2 97 50]\n",
      " 27900/50001: episode: 3100, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 44.000, mean reward:  4.889 [ 3.000,  8.000], mean action: 54.444 [2.000, 97.000],  loss: 7.192500, mae: 2.431210, mean_q: 4.731911\n",
      "[61 64 88 37 66 34 64  5 37 48]\n",
      " 27909/50001: episode: 3101, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 14.000, mean reward:  1.556 [-10.000,  7.000], mean action: 49.222 [5.000, 88.000],  loss: 8.537170, mae: 2.485348, mean_q: 4.740913\n",
      "[82 82  1 28 80 78 83 66 80 74]\n",
      " 27918/50001: episode: 3102, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 10.000, mean reward:  1.111 [-10.000,  6.000], mean action: 63.556 [1.000, 83.000],  loss: 7.897445, mae: 2.490662, mean_q: 4.862544\n",
      "[63 28 34 25 93 52 33 67 51 47]\n",
      " 27927/50001: episode: 3103, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 40.000, mean reward:  4.444 [ 3.000,  8.000], mean action: 47.778 [25.000, 93.000],  loss: 8.998574, mae: 2.377876, mean_q: 4.612665\n",
      "[40 34 68 98  1 42 79 11 96 20]\n",
      " 27936/50001: episode: 3104, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 35.000, mean reward:  3.889 [ 3.000,  5.000], mean action: 49.889 [1.000, 98.000],  loss: 5.984576, mae: 2.366454, mean_q: 4.571260\n",
      "[44 68  2 54 55  1 38 60 93 32]\n",
      " 27945/50001: episode: 3105, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 44.778 [1.000, 93.000],  loss: 9.299734, mae: 2.388109, mean_q: 4.602663\n",
      "[ 5 24 77 56 92 15 71 52 34 51]\n",
      " 27954/50001: episode: 3106, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 39.000, mean reward:  4.333 [ 2.000,  9.000], mean action: 52.444 [15.000, 92.000],  loss: 6.256015, mae: 2.366736, mean_q: 4.562199\n",
      "[32 91 53 79  2 24 75 30 10 78]\n",
      " 27963/50001: episode: 3107, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 33.000, mean reward:  3.667 [ 2.000,  5.000], mean action: 49.111 [2.000, 91.000],  loss: 6.981316, mae: 2.376847, mean_q: 4.531338\n",
      "[ 5 64 35 34 46 76 33 68 40 13]\n",
      " 27972/50001: episode: 3108, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 42.000, mean reward:  4.667 [ 3.000,  9.000], mean action: 45.444 [13.000, 76.000],  loss: 10.175795, mae: 2.435405, mean_q: 4.803144\n",
      "[95 61 14 98 60 73 76  1 57 50]\n",
      " 27981/50001: episode: 3109, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 41.000, mean reward:  4.556 [ 2.000,  9.000], mean action: 54.444 [1.000, 98.000],  loss: 7.546830, mae: 2.415007, mean_q: 4.627768\n",
      "[ 1  9 16 12 98 79 84 64 31 98]\n",
      " 27990/50001: episode: 3110, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 22.000, mean reward:  2.444 [-10.000,  7.000], mean action: 54.556 [9.000, 98.000],  loss: 9.258663, mae: 2.406565, mean_q: 4.685685\n",
      "[11 52 31 48 53 94 33 91 89 34]\n",
      " 27999/50001: episode: 3111, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 39.000, mean reward:  4.333 [ 3.000, 10.000], mean action: 58.333 [31.000, 94.000],  loss: 6.246962, mae: 2.417093, mean_q: 4.676610\n",
      "[78 95 48 32 97 92 51 95 12 79]\n",
      " 28008/50001: episode: 3112, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 26.000, mean reward:  2.889 [-10.000,  6.000], mean action: 66.778 [12.000, 97.000],  loss: 7.251244, mae: 2.364456, mean_q: 4.631466\n",
      "[ 1 51 31 24 33 46 62 30 68 97]\n",
      " 28017/50001: episode: 3113, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 40.000, mean reward:  4.444 [ 3.000,  8.000], mean action: 49.111 [24.000, 97.000],  loss: 8.418631, mae: 2.395589, mean_q: 4.635768\n",
      "[ 6 76  4  9 74 67 76 35 79 79]\n",
      " 28026/50001: episode: 3114, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward:  9.000, mean reward:  1.000 [-10.000,  7.000], mean action: 55.444 [4.000, 79.000],  loss: 7.164518, mae: 2.464478, mean_q: 4.789493\n",
      "[ 6 34 79 58 27 50 48 96 37 94]\n",
      " 28035/50001: episode: 3115, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 58.111 [27.000, 96.000],  loss: 4.525355, mae: 2.460373, mean_q: 4.669996\n",
      "[94 13 47 43 95 46 67  5 41 93]\n",
      " 28044/50001: episode: 3116, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 39.000, mean reward:  4.333 [ 2.000,  6.000], mean action: 50.000 [5.000, 95.000],  loss: 8.551790, mae: 2.449439, mean_q: 4.689821\n",
      "[53 26 28 14 16 10 86 50 93 48]\n",
      " 28053/50001: episode: 3117, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 42.000, mean reward:  4.667 [ 2.000,  8.000], mean action: 41.222 [10.000, 93.000],  loss: 5.887325, mae: 2.494742, mean_q: 4.826838\n",
      "[52 28 10 67 12 62 37 51 64 98]\n",
      " 28062/50001: episode: 3118, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 47.667 [10.000, 98.000],  loss: 8.028716, mae: 2.540985, mean_q: 4.901255\n",
      "[36 50 68 37 53 46 62 93 94 41]\n",
      " 28071/50001: episode: 3119, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 40.000, mean reward:  4.444 [ 3.000,  6.000], mean action: 60.444 [37.000, 94.000],  loss: 7.640918, mae: 2.478715, mean_q: 4.736421\n",
      "[ 1 60 79 85 11 48 73 74 60 88]\n",
      " 28080/50001: episode: 3120, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 24.000, mean reward:  2.667 [-10.000,  8.000], mean action: 64.222 [11.000, 88.000],  loss: 6.855884, mae: 2.445444, mean_q: 4.808026\n",
      "[ 5 64 19 99 20 23 51 50 28 24]\n",
      " 28089/50001: episode: 3121, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 42.000 [19.000, 99.000],  loss: 10.216924, mae: 2.469982, mean_q: 4.720932\n",
      "[22  5 53 28  1 39 54 79 24 98]\n",
      " 28098/50001: episode: 3122, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 38.000, mean reward:  4.222 [ 2.000,  6.000], mean action: 42.333 [1.000, 98.000],  loss: 7.410511, mae: 2.359630, mean_q: 4.502355\n",
      "[48 74 19 41  8 60 79 89 27 24]\n",
      " 28107/50001: episode: 3123, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 46.778 [8.000, 89.000],  loss: 5.584202, mae: 2.316833, mean_q: 4.448825\n",
      "[71 78 17 83 31 59 21 37 10 92]\n",
      " 28116/50001: episode: 3124, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 38.000, mean reward:  4.222 [ 3.000,  8.000], mean action: 47.556 [10.000, 92.000],  loss: 6.372577, mae: 2.391399, mean_q: 4.625373\n",
      "[45 91 84 57 34 89 48 51 57 98]\n",
      " 28125/50001: episode: 3125, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 26.000, mean reward:  2.889 [-10.000,  6.000], mean action: 67.667 [34.000, 98.000],  loss: 9.665072, mae: 2.408879, mean_q: 4.691171\n",
      "[65 78 23 11  2 44 95 38 81 51]\n",
      " 28134/50001: episode: 3126, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 47.000 [2.000, 95.000],  loss: 8.101889, mae: 2.350293, mean_q: 4.518421\n",
      "[53 41 33 79 37 69 13 20 31 13]\n",
      " 28143/50001: episode: 3127, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 37.333 [13.000, 79.000],  loss: 6.906682, mae: 2.238463, mean_q: 4.352353\n",
      "[67 42 77 34 46 44 34 40 27 66]\n",
      " 28152/50001: episode: 3128, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 29.000, mean reward:  3.222 [-10.000,  9.000], mean action: 45.556 [27.000, 77.000],  loss: 9.536782, mae: 2.292851, mean_q: 4.466660\n",
      "[44 48 31 74 31 38 95 55 40 42]\n",
      " 28161/50001: episode: 3129, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 24.000, mean reward:  2.667 [-10.000,  8.000], mean action: 50.444 [31.000, 95.000],  loss: 7.042038, mae: 2.303421, mean_q: 4.400236\n",
      "[74 95  4 50 51 59 67 28 48 70]\n",
      " 28170/50001: episode: 3130, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 36.000, mean reward:  4.000 [ 2.000,  6.000], mean action: 52.444 [4.000, 95.000],  loss: 7.058981, mae: 2.330875, mean_q: 4.487448\n",
      "[99 27 57 46 98 60 28 97 12 79]\n",
      " 28179/50001: episode: 3131, duration: 0.068s, episode steps:   9, steps per second: 133, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 56.000 [12.000, 98.000],  loss: 4.999395, mae: 2.332927, mean_q: 4.577207\n",
      "[88 28 15 87 47 37 21 74 31 98]\n",
      " 28188/50001: episode: 3132, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 48.667 [15.000, 98.000],  loss: 7.138655, mae: 2.345692, mean_q: 4.514369\n",
      "[46  6 37  2 85 42 73 14 48 79]\n",
      " 28197/50001: episode: 3133, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 41.000, mean reward:  4.556 [ 2.000,  7.000], mean action: 42.889 [2.000, 85.000],  loss: 8.059412, mae: 2.410623, mean_q: 4.618027\n",
      "[44 98 60 60  2 37 79 59 31  4]\n",
      " 28206/50001: episode: 3134, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 29.000, mean reward:  3.222 [-10.000,  6.000], mean action: 47.778 [2.000, 98.000],  loss: 6.738662, mae: 2.439013, mean_q: 4.743968\n",
      "[87  0 50 34 75  9 87 93 66 48]\n",
      " 28215/50001: episode: 3135, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 30.000, mean reward:  3.333 [-10.000,  7.000], mean action: 51.333 [0.000, 93.000],  loss: 4.118143, mae: 2.528636, mean_q: 4.785789\n",
      "[41 34 95 78  0 31 37 36 54  4]\n",
      " 28224/50001: episode: 3136, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 41.000 [0.000, 95.000],  loss: 9.348545, mae: 2.540460, mean_q: 4.873855\n",
      "[99 89 60 27  2 42 24 65 68 28]\n",
      " 28233/50001: episode: 3137, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 45.000 [2.000, 89.000],  loss: 7.327604, mae: 2.490441, mean_q: 4.780399\n",
      "[89 88 28 56 31 21 58 27 36 32]\n",
      " 28242/50001: episode: 3138, duration: 0.069s, episode steps:   9, steps per second: 131, episode reward: 36.000, mean reward:  4.000 [ 2.000,  7.000], mean action: 41.889 [21.000, 88.000],  loss: 6.394916, mae: 2.473302, mean_q: 4.769635\n",
      "[52 37  9 35 76 75 95  1  3 13]\n",
      " 28251/50001: episode: 3139, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 42.000, mean reward:  4.667 [ 2.000,  9.000], mean action: 38.222 [1.000, 95.000],  loss: 7.743968, mae: 2.501167, mean_q: 4.832444\n",
      "[32 28 59 87 95 34 62  2 10 97]\n",
      " 28260/50001: episode: 3140, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 52.667 [2.000, 97.000],  loss: 8.395308, mae: 2.512546, mean_q: 4.805868\n",
      "[48 34 95 98  1 53 46 62  1 13]\n",
      " 28269/50001: episode: 3141, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 26.000, mean reward:  2.889 [-10.000,  8.000], mean action: 44.778 [1.000, 98.000],  loss: 5.481310, mae: 2.555440, mean_q: 4.895544\n",
      "[ 2 13 40 30 37 90 40 34 95 69]\n",
      " 28278/50001: episode: 3142, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 28.000, mean reward:  3.111 [-10.000,  8.000], mean action: 49.778 [13.000, 95.000],  loss: 9.384457, mae: 2.560919, mean_q: 4.878382\n",
      "[ 9 46 99 74  9 89 88 96 45 14]\n",
      " 28287/50001: episode: 3143, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 62.222 [9.000, 99.000],  loss: 6.167948, mae: 2.436140, mean_q: 4.676236\n",
      "[85 30 13 13 13 10 84 32 32 88]\n",
      " 28296/50001: episode: 3144, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: -4.000, mean reward: -0.444 [-10.000,  7.000], mean action: 35.000 [10.000, 88.000],  loss: 5.719182, mae: 2.407215, mean_q: 4.645284\n",
      "[33 95 69 32 87 14 75 28 14 63]\n",
      " 28305/50001: episode: 3145, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 53.000 [14.000, 95.000],  loss: 7.113584, mae: 2.464732, mean_q: 4.718691\n",
      "[97 95  4 32 31 30 27 57 82 31]\n",
      " 28314/50001: episode: 3146, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 24.000, mean reward:  2.667 [-10.000,  5.000], mean action: 43.222 [4.000, 95.000],  loss: 9.631087, mae: 2.442533, mean_q: 4.777014\n",
      "[87 57 85 96 37 13 13 27 92 48]\n",
      " 28323/50001: episode: 3147, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 52.000 [13.000, 96.000],  loss: 7.363070, mae: 2.371851, mean_q: 4.640708\n",
      "[ 2 14 10 93 20  4 31 79 10 13]\n",
      " 28332/50001: episode: 3148, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 30.444 [4.000, 93.000],  loss: 7.414712, mae: 2.391038, mean_q: 4.671549\n",
      "[21 96 79 44 46 65 92 33 23 49]\n",
      " 28341/50001: episode: 3149, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 37.000, mean reward:  4.111 [ 3.000,  6.000], mean action: 58.556 [23.000, 96.000],  loss: 7.414565, mae: 2.416226, mean_q: 4.770394\n",
      "[66 51  2 34 90  1 95 13 76 54]\n",
      " 28350/50001: episode: 3150, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 46.222 [1.000, 95.000],  loss: 7.852779, mae: 2.371118, mean_q: 4.652102\n",
      "[43 37 62 12 27  1 60 43 83 32]\n",
      " 28359/50001: episode: 3151, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 28.000, mean reward:  3.111 [-10.000,  7.000], mean action: 39.667 [1.000, 83.000],  loss: 7.111708, mae: 2.398885, mean_q: 4.660905\n",
      "[48 30 64 57 19 98 13 60  8 88]\n",
      " 28368/50001: episode: 3152, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 39.000, mean reward:  4.333 [ 3.000,  8.000], mean action: 48.556 [8.000, 98.000],  loss: 8.343980, mae: 2.473905, mean_q: 4.775491\n",
      "[35 95 60 74 23 75 39 98 24 75]\n",
      " 28377/50001: episode: 3153, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 62.556 [23.000, 98.000],  loss: 6.717542, mae: 2.457308, mean_q: 4.698470\n",
      "[62 85 41 74 53 99 11 56 99 79]\n",
      " 28386/50001: episode: 3154, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 66.333 [11.000, 99.000],  loss: 8.853543, mae: 2.424635, mean_q: 4.723231\n",
      "[99 40 14 99 50 50 50 59 24 81]\n",
      " 28395/50001: episode: 3155, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: -7.000, mean reward: -0.778 [-10.000,  5.000], mean action: 51.889 [14.000, 99.000],  loss: 7.398470, mae: 2.336563, mean_q: 4.521769\n",
      "[ 9 49 94 12  0 53 66 59 90 97]\n",
      " 28404/50001: episode: 3156, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 39.000, mean reward:  4.333 [ 2.000,  8.000], mean action: 57.778 [0.000, 97.000],  loss: 7.956204, mae: 2.392530, mean_q: 4.623654\n",
      "[14 86 24 50  2 93 42  1 88 79]\n",
      " 28413/50001: episode: 3157, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 43.000, mean reward:  4.778 [ 3.000,  7.000], mean action: 51.667 [1.000, 93.000],  loss: 7.943464, mae: 2.403852, mean_q: 4.597463\n",
      "[34 37 82 38 12 73 48 97 42 40]\n",
      " 28422/50001: episode: 3158, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward: 37.000, mean reward:  4.111 [ 2.000,  6.000], mean action: 52.111 [12.000, 97.000],  loss: 7.996945, mae: 2.450122, mean_q: 4.718855\n",
      "[86 60 90  4 24 63 65 54 58 13]\n",
      " 28431/50001: episode: 3159, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 42.000, mean reward:  4.667 [ 3.000, 10.000], mean action: 47.889 [4.000, 90.000],  loss: 8.101446, mae: 2.404814, mean_q: 4.672443\n",
      "[57 37  6 30 35  1 21 78 41 50]\n",
      " 28440/50001: episode: 3160, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 40.000, mean reward:  4.444 [ 3.000, 10.000], mean action: 33.222 [1.000, 78.000],  loss: 8.950965, mae: 2.408719, mean_q: 4.602639\n",
      "[59 75 42 21 46 31 74 82 30 74]\n",
      " 28449/50001: episode: 3161, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 52.778 [21.000, 82.000],  loss: 6.365120, mae: 2.384005, mean_q: 4.702523\n",
      "[50 85 99 27 97 53 26 46 82 50]\n",
      " 28458/50001: episode: 3162, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 21.000, mean reward:  2.333 [-10.000,  6.000], mean action: 62.778 [26.000, 99.000],  loss: 7.149880, mae: 2.410757, mean_q: 4.686499\n",
      "[62 24 98 59 98 57 60 99 89 12]\n",
      " 28467/50001: episode: 3163, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 28.000, mean reward:  3.111 [-10.000,  9.000], mean action: 66.222 [12.000, 99.000],  loss: 8.043720, mae: 2.369038, mean_q: 4.581822\n",
      "[ 5 48  2 97 79 48 51 11 30 23]\n",
      " 28476/50001: episode: 3164, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 21.000, mean reward:  2.333 [-10.000,  5.000], mean action: 43.222 [2.000, 97.000],  loss: 7.268271, mae: 2.411427, mean_q: 4.675624\n",
      "[57  2 42 42 36 43 63 40 66 40]\n",
      " 28485/50001: episode: 3165, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward:  8.000, mean reward:  0.889 [-10.000,  8.000], mean action: 41.556 [2.000, 66.000],  loss: 6.659912, mae: 2.347671, mean_q: 4.620340\n",
      "[36  8 54 92 60 45 95 31 37 92]\n",
      " 28494/50001: episode: 3166, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 30.000, mean reward:  3.333 [-10.000,  8.000], mean action: 57.111 [8.000, 95.000],  loss: 6.234632, mae: 2.410664, mean_q: 4.610636\n",
      "[28 13 68 31 45  4 14 75 37 31]\n",
      " 28503/50001: episode: 3167, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 35.333 [4.000, 75.000],  loss: 7.961720, mae: 2.433229, mean_q: 4.670244\n",
      "[11 41 95 33  2 12 47 44 23 42]\n",
      " 28512/50001: episode: 3168, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 40.000, mean reward:  4.444 [ 3.000,  6.000], mean action: 37.667 [2.000, 95.000],  loss: 7.951513, mae: 2.422060, mean_q: 4.642078\n",
      "[51 83 53 52 74 93 50 67 27 28]\n",
      " 28521/50001: episode: 3169, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 58.556 [27.000, 93.000],  loss: 8.112129, mae: 2.439200, mean_q: 4.686000\n",
      "[47 34 24 58 59 64 69 72  4 93]\n",
      " 28530/50001: episode: 3170, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 37.000, mean reward:  4.111 [ 2.000,  7.000], mean action: 53.000 [4.000, 93.000],  loss: 8.364339, mae: 2.441063, mean_q: 4.663434\n",
      "[74 87  8 53 67 60 88 59  5 48]\n",
      " 28539/50001: episode: 3171, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 52.778 [5.000, 88.000],  loss: 8.412863, mae: 2.425947, mean_q: 4.628127\n",
      "[30 91  5 87 74 84 88 60 52 87]\n",
      " 28548/50001: episode: 3172, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 69.778 [5.000, 91.000],  loss: 7.682108, mae: 2.385687, mean_q: 4.572616\n",
      "[54 95 12 31 11 82 46 10 12 11]\n",
      " 28557/50001: episode: 3173, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward:  9.000, mean reward:  1.000 [-10.000,  5.000], mean action: 34.444 [10.000, 95.000],  loss: 8.228130, mae: 2.363013, mean_q: 4.533461\n",
      "[ 8 88  2 68 43 77 83  9 98 20]\n",
      " 28566/50001: episode: 3174, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 37.000, mean reward:  4.111 [ 2.000,  7.000], mean action: 54.222 [2.000, 98.000],  loss: 8.137989, mae: 2.401083, mean_q: 4.584969\n",
      "[ 1 46 45 37  9 50 48  4 69 24]\n",
      " 28575/50001: episode: 3175, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 39.000, mean reward:  4.333 [ 2.000,  6.000], mean action: 36.889 [4.000, 69.000],  loss: 9.893463, mae: 2.393946, mean_q: 4.548863\n",
      "[40 98 50 46 41 63 78 10 81 20]\n",
      " 28584/50001: episode: 3176, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 36.000, mean reward:  4.000 [ 3.000,  5.000], mean action: 54.111 [10.000, 98.000],  loss: 6.406337, mae: 2.355158, mean_q: 4.551593\n",
      "[23 79 75 28 92 82 34 37  4 37]\n",
      " 28593/50001: episode: 3177, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 52.000 [4.000, 92.000],  loss: 7.865095, mae: 2.395939, mean_q: 4.604776\n",
      "[99 67 27 47 79 23 90 98 94 28]\n",
      " 28602/50001: episode: 3178, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 38.000, mean reward:  4.222 [ 3.000,  7.000], mean action: 61.444 [23.000, 98.000],  loss: 4.449825, mae: 2.402054, mean_q: 4.567335\n",
      "[34 98 80 49 31 76 37 88 12 34]\n",
      " 28611/50001: episode: 3179, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 56.111 [12.000, 98.000],  loss: 7.465381, mae: 2.487140, mean_q: 4.693602\n",
      "[ 0 46 61  2 92 29  9 32 37 69]\n",
      " 28620/50001: episode: 3180, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 41.000, mean reward:  4.556 [ 2.000,  8.000], mean action: 41.889 [2.000, 92.000],  loss: 5.876818, mae: 2.520675, mean_q: 4.832465\n",
      "[23 34 50 31 98 33 35 42 94 60]\n",
      " 28629/50001: episode: 3181, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 53.000 [31.000, 98.000],  loss: 7.138212, mae: 2.508132, mean_q: 4.788539\n",
      "[24 34 13 93 36 20  0 74 28 13]\n",
      " 28638/50001: episode: 3182, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 21.000, mean reward:  2.333 [-10.000,  7.000], mean action: 34.556 [0.000, 93.000],  loss: 9.164354, mae: 2.450881, mean_q: 4.669582\n",
      "[35 70 37  3 32 91 24 28 95 35]\n",
      " 28647/50001: episode: 3183, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 28.000, mean reward:  3.111 [-10.000,  9.000], mean action: 46.111 [3.000, 95.000],  loss: 6.641871, mae: 2.545321, mean_q: 4.857301\n",
      "[32 53 88  1 95 32 83 11 41 89]\n",
      " 28656/50001: episode: 3184, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 54.778 [1.000, 95.000],  loss: 5.979813, mae: 2.480561, mean_q: 4.741691\n",
      "[99 99 28 11 60 67 91 46 98 69]\n",
      " 28665/50001: episode: 3185, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 63.222 [11.000, 99.000],  loss: 8.412928, mae: 2.566312, mean_q: 4.948287\n",
      "[20 31 93 23 88 37 68 32 89 50]\n",
      " 28674/50001: episode: 3186, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 56.778 [23.000, 93.000],  loss: 6.601232, mae: 2.594732, mean_q: 4.966373\n",
      "[55 50 45 49 37 68 40 28 48 46]\n",
      " 28683/50001: episode: 3187, duration: 0.069s, episode steps:   9, steps per second: 131, episode reward: 40.000, mean reward:  4.444 [ 2.000,  6.000], mean action: 45.667 [28.000, 68.000],  loss: 8.431228, mae: 2.501312, mean_q: 4.886976\n",
      "[73 53 64 34 20 34 38 29 27 42]\n",
      " 28692/50001: episode: 3188, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 37.889 [20.000, 64.000],  loss: 6.517772, mae: 2.375747, mean_q: 4.554178\n",
      "[80 91 30 67 47 31 47  1  4 92]\n",
      " 28701/50001: episode: 3189, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 45.556 [1.000, 92.000],  loss: 7.453756, mae: 2.493068, mean_q: 4.771654\n",
      "[66  4 62 34 99  5 46 28  5 18]\n",
      " 28710/50001: episode: 3190, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 33.444 [4.000, 99.000],  loss: 6.936162, mae: 2.425095, mean_q: 4.578535\n",
      "[ 9  5 99 46 41 59 50 37 92 12]\n",
      " 28719/50001: episode: 3191, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 49.000 [5.000, 99.000],  loss: 6.009280, mae: 2.459949, mean_q: 4.686927\n",
      "[23 89 94 41 98 13 93 18 18 32]\n",
      " 28728/50001: episode: 3192, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 27.000, mean reward:  3.000 [-10.000,  8.000], mean action: 55.111 [13.000, 98.000],  loss: 8.628643, mae: 2.471081, mean_q: 4.714983\n",
      "[19 26 57 98  8 21  8 49 24 44]\n",
      " 28737/50001: episode: 3193, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 37.222 [8.000, 98.000],  loss: 10.172017, mae: 2.481712, mean_q: 4.784760\n",
      "[72 35 58  1 13 46 37 66 88 21]\n",
      " 28746/50001: episode: 3194, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 42.000, mean reward:  4.667 [ 3.000,  6.000], mean action: 40.556 [1.000, 88.000],  loss: 10.475559, mae: 2.386073, mean_q: 4.604803\n",
      "[60 22  2 18 27 10 33 69 93 67]\n",
      " 28755/50001: episode: 3195, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 33.000, mean reward:  3.667 [ 2.000,  5.000], mean action: 37.889 [2.000, 93.000],  loss: 9.068048, mae: 2.319083, mean_q: 4.452589\n",
      "[87  2 36 44 56 43 62 64 85 57]\n",
      " 28764/50001: episode: 3196, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 38.000, mean reward:  4.222 [ 2.000,  8.000], mean action: 49.889 [2.000, 85.000],  loss: 8.243851, mae: 2.264437, mean_q: 4.332618\n",
      "[88 50 49 14 13 70 80 68 37 14]\n",
      " 28773/50001: episode: 3197, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 22.000, mean reward:  2.444 [-10.000,  8.000], mean action: 43.889 [13.000, 80.000],  loss: 7.365245, mae: 2.310022, mean_q: 4.460621\n",
      "[51 60 60 35 77 95 52 44 28  1]\n",
      " 28782/50001: episode: 3198, duration: 0.071s, episode steps:   9, steps per second: 127, episode reward: 28.000, mean reward:  3.111 [-10.000,  8.000], mean action: 50.222 [1.000, 95.000],  loss: 7.705420, mae: 2.386938, mean_q: 4.538621\n",
      "[67 35 16 12  2 82 74 73 48  1]\n",
      " 28791/50001: episode: 3199, duration: 0.072s, episode steps:   9, steps per second: 124, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 38.111 [1.000, 82.000],  loss: 6.687141, mae: 2.394454, mean_q: 4.526359\n",
      "[72 87 98  6 56  6 37 60  5 52]\n",
      " 28800/50001: episode: 3200, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 45.222 [5.000, 98.000],  loss: 5.996929, mae: 2.410520, mean_q: 4.573683\n",
      "[ 1 46 20  2 63 30 37 50 88 79]\n",
      " 28809/50001: episode: 3201, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 44.000, mean reward:  4.889 [ 3.000,  7.000], mean action: 46.111 [2.000, 88.000],  loss: 7.274433, mae: 2.412663, mean_q: 4.587220\n",
      "[25 28 82 43  2 66 76 46 42 95]\n",
      " 28818/50001: episode: 3202, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 45.000, mean reward:  5.000 [ 2.000, 11.000], mean action: 53.333 [2.000, 95.000],  loss: 10.055295, mae: 2.478042, mean_q: 4.738732\n",
      "[12 82 75 37 98 33 29 52 14 75]\n",
      " 28827/50001: episode: 3203, duration: 0.068s, episode steps:   9, steps per second: 133, episode reward: 21.000, mean reward:  2.333 [-10.000,  6.000], mean action: 55.000 [14.000, 98.000],  loss: 8.022327, mae: 2.456973, mean_q: 4.681242\n",
      "[52  4 13 26 12 79 30 64 31  8]\n",
      " 28836/50001: episode: 3204, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward: 35.000, mean reward:  3.889 [ 2.000,  6.000], mean action: 29.667 [4.000, 79.000],  loss: 7.608117, mae: 2.475518, mean_q: 4.693610\n",
      "[40 28 25 59 48 53  9 46 13 46]\n",
      " 28845/50001: episode: 3205, duration: 0.069s, episode steps:   9, steps per second: 131, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 36.333 [9.000, 59.000],  loss: 6.753930, mae: 2.426171, mean_q: 4.681918\n",
      "[ 9 98 85 21 48 42 89 77 90 96]\n",
      " 28854/50001: episode: 3206, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 37.000, mean reward:  4.111 [ 3.000,  6.000], mean action: 71.778 [21.000, 98.000],  loss: 5.219512, mae: 2.479271, mean_q: 4.697476\n",
      "[42 40  1 10  2 53 62 46 48 95]\n",
      " 28863/50001: episode: 3207, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 45.000, mean reward:  5.000 [ 3.000, 10.000], mean action: 39.667 [1.000, 95.000],  loss: 6.791067, mae: 2.455003, mean_q: 4.691889\n",
      "[27 67 45 88 31 46 48 46 16 28]\n",
      " 28872/50001: episode: 3208, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 46.111 [16.000, 88.000],  loss: 9.035786, mae: 2.480692, mean_q: 4.635668\n",
      "[56 68 13 37 75 48 89 11 82 74]\n",
      " 28881/50001: episode: 3209, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 55.222 [11.000, 89.000],  loss: 8.562164, mae: 2.398806, mean_q: 4.583250\n",
      "[28 54 53 57 96 32 62 68 98 31]\n",
      " 28890/50001: episode: 3210, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 61.222 [31.000, 98.000],  loss: 8.648945, mae: 2.347974, mean_q: 4.477788\n",
      "[85 32 10 97 92 74 31  2 95 69]\n",
      " 28899/50001: episode: 3211, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 39.000, mean reward:  4.333 [ 3.000,  9.000], mean action: 55.778 [2.000, 97.000],  loss: 8.278136, mae: 2.327688, mean_q: 4.518181\n",
      "[78 89 28 98  9 31 81 56 50 98]\n",
      " 28908/50001: episode: 3212, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 27.000, mean reward:  3.000 [-10.000,  8.000], mean action: 60.000 [9.000, 98.000],  loss: 8.218386, mae: 2.289475, mean_q: 4.394465\n",
      "[40 37 73 75 63 53 88 45 75 33]\n",
      " 28917/50001: episode: 3213, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 60.222 [33.000, 88.000],  loss: 7.258471, mae: 2.290999, mean_q: 4.401175\n",
      "[21  4 48 42 18 75 74 46 26 97]\n",
      " 28926/50001: episode: 3214, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 39.000, mean reward:  4.333 [ 2.000,  8.000], mean action: 47.778 [4.000, 97.000],  loss: 7.832059, mae: 2.301984, mean_q: 4.429924\n",
      "[24 67 58 10 68 88 13 37 30 23]\n",
      " 28935/50001: episode: 3215, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 43.778 [10.000, 88.000],  loss: 8.729855, mae: 2.334121, mean_q: 4.432000\n",
      "[59 34 23 67 13 99 67 46 50 64]\n",
      " 28944/50001: episode: 3216, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 51.444 [13.000, 99.000],  loss: 6.770173, mae: 2.326390, mean_q: 4.420115\n",
      "[87 68 75 96 88 32 97 98 56 64]\n",
      " 28953/50001: episode: 3217, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 38.000, mean reward:  4.222 [ 3.000,  5.000], mean action: 74.889 [32.000, 98.000],  loss: 7.587208, mae: 2.277492, mean_q: 4.355922\n",
      "[ 5 54  0 39 77  1 34 28 84 97]\n",
      " 28962/50001: episode: 3218, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 42.000, mean reward:  4.667 [ 2.000,  8.000], mean action: 46.000 [0.000, 97.000],  loss: 6.172492, mae: 2.304508, mean_q: 4.438836\n",
      "[56 41 98  1  8 11 47 48 46 66]\n",
      " 28971/50001: episode: 3219, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 45.000, mean reward:  5.000 [ 3.000,  8.000], mean action: 40.667 [1.000, 98.000],  loss: 6.066645, mae: 2.357704, mean_q: 4.499404\n",
      "[83 96 23  6  2 40 14 92 42 24]\n",
      " 28980/50001: episode: 3220, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 37.667 [2.000, 96.000],  loss: 7.819315, mae: 2.443484, mean_q: 4.676654\n",
      "[43 42 88 50 92 34 11  4 66 71]\n",
      " 28989/50001: episode: 3221, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 38.000, mean reward:  4.222 [ 2.000,  6.000], mean action: 50.889 [4.000, 92.000],  loss: 8.137449, mae: 2.454057, mean_q: 4.614118\n",
      "[22 67 89 55 98 27 11 95 75 89]\n",
      " 28998/50001: episode: 3222, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 28.000, mean reward:  3.111 [-10.000,  8.000], mean action: 67.333 [11.000, 98.000],  loss: 5.462046, mae: 2.482915, mean_q: 4.743062\n",
      "[63  5 25 73 88 60 93 48 82 12]\n",
      " 29007/50001: episode: 3223, duration: 0.068s, episode steps:   9, steps per second: 133, episode reward: 43.000, mean reward:  4.778 [ 2.000,  9.000], mean action: 54.000 [5.000, 93.000],  loss: 7.726715, mae: 2.541661, mean_q: 4.793417\n",
      "[42 15 88 48  1 50  2 32 27 27]\n",
      " 29016/50001: episode: 3224, duration: 0.070s, episode steps:   9, steps per second: 129, episode reward: 26.000, mean reward:  2.889 [-10.000,  6.000], mean action: 32.222 [1.000, 88.000],  loss: 6.434179, mae: 2.526562, mean_q: 4.830503\n",
      "[57 41 32  2  6  4 52 63 69 48]\n",
      " 29025/50001: episode: 3225, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward: 40.000, mean reward:  4.444 [ 3.000,  8.000], mean action: 35.222 [2.000, 69.000],  loss: 11.183947, mae: 2.487061, mean_q: 4.808845\n",
      "[64 96 47 35 75 52 91 91 98 89]\n",
      " 29034/50001: episode: 3226, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 74.889 [35.000, 98.000],  loss: 6.160807, mae: 2.486624, mean_q: 4.729331\n",
      "[90 73 64  4 12 12 79 28 89 51]\n",
      " 29043/50001: episode: 3227, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 23.000, mean reward:  2.556 [-10.000,  5.000], mean action: 45.778 [4.000, 89.000],  loss: 9.782993, mae: 2.392385, mean_q: 4.598763\n",
      "[ 4  2  8 59 85  6 20 13  8 54]\n",
      " 29052/50001: episode: 3228, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 21.000, mean reward:  2.333 [-10.000,  7.000], mean action: 28.333 [2.000, 85.000],  loss: 8.966196, mae: 2.370064, mean_q: 4.556245\n",
      "[41 85 60 10 30 23 33 14 20 63]\n",
      " 29061/50001: episode: 3229, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 37.000, mean reward:  4.111 [ 3.000,  6.000], mean action: 37.556 [10.000, 85.000],  loss: 6.681376, mae: 2.443703, mean_q: 4.663646\n",
      "[21 28 26 64 69 46 14 77 12 40]\n",
      " 29070/50001: episode: 3230, duration: 0.072s, episode steps:   9, steps per second: 125, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 41.778 [12.000, 77.000],  loss: 9.001760, mae: 2.375843, mean_q: 4.534624\n",
      "[11 24 23 70 24 91 51 74 10 88]\n",
      " 29079/50001: episode: 3231, duration: 0.068s, episode steps:   9, steps per second: 133, episode reward: 25.000, mean reward:  2.778 [-10.000,  9.000], mean action: 50.556 [10.000, 91.000],  loss: 7.774331, mae: 2.367611, mean_q: 4.497304\n",
      "[86 95 28  9 51 48 74 56 91 50]\n",
      " 29088/50001: episode: 3232, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 40.000, mean reward:  4.444 [ 3.000,  8.000], mean action: 55.778 [9.000, 95.000],  loss: 7.505503, mae: 2.296803, mean_q: 4.443932\n",
      "[28  1 48 67 45 64 59 86 64 14]\n",
      " 29097/50001: episode: 3233, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 21.000, mean reward:  2.333 [-10.000,  8.000], mean action: 49.778 [1.000, 86.000],  loss: 8.066933, mae: 2.387000, mean_q: 4.561148\n",
      "[24 40 79 57 76 31 82 62 31 13]\n",
      " 29106/50001: episode: 3234, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 24.000, mean reward:  2.667 [-10.000,  8.000], mean action: 52.333 [13.000, 82.000],  loss: 7.343634, mae: 2.371978, mean_q: 4.566163\n",
      "[94 42 20 98 96 31 62 14 14 14]\n",
      " 29115/50001: episode: 3235, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward:  9.000, mean reward:  1.000 [-10.000,  6.000], mean action: 43.444 [14.000, 98.000],  loss: 8.303490, mae: 2.381933, mean_q: 4.588686\n",
      "[67 59 14 34 77 34  5 34 82 47]\n",
      " 29124/50001: episode: 3236, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 10.000, mean reward:  1.111 [-10.000,  7.000], mean action: 42.889 [5.000, 82.000],  loss: 10.889260, mae: 2.352134, mean_q: 4.509374\n",
      "[81 51 37 85 37 98 46 51  9 40]\n",
      " 29133/50001: episode: 3237, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 11.000, mean reward:  1.222 [-10.000,  5.000], mean action: 50.444 [9.000, 98.000],  loss: 8.298223, mae: 2.328174, mean_q: 4.455642\n",
      "[49  9 48  4 11 95 62 27 36 97]\n",
      " 29142/50001: episode: 3238, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 43.222 [4.000, 97.000],  loss: 5.519837, mae: 2.269549, mean_q: 4.377744\n",
      "[98 95  4 18 23 41 40 24 33 69]\n",
      " 29151/50001: episode: 3239, duration: 0.072s, episode steps:   9, steps per second: 124, episode reward: 36.000, mean reward:  4.000 [ 2.000,  5.000], mean action: 38.556 [4.000, 95.000],  loss: 7.383010, mae: 2.398388, mean_q: 4.577071\n",
      "[ 5 13 21 92 64 74 80 93 34 31]\n",
      " 29160/50001: episode: 3240, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 40.000, mean reward:  4.444 [ 3.000,  9.000], mean action: 55.778 [13.000, 93.000],  loss: 8.273507, mae: 2.390412, mean_q: 4.586645\n",
      "[97 34 31 44 31 88  1 51 96 46]\n",
      " 29169/50001: episode: 3241, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 46.889 [1.000, 96.000],  loss: 8.151462, mae: 2.332601, mean_q: 4.495565\n",
      "[98 95 60 40 37  4 12 66 12  5]\n",
      " 29178/50001: episode: 3242, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 25.000, mean reward:  2.778 [-10.000,  5.000], mean action: 36.778 [4.000, 95.000],  loss: 7.064169, mae: 2.440323, mean_q: 4.631290\n",
      "[14 12 28 53 60 82  2 23 36 60]\n",
      " 29187/50001: episode: 3243, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 20.000, mean reward:  2.222 [-10.000,  5.000], mean action: 39.556 [2.000, 82.000],  loss: 8.003897, mae: 2.402264, mean_q: 4.645221\n",
      "[39  2 87 14 77  5 46 10 82 14]\n",
      " 29196/50001: episode: 3244, duration: 0.070s, episode steps:   9, steps per second: 128, episode reward: 22.000, mean reward:  2.444 [-10.000,  5.000], mean action: 37.444 [2.000, 87.000],  loss: 7.660015, mae: 2.466964, mean_q: 4.789262\n",
      "[82 70 40  9 60 10 86 12  4 44]\n",
      " 29205/50001: episode: 3245, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 37.222 [4.000, 86.000],  loss: 5.717446, mae: 2.494849, mean_q: 4.740116\n",
      "[39 78 51  2  9 34 72 80 33 15]\n",
      " 29214/50001: episode: 3246, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 35.000, mean reward:  3.889 [ 2.000,  6.000], mean action: 41.556 [2.000, 80.000],  loss: 6.196094, mae: 2.421852, mean_q: 4.587076\n",
      "[36 24 88 38 99 70 12 49 72 50]\n",
      " 29223/50001: episode: 3247, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 41.000, mean reward:  4.556 [ 2.000, 11.000], mean action: 55.778 [12.000, 99.000],  loss: 7.943438, mae: 2.511009, mean_q: 4.826101\n",
      "[46 89 32 87 93 88 57 16 13 28]\n",
      " 29232/50001: episode: 3248, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 55.889 [13.000, 93.000],  loss: 6.639362, mae: 2.421574, mean_q: 4.672347\n",
      "[64 69 28 84 92 95  9 60 98 79]\n",
      " 29241/50001: episode: 3249, duration: 0.066s, episode steps:   9, steps per second: 135, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 68.222 [9.000, 98.000],  loss: 7.175190, mae: 2.450958, mean_q: 4.724520\n",
      "[78 85 34 13 60 60 86 88 49 86]\n",
      " 29250/50001: episode: 3250, duration: 0.070s, episode steps:   9, steps per second: 128, episode reward:  9.000, mean reward:  1.000 [-10.000,  6.000], mean action: 62.333 [13.000, 88.000],  loss: 5.795280, mae: 2.511659, mean_q: 4.791098\n",
      "[ 6 49 74  4 48 91 88 95 40 34]\n",
      " 29259/50001: episode: 3251, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 46.000, mean reward:  5.111 [ 3.000,  9.000], mean action: 58.111 [4.000, 95.000],  loss: 6.999771, mae: 2.528991, mean_q: 4.830364\n",
      "[17 34 72 13 88 43 49 82 36 75]\n",
      " 29268/50001: episode: 3252, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 38.000, mean reward:  4.222 [ 2.000,  8.000], mean action: 54.667 [13.000, 88.000],  loss: 3.572570, mae: 2.556860, mean_q: 5.025128\n",
      "[36 72 51 34 89 49 28 82 98 89]\n",
      " 29277/50001: episode: 3253, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 26.000, mean reward:  2.889 [-10.000,  6.000], mean action: 65.778 [28.000, 98.000],  loss: 6.747683, mae: 2.574486, mean_q: 4.923884\n",
      "[92 13 53 23 73 89 35 68  1 34]\n",
      " 29286/50001: episode: 3254, duration: 0.072s, episode steps:   9, steps per second: 125, episode reward: 39.000, mean reward:  4.333 [ 2.000,  9.000], mean action: 43.222 [1.000, 89.000],  loss: 7.472439, mae: 2.483245, mean_q: 4.712388\n",
      "[81 34 34 48  2  1 98 31 67 33]\n",
      " 29295/50001: episode: 3255, duration: 0.072s, episode steps:   9, steps per second: 125, episode reward: 21.000, mean reward:  2.333 [-10.000,  5.000], mean action: 38.667 [1.000, 98.000],  loss: 7.750253, mae: 2.567786, mean_q: 4.944283\n",
      "[26 60 31 99 92 42 52 34  4 85]\n",
      " 29304/50001: episode: 3256, duration: 0.071s, episode steps:   9, steps per second: 127, episode reward: 39.000, mean reward:  4.333 [ 3.000,  8.000], mean action: 55.444 [4.000, 99.000],  loss: 6.622713, mae: 2.536586, mean_q: 4.816900\n",
      "[ 8 67 12 93  6 38 74  6 37 11]\n",
      " 29313/50001: episode: 3257, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward: 25.000, mean reward:  2.778 [-10.000,  9.000], mean action: 38.222 [6.000, 93.000],  loss: 9.153276, mae: 2.465554, mean_q: 4.720124\n",
      "[20 51 31 37  1 96 49  5 88 79]\n",
      " 29322/50001: episode: 3258, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 48.556 [1.000, 96.000],  loss: 8.959991, mae: 2.431542, mean_q: 4.604668\n",
      "[44 41 71 95 80 60 48 66 96 87]\n",
      " 29331/50001: episode: 3259, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 39.000, mean reward:  4.333 [ 2.000,  6.000], mean action: 71.556 [41.000, 96.000],  loss: 5.697680, mae: 2.420117, mean_q: 4.699770\n",
      "[49 63 31 23 99 35 52 32 74 90]\n",
      " 29340/50001: episode: 3260, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 55.444 [23.000, 99.000],  loss: 6.486595, mae: 2.459627, mean_q: 4.712569\n",
      "[60 49 42 46 21  9 56 56 21 37]\n",
      " 29349/50001: episode: 3261, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward: 13.000, mean reward:  1.444 [-10.000, 10.000], mean action: 37.444 [9.000, 56.000],  loss: 5.727365, mae: 2.508416, mean_q: 4.791397\n",
      "[52 23 41 95 52 30 20 10 50 73]\n",
      " 29358/50001: episode: 3262, duration: 0.086s, episode steps:   9, steps per second: 104, episode reward: 24.000, mean reward:  2.667 [-10.000,  8.000], mean action: 43.778 [10.000, 95.000],  loss: 7.541677, mae: 2.556828, mean_q: 4.917625\n",
      "[40 84 32 96 81 90 89 91 98 34]\n",
      " 29367/50001: episode: 3263, duration: 0.086s, episode steps:   9, steps per second: 104, episode reward: 41.000, mean reward:  4.556 [ 3.000, 10.000], mean action: 77.222 [32.000, 98.000],  loss: 5.610031, mae: 2.565665, mean_q: 4.943051\n",
      "[71 13 96 34 79 15 61 50 65 28]\n",
      " 29376/50001: episode: 3264, duration: 0.087s, episode steps:   9, steps per second: 104, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 49.000 [13.000, 96.000],  loss: 7.559845, mae: 2.607477, mean_q: 5.024268\n",
      "[16 84 33 53  2 98  6 95 31 24]\n",
      " 29385/50001: episode: 3265, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward: 43.000, mean reward:  4.778 [ 3.000,  8.000], mean action: 47.333 [2.000, 98.000],  loss: 9.502003, mae: 2.544081, mean_q: 4.796988\n",
      "[11 48 97 48 50 55 32 40 34 88]\n",
      " 29394/50001: episode: 3266, duration: 0.086s, episode steps:   9, steps per second: 104, episode reward: 28.000, mean reward:  3.111 [-10.000,  8.000], mean action: 54.667 [32.000, 97.000],  loss: 8.132071, mae: 2.496822, mean_q: 4.727751\n",
      "[88 31 31 12  8  5 88 64 14 50]\n",
      " 29403/50001: episode: 3267, duration: 0.078s, episode steps:   9, steps per second: 115, episode reward: 13.000, mean reward:  1.444 [-10.000,  8.000], mean action: 33.667 [5.000, 88.000],  loss: 6.807543, mae: 2.510042, mean_q: 4.776045\n",
      "[50 57 59 95 28 36 46  5 44 76]\n",
      " 29412/50001: episode: 3268, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 34.000, mean reward:  3.778 [ 2.000,  5.000], mean action: 49.556 [5.000, 95.000],  loss: 7.132244, mae: 2.494741, mean_q: 4.717395\n",
      "[45 13 44 81 62 95 13 95 97 81]\n",
      " 29421/50001: episode: 3269, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: -4.000, mean reward: -0.444 [-10.000,  7.000], mean action: 64.556 [13.000, 97.000],  loss: 5.999043, mae: 2.485702, mean_q: 4.768958\n",
      "[12 89 97 34 83 34 79 50 64 50]\n",
      " 29430/50001: episode: 3270, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward:  8.000, mean reward:  0.889 [-10.000,  6.000], mean action: 64.444 [34.000, 97.000],  loss: 6.941757, mae: 2.510146, mean_q: 4.806867\n",
      "[75 56 12 27 21 88 96 52 64 46]\n",
      " 29439/50001: episode: 3271, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 51.333 [12.000, 96.000],  loss: 4.973461, mae: 2.483455, mean_q: 4.727273\n",
      "[46 41 92 28 93 56 13 96 48 12]\n",
      " 29448/50001: episode: 3272, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 53.222 [12.000, 96.000],  loss: 6.948542, mae: 2.508268, mean_q: 4.848395\n",
      "[ 4 74 13 88 46 32 79  4 28 34]\n",
      " 29457/50001: episode: 3273, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 27.000, mean reward:  3.000 [-10.000,  8.000], mean action: 44.222 [4.000, 88.000],  loss: 5.879815, mae: 2.490230, mean_q: 4.792346\n",
      "[65 33 80 93 34 37 48 52 48 64]\n",
      " 29466/50001: episode: 3274, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 54.333 [33.000, 93.000],  loss: 6.874597, mae: 2.450398, mean_q: 4.691529\n",
      "[12 42 97 53 26 37 13 88 37 48]\n",
      " 29475/50001: episode: 3275, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 49.000 [13.000, 97.000],  loss: 7.184811, mae: 2.539546, mean_q: 4.944718\n",
      "[ 1 83 34 21 52 15 47 17 27 22]\n",
      " 29484/50001: episode: 3276, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 35.333 [15.000, 83.000],  loss: 6.963585, mae: 2.501338, mean_q: 4.692890\n",
      "[ 3  4 66  1 95 84 27 41 97 35]\n",
      " 29493/50001: episode: 3277, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 50.000 [1.000, 97.000],  loss: 4.887735, mae: 2.549063, mean_q: 4.895553\n",
      "[82 34 84 87 76 34 21 12 57 79]\n",
      " 29502/50001: episode: 3278, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 53.778 [12.000, 87.000],  loss: 8.353310, mae: 2.539070, mean_q: 4.956068\n",
      "[75 59 28 82 81 16  2 85 50 32]\n",
      " 29511/50001: episode: 3279, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 39.000, mean reward:  4.333 [ 3.000,  8.000], mean action: 48.333 [2.000, 85.000],  loss: 10.759510, mae: 2.468597, mean_q: 4.783471\n",
      "[41 96 92 37 41 20 51 21 24 37]\n",
      " 29520/50001: episode: 3280, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward:  7.000, mean reward:  0.778 [-10.000,  6.000], mean action: 46.556 [20.000, 96.000],  loss: 5.617499, mae: 2.325546, mean_q: 4.498815\n",
      "[ 3 95 41 95 13 50 33 59 84 14]\n",
      " 29529/50001: episode: 3281, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 53.778 [13.000, 95.000],  loss: 9.132408, mae: 2.392971, mean_q: 4.671039\n",
      "[98 96 64 34  2 87 95 72  2 34]\n",
      " 29538/50001: episode: 3282, duration: 0.068s, episode steps:   9, steps per second: 133, episode reward:  8.000, mean reward:  0.889 [-10.000,  7.000], mean action: 54.000 [2.000, 96.000],  loss: 4.861436, mae: 2.401684, mean_q: 4.686930\n",
      "[64 51 95 53 34 88 53 99  4 58]\n",
      " 29547/50001: episode: 3283, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 59.444 [4.000, 99.000],  loss: 9.751991, mae: 2.355770, mean_q: 4.538427\n",
      "[19 95 82 62 98 57 15 34 50 31]\n",
      " 29556/50001: episode: 3284, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 43.000, mean reward:  4.778 [ 2.000,  8.000], mean action: 58.222 [15.000, 98.000],  loss: 7.285703, mae: 2.422895, mean_q: 4.740304\n",
      "[57 57 89 84 13  4  1 88 98 88]\n",
      " 29565/50001: episode: 3285, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 12.000, mean reward:  1.333 [-10.000,  6.000], mean action: 58.000 [1.000, 98.000],  loss: 5.574307, mae: 2.430145, mean_q: 4.632558\n",
      "[18 98 63 32 32 48 74 98 91 32]\n",
      " 29574/50001: episode: 3286, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: -6.000, mean reward: -0.667 [-10.000,  5.000], mean action: 63.111 [32.000, 98.000],  loss: 8.955176, mae: 2.433423, mean_q: 4.693594\n",
      "[63 20 10 24  4 95 13 37 88 37]\n",
      " 29583/50001: episode: 3287, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 29.000, mean reward:  3.222 [-10.000,  6.000], mean action: 36.444 [4.000, 95.000],  loss: 5.765638, mae: 2.456378, mean_q: 4.805316\n",
      "[22 68 40 38 52 87 14 95 14 62]\n",
      " 29592/50001: episode: 3288, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 24.000, mean reward:  2.667 [-10.000,  9.000], mean action: 52.222 [14.000, 95.000],  loss: 8.190912, mae: 2.544568, mean_q: 4.868888\n",
      "[10 76 59 93  2  4 11 79  9 92]\n",
      " 29601/50001: episode: 3289, duration: 0.074s, episode steps:   9, steps per second: 122, episode reward: 40.000, mean reward:  4.444 [ 3.000,  6.000], mean action: 47.222 [2.000, 93.000],  loss: 8.750970, mae: 2.578777, mean_q: 4.933524\n",
      "[24 61  8 40 11 85 95 50 62 50]\n",
      " 29610/50001: episode: 3290, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 51.333 [8.000, 95.000],  loss: 8.286987, mae: 2.483695, mean_q: 4.749961\n",
      "[58  4 37 75 53 99 88 97 64 37]\n",
      " 29619/50001: episode: 3291, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 61.556 [4.000, 99.000],  loss: 6.348188, mae: 2.412565, mean_q: 4.705598\n",
      "[39 84 28 24 93 77 34 37 37 53]\n",
      " 29628/50001: episode: 3292, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 51.889 [24.000, 93.000],  loss: 7.343516, mae: 2.466156, mean_q: 4.738667\n",
      "[41 50  4 74 96 82 11 34 37 95]\n",
      " 29637/50001: episode: 3293, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 45.000, mean reward:  5.000 [ 3.000,  9.000], mean action: 53.667 [4.000, 96.000],  loss: 6.406823, mae: 2.561582, mean_q: 4.798213\n",
      "[13 52 23 70 93 78 95 71 94 54]\n",
      " 29646/50001: episode: 3294, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 32.000, mean reward:  3.556 [ 2.000,  7.000], mean action: 70.000 [23.000, 95.000],  loss: 5.843444, mae: 2.462492, mean_q: 4.700352\n",
      "[13 51 92 18 31 68 95 37 60 13]\n",
      " 29655/50001: episode: 3295, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 51.667 [13.000, 95.000],  loss: 10.293400, mae: 2.579978, mean_q: 4.910933\n",
      "[11 46  4  6 30 38 31 26 53 34]\n",
      " 29664/50001: episode: 3296, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 41.000, mean reward:  4.556 [ 2.000, 10.000], mean action: 29.778 [4.000, 53.000],  loss: 8.769464, mae: 2.464046, mean_q: 4.718271\n",
      "[24  2 34  2 55 24 85 24  9 32]\n",
      " 29673/50001: episode: 3297, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: -4.000, mean reward: -0.444 [-10.000,  7.000], mean action: 29.667 [2.000, 85.000],  loss: 7.608233, mae: 2.404647, mean_q: 4.575184\n",
      "[25 13 47 89 31 68 34 16 11 34]\n",
      " 29682/50001: episode: 3298, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 21.000, mean reward:  2.333 [-10.000,  6.000], mean action: 38.111 [11.000, 89.000],  loss: 5.248766, mae: 2.428859, mean_q: 4.737055\n",
      "[78 41 13  6 44 34 24 57 48 24]\n",
      " 29691/50001: episode: 3299, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 29.000, mean reward:  3.222 [-10.000,  7.000], mean action: 32.333 [6.000, 57.000],  loss: 6.903870, mae: 2.519859, mean_q: 4.742737\n",
      "[13  3 46 23  4 10 62 82 13 48]\n",
      " 29700/50001: episode: 3300, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 32.333 [3.000, 82.000],  loss: 9.424412, mae: 2.471961, mean_q: 4.690905\n",
      "[95 94 38 11 25 75 87 94 51 91]\n",
      " 29709/50001: episode: 3301, duration: 0.070s, episode steps:   9, steps per second: 129, episode reward: 19.000, mean reward:  2.111 [-10.000,  5.000], mean action: 62.889 [11.000, 94.000],  loss: 8.110618, mae: 2.443175, mean_q: 4.724838\n",
      "[54 96 94 95 13 41 47 38 12 97]\n",
      " 29718/50001: episode: 3302, duration: 0.070s, episode steps:   9, steps per second: 129, episode reward: 43.000, mean reward:  4.778 [ 2.000,  8.000], mean action: 59.222 [12.000, 97.000],  loss: 8.687734, mae: 2.378654, mean_q: 4.571810\n",
      "[65 83 97 95 47 14 23  4 99 50]\n",
      " 29727/50001: episode: 3303, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 56.889 [4.000, 99.000],  loss: 6.374933, mae: 2.410214, mean_q: 4.656755\n",
      "[71 98 80 88 46 73 34  8 83 79]\n",
      " 29736/50001: episode: 3304, duration: 0.069s, episode steps:   9, steps per second: 131, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 65.444 [8.000, 98.000],  loss: 9.731810, mae: 2.413110, mean_q: 4.656203\n",
      "[81 76 58 78 32 16 92 93 31 37]\n",
      " 29745/50001: episode: 3305, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 41.000, mean reward:  4.556 [ 3.000,  9.000], mean action: 57.000 [16.000, 93.000],  loss: 10.021091, mae: 2.377468, mean_q: 4.601776\n",
      "[21 13 10 44 49 37 72  4 99 50]\n",
      " 29754/50001: episode: 3306, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 40.000, mean reward:  4.444 [ 2.000,  9.000], mean action: 42.000 [4.000, 99.000],  loss: 5.907911, mae: 2.287878, mean_q: 4.473066\n",
      "[49 30 74 67  9  3  9  6 95 47]\n",
      " 29763/50001: episode: 3307, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 29.000, mean reward:  3.222 [-10.000, 10.000], mean action: 37.778 [3.000, 95.000],  loss: 6.661328, mae: 2.295932, mean_q: 4.453893\n",
      "[87  5 41 52 46 79 50 27  0 96]\n",
      " 29772/50001: episode: 3308, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 37.000, mean reward:  4.111 [ 3.000,  6.000], mean action: 44.000 [0.000, 96.000],  loss: 8.310303, mae: 2.389635, mean_q: 4.641187\n",
      "[80 23 50 16 53 51 82 45 97 87]\n",
      " 29781/50001: episode: 3309, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 35.000, mean reward:  3.889 [ 2.000,  8.000], mean action: 56.000 [16.000, 97.000],  loss: 5.446991, mae: 2.315750, mean_q: 4.514599\n",
      "[30 88 74 40 31  1 18 11 24 50]\n",
      " 29790/50001: episode: 3310, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 42.000, mean reward:  4.667 [ 3.000,  9.000], mean action: 37.444 [1.000, 88.000],  loss: 6.120134, mae: 2.395340, mean_q: 4.714570\n",
      "[29 68 65 83  1 64 10 47 20 81]\n",
      " 29799/50001: episode: 3311, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 36.000, mean reward:  4.000 [ 2.000,  7.000], mean action: 48.778 [1.000, 83.000],  loss: 8.179704, mae: 2.471844, mean_q: 4.752602\n",
      "[ 7  9 33  4 43 44  8 27 40 50]\n",
      " 29808/50001: episode: 3312, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 43.000, mean reward:  4.778 [ 3.000, 10.000], mean action: 28.667 [4.000, 50.000],  loss: 10.033123, mae: 2.455689, mean_q: 4.692993\n",
      "[14 50  2 68 55 50 89 23 90 14]\n",
      " 29817/50001: episode: 3313, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward:  7.000, mean reward:  0.778 [-10.000,  5.000], mean action: 49.000 [2.000, 90.000],  loss: 9.254493, mae: 2.319923, mean_q: 4.548765\n",
      "[80 16 96 69 54 52 98 10 76 12]\n",
      " 29826/50001: episode: 3314, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 41.000, mean reward:  4.556 [ 3.000, 10.000], mean action: 53.667 [10.000, 98.000],  loss: 6.086114, mae: 2.328611, mean_q: 4.533380\n",
      "[43 28 95 37 11  2 77 27 89 92]\n",
      " 29835/50001: episode: 3315, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 38.000, mean reward:  4.222 [ 3.000,  5.000], mean action: 50.889 [2.000, 95.000],  loss: 7.942859, mae: 2.348726, mean_q: 4.594296\n",
      "[17 30 37 70 55 12 70 33 94 34]\n",
      " 29844/50001: episode: 3316, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 29.000, mean reward:  3.222 [-10.000, 11.000], mean action: 48.333 [12.000, 94.000],  loss: 9.881872, mae: 2.277335, mean_q: 4.443351\n",
      "[73 28 97 15 37 78 11 88 83 46]\n",
      " 29853/50001: episode: 3317, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 41.000, mean reward:  4.556 [ 2.000,  7.000], mean action: 53.667 [11.000, 97.000],  loss: 7.126690, mae: 2.228930, mean_q: 4.305333\n",
      "[78 74 34 59 82 83 83 23 16 12]\n",
      " 29862/50001: episode: 3318, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 26.000, mean reward:  2.889 [-10.000,  9.000], mean action: 51.778 [12.000, 83.000],  loss: 9.109027, mae: 2.354018, mean_q: 4.616206\n",
      "[55 50  9 20 51 88 11 88 97 83]\n",
      " 29871/50001: episode: 3319, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 55.222 [9.000, 97.000],  loss: 5.380241, mae: 2.371870, mean_q: 4.571267\n",
      "[38 28 37 21  4 25 47 24 93 46]\n",
      " 29880/50001: episode: 3320, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 36.111 [4.000, 93.000],  loss: 5.439564, mae: 2.391954, mean_q: 4.636817\n",
      "[55 95 30 24 41 11  2 33 50 92]\n",
      " 29889/50001: episode: 3321, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 40.000, mean reward:  4.444 [ 3.000,  8.000], mean action: 42.000 [2.000, 95.000],  loss: 6.320610, mae: 2.442092, mean_q: 4.671915\n",
      "[36  3 38 88 32 93 32 96 97 97]\n",
      " 29898/50001: episode: 3322, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 12.000, mean reward:  1.333 [-10.000,  7.000], mean action: 64.000 [3.000, 97.000],  loss: 7.072634, mae: 2.538992, mean_q: 4.876181\n",
      "[37 89 60 90 40 35 65 69 31 14]\n",
      " 29907/50001: episode: 3323, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 38.000, mean reward:  4.222 [ 3.000,  7.000], mean action: 54.778 [14.000, 90.000],  loss: 5.943107, mae: 2.535537, mean_q: 4.888684\n",
      "[94 51  6 99 76 88 48 76 64 89]\n",
      " 29916/50001: episode: 3324, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 66.333 [6.000, 99.000],  loss: 9.848755, mae: 2.526925, mean_q: 4.860181\n",
      "[ 3 61 90 70 14 76 48 47 88 13]\n",
      " 29925/50001: episode: 3325, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 45.000, mean reward:  5.000 [ 2.000,  8.000], mean action: 56.333 [13.000, 90.000],  loss: 5.314398, mae: 2.460154, mean_q: 4.739279\n",
      "[22 37 55 18 12 14 35 50 97 89]\n",
      " 29934/50001: episode: 3326, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 45.222 [12.000, 97.000],  loss: 5.782965, mae: 2.463489, mean_q: 4.745047\n",
      "[54 72 85 54 98 21 57 99 29 43]\n",
      " 29943/50001: episode: 3327, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward: 20.000, mean reward:  2.222 [-10.000,  6.000], mean action: 62.000 [21.000, 99.000],  loss: 7.134283, mae: 2.574693, mean_q: 4.908805\n",
      "[52 49 89 31 82 42 86  4 60 50]\n",
      " 29952/50001: episode: 3328, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 44.000, mean reward:  4.889 [ 3.000,  9.000], mean action: 54.778 [4.000, 89.000],  loss: 9.493072, mae: 2.542312, mean_q: 4.832647\n",
      "[43  3 44 55 24 31 98 63 16 48]\n",
      " 29961/50001: episode: 3329, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 43.000, mean reward:  4.778 [ 3.000,  9.000], mean action: 42.444 [3.000, 98.000],  loss: 9.042435, mae: 2.455157, mean_q: 4.733540\n",
      "[66 53  1 95 78 62 67  3 24 67]\n",
      " 29970/50001: episode: 3330, duration: 0.087s, episode steps:   9, steps per second: 103, episode reward: 20.000, mean reward:  2.222 [-10.000,  6.000], mean action: 50.000 [1.000, 95.000],  loss: 6.459834, mae: 2.433775, mean_q: 4.719906\n",
      "[ 0 54 78 50 24 40 57  9  4 89]\n",
      " 29979/50001: episode: 3331, duration: 0.087s, episode steps:   9, steps per second: 103, episode reward: 41.000, mean reward:  4.556 [ 3.000,  6.000], mean action: 45.000 [4.000, 89.000],  loss: 8.091201, mae: 2.372562, mean_q: 4.643929\n",
      "[ 0 93 47 68 34 69 13 97 82 48]\n",
      " 29988/50001: episode: 3332, duration: 0.078s, episode steps:   9, steps per second: 116, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 61.222 [13.000, 97.000],  loss: 8.129310, mae: 2.427383, mean_q: 4.696318\n",
      "[11 40 74 98 93 24 88  2 50 73]\n",
      " 29997/50001: episode: 3333, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 60.222 [2.000, 98.000],  loss: 7.358091, mae: 2.434926, mean_q: 4.741126\n",
      "[72 95  2 28 23  8 11 46 10 63]\n",
      " 30006/50001: episode: 3334, duration: 0.069s, episode steps:   9, steps per second: 130, episode reward: 38.000, mean reward:  4.222 [ 3.000,  5.000], mean action: 31.778 [2.000, 95.000],  loss: 7.407689, mae: 2.413566, mean_q: 4.716250\n",
      "[94 11 75 30  3 50 75 96 66 88]\n",
      " 30015/50001: episode: 3335, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward: 28.000, mean reward:  3.111 [-10.000,  9.000], mean action: 54.889 [3.000, 96.000],  loss: 7.883538, mae: 2.412410, mean_q: 4.691551\n",
      "[ 5 25 93 91  1 38 42 47 95 97]\n",
      " 30024/50001: episode: 3336, duration: 0.092s, episode steps:   9, steps per second:  97, episode reward: 45.000, mean reward:  5.000 [ 2.000,  9.000], mean action: 58.778 [1.000, 97.000],  loss: 6.713775, mae: 2.399392, mean_q: 4.639827\n",
      "[77 60 74 43 60 57 78 23 88 52]\n",
      " 30033/50001: episode: 3337, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward: 26.000, mean reward:  2.889 [-10.000,  8.000], mean action: 59.444 [23.000, 88.000],  loss: 8.990712, mae: 2.419147, mean_q: 4.685808\n",
      "[31 30 40 98 75 62  1 38 24 16]\n",
      " 30042/50001: episode: 3338, duration: 0.086s, episode steps:   9, steps per second: 104, episode reward: 36.000, mean reward:  4.000 [ 2.000,  6.000], mean action: 42.667 [1.000, 98.000],  loss: 7.767532, mae: 2.375095, mean_q: 4.626116\n",
      "[60 98 47 91 62 90 20 11 69  7]\n",
      " 30051/50001: episode: 3339, duration: 0.076s, episode steps:   9, steps per second: 118, episode reward: 32.000, mean reward:  3.556 [ 2.000,  5.000], mean action: 55.000 [7.000, 98.000],  loss: 7.072055, mae: 2.461152, mean_q: 4.791044\n",
      "[92  1 91 98 31 97 27 95 14 97]\n",
      " 30060/50001: episode: 3340, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 29.000, mean reward:  3.222 [-10.000,  8.000], mean action: 61.222 [1.000, 98.000],  loss: 7.481718, mae: 2.492005, mean_q: 4.775260\n",
      "[87 84 23 62 15 99 28 75 71 46]\n",
      " 30069/50001: episode: 3341, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 37.000, mean reward:  4.111 [ 2.000,  7.000], mean action: 55.889 [15.000, 99.000],  loss: 7.038762, mae: 2.435019, mean_q: 4.700262\n",
      "[31 34 56 17 66 34 98 29 62 50]\n",
      " 30078/50001: episode: 3342, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 24.000, mean reward:  2.667 [-10.000,  9.000], mean action: 49.556 [17.000, 98.000],  loss: 3.521286, mae: 2.486215, mean_q: 4.848724\n",
      "[21 16 30 75 32 46 67 14 63 33]\n",
      " 30087/50001: episode: 3343, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 41.000, mean reward:  4.556 [ 3.000,  6.000], mean action: 41.778 [14.000, 75.000],  loss: 10.065875, mae: 2.535941, mean_q: 4.929664\n",
      "[ 4 46 47 50 46 62 13 14 69 48]\n",
      " 30096/50001: episode: 3344, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 43.889 [13.000, 69.000],  loss: 7.892626, mae: 2.470366, mean_q: 4.699755\n",
      "[67 53 37 62 66 68 34 37 42 31]\n",
      " 30105/50001: episode: 3345, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 47.778 [31.000, 68.000],  loss: 8.726988, mae: 2.448778, mean_q: 4.705370\n",
      "[63 11 25 37 91 45 82 96 66 50]\n",
      " 30114/50001: episode: 3346, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 42.000, mean reward:  4.667 [ 2.000,  9.000], mean action: 55.889 [11.000, 96.000],  loss: 7.568804, mae: 2.382725, mean_q: 4.652228\n",
      "[ 0 17 24  2 16 97  7 95 64 10]\n",
      " 30123/50001: episode: 3347, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 40.000, mean reward:  4.444 [ 2.000,  9.000], mean action: 36.889 [2.000, 97.000],  loss: 6.856332, mae: 2.346938, mean_q: 4.562023\n",
      "[28 12 66 12 31 42  0 32 57  6]\n",
      " 30132/50001: episode: 3348, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 21.000, mean reward:  2.333 [-10.000,  5.000], mean action: 28.667 [0.000, 66.000],  loss: 6.891027, mae: 2.363050, mean_q: 4.550142\n",
      "[ 1 13 34 36 92 76 13 13 94 47]\n",
      " 30141/50001: episode: 3349, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward:  7.000, mean reward:  0.778 [-10.000,  7.000], mean action: 46.444 [13.000, 94.000],  loss: 9.395741, mae: 2.413278, mean_q: 4.641981\n",
      "[99 43 35 27 60 17 73 27  5 24]\n",
      " 30150/50001: episode: 3350, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 22.000, mean reward:  2.444 [-10.000,  8.000], mean action: 34.556 [5.000, 73.000],  loss: 6.824013, mae: 2.317472, mean_q: 4.463657\n",
      "[21 60  4  4 28 36 33 12 80 49]\n",
      " 30159/50001: episode: 3351, duration: 0.073s, episode steps:   9, steps per second: 123, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 34.000 [4.000, 80.000],  loss: 6.079870, mae: 2.347237, mean_q: 4.569982\n",
      "[49 95 15 40 46  6 48 82 50 61]\n",
      " 30168/50001: episode: 3352, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 41.000, mean reward:  4.556 [ 2.000,  8.000], mean action: 49.222 [6.000, 95.000],  loss: 6.793320, mae: 2.417427, mean_q: 4.599470\n",
      "[53 48 34 14 23 28 98 21 57 79]\n",
      " 30177/50001: episode: 3353, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 37.000, mean reward:  4.111 [ 3.000,  6.000], mean action: 44.667 [14.000, 98.000],  loss: 8.217197, mae: 2.418672, mean_q: 4.671349\n",
      "[ 1 95 74 55 28 33 47 83 88 42]\n",
      " 30186/50001: episode: 3354, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 60.556 [28.000, 95.000],  loss: 9.119114, mae: 2.411458, mean_q: 4.616209\n",
      "[32 95 86 55 13 72 79 28 79 74]\n",
      " 30195/50001: episode: 3355, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 64.556 [13.000, 95.000],  loss: 5.758352, mae: 2.409302, mean_q: 4.612958\n",
      "[44 13 76 93 47 74 30 66 66 88]\n",
      " 30204/50001: episode: 3356, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 28.000, mean reward:  3.111 [-10.000,  8.000], mean action: 61.444 [13.000, 93.000],  loss: 8.309733, mae: 2.434463, mean_q: 4.670796\n",
      "[54 14 57 48 69 27 28 71 66 32]\n",
      " 30213/50001: episode: 3357, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 45.778 [14.000, 71.000],  loss: 6.454015, mae: 2.440340, mean_q: 4.726318\n",
      "[32 34 96 62 95 88 88 17  1  1]\n",
      " 30222/50001: episode: 3358, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 11.000, mean reward:  1.222 [-10.000,  7.000], mean action: 53.556 [1.000, 96.000],  loss: 8.674976, mae: 2.474349, mean_q: 4.749881\n",
      "[56 48 81 37 95 13  4 12 40  5]\n",
      " 30231/50001: episode: 3359, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 37.000, mean reward:  4.111 [ 2.000,  6.000], mean action: 37.222 [4.000, 95.000],  loss: 7.659633, mae: 2.324242, mean_q: 4.473312\n",
      "[72 37  4  2 79  5 71 30 50 89]\n",
      " 30240/50001: episode: 3360, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 40.778 [2.000, 89.000],  loss: 6.687223, mae: 2.338428, mean_q: 4.522481\n",
      "[92 51 13 30 79 57 62  5 59  9]\n",
      " 30249/50001: episode: 3361, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 40.556 [5.000, 79.000],  loss: 6.871986, mae: 2.430459, mean_q: 4.633955\n",
      "[33 52  9 45  4 23 98 83  5 24]\n",
      " 30258/50001: episode: 3362, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 38.111 [4.000, 98.000],  loss: 7.237211, mae: 2.408795, mean_q: 4.674530\n",
      "[63 26 28 19 86 84 34 46 79 23]\n",
      " 30267/50001: episode: 3363, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 41.000, mean reward:  4.556 [ 2.000,  8.000], mean action: 47.222 [19.000, 86.000],  loss: 8.546011, mae: 2.497008, mean_q: 4.733782\n",
      "[97 34 41 26 89  2 41 35  0 30]\n",
      " 30276/50001: episode: 3364, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 33.111 [0.000, 89.000],  loss: 7.393633, mae: 2.323415, mean_q: 4.478601\n",
      "[ 7  5  2 27 67 34 28 13 35 34]\n",
      " 30285/50001: episode: 3365, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 27.222 [2.000, 67.000],  loss: 7.880353, mae: 2.281862, mean_q: 4.367219\n",
      "[26 86 20 85 95 55 48 28 57  6]\n",
      " 30294/50001: episode: 3366, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 53.333 [6.000, 95.000],  loss: 6.711988, mae: 2.365619, mean_q: 4.630180\n",
      "[86 50 17 48 62 30 99 13  4 32]\n",
      " 30303/50001: episode: 3367, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 39.444 [4.000, 99.000],  loss: 8.936995, mae: 2.354230, mean_q: 4.581505\n",
      "[83 89 14 37 48 67 95 28 82 87]\n",
      " 30312/50001: episode: 3368, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 38.000, mean reward:  4.222 [ 3.000,  7.000], mean action: 60.778 [14.000, 95.000],  loss: 6.470420, mae: 2.386634, mean_q: 4.551630\n",
      "[59 24 28 59 36 64 26 50 66 81]\n",
      " 30321/50001: episode: 3369, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 48.222 [24.000, 81.000],  loss: 8.622077, mae: 2.344293, mean_q: 4.595646\n",
      "[35  2  4 34 66 17 59 37 34 98]\n",
      " 30330/50001: episode: 3370, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 39.000 [2.000, 98.000],  loss: 7.521862, mae: 2.431308, mean_q: 4.677254\n",
      "[25 64 24  1 96 14 32 27 39 73]\n",
      " 30339/50001: episode: 3371, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 33.000, mean reward:  3.667 [ 2.000,  5.000], mean action: 41.111 [1.000, 96.000],  loss: 8.854692, mae: 2.429662, mean_q: 4.668327\n",
      "[ 0 31  4  9 94 83 56 73 23 79]\n",
      " 30348/50001: episode: 3372, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 36.000, mean reward:  4.000 [ 2.000,  7.000], mean action: 50.222 [4.000, 94.000],  loss: 9.037967, mae: 2.406203, mean_q: 4.675556\n",
      "[22 72 62  4 87 95 96 88 89 18]\n",
      " 30357/50001: episode: 3373, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 67.889 [4.000, 96.000],  loss: 6.114935, mae: 2.331216, mean_q: 4.490852\n",
      "[60 93 80 85  1 30 99 48 90 50]\n",
      " 30366/50001: episode: 3374, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 64.000 [1.000, 99.000],  loss: 5.406880, mae: 2.319000, mean_q: 4.428259\n",
      "[ 6 74 34 31 91 12 86 61 93 50]\n",
      " 30375/50001: episode: 3375, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 42.000, mean reward:  4.667 [ 3.000, 10.000], mean action: 59.111 [12.000, 93.000],  loss: 8.607121, mae: 2.402864, mean_q: 4.621862\n",
      "[54 37 37 98 97 45 44 67 19 28]\n",
      " 30384/50001: episode: 3376, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 24.000, mean reward:  2.667 [-10.000,  8.000], mean action: 52.444 [19.000, 98.000],  loss: 8.646520, mae: 2.364032, mean_q: 4.479570\n",
      "[66 91 97 14 27 63 37  7 88  6]\n",
      " 30393/50001: episode: 3377, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 37.000, mean reward:  4.111 [ 1.000,  7.000], mean action: 47.778 [6.000, 97.000],  loss: 5.891892, mae: 2.372888, mean_q: 4.490983\n",
      "[26 95 37 95 28  6 10 63 13 66]\n",
      " 30402/50001: episode: 3378, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 28.000, mean reward:  3.111 [-10.000,  7.000], mean action: 45.889 [6.000, 95.000],  loss: 6.232055, mae: 2.331292, mean_q: 4.458011\n",
      "[52  1 44 44 35 10 27 31 40 30]\n",
      " 30411/50001: episode: 3379, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 29.111 [1.000, 44.000],  loss: 9.867733, mae: 2.353503, mean_q: 4.564126\n",
      "[24 12 24 66 68 18 49 98 73 93]\n",
      " 30420/50001: episode: 3380, duration: 0.086s, episode steps:   9, steps per second: 105, episode reward: 22.000, mean reward:  2.444 [-10.000,  8.000], mean action: 55.667 [12.000, 98.000],  loss: 6.891313, mae: 2.351328, mean_q: 4.534723\n",
      "[ 3 48 60  1 41 66 13 14  9 96]\n",
      " 30429/50001: episode: 3381, duration: 0.087s, episode steps:   9, steps per second: 104, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 38.667 [1.000, 96.000],  loss: 7.012085, mae: 2.425942, mean_q: 4.676421\n",
      "[14 41 40 22 97 51 95 31 40 53]\n",
      " 30438/50001: episode: 3382, duration: 0.070s, episode steps:   9, steps per second: 129, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 52.222 [22.000, 97.000],  loss: 8.799388, mae: 2.434059, mean_q: 4.661460\n",
      "[75 28 27 70 50  2 69 40 14 37]\n",
      " 30447/50001: episode: 3383, duration: 0.078s, episode steps:   9, steps per second: 116, episode reward: 41.000, mean reward:  4.556 [ 2.000,  8.000], mean action: 37.444 [2.000, 70.000],  loss: 6.683337, mae: 2.403614, mean_q: 4.622374\n",
      "[58 22 41  4 52 60 91 83 85 67]\n",
      " 30456/50001: episode: 3384, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 34.000, mean reward:  3.778 [ 3.000,  5.000], mean action: 56.111 [4.000, 91.000],  loss: 5.957377, mae: 2.461411, mean_q: 4.769534\n",
      "[31  9 66 53 19 48 53 53 95 84]\n",
      " 30465/50001: episode: 3385, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 11.000, mean reward:  1.222 [-10.000,  9.000], mean action: 53.333 [9.000, 95.000],  loss: 9.082225, mae: 2.505934, mean_q: 4.816199\n",
      "[59 34 58 95 85 60 32 38  2 59]\n",
      " 30474/50001: episode: 3386, duration: 0.078s, episode steps:   9, steps per second: 115, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 51.444 [2.000, 95.000],  loss: 6.221051, mae: 2.546384, mean_q: 4.900782\n",
      "[21 41 37 76 52 31  1 88 10 32]\n",
      " 30483/50001: episode: 3387, duration: 0.077s, episode steps:   9, steps per second: 116, episode reward: 44.000, mean reward:  4.889 [ 3.000,  7.000], mean action: 40.889 [1.000, 88.000],  loss: 8.601814, mae: 2.499332, mean_q: 4.788170\n",
      "[34 40 31 85 44 75 13  0 46 60]\n",
      " 30492/50001: episode: 3388, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 43.778 [0.000, 85.000],  loss: 8.195801, mae: 2.454167, mean_q: 4.747537\n",
      "[85 37 66 37 67 72 63 44 37 97]\n",
      " 30501/50001: episode: 3389, duration: 0.088s, episode steps:   9, steps per second: 103, episode reward:  8.000, mean reward:  0.889 [-10.000,  7.000], mean action: 57.778 [37.000, 97.000],  loss: 6.476461, mae: 2.479158, mean_q: 4.725170\n",
      "[48  4 43 95 34  6 67 66 31 37]\n",
      " 30510/50001: episode: 3390, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward: 43.000, mean reward:  4.778 [ 2.000,  8.000], mean action: 42.556 [4.000, 95.000],  loss: 6.432918, mae: 2.446930, mean_q: 4.704257\n",
      "[38 46 16 86 10 13 93 32 34 37]\n",
      " 30519/50001: episode: 3391, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward: 48.000, mean reward:  5.333 [ 3.000,  9.000], mean action: 40.778 [10.000, 93.000],  loss: 7.584936, mae: 2.418831, mean_q: 4.692905\n",
      "[73 32 56 48 56 53 38 95 55  7]\n",
      " 30528/50001: episode: 3392, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward: 21.000, mean reward:  2.333 [-10.000,  9.000], mean action: 48.889 [7.000, 95.000],  loss: 7.747694, mae: 2.450259, mean_q: 4.768874\n",
      "[26 46 34 81 13 11 42 96 34 82]\n",
      " 30537/50001: episode: 3393, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward: 22.000, mean reward:  2.444 [-10.000,  5.000], mean action: 48.778 [11.000, 96.000],  loss: 8.548936, mae: 2.496291, mean_q: 4.714103\n",
      "[46 28 37 50 84 48 19 26 41 16]\n",
      " 30546/50001: episode: 3394, duration: 0.088s, episode steps:   9, steps per second: 103, episode reward: 35.000, mean reward:  3.889 [ 2.000,  6.000], mean action: 38.778 [16.000, 84.000],  loss: 7.186515, mae: 2.479344, mean_q: 4.763824\n",
      "[79 48 90 81 50 23 20 34 40 12]\n",
      " 30555/50001: episode: 3395, duration: 0.074s, episode steps:   9, steps per second: 122, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 44.222 [12.000, 90.000],  loss: 7.315868, mae: 2.328429, mean_q: 4.503691\n",
      "[29  2 37 80 47 46 95 14 28 62]\n",
      " 30564/50001: episode: 3396, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 41.000, mean reward:  4.556 [ 2.000,  7.000], mean action: 45.667 [2.000, 95.000],  loss: 6.499876, mae: 2.413177, mean_q: 4.648047\n",
      "[40 95 34 86 62  9 98 56 28 48]\n",
      " 30573/50001: episode: 3397, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 42.000, mean reward:  4.667 [ 2.000,  8.000], mean action: 57.333 [9.000, 98.000],  loss: 8.956747, mae: 2.408596, mean_q: 4.644668\n",
      "[50 46 11 31  4  4 21 40 54 13]\n",
      " 30582/50001: episode: 3398, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 26.000, mean reward:  2.889 [-10.000,  9.000], mean action: 24.889 [4.000, 54.000],  loss: 8.637264, mae: 2.438575, mean_q: 4.698272\n",
      "[52 74 92 37 16  6 14 89  1 99]\n",
      " 30591/50001: episode: 3399, duration: 0.082s, episode steps:   9, steps per second: 110, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 47.556 [1.000, 99.000],  loss: 5.393623, mae: 2.398102, mean_q: 4.596834\n",
      "[39 96 12 56 91 98 95 98 88 50]\n",
      " 30600/50001: episode: 3400, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 31.000, mean reward:  3.444 [-10.000,  8.000], mean action: 76.000 [12.000, 98.000],  loss: 7.523390, mae: 2.401038, mean_q: 4.611938\n",
      "[57 50 25 36 23 92 88 16 69 46]\n",
      " 30609/50001: episode: 3401, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 35.000, mean reward:  3.889 [ 2.000,  6.000], mean action: 49.444 [16.000, 92.000],  loss: 7.036786, mae: 2.422388, mean_q: 4.659762\n",
      "[35 95  2 59 47 24 84 50 28 50]\n",
      " 30618/50001: episode: 3402, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 48.778 [2.000, 95.000],  loss: 7.367124, mae: 2.511306, mean_q: 4.836598\n",
      "[26 53 36 75 11 84 70  8  9 95]\n",
      " 30627/50001: episode: 3403, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 42.000, mean reward:  4.667 [ 2.000, 12.000], mean action: 49.000 [8.000, 95.000],  loss: 6.304269, mae: 2.509349, mean_q: 4.820518\n",
      "[23 48 50 99 48 77  9 96 13 97]\n",
      " 30636/50001: episode: 3404, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 59.667 [9.000, 99.000],  loss: 6.620807, mae: 2.486026, mean_q: 4.764959\n",
      "[22 13 12 62 99 17 79 31 14 32]\n",
      " 30645/50001: episode: 3405, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 37.000, mean reward:  4.111 [ 2.000,  6.000], mean action: 39.889 [12.000, 99.000],  loss: 6.282542, mae: 2.478614, mean_q: 4.768755\n",
      "[90  0 34 34 59 71 95 89 24 31]\n",
      " 30654/50001: episode: 3406, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 26.000, mean reward:  2.889 [-10.000,  8.000], mean action: 48.556 [0.000, 95.000],  loss: 9.344961, mae: 2.518603, mean_q: 4.808299\n",
      "[78 37 33 73  1  1 89 44 43 23]\n",
      " 30663/50001: episode: 3407, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 38.222 [1.000, 89.000],  loss: 7.588372, mae: 2.435646, mean_q: 4.637933\n",
      "[35 28 98 76 68 24  1 37 89 79]\n",
      " 30672/50001: episode: 3408, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 43.000, mean reward:  4.778 [ 3.000,  7.000], mean action: 55.556 [1.000, 98.000],  loss: 9.355762, mae: 2.432145, mean_q: 4.749708\n",
      "[40 34 54 16  9 61 53  9 14 44]\n",
      " 30681/50001: episode: 3409, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 21.000, mean reward:  2.333 [-10.000,  6.000], mean action: 32.667 [9.000, 61.000],  loss: 7.437307, mae: 2.320166, mean_q: 4.531366\n",
      "[31 32  1 96 51 99 31 83 12 30]\n",
      " 30690/50001: episode: 3410, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 48.333 [1.000, 99.000],  loss: 10.831119, mae: 2.380393, mean_q: 4.583380\n",
      "[26 14 10 95 30 75 28 20 28 88]\n",
      " 30699/50001: episode: 3411, duration: 0.069s, episode steps:   9, steps per second: 130, episode reward: 28.000, mean reward:  3.111 [-10.000,  8.000], mean action: 43.111 [10.000, 95.000],  loss: 5.446247, mae: 2.297130, mean_q: 4.427893\n",
      "[40 96 86 46 28 13 48 65 60 88]\n",
      " 30708/50001: episode: 3412, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 43.000, mean reward:  4.778 [ 2.000,  8.000], mean action: 58.889 [13.000, 96.000],  loss: 7.902279, mae: 2.285174, mean_q: 4.448528\n",
      "[58 95 51 45 82 76 95  1 60 31]\n",
      " 30717/50001: episode: 3413, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 59.556 [1.000, 95.000],  loss: 7.923820, mae: 2.379500, mean_q: 4.547750\n",
      "[81 30 45 63 27 60 36  8  4  6]\n",
      " 30726/50001: episode: 3414, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 35.000, mean reward:  3.889 [ 2.000,  7.000], mean action: 31.000 [4.000, 63.000],  loss: 7.704381, mae: 2.423429, mean_q: 4.654942\n",
      "[69 37 47 46 93 70 27 41 57 60]\n",
      " 30735/50001: episode: 3415, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 41.000, mean reward:  4.556 [ 2.000,  7.000], mean action: 53.111 [27.000, 93.000],  loss: 7.409091, mae: 2.442268, mean_q: 4.614987\n",
      "[94 31 12 37 86 96 51  6 62 34]\n",
      " 30744/50001: episode: 3416, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 41.000, mean reward:  4.556 [ 3.000, 10.000], mean action: 46.111 [6.000, 96.000],  loss: 7.334938, mae: 2.441215, mean_q: 4.709378\n",
      "[64 75 61 56 42 60 44 66 99 50]\n",
      " 30753/50001: episode: 3417, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 41.000, mean reward:  4.556 [ 2.000,  9.000], mean action: 61.444 [42.000, 99.000],  loss: 7.955954, mae: 2.411191, mean_q: 4.611327\n",
      "[74 46 95 88 76 57 88 40 68 23]\n",
      " 30762/50001: episode: 3418, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 64.556 [23.000, 95.000],  loss: 8.089287, mae: 2.427018, mean_q: 4.679041\n",
      "[11 94 68 13 13 98 39 24 48 88]\n",
      " 30771/50001: episode: 3419, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 28.000, mean reward:  3.111 [-10.000,  8.000], mean action: 53.889 [13.000, 98.000],  loss: 9.229130, mae: 2.353485, mean_q: 4.535516\n",
      "[70 43 82 34  1 79 74 44 59 66]\n",
      " 30780/50001: episode: 3420, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 41.000, mean reward:  4.556 [ 2.000,  8.000], mean action: 53.556 [1.000, 82.000],  loss: 8.338627, mae: 2.417532, mean_q: 4.634711\n",
      "[43 30  1 24 32 70 95 88 98 82]\n",
      " 30789/50001: episode: 3421, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 57.778 [1.000, 98.000],  loss: 7.902483, mae: 2.419863, mean_q: 4.735865\n",
      "[78 66 88 92 75 46 95 83 40 66]\n",
      " 30798/50001: episode: 3422, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 72.333 [40.000, 95.000],  loss: 9.648469, mae: 2.385345, mean_q: 4.613883\n",
      "[63 31 50  2 45 53 95  6 52 81]\n",
      " 30807/50001: episode: 3423, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 37.000, mean reward:  4.111 [ 2.000,  7.000], mean action: 46.111 [2.000, 95.000],  loss: 6.052312, mae: 2.376722, mean_q: 4.549545\n",
      "[89 43 95 32 41 58 93  2 97 69]\n",
      " 30816/50001: episode: 3424, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 38.000, mean reward:  4.222 [ 2.000,  6.000], mean action: 58.889 [2.000, 97.000],  loss: 6.336228, mae: 2.389896, mean_q: 4.579096\n",
      "[56 88  8  6 11 33 16 30  4 75]\n",
      " 30825/50001: episode: 3425, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 30.111 [4.000, 88.000],  loss: 7.921832, mae: 2.418206, mean_q: 4.678986\n",
      "[26 75 26 82  2 90 75 55 42 48]\n",
      " 30834/50001: episode: 3426, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 14.000, mean reward:  1.556 [-10.000,  8.000], mean action: 55.000 [2.000, 90.000],  loss: 9.026515, mae: 2.412254, mean_q: 4.613327\n",
      "[87 43 42 37 82 93 36 62 40 12]\n",
      " 30843/50001: episode: 3427, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 41.000, mean reward:  4.556 [ 2.000, 10.000], mean action: 49.667 [12.000, 93.000],  loss: 8.491603, mae: 2.435668, mean_q: 4.615653\n",
      "[ 1 28 48 84 54 60 74 14 40 72]\n",
      " 30852/50001: episode: 3428, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 34.000, mean reward:  3.778 [ 2.000,  5.000], mean action: 52.667 [14.000, 84.000],  loss: 8.174853, mae: 2.451132, mean_q: 4.707833\n",
      "[44 95 69 14 57 98 95 51 51 16]\n",
      " 30861/50001: episode: 3429, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward:  8.000, mean reward:  0.889 [-10.000,  5.000], mean action: 60.667 [14.000, 98.000],  loss: 5.982990, mae: 2.479434, mean_q: 4.816205\n",
      "[71 83 32 80 31 15 88 82 98 32]\n",
      " 30870/50001: episode: 3430, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 60.111 [15.000, 98.000],  loss: 6.239696, mae: 2.534150, mean_q: 4.887204\n",
      "[20 30 98 27 14 66 99 73 12 77]\n",
      " 30879/50001: episode: 3431, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 36.000, mean reward:  4.000 [ 2.000,  7.000], mean action: 55.111 [12.000, 99.000],  loss: 9.139075, mae: 2.509904, mean_q: 4.799700\n",
      "[98 99 28 27 98 59 15 82 31 97]\n",
      " 30888/50001: episode: 3432, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 24.000, mean reward:  2.667 [-10.000,  8.000], mean action: 59.556 [15.000, 99.000],  loss: 5.542312, mae: 2.525749, mean_q: 4.862934\n",
      "[62 37 76  2 95 47 49 28 28  7]\n",
      " 30897/50001: episode: 3433, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 41.000 [2.000, 95.000],  loss: 7.435456, mae: 2.507398, mean_q: 4.909073\n",
      "[39 14 27 66 37 16 13 76 86 71]\n",
      " 30906/50001: episode: 3434, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 35.000, mean reward:  3.889 [ 3.000,  6.000], mean action: 45.111 [13.000, 86.000],  loss: 4.552720, mae: 2.524249, mean_q: 4.918086\n",
      "[42 96 92 47 41 57  4 13 79 79]\n",
      " 30915/50001: episode: 3435, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 56.444 [4.000, 96.000],  loss: 8.384513, mae: 2.578102, mean_q: 5.015473\n",
      "[ 1 97 98 32 97 88 11 13 46 46]\n",
      " 30924/50001: episode: 3436, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 11.000, mean reward:  1.222 [-10.000,  6.000], mean action: 58.667 [11.000, 98.000],  loss: 10.230536, mae: 2.557217, mean_q: 4.884863\n",
      "[49 60 39 57 95 43 23 60 98 46]\n",
      " 30933/50001: episode: 3437, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 57.889 [23.000, 98.000],  loss: 7.024549, mae: 2.339391, mean_q: 4.567472\n",
      "[63 87 66  4 69 42  4 25 62 47]\n",
      " 30942/50001: episode: 3438, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 23.000, mean reward:  2.556 [-10.000,  8.000], mean action: 45.111 [4.000, 87.000],  loss: 8.559854, mae: 2.346136, mean_q: 4.543369\n",
      "[65 46 32 24 41 14 55 77 60 34]\n",
      " 30951/50001: episode: 3439, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 44.000, mean reward:  4.889 [ 3.000, 10.000], mean action: 42.556 [14.000, 77.000],  loss: 8.140975, mae: 2.368339, mean_q: 4.582141\n",
      "[58 30 55 40 21 57  2 94 27 68]\n",
      " 30960/50001: episode: 3440, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 43.778 [2.000, 94.000],  loss: 5.839116, mae: 2.359946, mean_q: 4.528340\n",
      "[62 92 23 23  8  2 53 63  5 10]\n",
      " 30969/50001: episode: 3441, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 21.000, mean reward:  2.333 [-10.000,  5.000], mean action: 31.000 [2.000, 92.000],  loss: 8.698933, mae: 2.341551, mean_q: 4.570122\n",
      "[69 54  2  2 30 50 23 50 93 63]\n",
      " 30978/50001: episode: 3442, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 10.000, mean reward:  1.111 [-10.000,  6.000], mean action: 40.778 [2.000, 93.000],  loss: 9.102242, mae: 2.390894, mean_q: 4.640731\n",
      "[ 7 37 41  9 99 75 67  2 28 16]\n",
      " 30987/50001: episode: 3443, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 41.000, mean reward:  4.556 [ 3.000,  6.000], mean action: 41.556 [2.000, 99.000],  loss: 8.253654, mae: 2.320380, mean_q: 4.508797\n",
      "[63 52 17 17 37 63 86 48 34 10]\n",
      " 30996/50001: episode: 3444, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 15.000, mean reward:  1.667 [-10.000,  9.000], mean action: 40.444 [10.000, 86.000],  loss: 9.163618, mae: 2.300929, mean_q: 4.463122\n",
      "[98 85 24 85 61  6 31  2 78  6]\n",
      " 31005/50001: episode: 3445, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward:  8.000, mean reward:  0.889 [-10.000,  6.000], mean action: 42.000 [2.000, 85.000],  loss: 5.111676, mae: 2.335513, mean_q: 4.567420\n",
      "[16 62 67 92 50  8 89 88  8 94]\n",
      " 31014/50001: episode: 3446, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 62.000 [8.000, 94.000],  loss: 6.370450, mae: 2.260829, mean_q: 4.379162\n",
      "[21 44 60 21 98 88 41 78 53 53]\n",
      " 31023/50001: episode: 3447, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward:  9.000, mean reward:  1.000 [-10.000,  6.000], mean action: 59.556 [21.000, 98.000],  loss: 5.974953, mae: 2.403894, mean_q: 4.707049\n",
      "[19 42  9 49 97 34 51 49 79 31]\n",
      " 31032/50001: episode: 3448, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 26.000, mean reward:  2.889 [-10.000,  6.000], mean action: 49.000 [9.000, 97.000],  loss: 7.569474, mae: 2.456932, mean_q: 4.788858\n",
      "[26 46 30 36 46 95 95 13  0 99]\n",
      " 31041/50001: episode: 3449, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 12.000, mean reward:  1.333 [-10.000,  7.000], mean action: 51.111 [0.000, 99.000],  loss: 9.666023, mae: 2.478345, mean_q: 4.863585\n",
      "[56 37 89 56 44 25 99 88 13 28]\n",
      " 31050/50001: episode: 3450, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 53.222 [13.000, 99.000],  loss: 7.716200, mae: 2.451981, mean_q: 4.754781\n",
      "[54 57 60 56 59  4 14 14 90 46]\n",
      " 31059/50001: episode: 3451, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 44.444 [4.000, 90.000],  loss: 7.286837, mae: 2.388382, mean_q: 4.592888\n",
      "[39 14 95 90  9 46 14 12 24  1]\n",
      " 31068/50001: episode: 3452, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 29.000, mean reward:  3.222 [-10.000,  7.000], mean action: 33.889 [1.000, 95.000],  loss: 7.498107, mae: 2.432897, mean_q: 4.711737\n",
      "[ 1 58 32 76 13 69 52  4 69 88]\n",
      " 31077/50001: episode: 3453, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 51.222 [4.000, 88.000],  loss: 8.128019, mae: 2.469697, mean_q: 4.763323\n",
      "[69 34 13 31 89 43 95 98 24 31]\n",
      " 31086/50001: episode: 3454, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 50.889 [13.000, 98.000],  loss: 9.494340, mae: 2.420947, mean_q: 4.714303\n",
      "[96 75 76 46 45 28 37  1  4  1]\n",
      " 31095/50001: episode: 3455, duration: 0.071s, episode steps:   9, steps per second: 127, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 34.778 [1.000, 76.000],  loss: 7.803154, mae: 2.383560, mean_q: 4.674575\n",
      "[48 24 76 34 68 24 57 31 14 90]\n",
      " 31104/50001: episode: 3456, duration: 0.069s, episode steps:   9, steps per second: 131, episode reward: 24.000, mean reward:  2.667 [-10.000,  5.000], mean action: 46.444 [14.000, 90.000],  loss: 7.211851, mae: 2.378212, mean_q: 4.598204\n",
      "[37 62 60 85  6 12 66 75 66 76]\n",
      " 31113/50001: episode: 3457, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 56.444 [6.000, 85.000],  loss: 7.067877, mae: 2.350617, mean_q: 4.580007\n",
      "[33 89 30  4 19 43 88  9 50 66]\n",
      " 31122/50001: episode: 3458, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 43.000, mean reward:  4.778 [ 2.000,  8.000], mean action: 44.222 [4.000, 89.000],  loss: 5.750633, mae: 2.436761, mean_q: 4.629611\n",
      "[22 94 49 48  9 33 98 63 50 88]\n",
      " 31131/50001: episode: 3459, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 43.000, mean reward:  4.778 [ 2.000,  8.000], mean action: 59.111 [9.000, 98.000],  loss: 8.474050, mae: 2.450793, mean_q: 4.632608\n",
      "[47 89 37 74 95  4 83 45 60 48]\n",
      " 31140/50001: episode: 3460, duration: 0.068s, episode steps:   9, steps per second: 133, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 59.444 [4.000, 95.000],  loss: 8.313964, mae: 2.428807, mean_q: 4.726112\n",
      "[34 74 47 39 98 49 52 57  4 57]\n",
      " 31149/50001: episode: 3461, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 53.000 [4.000, 98.000],  loss: 7.425555, mae: 2.387323, mean_q: 4.581185\n",
      "[60 46 28 35 42 26  4  6 36 96]\n",
      " 31158/50001: episode: 3462, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 34.000, mean reward:  3.778 [ 2.000,  6.000], mean action: 35.444 [4.000, 96.000],  loss: 8.624577, mae: 2.360926, mean_q: 4.619130\n",
      "[49 76 66 90 35 37 98 98 37 37]\n",
      " 31167/50001: episode: 3463, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward: -5.000, mean reward: -0.556 [-10.000,  6.000], mean action: 63.778 [35.000, 98.000],  loss: 5.995874, mae: 2.430754, mean_q: 4.653646\n",
      "[ 8  1  8 16 81 79 59 88 93 52]\n",
      " 31176/50001: episode: 3464, duration: 0.077s, episode steps:   9, steps per second: 117, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 53.000 [1.000, 93.000],  loss: 8.088866, mae: 2.421431, mean_q: 4.755654\n",
      "[96 79 79 41 33 60  1 41 57 37]\n",
      " 31185/50001: episode: 3465, duration: 0.083s, episode steps:   9, steps per second: 109, episode reward: 17.000, mean reward:  1.889 [-10.000,  9.000], mean action: 47.556 [1.000, 79.000],  loss: 6.017001, mae: 2.413907, mean_q: 4.666071\n",
      "[26 24  9 36 31 78 95 76 93  8]\n",
      " 31194/50001: episode: 3466, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 39.000, mean reward:  4.333 [ 2.000,  8.000], mean action: 50.000 [8.000, 95.000],  loss: 8.716587, mae: 2.382924, mean_q: 4.611686\n",
      "[23 27  1 12 89 17 52 44 85 40]\n",
      " 31203/50001: episode: 3467, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 35.000, mean reward:  3.889 [ 2.000,  7.000], mean action: 40.778 [1.000, 89.000],  loss: 8.593852, mae: 2.423556, mean_q: 4.706530\n",
      "[22  8 12 70 61 47 63 50 64 31]\n",
      " 31212/50001: episode: 3468, duration: 0.070s, episode steps:   9, steps per second: 129, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 45.111 [8.000, 70.000],  loss: 5.871707, mae: 2.462140, mean_q: 4.680636\n",
      "[86 11 32 84 55  9 81 84 88 66]\n",
      " 31221/50001: episode: 3469, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 27.000, mean reward:  3.000 [-10.000,  8.000], mean action: 56.667 [9.000, 88.000],  loss: 8.481872, mae: 2.525783, mean_q: 4.856418\n",
      "[69 36 62 66 59 85 27 30  1 67]\n",
      " 31230/50001: episode: 3470, duration: 0.068s, episode steps:   9, steps per second: 133, episode reward: 38.000, mean reward:  4.222 [ 2.000,  8.000], mean action: 48.111 [1.000, 85.000],  loss: 5.263927, mae: 2.483181, mean_q: 4.731428\n",
      "[28  9 95  2  4 15 95 42 96 69]\n",
      " 31239/50001: episode: 3471, duration: 0.083s, episode steps:   9, steps per second: 109, episode reward: 20.000, mean reward:  2.222 [-10.000,  5.000], mean action: 47.444 [2.000, 96.000],  loss: 8.325405, mae: 2.490808, mean_q: 4.798460\n",
      "[ 7 13 90 28 88 30 88  2 67 74]\n",
      " 31248/50001: episode: 3472, duration: 0.087s, episode steps:   9, steps per second: 104, episode reward: 26.000, mean reward:  2.889 [-10.000,  6.000], mean action: 53.333 [2.000, 90.000],  loss: 5.393521, mae: 2.503325, mean_q: 4.816113\n",
      "[70 50 37  4 68  4 95 31 75 50]\n",
      " 31257/50001: episode: 3473, duration: 0.075s, episode steps:   9, steps per second: 119, episode reward: 13.000, mean reward:  1.444 [-10.000,  7.000], mean action: 46.000 [4.000, 95.000],  loss: 7.984669, mae: 2.507810, mean_q: 4.831660\n",
      "[76 96 89 14 53 68 95 33 12 31]\n",
      " 31266/50001: episode: 3474, duration: 0.072s, episode steps:   9, steps per second: 125, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 54.556 [12.000, 96.000],  loss: 8.043446, mae: 2.513447, mean_q: 4.825572\n",
      "[44 34 32 62 37 70 88 92 48 93]\n",
      " 31275/50001: episode: 3475, duration: 0.087s, episode steps:   9, steps per second: 104, episode reward: 42.000, mean reward:  4.667 [ 2.000,  7.000], mean action: 61.778 [32.000, 93.000],  loss: 7.326252, mae: 2.494838, mean_q: 4.799647\n",
      "[25 20 30 87 75 88  2 51 64 50]\n",
      " 31284/50001: episode: 3476, duration: 0.076s, episode steps:   9, steps per second: 119, episode reward: 42.000, mean reward:  4.667 [ 3.000,  9.000], mean action: 51.889 [2.000, 88.000],  loss: 9.550558, mae: 2.400555, mean_q: 4.586048\n",
      "[35  5 54 25 19 86 62 80 98 79]\n",
      " 31293/50001: episode: 3477, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 38.000, mean reward:  4.222 [ 3.000,  8.000], mean action: 56.444 [5.000, 98.000],  loss: 6.950172, mae: 2.394081, mean_q: 4.705931\n",
      "[89 89 31 81 93 90 34 38 43 69]\n",
      " 31302/50001: episode: 3478, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 63.111 [31.000, 93.000],  loss: 6.589317, mae: 2.415185, mean_q: 4.640626\n",
      "[27 51 84 75 74 34 95 32 40 40]\n",
      " 31311/50001: episode: 3479, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 28.000, mean reward:  3.111 [-10.000,  7.000], mean action: 58.333 [32.000, 95.000],  loss: 4.967826, mae: 2.478734, mean_q: 4.760077\n",
      "[36 96 37  1 56 74 40 92 46 89]\n",
      " 31320/50001: episode: 3480, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 59.000 [1.000, 96.000],  loss: 5.670466, mae: 2.484934, mean_q: 4.781543\n",
      "[13 60 23 86 93 37 74 12  2 88]\n",
      " 31329/50001: episode: 3481, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 40.000, mean reward:  4.444 [ 2.000,  6.000], mean action: 52.778 [2.000, 93.000],  loss: 5.790391, mae: 2.529240, mean_q: 4.814202\n",
      "[79 51 60 21 63 44 14 89 50 69]\n",
      " 31338/50001: episode: 3482, duration: 0.071s, episode steps:   9, steps per second: 127, episode reward: 39.000, mean reward:  4.333 [ 3.000,  8.000], mean action: 51.222 [14.000, 89.000],  loss: 8.279943, mae: 2.646780, mean_q: 4.994630\n",
      "[68  8 87 46 95 18 34 27 98 29]\n",
      " 31347/50001: episode: 3483, duration: 0.087s, episode steps:   9, steps per second: 104, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 49.111 [8.000, 98.000],  loss: 6.109337, mae: 2.463522, mean_q: 4.686158\n",
      "[62 34 94 34 81 14  4 82 31 13]\n",
      " 31356/50001: episode: 3484, duration: 0.074s, episode steps:   9, steps per second: 122, episode reward: 27.000, mean reward:  3.000 [-10.000,  8.000], mean action: 43.000 [4.000, 94.000],  loss: 8.659397, mae: 2.581349, mean_q: 4.963957\n",
      "[12  5 32 95 88 82  0 95 12 97]\n",
      " 31365/50001: episode: 3485, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward:  9.000, mean reward:  1.000 [-10.000,  7.000], mean action: 56.222 [0.000, 97.000],  loss: 11.855749, mae: 2.527503, mean_q: 4.837266\n",
      "[13 88 60 48 96 63 50 88  4 37]\n",
      " 31374/50001: episode: 3486, duration: 0.073s, episode steps:   9, steps per second: 123, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 59.333 [4.000, 96.000],  loss: 6.689794, mae: 2.352994, mean_q: 4.526931\n",
      "[ 0 98 60 13 41 92 10 79 88 21]\n",
      " 31383/50001: episode: 3487, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 55.778 [10.000, 98.000],  loss: 7.613703, mae: 2.350638, mean_q: 4.568389\n",
      "[99 94 77 40  4 34 95 94 68 73]\n",
      " 31392/50001: episode: 3488, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 21.000, mean reward:  2.333 [-10.000,  7.000], mean action: 64.333 [4.000, 95.000],  loss: 8.171089, mae: 2.357572, mean_q: 4.574536\n",
      "[76 52  0 50 24 67 51 95 79 12]\n",
      " 31401/50001: episode: 3489, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 45.000, mean reward:  5.000 [ 2.000,  8.000], mean action: 47.778 [0.000, 95.000],  loss: 6.241671, mae: 2.425269, mean_q: 4.619320\n",
      "[ 3 88 95 13 42 79 65 99  4 12]\n",
      " 31410/50001: episode: 3490, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 42.000, mean reward:  4.667 [ 2.000,  8.000], mean action: 55.222 [4.000, 99.000],  loss: 5.953886, mae: 2.480733, mean_q: 4.697455\n",
      "[54 33 77 34 93 70  2 95 57 79]\n",
      " 31419/50001: episode: 3491, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 43.000, mean reward:  4.778 [ 2.000,  8.000], mean action: 60.000 [2.000, 95.000],  loss: 7.970730, mae: 2.400211, mean_q: 4.549776\n",
      "[24 14 67 17 64 70 24 46 67 54]\n",
      " 31428/50001: episode: 3492, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward:  5.000, mean reward:  0.556 [-10.000,  6.000], mean action: 47.000 [14.000, 70.000],  loss: 5.767488, mae: 2.469459, mean_q: 4.692694\n",
      "[ 8 53 11 79  3 18 95  4 23 42]\n",
      " 31437/50001: episode: 3493, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 39.000, mean reward:  4.333 [ 2.000,  8.000], mean action: 36.444 [3.000, 95.000],  loss: 5.751779, mae: 2.534569, mean_q: 4.802905\n",
      "[49 62 28 11 90  2 18 95 50 77]\n",
      " 31446/50001: episode: 3494, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 48.111 [2.000, 95.000],  loss: 4.374615, mae: 2.564133, mean_q: 4.867875\n",
      "[91 92 60 84 56 10 95 69  4  4]\n",
      " 31455/50001: episode: 3495, duration: 0.077s, episode steps:   9, steps per second: 117, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 52.667 [4.000, 95.000],  loss: 8.587353, mae: 2.632946, mean_q: 5.020089\n",
      "[66  5 31 13 62 59 39 95 62 52]\n",
      " 31464/50001: episode: 3496, duration: 0.073s, episode steps:   9, steps per second: 123, episode reward: 24.000, mean reward:  2.667 [-10.000,  9.000], mean action: 46.444 [5.000, 95.000],  loss: 6.024188, mae: 2.573192, mean_q: 4.881282\n",
      "[67 44 14 67  2 81 24 67 83 97]\n",
      " 31473/50001: episode: 3497, duration: 0.074s, episode steps:   9, steps per second: 122, episode reward: 11.000, mean reward:  1.222 [-10.000,  8.000], mean action: 53.222 [2.000, 97.000],  loss: 5.009336, mae: 2.558576, mean_q: 4.902819\n",
      "[21 32 54 95 75 57 68 97 28 28]\n",
      " 31482/50001: episode: 3498, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 27.000, mean reward:  3.000 [-10.000,  6.000], mean action: 59.333 [28.000, 97.000],  loss: 9.597672, mae: 2.523341, mean_q: 4.838442\n",
      "[97 60 67 73 14 33 42 96 65 88]\n",
      " 31491/50001: episode: 3499, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 37.000, mean reward:  4.111 [ 2.000,  9.000], mean action: 59.778 [14.000, 96.000],  loss: 6.945028, mae: 2.594628, mean_q: 4.914061\n",
      "[70  6  2 56 75 97 30 13 34 13]\n",
      " 31500/50001: episode: 3500, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 30.000, mean reward:  3.333 [-10.000,  8.000], mean action: 36.222 [2.000, 97.000],  loss: 8.485160, mae: 2.511687, mean_q: 4.744783\n",
      "[15 52 88 74 77 31 38  8 31 24]\n",
      " 31509/50001: episode: 3501, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 21.000, mean reward:  2.333 [-10.000,  7.000], mean action: 47.000 [8.000, 88.000],  loss: 6.246605, mae: 2.439091, mean_q: 4.688913\n",
      "[45 16 67 93 49 96 37 54 44 97]\n",
      " 31518/50001: episode: 3502, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 40.000, mean reward:  4.444 [ 3.000,  9.000], mean action: 61.444 [16.000, 97.000],  loss: 6.761508, mae: 2.425848, mean_q: 4.606558\n",
      "[74 31 99 64 95 23 70  1 44 84]\n",
      " 31527/50001: episode: 3503, duration: 0.071s, episode steps:   9, steps per second: 127, episode reward: 36.000, mean reward:  4.000 [ 2.000,  6.000], mean action: 56.778 [1.000, 99.000],  loss: 7.758193, mae: 2.480296, mean_q: 4.674645\n",
      "[29 71 47 38 69 32 34 34 61 48]\n",
      " 31536/50001: episode: 3504, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 26.000, mean reward:  2.889 [-10.000,  8.000], mean action: 48.222 [32.000, 71.000],  loss: 9.912759, mae: 2.458484, mean_q: 4.678815\n",
      "[14 95 76 14 31 76 18 88 37  1]\n",
      " 31545/50001: episode: 3505, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 17.000, mean reward:  1.889 [-10.000,  8.000], mean action: 48.444 [1.000, 95.000],  loss: 9.699368, mae: 2.368845, mean_q: 4.495542\n",
      "[75 31 89 13 31 13 92 93 75 55]\n",
      " 31554/50001: episode: 3506, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: -8.000, mean reward: -0.889 [-10.000,  5.000], mean action: 54.667 [13.000, 93.000],  loss: 7.651570, mae: 2.285844, mean_q: 4.398955\n",
      "[70 53 28 35  0  2 76 28 32 23]\n",
      " 31563/50001: episode: 3507, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 30.778 [0.000, 76.000],  loss: 7.912081, mae: 2.331740, mean_q: 4.492470\n",
      "[49 28 32 20 63  2 82 50 34 50]\n",
      " 31572/50001: episode: 3508, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 29.000, mean reward:  3.222 [-10.000,  8.000], mean action: 40.111 [2.000, 82.000],  loss: 7.603889, mae: 2.262493, mean_q: 4.385687\n",
      "[72 41 37 59 48 91 98 81 36 14]\n",
      " 31581/50001: episode: 3509, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 56.111 [14.000, 98.000],  loss: 7.085895, mae: 2.351276, mean_q: 4.498085\n",
      "[32 92 80  4 95  4 82 31 13 99]\n",
      " 31590/50001: episode: 3510, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 55.556 [4.000, 99.000],  loss: 7.657082, mae: 2.443704, mean_q: 4.681794\n",
      "[87 14 98 84 99 21 35 88 90 81]\n",
      " 31599/50001: episode: 3511, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 38.000, mean reward:  4.222 [ 3.000,  7.000], mean action: 67.778 [14.000, 99.000],  loss: 8.743460, mae: 2.430594, mean_q: 4.694643\n",
      "[23 50 32 20 31 13 23 23 67 79]\n",
      " 31608/50001: episode: 3512, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward:  9.000, mean reward:  1.000 [-10.000,  6.000], mean action: 37.556 [13.000, 79.000],  loss: 6.014728, mae: 2.381270, mean_q: 4.632405\n",
      "[56 31 27 35 60 33 83 14 13 12]\n",
      " 31617/50001: episode: 3513, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 34.222 [12.000, 83.000],  loss: 6.081236, mae: 2.479395, mean_q: 4.727809\n",
      "[90 86 89 69 56 29  4  1 14 95]\n",
      " 31626/50001: episode: 3514, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 46.000, mean reward:  5.111 [ 3.000, 10.000], mean action: 49.222 [1.000, 95.000],  loss: 6.691555, mae: 2.465378, mean_q: 4.670526\n",
      "[25  5 82 98 30 93 42 95 31 37]\n",
      " 31635/50001: episode: 3515, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 47.000, mean reward:  5.222 [ 3.000,  8.000], mean action: 57.000 [5.000, 98.000],  loss: 5.077638, mae: 2.478082, mean_q: 4.736559\n",
      "[60 95 89 23 12  5 12 13 30 83]\n",
      " 31644/50001: episode: 3516, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 40.222 [5.000, 95.000],  loss: 8.551255, mae: 2.528824, mean_q: 4.781487\n",
      "[99  5 30 34 34 96 28  9 30 16]\n",
      " 31653/50001: episode: 3517, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 10.000, mean reward:  1.111 [-10.000,  6.000], mean action: 31.333 [5.000, 96.000],  loss: 6.448790, mae: 2.587960, mean_q: 4.971857\n",
      "[ 0 95 53 27 41 60 10 50 50 52]\n",
      " 31662/50001: episode: 3518, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 48.667 [10.000, 95.000],  loss: 7.915911, mae: 2.578877, mean_q: 4.889392\n",
      "[62  8 71 32 67 25 76 29 50 42]\n",
      " 31671/50001: episode: 3519, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 38.000, mean reward:  4.222 [ 2.000,  9.000], mean action: 44.444 [8.000, 76.000],  loss: 8.977737, mae: 2.513999, mean_q: 4.775375\n",
      "[13 28 99 13 14 23 97  1 55  1]\n",
      " 31680/50001: episode: 3520, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward:  6.000, mean reward:  0.667 [-10.000,  5.000], mean action: 36.778 [1.000, 99.000],  loss: 7.005398, mae: 2.448611, mean_q: 4.667923\n",
      "[86 50 75  1 53 56 97 69 94 54]\n",
      " 31689/50001: episode: 3521, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 35.000, mean reward:  3.889 [ 3.000,  5.000], mean action: 61.000 [1.000, 97.000],  loss: 7.428475, mae: 2.417345, mean_q: 4.694887\n",
      "[ 5 52 53 53 42 13 86 66 31 65]\n",
      " 31698/50001: episode: 3522, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 51.222 [13.000, 86.000],  loss: 5.994596, mae: 2.456178, mean_q: 4.746490\n",
      "[93 15 24 83 56 78 37 50 89 31]\n",
      " 31707/50001: episode: 3523, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 41.000, mean reward:  4.556 [ 2.000,  7.000], mean action: 51.444 [15.000, 89.000],  loss: 6.797609, mae: 2.533961, mean_q: 4.852078\n",
      "[25 35 50 59 73 13 69 37 14 51]\n",
      " 31716/50001: episode: 3524, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 44.556 [13.000, 73.000],  loss: 8.644437, mae: 2.467464, mean_q: 4.688857\n",
      "[27 30 12 18 79 31 27 89 50 50]\n",
      " 31725/50001: episode: 3525, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 10.000, mean reward:  1.111 [-10.000,  8.000], mean action: 42.889 [12.000, 89.000],  loss: 6.688560, mae: 2.495313, mean_q: 4.686532\n",
      "[79 51 97 93 40 21  8 95 50 27]\n",
      " 31734/50001: episode: 3526, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 53.556 [8.000, 97.000],  loss: 6.853765, mae: 2.444090, mean_q: 4.683466\n",
      "[ 3 85 29 31 20 29 18  1 88 79]\n",
      " 31743/50001: episode: 3527, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 29.000, mean reward:  3.222 [-10.000,  8.000], mean action: 42.222 [1.000, 88.000],  loss: 9.493244, mae: 2.398445, mean_q: 4.577873\n",
      "[32 31 65 86 50 82 66 34 31 34]\n",
      " 31752/50001: episode: 3528, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 12.000, mean reward:  1.333 [-10.000,  8.000], mean action: 53.222 [31.000, 86.000],  loss: 7.213639, mae: 2.378061, mean_q: 4.555672\n",
      "[80 82 96 95 12 43 34 29 12 48]\n",
      " 31761/50001: episode: 3529, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 50.111 [12.000, 96.000],  loss: 7.098490, mae: 2.315895, mean_q: 4.452087\n",
      "[ 9 96 47 94 31 67 96 50 85 62]\n",
      " 31770/50001: episode: 3530, duration: 0.070s, episode steps:   9, steps per second: 129, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 69.778 [31.000, 96.000],  loss: 6.254619, mae: 2.404438, mean_q: 4.621381\n",
      "[29 59 37 37 61  4 67 34 77 50]\n",
      " 31779/50001: episode: 3531, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 28.000, mean reward:  3.111 [-10.000,  8.000], mean action: 47.333 [4.000, 77.000],  loss: 8.545628, mae: 2.438077, mean_q: 4.659275\n",
      "[20 95 31 46 15 48 33 46 37 31]\n",
      " 31788/50001: episode: 3532, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 12.000, mean reward:  1.333 [-10.000,  8.000], mean action: 42.444 [15.000, 95.000],  loss: 5.719450, mae: 2.446567, mean_q: 4.721394\n",
      "[50 40 93 98 59 75 30 46 42 37]\n",
      " 31797/50001: episode: 3533, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 42.000, mean reward:  4.667 [ 3.000,  9.000], mean action: 57.778 [30.000, 98.000],  loss: 9.070049, mae: 2.433762, mean_q: 4.611109\n",
      "[53 69 37 48 64 48 93 88 88 50]\n",
      " 31806/50001: episode: 3534, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward: 13.000, mean reward:  1.444 [-10.000,  8.000], mean action: 65.000 [37.000, 93.000],  loss: 8.086669, mae: 2.429579, mean_q: 4.669211\n",
      "[23 28 35 55 63 89 33 37 66 46]\n",
      " 31815/50001: episode: 3535, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 43.000, mean reward:  4.778 [ 2.000,  8.000], mean action: 50.222 [28.000, 89.000],  loss: 6.876284, mae: 2.373938, mean_q: 4.546330\n",
      "[47 37 34 34 98 49 50 60 64 75]\n",
      " 31824/50001: episode: 3536, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 26.000, mean reward:  2.889 [-10.000,  6.000], mean action: 55.667 [34.000, 98.000],  loss: 5.685866, mae: 2.415201, mean_q: 4.754883\n",
      "[96 54 95 14 39 46 57 97 87 69]\n",
      " 31833/50001: episode: 3537, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 62.000 [14.000, 97.000],  loss: 7.741372, mae: 2.428245, mean_q: 4.624674\n",
      "[95  8 52 95 50 13 32 97 89 42]\n",
      " 31842/50001: episode: 3538, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 23.000, mean reward:  2.556 [-10.000,  5.000], mean action: 53.111 [8.000, 97.000],  loss: 5.838981, mae: 2.523563, mean_q: 4.906547\n",
      "[33 31 41 49 95 50 48 43 93  6]\n",
      " 31851/50001: episode: 3539, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 39.000, mean reward:  4.333 [ 2.000,  6.000], mean action: 50.667 [6.000, 95.000],  loss: 7.876130, mae: 2.467678, mean_q: 4.728345\n",
      "[50 45 50 10 48 48 23 83 27 24]\n",
      " 31860/50001: episode: 3540, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward:  8.000, mean reward:  0.889 [-10.000,  7.000], mean action: 39.778 [10.000, 83.000],  loss: 6.986094, mae: 2.549853, mean_q: 4.949059\n",
      "[90 34 48 32 80 88  2 60 53 17]\n",
      " 31869/50001: episode: 3541, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 35.000, mean reward:  3.889 [ 2.000,  5.000], mean action: 46.000 [2.000, 88.000],  loss: 9.216674, mae: 2.492233, mean_q: 4.746938\n",
      "[89 23 49 34 95 30 94 34 98 46]\n",
      " 31878/50001: episode: 3542, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 26.000, mean reward:  2.889 [-10.000,  6.000], mean action: 55.889 [23.000, 98.000],  loss: 6.363775, mae: 2.445937, mean_q: 4.712591\n",
      "[18 16 54 91 89 84 21 95  4 97]\n",
      " 31887/50001: episode: 3543, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 44.000, mean reward:  4.889 [ 3.000,  9.000], mean action: 61.222 [4.000, 97.000],  loss: 7.711606, mae: 2.384848, mean_q: 4.640997\n",
      "[63 15  2 23 42  1 78 14 71 39]\n",
      " 31896/50001: episode: 3544, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 32.000, mean reward:  3.556 [ 2.000,  5.000], mean action: 31.667 [1.000, 78.000],  loss: 9.089164, mae: 2.414818, mean_q: 4.677930\n",
      "[43 74 64 14 95 42 89 76 67 74]\n",
      " 31905/50001: episode: 3545, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 21.000, mean reward:  2.333 [-10.000,  6.000], mean action: 66.111 [14.000, 95.000],  loss: 8.651274, mae: 2.498095, mean_q: 4.713264\n",
      "[85 62  6 97 98 42 45 37 72 50]\n",
      " 31914/50001: episode: 3546, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 42.000, mean reward:  4.667 [ 2.000,  9.000], mean action: 56.556 [6.000, 98.000],  loss: 6.583056, mae: 2.399349, mean_q: 4.569052\n",
      "[56 13  6 30 42 85 44 40 74 66]\n",
      " 31923/50001: episode: 3547, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 40.000, mean reward:  4.444 [ 3.000,  8.000], mean action: 44.444 [6.000, 85.000],  loss: 7.151509, mae: 2.434786, mean_q: 4.691031\n",
      "[88 34 31 87 82 48 69 13  3  4]\n",
      " 31932/50001: episode: 3548, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 41.222 [3.000, 87.000],  loss: 7.122592, mae: 2.426185, mean_q: 4.669310\n",
      "[50  1 26 11 44 92 17 95 88 25]\n",
      " 31941/50001: episode: 3549, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 36.000, mean reward:  4.000 [ 2.000,  9.000], mean action: 44.333 [1.000, 95.000],  loss: 7.103137, mae: 2.442173, mean_q: 4.671462\n",
      "[90 92 40 90 98 32 14 60 73 75]\n",
      " 31950/50001: episode: 3550, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 63.778 [14.000, 98.000],  loss: 8.148333, mae: 2.423610, mean_q: 4.659174\n",
      "[ 6 74  8  2 49 89 31 95 31 37]\n",
      " 31959/50001: episode: 3551, duration: 0.072s, episode steps:   9, steps per second: 125, episode reward: 28.000, mean reward:  3.111 [-10.000,  8.000], mean action: 46.222 [2.000, 95.000],  loss: 7.105997, mae: 2.474793, mean_q: 4.777040\n",
      "[96  4 97 98 19 65 79  2  4 88]\n",
      " 31968/50001: episode: 3552, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 50.667 [2.000, 98.000],  loss: 8.061113, mae: 2.487950, mean_q: 4.796109\n",
      "[58 37 31 43 67 84 24 78 94 79]\n",
      " 31977/50001: episode: 3553, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 36.000, mean reward:  4.000 [ 2.000,  7.000], mean action: 59.667 [24.000, 94.000],  loss: 8.026012, mae: 2.487285, mean_q: 4.797337\n",
      "[43 28 45 74 19 23 46 53 40  8]\n",
      " 31986/50001: episode: 3554, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 38.000, mean reward:  4.222 [ 2.000,  6.000], mean action: 37.333 [8.000, 74.000],  loss: 6.931271, mae: 2.494528, mean_q: 4.752890\n",
      "[20 37 87 13 98 28 53 27 46 51]\n",
      " 31995/50001: episode: 3555, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 42.000, mean reward:  4.667 [ 3.000,  6.000], mean action: 48.889 [13.000, 98.000],  loss: 9.661601, mae: 2.463225, mean_q: 4.731771\n",
      "[ 1 88 97 49 65 48 95 34 42 48]\n",
      " 32004/50001: episode: 3556, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 62.889 [34.000, 97.000],  loss: 8.072891, mae: 2.399165, mean_q: 4.616791\n",
      "[70 95 55 60 95 34 43 13 18 94]\n",
      " 32013/50001: episode: 3557, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 56.333 [13.000, 95.000],  loss: 8.122126, mae: 2.381071, mean_q: 4.647990\n",
      "[89 95 82 56 94 12  8 86 98 47]\n",
      " 32022/50001: episode: 3558, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 40.000, mean reward:  4.444 [ 3.000,  8.000], mean action: 64.222 [8.000, 98.000],  loss: 8.395317, mae: 2.357306, mean_q: 4.543964\n",
      "[71 96 40 96 32 26 35 23  4 34]\n",
      " 32031/50001: episode: 3559, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 29.000, mean reward:  3.222 [-10.000, 10.000], mean action: 42.889 [4.000, 96.000],  loss: 7.853045, mae: 2.359592, mean_q: 4.579461\n",
      "[44 13 20 32 98 31 28 83 12 76]\n",
      " 32040/50001: episode: 3560, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 43.667 [12.000, 98.000],  loss: 7.459265, mae: 2.354239, mean_q: 4.531960\n",
      "[74 54 95 80 54 57 74 88 14 37]\n",
      " 32049/50001: episode: 3561, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 17.000, mean reward:  1.889 [-10.000,  8.000], mean action: 61.444 [14.000, 95.000],  loss: 7.416869, mae: 2.448235, mean_q: 4.739034\n",
      "[25 52 28 91 59 54 13 33  2 34]\n",
      " 32058/50001: episode: 3562, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 42.000, mean reward:  4.667 [ 3.000,  9.000], mean action: 40.667 [2.000, 91.000],  loss: 7.629914, mae: 2.477659, mean_q: 4.748411\n",
      "[ 2  0 66  3  4 36 20 24 86 24]\n",
      " 32067/50001: episode: 3563, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 19.000, mean reward:  2.111 [-10.000,  6.000], mean action: 29.222 [0.000, 86.000],  loss: 6.703439, mae: 2.470480, mean_q: 4.739787\n",
      "[65 58 58 79 23 51 13 46 14 74]\n",
      " 32076/50001: episode: 3564, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 27.000, mean reward:  3.000 [-10.000,  6.000], mean action: 46.222 [13.000, 79.000],  loss: 7.218111, mae: 2.443757, mean_q: 4.660384\n",
      "[87 32 92 52 95 57 82 64 52 79]\n",
      " 32085/50001: episode: 3565, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 67.222 [32.000, 95.000],  loss: 7.670669, mae: 2.395974, mean_q: 4.588359\n",
      "[ 7 22 25 82 17 18 13 97 30 93]\n",
      " 32094/50001: episode: 3566, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 43.000, mean reward:  4.778 [ 3.000,  7.000], mean action: 44.111 [13.000, 97.000],  loss: 5.854710, mae: 2.420705, mean_q: 4.621995\n",
      "[39 59 31 86 37 60 24 27 40 90]\n",
      " 32103/50001: episode: 3567, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 42.000, mean reward:  4.667 [ 3.000,  6.000], mean action: 50.444 [24.000, 90.000],  loss: 7.605319, mae: 2.512547, mean_q: 4.848365\n",
      "[63 34 73 37 98 69 60 50 88 79]\n",
      " 32112/50001: episode: 3568, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 44.000, mean reward:  4.889 [ 2.000,  7.000], mean action: 65.333 [34.000, 98.000],  loss: 7.109775, mae: 2.488251, mean_q: 4.799117\n",
      "[47 63 25 35 14  1 72 10 69 78]\n",
      " 32121/50001: episode: 3569, duration: 0.069s, episode steps:   9, steps per second: 130, episode reward: 31.000, mean reward:  3.444 [ 2.000,  5.000], mean action: 40.778 [1.000, 78.000],  loss: 8.672065, mae: 2.489757, mean_q: 4.817924\n",
      "[ 0 95 37 13  8 51 74 76 49 12]\n",
      " 32130/50001: episode: 3570, duration: 0.072s, episode steps:   9, steps per second: 125, episode reward: 39.000, mean reward:  4.333 [ 2.000,  9.000], mean action: 46.111 [8.000, 95.000],  loss: 7.007123, mae: 2.458691, mean_q: 4.689797\n",
      "[44 23 62 82 85 82 95 27 78 13]\n",
      " 32139/50001: episode: 3571, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 28.000, mean reward:  3.111 [-10.000,  9.000], mean action: 60.778 [13.000, 95.000],  loss: 7.111188, mae: 2.486263, mean_q: 4.777921\n",
      "[61 93  5 97 28 62 98 13 60 19]\n",
      " 32148/50001: episode: 3572, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 52.778 [5.000, 98.000],  loss: 8.143475, mae: 2.492961, mean_q: 4.910151\n",
      "[86 60 90 91 91 60 46  4 13  1]\n",
      " 32157/50001: episode: 3573, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 17.000, mean reward:  1.889 [-10.000,  8.000], mean action: 50.667 [1.000, 91.000],  loss: 5.643659, mae: 2.453363, mean_q: 4.746035\n",
      "[87 48 58 42 27 58  1 73 56 70]\n",
      " 32166/50001: episode: 3574, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 19.000, mean reward:  2.111 [-10.000,  6.000], mean action: 48.111 [1.000, 73.000],  loss: 7.764427, mae: 2.508064, mean_q: 4.826663\n",
      "[21 21 41 31 95 78 68 31 24 16]\n",
      " 32175/50001: episode: 3575, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 11.000, mean reward:  1.222 [-10.000,  6.000], mean action: 45.000 [16.000, 95.000],  loss: 7.645258, mae: 2.565144, mean_q: 4.909321\n",
      "[25 31 95 53 97 57 69 42 31 12]\n",
      " 32184/50001: episode: 3576, duration: 0.069s, episode steps:   9, steps per second: 130, episode reward: 27.000, mean reward:  3.000 [-10.000,  8.000], mean action: 54.111 [12.000, 97.000],  loss: 5.541046, mae: 2.573815, mean_q: 4.893947\n",
      "[55 28 18 86 36 79 98 88 58  9]\n",
      " 32193/50001: episode: 3577, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 42.000, mean reward:  4.667 [ 2.000,  7.000], mean action: 55.556 [9.000, 98.000],  loss: 5.538832, mae: 2.580605, mean_q: 4.946694\n",
      "[30 89 37 46 75 23 53 89 34 80]\n",
      " 32202/50001: episode: 3578, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 58.444 [23.000, 89.000],  loss: 6.829725, mae: 2.631010, mean_q: 5.001513\n",
      "[73 58 29 99 27 50 47 21 84 50]\n",
      " 32211/50001: episode: 3579, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 21.000, mean reward:  2.333 [-10.000,  6.000], mean action: 51.667 [21.000, 99.000],  loss: 7.790247, mae: 2.573400, mean_q: 4.901180\n",
      "[33 91 41 19 41  5 70 89  5 79]\n",
      " 32220/50001: episode: 3580, duration: 0.068s, episode steps:   9, steps per second: 133, episode reward: 10.000, mean reward:  1.111 [-10.000,  8.000], mean action: 48.889 [5.000, 91.000],  loss: 8.396069, mae: 2.587024, mean_q: 4.934602\n",
      "[95 60 95 40 82 31 90 84 55 50]\n",
      " 32229/50001: episode: 3581, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 23.000, mean reward:  2.556 [-10.000,  9.000], mean action: 65.222 [31.000, 95.000],  loss: 7.686584, mae: 2.596733, mean_q: 4.980969\n",
      "[38 98 64 78 37 50 62 46 98 51]\n",
      " 32238/50001: episode: 3582, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 27.000, mean reward:  3.000 [-10.000,  6.000], mean action: 64.889 [37.000, 98.000],  loss: 8.679716, mae: 2.508099, mean_q: 4.763255\n",
      "[54 35 84 14 44 25 88 12 40 34]\n",
      " 32247/50001: episode: 3583, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 45.000, mean reward:  5.000 [ 3.000, 10.000], mean action: 41.778 [12.000, 88.000],  loss: 6.125902, mae: 2.470196, mean_q: 4.774606\n",
      "[69 41 37 33 92 28 60 94 48 79]\n",
      " 32256/50001: episode: 3584, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 56.889 [28.000, 94.000],  loss: 5.998584, mae: 2.453825, mean_q: 4.755236\n",
      "[71  1 24 95 34 62 46 32  5 37]\n",
      " 32265/50001: episode: 3585, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 46.000, mean reward:  5.111 [ 3.000,  8.000], mean action: 37.333 [1.000, 95.000],  loss: 7.326663, mae: 2.530987, mean_q: 4.850094\n",
      "[16 83 77 82 65 75 37 96 34 12]\n",
      " 32274/50001: episode: 3586, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 45.000, mean reward:  5.000 [ 3.000,  9.000], mean action: 62.333 [12.000, 96.000],  loss: 5.174166, mae: 2.561000, mean_q: 4.907171\n",
      "[ 4 95 20 90 98 14 49 13 41  1]\n",
      " 32283/50001: episode: 3587, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 46.778 [1.000, 98.000],  loss: 8.919866, mae: 2.573855, mean_q: 4.960680\n",
      "[69 50 97 77 98 13  1 89 41  4]\n",
      " 32292/50001: episode: 3588, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 40.000, mean reward:  4.444 [ 3.000,  6.000], mean action: 52.222 [1.000, 98.000],  loss: 10.295490, mae: 2.500400, mean_q: 4.748291\n",
      "[21 41 34 24 53 86 51 82 88  9]\n",
      " 32301/50001: episode: 3589, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 52.000 [9.000, 88.000],  loss: 9.467843, mae: 2.387146, mean_q: 4.691130\n",
      "[24 51 37  4 98 21 41 32 31 79]\n",
      " 32310/50001: episode: 3590, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 40.000, mean reward:  4.444 [ 3.000,  6.000], mean action: 43.778 [4.000, 98.000],  loss: 8.227798, mae: 2.308608, mean_q: 4.438350\n",
      "[19 50 42 31 97 13 95 18 40 48]\n",
      " 32319/50001: episode: 3591, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 48.222 [13.000, 97.000],  loss: 7.617296, mae: 2.334759, mean_q: 4.508493\n",
      "[91 66 46 10 10  4 88 76 48 90]\n",
      " 32328/50001: episode: 3592, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 48.667 [4.000, 90.000],  loss: 6.216099, mae: 2.361563, mean_q: 4.556722\n",
      "[16 50 91 84 15 17 43 88 58 48]\n",
      " 32337/50001: episode: 3593, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 40.000, mean reward:  4.444 [ 2.000,  9.000], mean action: 54.889 [15.000, 91.000],  loss: 6.310185, mae: 2.413648, mean_q: 4.595032\n",
      "[58 27 79 79 85 10 33 14 66 50]\n",
      " 32346/50001: episode: 3594, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 27.000, mean reward:  3.000 [-10.000,  9.000], mean action: 49.222 [10.000, 85.000],  loss: 10.120479, mae: 2.524453, mean_q: 4.900412\n",
      "[ 5 95 85 99 28 75 48 79 34 37]\n",
      " 32355/50001: episode: 3595, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 45.000, mean reward:  5.000 [ 3.000,  8.000], mean action: 64.444 [28.000, 99.000],  loss: 7.821982, mae: 2.404405, mean_q: 4.643160\n",
      "[52 95 49 56 28 66 16 89 85 50]\n",
      " 32364/50001: episode: 3596, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 43.000, mean reward:  4.778 [ 3.000,  9.000], mean action: 59.333 [16.000, 95.000],  loss: 6.375916, mae: 2.459841, mean_q: 4.779565\n",
      "[ 9 20 73 17 93 97 95  9 51 97]\n",
      " 32373/50001: episode: 3597, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 12.000, mean reward:  1.333 [-10.000,  8.000], mean action: 61.333 [9.000, 97.000],  loss: 9.787505, mae: 2.500632, mean_q: 4.746994\n",
      "[19 38  2 76 53 88 11  1 46 56]\n",
      " 32382/50001: episode: 3598, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward: 43.000, mean reward:  4.778 [ 3.000,  7.000], mean action: 41.222 [1.000, 88.000],  loss: 4.450700, mae: 2.433331, mean_q: 4.704258\n",
      "[13 95  2 97 94 50 48 31  8 94]\n",
      " 32391/50001: episode: 3599, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 21.000, mean reward:  2.333 [-10.000,  5.000], mean action: 57.667 [2.000, 97.000],  loss: 8.968832, mae: 2.442611, mean_q: 4.676277\n",
      "[69 12 37 70 27 37 66 96 37 34]\n",
      " 32400/50001: episode: 3600, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 11.000, mean reward:  1.222 [-10.000,  8.000], mean action: 46.222 [12.000, 96.000],  loss: 7.610516, mae: 2.428773, mean_q: 4.688903\n",
      "[96 34 37 45 35  4 62 60 85 54]\n",
      " 32409/50001: episode: 3601, duration: 0.066s, episode steps:   9, steps per second: 135, episode reward: 38.000, mean reward:  4.222 [ 2.000,  6.000], mean action: 46.222 [4.000, 85.000],  loss: 5.184658, mae: 2.445677, mean_q: 4.749217\n",
      "[67 57 28 89  2 17 47 36 80 31]\n",
      " 32418/50001: episode: 3602, duration: 0.109s, episode steps:   9, steps per second:  83, episode reward: 37.000, mean reward:  4.111 [ 2.000,  7.000], mean action: 43.000 [2.000, 89.000],  loss: 6.393489, mae: 2.478432, mean_q: 4.828436\n",
      "[67 34 48 13 38 26 93 34 89 50]\n",
      " 32427/50001: episode: 3603, duration: 0.108s, episode steps:   9, steps per second:  84, episode reward: 24.000, mean reward:  2.667 [-10.000,  8.000], mean action: 47.222 [13.000, 93.000],  loss: 6.947603, mae: 2.483876, mean_q: 4.796893\n",
      "[69 14 44 33 54 46 71 97 34 31]\n",
      " 32436/50001: episode: 3604, duration: 0.108s, episode steps:   9, steps per second:  83, episode reward: 41.000, mean reward:  4.556 [ 2.000,  9.000], mean action: 47.111 [14.000, 97.000],  loss: 8.235568, mae: 2.513878, mean_q: 4.859536\n",
      "[ 3 52 75 73 18 11 79 42 89 88]\n",
      " 32445/50001: episode: 3605, duration: 0.106s, episode steps:   9, steps per second:  85, episode reward: 41.000, mean reward:  4.556 [ 2.000,  9.000], mean action: 58.556 [11.000, 89.000],  loss: 7.386783, mae: 2.501740, mean_q: 4.855251\n",
      "[66 24 92 53 68 21 14 32 58 89]\n",
      " 32454/50001: episode: 3606, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward: 37.000, mean reward:  4.111 [ 3.000,  6.000], mean action: 50.111 [14.000, 92.000],  loss: 7.046239, mae: 2.480007, mean_q: 4.809598\n",
      "[ 4 50 79 27 98 21 47 53 12 74]\n",
      " 32463/50001: episode: 3607, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 51.222 [12.000, 98.000],  loss: 5.938761, mae: 2.399977, mean_q: 4.650891\n",
      "[28 24 68 97 97 34 32 34 89 50]\n",
      " 32472/50001: episode: 3608, duration: 0.078s, episode steps:   9, steps per second: 116, episode reward: 11.000, mean reward:  1.222 [-10.000,  7.000], mean action: 58.333 [24.000, 97.000],  loss: 7.247525, mae: 2.459401, mean_q: 4.727453\n",
      "[71 97 34 11 58 67 97 37  1 50]\n",
      " 32481/50001: episode: 3609, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 28.000, mean reward:  3.111 [-10.000,  7.000], mean action: 50.222 [1.000, 97.000],  loss: 4.263627, mae: 2.454298, mean_q: 4.740459\n",
      "[72  5 16 19 79 21 13 22 93 79]\n",
      " 32490/50001: episode: 3610, duration: 0.073s, episode steps:   9, steps per second: 124, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 38.556 [5.000, 93.000],  loss: 8.859612, mae: 2.500386, mean_q: 4.827786\n",
      "[29 12 90 63 61 91 16 50 66 23]\n",
      " 32499/50001: episode: 3611, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward: 43.000, mean reward:  4.778 [ 2.000,  8.000], mean action: 52.444 [12.000, 91.000],  loss: 8.291125, mae: 2.496510, mean_q: 4.813891\n",
      "[72 95 24 78 32 66 14 75 83 47]\n",
      " 32508/50001: episode: 3612, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward: 43.000, mean reward:  4.778 [ 3.000,  8.000], mean action: 57.111 [14.000, 95.000],  loss: 8.989396, mae: 2.428930, mean_q: 4.710670\n",
      "[17 88 64 13 76 46 78 96 83 23]\n",
      " 32517/50001: episode: 3613, duration: 0.087s, episode steps:   9, steps per second: 103, episode reward: 38.000, mean reward:  4.222 [ 3.000,  7.000], mean action: 63.000 [13.000, 96.000],  loss: 9.480663, mae: 2.390024, mean_q: 4.564538\n",
      "[67 53 12 47 40 18 99 82 18 34]\n",
      " 32526/50001: episode: 3614, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward: 27.000, mean reward:  3.000 [-10.000, 10.000], mean action: 44.778 [12.000, 99.000],  loss: 8.025166, mae: 2.352350, mean_q: 4.648402\n",
      "[46 95 90 48 40  2 66 28 68 20]\n",
      " 32535/50001: episode: 3615, duration: 0.091s, episode steps:   9, steps per second:  98, episode reward: 37.000, mean reward:  4.111 [ 3.000,  5.000], mean action: 50.778 [2.000, 95.000],  loss: 7.383543, mae: 2.313428, mean_q: 4.427190\n",
      "[63 76 74 34 37 50 94 88 98 32]\n",
      " 32544/50001: episode: 3616, duration: 0.085s, episode steps:   9, steps per second: 106, episode reward: 42.000, mean reward:  4.667 [ 3.000,  6.000], mean action: 64.778 [32.000, 98.000],  loss: 6.711715, mae: 2.346614, mean_q: 4.557320\n",
      "[50 16  6 21 65 94 30  2 10  9]\n",
      " 32553/50001: episode: 3617, duration: 0.086s, episode steps:   9, steps per second: 104, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 28.111 [2.000, 94.000],  loss: 6.647104, mae: 2.435594, mean_q: 4.764452\n",
      "[56 33 29 97 90 34  1 56 86 12]\n",
      " 32562/50001: episode: 3618, duration: 0.082s, episode steps:   9, steps per second: 110, episode reward: 27.000, mean reward:  3.000 [-10.000,  9.000], mean action: 48.667 [1.000, 97.000],  loss: 6.825046, mae: 2.477227, mean_q: 4.810689\n",
      "[91 20  2 98 97 88 23 34 76 50]\n",
      " 32571/50001: episode: 3619, duration: 0.075s, episode steps:   9, steps per second: 120, episode reward: 44.000, mean reward:  4.889 [ 3.000,  8.000], mean action: 54.222 [2.000, 98.000],  loss: 6.767937, mae: 2.495300, mean_q: 4.742609\n",
      "[10 32 23 34  9 68 89 50 34 94]\n",
      " 32580/50001: episode: 3620, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 48.111 [9.000, 94.000],  loss: 5.621533, mae: 2.528713, mean_q: 4.895437\n",
      "[39 59 82 95 88  1 43 13 39  1]\n",
      " 32589/50001: episode: 3621, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward: 12.000, mean reward:  1.333 [-10.000,  7.000], mean action: 46.778 [1.000, 95.000],  loss: 9.992496, mae: 2.512627, mean_q: 4.829562\n",
      "[50 30 67  1 33 28 84  2 55 50]\n",
      " 32598/50001: episode: 3622, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward: 20.000, mean reward:  2.222 [-10.000,  5.000], mean action: 38.889 [1.000, 84.000],  loss: 7.901121, mae: 2.393441, mean_q: 4.591831\n",
      "[84 35 50 88 98 67 20 76 48 57]\n",
      " 32607/50001: episode: 3623, duration: 0.073s, episode steps:   9, steps per second: 123, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 59.889 [20.000, 98.000],  loss: 7.371473, mae: 2.389396, mean_q: 4.605170\n",
      "[18 95 94  1  4  4 44 88 42 66]\n",
      " 32616/50001: episode: 3624, duration: 0.075s, episode steps:   9, steps per second: 119, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 48.667 [1.000, 95.000],  loss: 6.996439, mae: 2.404680, mean_q: 4.650699\n",
      "[74 76 16 34 64 66 88 48 28 71]\n",
      " 32625/50001: episode: 3625, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 40.000, mean reward:  4.444 [ 2.000,  6.000], mean action: 54.556 [16.000, 88.000],  loss: 7.693164, mae: 2.436142, mean_q: 4.590542\n",
      "[23 42 41 10 27 15 66  4 77 46]\n",
      " 32634/50001: episode: 3626, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 36.444 [4.000, 77.000],  loss: 8.427987, mae: 2.430506, mean_q: 4.679142\n",
      "[52 46 82 97 31  1 88 12 28 75]\n",
      " 32643/50001: episode: 3627, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 44.000, mean reward:  4.889 [ 4.000,  6.000], mean action: 51.111 [1.000, 97.000],  loss: 6.795195, mae: 2.461205, mean_q: 4.746047\n",
      "[64 78 51 23 93 74 23 50 94  6]\n",
      " 32652/50001: episode: 3628, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 54.667 [6.000, 94.000],  loss: 7.512110, mae: 2.429131, mean_q: 4.685381\n",
      "[47 95 85 24 34 21  9 95 60 74]\n",
      " 32661/50001: episode: 3629, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 23.000, mean reward:  2.556 [-10.000,  5.000], mean action: 55.222 [9.000, 95.000],  loss: 7.979269, mae: 2.399689, mean_q: 4.709930\n",
      "[44 34 88 93 62 33 74 89 93 97]\n",
      " 32670/50001: episode: 3630, duration: 0.066s, episode steps:   9, steps per second: 135, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 73.667 [33.000, 97.000],  loss: 4.925908, mae: 2.404956, mean_q: 4.635565\n",
      "[40 28 35 34 14 82 10 37 96 50]\n",
      " 32679/50001: episode: 3631, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 42.889 [10.000, 96.000],  loss: 6.921943, mae: 2.487748, mean_q: 4.811069\n",
      "[ 2 14 36 17 29 20  2 59 85 50]\n",
      " 32688/50001: episode: 3632, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 22.000, mean reward:  2.444 [-10.000, 10.000], mean action: 34.667 [2.000, 85.000],  loss: 6.227209, mae: 2.548226, mean_q: 4.847910\n",
      "[99 89 37 23 97 95 60 23 23 54]\n",
      " 32697/50001: episode: 3633, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward:  8.000, mean reward:  0.889 [-10.000,  6.000], mean action: 55.667 [23.000, 97.000],  loss: 7.214602, mae: 2.478709, mean_q: 4.845411\n",
      "[12 51 34 43 82 37 68 37  6 41]\n",
      " 32706/50001: episode: 3634, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 44.333 [6.000, 82.000],  loss: 8.548675, mae: 2.498789, mean_q: 4.780214\n",
      "[84 34 27 23 82 91 62 68 39 63]\n",
      " 32715/50001: episode: 3635, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 33.000, mean reward:  3.667 [ 2.000,  6.000], mean action: 54.333 [23.000, 91.000],  loss: 5.791429, mae: 2.431527, mean_q: 4.707087\n",
      "[97 53 89 28 90 99 68  1  1  5]\n",
      " 32724/50001: episode: 3636, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 48.222 [1.000, 99.000],  loss: 5.973359, mae: 2.464472, mean_q: 4.690656\n",
      "[52 62 38 31 42 28 82 56 94 13]\n",
      " 32733/50001: episode: 3637, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 40.000, mean reward:  4.444 [ 2.000,  9.000], mean action: 49.556 [13.000, 94.000],  loss: 5.017616, mae: 2.493010, mean_q: 4.758013\n",
      "[25 51 13 93 93 13 66 24 98 51]\n",
      " 32742/50001: episode: 3638, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: -3.000, mean reward: -0.333 [-10.000,  6.000], mean action: 55.778 [13.000, 98.000],  loss: 8.914534, mae: 2.588769, mean_q: 5.002300\n",
      "[42  1 30 60 79 51 83 28 88 28]\n",
      " 32751/50001: episode: 3639, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 49.778 [1.000, 88.000],  loss: 5.317142, mae: 2.526944, mean_q: 4.856501\n",
      "[64 28 37 34 81 71 41 97 37 98]\n",
      " 32760/50001: episode: 3640, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 26.000, mean reward:  2.889 [-10.000,  6.000], mean action: 58.222 [28.000, 98.000],  loss: 5.896664, mae: 2.577773, mean_q: 4.946416\n",
      "[50 72 71 97  0 59 59 12 78 50]\n",
      " 32769/50001: episode: 3641, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward:  6.000, mean reward:  0.667 [-10.000,  7.000], mean action: 55.333 [0.000, 97.000],  loss: 4.862976, mae: 2.563589, mean_q: 4.914257\n",
      "[21 74  1 53 97 99  1 60 54 88]\n",
      " 32778/50001: episode: 3642, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 26.000, mean reward:  2.889 [-10.000,  8.000], mean action: 58.556 [1.000, 99.000],  loss: 7.339308, mae: 2.589445, mean_q: 4.885346\n",
      "[49  6 85 95 75 74 34 46 97 50]\n",
      " 32787/50001: episode: 3643, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 46.000, mean reward:  5.111 [ 3.000,  8.000], mean action: 62.444 [6.000, 97.000],  loss: 6.026230, mae: 2.589862, mean_q: 4.986604\n",
      "[44 74 13 68 30 35 20 91  5 52]\n",
      " 32796/50001: episode: 3644, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 43.111 [5.000, 91.000],  loss: 7.395562, mae: 2.526216, mean_q: 4.785110\n",
      "[36 35 49 96  2 25 18 27 48 48]\n",
      " 32805/50001: episode: 3645, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 38.667 [2.000, 96.000],  loss: 5.964973, mae: 2.570182, mean_q: 4.975407\n",
      "[79 34 97 68  6 79 60 48 31 78]\n",
      " 32814/50001: episode: 3646, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 55.667 [6.000, 97.000],  loss: 6.928031, mae: 2.497413, mean_q: 4.701924\n",
      "[17 16  8 11 93 59 83 74 66 85]\n",
      " 32823/50001: episode: 3647, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 37.000, mean reward:  4.111 [ 3.000,  7.000], mean action: 55.000 [8.000, 93.000],  loss: 6.886960, mae: 2.533873, mean_q: 4.811844\n",
      "[48 27 98 34 82 28 14 23 74 70]\n",
      " 32832/50001: episode: 3648, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 35.000, mean reward:  3.889 [ 3.000,  5.000], mean action: 50.000 [14.000, 98.000],  loss: 7.752398, mae: 2.615086, mean_q: 5.020760\n",
      "[37 84  4 34 25 12  7 76 17 10]\n",
      " 32841/50001: episode: 3649, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 30.000, mean reward:  3.333 [ 1.000,  5.000], mean action: 29.889 [4.000, 84.000],  loss: 7.534859, mae: 2.491028, mean_q: 4.807049\n",
      "[10  4 20 35 79 79 98 94 48 57]\n",
      " 32850/50001: episode: 3650, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 57.111 [4.000, 98.000],  loss: 8.011037, mae: 2.596033, mean_q: 4.955494\n",
      "[75 95 60 84 86  4 60  6 52  1]\n",
      " 32859/50001: episode: 3651, duration: 0.070s, episode steps:   9, steps per second: 128, episode reward: 26.000, mean reward:  2.889 [-10.000,  9.000], mean action: 49.778 [1.000, 95.000],  loss: 6.528491, mae: 2.593250, mean_q: 4.952393\n",
      "[55 95 46 34 55 18 87 40 50 32]\n",
      " 32868/50001: episode: 3652, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 30.000, mean reward:  3.333 [-10.000,  8.000], mean action: 50.778 [18.000, 95.000],  loss: 6.639988, mae: 2.579786, mean_q: 4.914952\n",
      "[35 21 62 14 85 42 95 71  4  5]\n",
      " 32877/50001: episode: 3653, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 38.000, mean reward:  4.222 [ 2.000,  8.000], mean action: 44.333 [4.000, 95.000],  loss: 6.944799, mae: 2.566292, mean_q: 4.933364\n",
      "[11 95 64 66 34 91 67 48 28 23]\n",
      " 32886/50001: episode: 3654, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 41.000, mean reward:  4.556 [ 3.000,  6.000], mean action: 57.333 [23.000, 95.000],  loss: 7.378762, mae: 2.480436, mean_q: 4.757627\n",
      "[25 23  1 59 91 83 41 27  4 86]\n",
      " 32895/50001: episode: 3655, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 36.000, mean reward:  4.000 [ 3.000,  6.000], mean action: 46.111 [1.000, 91.000],  loss: 6.868963, mae: 2.523692, mean_q: 4.894718\n",
      "[ 6  9 32  3 73 71 33 71 99  5]\n",
      " 32904/50001: episode: 3656, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 22.000, mean reward:  2.444 [-10.000,  7.000], mean action: 44.000 [3.000, 99.000],  loss: 7.409826, mae: 2.507858, mean_q: 4.845411\n",
      "[41 11 47 95 88 94 95 50 14 37]\n",
      " 32913/50001: episode: 3657, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 28.000, mean reward:  3.111 [-10.000,  7.000], mean action: 59.000 [11.000, 95.000],  loss: 10.591548, mae: 2.527245, mean_q: 4.792080\n",
      "[12 62 37  2 95 32 54 17 53  6]\n",
      " 32922/50001: episode: 3658, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 36.000, mean reward:  4.000 [ 2.000,  6.000], mean action: 39.778 [2.000, 95.000],  loss: 8.411312, mae: 2.474302, mean_q: 4.708131\n",
      "[27 34 67 74 93 34 88 57 52 52]\n",
      " 32931/50001: episode: 3659, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward:  9.000, mean reward:  1.000 [-10.000,  5.000], mean action: 61.222 [34.000, 93.000],  loss: 7.408735, mae: 2.388669, mean_q: 4.625994\n",
      "[84  1 34 10 84  6 27 21 32 25]\n",
      " 32940/50001: episode: 3660, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 26.667 [1.000, 84.000],  loss: 8.168966, mae: 2.343858, mean_q: 4.487056\n",
      "[47 40 47 28 97 88  1  9 86 46]\n",
      " 32949/50001: episode: 3661, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 49.111 [1.000, 97.000],  loss: 6.102114, mae: 2.324383, mean_q: 4.514170\n",
      "[74 82 24 34 68 69 13 10  4  5]\n",
      " 32958/50001: episode: 3662, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 34.333 [4.000, 82.000],  loss: 9.641989, mae: 2.327956, mean_q: 4.556979\n",
      "[92 91 31 95 27 42 37 10 89 78]\n",
      " 32967/50001: episode: 3663, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 55.556 [10.000, 95.000],  loss: 5.716510, mae: 2.398595, mean_q: 4.665477\n",
      "[85 20 36  8 60 95 37 62 98 37]\n",
      " 32976/50001: episode: 3664, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 50.333 [8.000, 98.000],  loss: 7.613422, mae: 2.452772, mean_q: 4.715496\n",
      "[10 34 37 82 92 37 95 34 98  7]\n",
      " 32985/50001: episode: 3665, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward:  7.000, mean reward:  0.778 [-10.000,  6.000], mean action: 57.333 [7.000, 98.000],  loss: 7.133333, mae: 2.410199, mean_q: 4.653173\n",
      "[10  4 96 27 21  9 37 97 89 79]\n",
      " 32994/50001: episode: 3666, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 51.000 [4.000, 97.000],  loss: 5.573566, mae: 2.407895, mean_q: 4.618075\n",
      "[90 28 34 41 62 96 91 66 14 45]\n",
      " 33003/50001: episode: 3667, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 53.000 [14.000, 96.000],  loss: 3.849465, mae: 2.423269, mean_q: 4.658812\n",
      "[16 90  5 90 93  4 50 83 10 50]\n",
      " 33012/50001: episode: 3668, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward:  9.000, mean reward:  1.000 [-10.000,  6.000], mean action: 52.778 [4.000, 93.000],  loss: 8.066318, mae: 2.518519, mean_q: 4.904943\n",
      "[57 38 91 82  1 38 50 13 96 46]\n",
      " 33021/50001: episode: 3669, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 28.000, mean reward:  3.111 [-10.000,  7.000], mean action: 50.556 [1.000, 96.000],  loss: 4.330975, mae: 2.519469, mean_q: 4.840984\n",
      "[83 13 27 82 44 60 76 37  2 66]\n",
      " 33030/50001: episode: 3670, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward: 44.000, mean reward:  4.889 [ 3.000,  8.000], mean action: 45.222 [2.000, 82.000],  loss: 8.847569, mae: 2.552402, mean_q: 4.939276\n",
      "[49 42 24 11  7 99 34 16 21  0]\n",
      " 33039/50001: episode: 3671, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 35.000, mean reward:  3.889 [ 2.000,  7.000], mean action: 28.222 [0.000, 99.000],  loss: 8.058675, mae: 2.546182, mean_q: 4.837755\n",
      "[79  6 57 98 99 37 62  5 27 20]\n",
      " 33048/50001: episode: 3672, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 37.000, mean reward:  4.111 [ 3.000,  6.000], mean action: 45.667 [5.000, 99.000],  loss: 9.880279, mae: 2.477561, mean_q: 4.715242\n",
      "[12 37 38 14  3 30 13 64 59 17]\n",
      " 33057/50001: episode: 3673, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 32.000, mean reward:  3.556 [ 2.000,  6.000], mean action: 30.556 [3.000, 64.000],  loss: 8.488199, mae: 2.438642, mean_q: 4.662251\n",
      "[72 71 78 24 65 77  9 41 39 27]\n",
      " 33066/50001: episode: 3674, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 39.000, mean reward:  4.333 [ 2.000,  8.000], mean action: 47.889 [9.000, 78.000],  loss: 8.731457, mae: 2.441617, mean_q: 4.683746\n",
      "[52 13 41 93 50 89  0 88 97 23]\n",
      " 33075/50001: episode: 3675, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 54.889 [0.000, 97.000],  loss: 6.940123, mae: 2.397408, mean_q: 4.654236\n",
      "[92 89 28 30 58 34 37 33 73 97]\n",
      " 33084/50001: episode: 3676, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 53.222 [28.000, 97.000],  loss: 4.921467, mae: 2.465555, mean_q: 4.735731\n",
      "[37 52 12 11 60 62  0 96 79 52]\n",
      " 33093/50001: episode: 3677, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 18.000, mean reward:  2.000 [-10.000,  6.000], mean action: 47.111 [0.000, 96.000],  loss: 8.527408, mae: 2.498269, mean_q: 4.894659\n",
      "[95  4 79 53 47 48 99 66 31  8]\n",
      " 33102/50001: episode: 3678, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 34.000, mean reward:  3.778 [ 2.000,  5.000], mean action: 48.333 [4.000, 99.000],  loss: 8.693783, mae: 2.478137, mean_q: 4.712729\n",
      "[18 37 95  1 14 88 88 53  2 43]\n",
      " 33111/50001: episode: 3679, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 24.000, mean reward:  2.667 [-10.000,  5.000], mean action: 46.778 [1.000, 95.000],  loss: 7.734710, mae: 2.538458, mean_q: 4.878583\n",
      "[49 46 74 65 95 57 57  8  1 95]\n",
      " 33120/50001: episode: 3680, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 10.000, mean reward:  1.111 [-10.000,  7.000], mean action: 55.333 [1.000, 95.000],  loss: 7.360860, mae: 2.538557, mean_q: 4.797516\n",
      "[40 42 56 21 38 99  4 80 88 12]\n",
      " 33129/50001: episode: 3681, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 39.000, mean reward:  4.333 [ 2.000,  9.000], mean action: 48.889 [4.000, 99.000],  loss: 8.346754, mae: 2.490022, mean_q: 4.798021\n",
      "[11 82  1 50 30 79 33 30 66 27]\n",
      " 33138/50001: episode: 3682, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 29.000, mean reward:  3.222 [-10.000,  8.000], mean action: 44.222 [1.000, 82.000],  loss: 7.328677, mae: 2.468854, mean_q: 4.725017\n",
      "[ 2 18 55 35 18 59 27 89 28 57]\n",
      " 33147/50001: episode: 3683, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 42.889 [18.000, 89.000],  loss: 5.168780, mae: 2.367086, mean_q: 4.582130\n",
      "[61 27 63  4 76 49 67  5  5 39]\n",
      " 33156/50001: episode: 3684, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 19.000, mean reward:  2.111 [-10.000,  5.000], mean action: 37.222 [4.000, 76.000],  loss: 6.294430, mae: 2.463161, mean_q: 4.740294\n",
      "[67  5 84 53 50 15 68 12 97 97]\n",
      " 33165/50001: episode: 3685, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 53.444 [5.000, 97.000],  loss: 8.026193, mae: 2.518284, mean_q: 4.828374\n",
      "[11 94  2 90 41 38 71 50 10 28]\n",
      " 33174/50001: episode: 3686, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 41.000, mean reward:  4.556 [ 2.000,  8.000], mean action: 47.111 [2.000, 94.000],  loss: 7.340464, mae: 2.577830, mean_q: 4.907738\n",
      "[58 66 53 53  1 48 47 58 99 93]\n",
      " 33183/50001: episode: 3687, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 10.000, mean reward:  1.111 [-10.000,  6.000], mean action: 57.556 [1.000, 99.000],  loss: 8.250958, mae: 2.553331, mean_q: 4.871421\n",
      "[89 34 74 41 97 27 97 34  4  4]\n",
      " 33192/50001: episode: 3688, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: -4.000, mean reward: -0.444 [-10.000,  5.000], mean action: 45.778 [4.000, 97.000],  loss: 8.197215, mae: 2.449823, mean_q: 4.769416\n",
      "[26 83  0 24 51 33 88 82 37 32]\n",
      " 33201/50001: episode: 3689, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 44.000, mean reward:  4.889 [ 2.000,  8.000], mean action: 47.778 [0.000, 88.000],  loss: 4.467594, mae: 2.462014, mean_q: 4.751436\n",
      "[12 47 23 18 24 93 37 48 72 46]\n",
      " 33210/50001: episode: 3690, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 36.000, mean reward:  4.000 [ 1.000,  7.000], mean action: 45.333 [18.000, 93.000],  loss: 5.748847, mae: 2.494375, mean_q: 4.862216\n",
      "[73 12 24 54 75 88 95 12 73 50]\n",
      " 33219/50001: episode: 3691, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 17.000, mean reward:  1.889 [-10.000,  9.000], mean action: 53.667 [12.000, 95.000],  loss: 8.514922, mae: 2.469959, mean_q: 4.704675\n",
      "[81 67 88 98 16 82 48 32 42 73]\n",
      " 33228/50001: episode: 3692, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 36.000, mean reward:  4.000 [ 2.000,  6.000], mean action: 60.667 [16.000, 98.000],  loss: 7.247704, mae: 2.529618, mean_q: 4.892049\n",
      "[86  5 54 24 12 21 93 88 48 32]\n",
      " 33237/50001: episode: 3693, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 44.000, mean reward:  4.889 [ 3.000,  7.000], mean action: 41.889 [5.000, 93.000],  loss: 6.839468, mae: 2.507206, mean_q: 4.818056\n",
      "[57 27 13 31 57 31 24 41 23 75]\n",
      " 33246/50001: episode: 3694, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 11.000, mean reward:  1.222 [-10.000,  6.000], mean action: 35.778 [13.000, 75.000],  loss: 8.667365, mae: 2.472256, mean_q: 4.719004\n",
      "[ 1 62 51 98 95  4 67 48 76 31]\n",
      " 33255/50001: episode: 3695, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 59.111 [4.000, 98.000],  loss: 8.455087, mae: 2.464041, mean_q: 4.706670\n",
      "[ 1 98 93 31 95  9 99 66 40 27]\n",
      " 33264/50001: episode: 3696, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 42.000, mean reward:  4.667 [ 4.000,  6.000], mean action: 62.000 [9.000, 99.000],  loss: 8.435946, mae: 2.410048, mean_q: 4.705069\n",
      "[55 23 42 67 10  9 77 69 62 42]\n",
      " 33273/50001: episode: 3697, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 18.000, mean reward:  2.000 [-10.000,  5.000], mean action: 44.556 [9.000, 77.000],  loss: 6.201160, mae: 2.402922, mean_q: 4.610390\n",
      "[28 95 28 28 76 46 47 34  6 75]\n",
      " 33282/50001: episode: 3698, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 14.000, mean reward:  1.556 [-10.000,  7.000], mean action: 48.333 [6.000, 95.000],  loss: 6.342745, mae: 2.448860, mean_q: 4.733890\n",
      "[32 41 98 92 14 37  9 68 12 46]\n",
      " 33291/50001: episode: 3699, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 46.333 [9.000, 98.000],  loss: 6.826304, mae: 2.451210, mean_q: 4.692776\n",
      "[ 6 32 13 57  9 58  0 79 20 64]\n",
      " 33300/50001: episode: 3700, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 35.000, mean reward:  3.889 [ 2.000,  6.000], mean action: 36.889 [0.000, 79.000],  loss: 8.349766, mae: 2.490814, mean_q: 4.810175\n",
      "[61 46 97 10 95 27 46 82 12 80]\n",
      " 33309/50001: episode: 3701, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 55.000 [10.000, 97.000],  loss: 8.565926, mae: 2.531898, mean_q: 4.867331\n",
      "[43 31 68 79 41 75 30 32 25 95]\n",
      " 33318/50001: episode: 3702, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 45.000, mean reward:  5.000 [ 3.000, 11.000], mean action: 52.889 [25.000, 95.000],  loss: 7.704040, mae: 2.492967, mean_q: 4.755745\n",
      "[38 60 32 50 98 17 98 92 31 64]\n",
      " 33327/50001: episode: 3703, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 60.222 [17.000, 98.000],  loss: 4.534933, mae: 2.555583, mean_q: 4.830889\n",
      "[56 19 75 64  1 92 74  3 67 50]\n",
      " 33336/50001: episode: 3704, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 38.000, mean reward:  4.222 [ 3.000,  9.000], mean action: 49.444 [1.000, 92.000],  loss: 7.056343, mae: 2.538314, mean_q: 4.815461\n",
      "[95 31 93 81  1 88 95 78 42 34]\n",
      " 33345/50001: episode: 3705, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 26.000, mean reward:  2.889 [-10.000,  9.000], mean action: 60.333 [1.000, 95.000],  loss: 8.102978, mae: 2.533326, mean_q: 4.822779\n",
      "[14 42 97 24 98 23 41 85  7 75]\n",
      " 33354/50001: episode: 3706, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 35.000, mean reward:  3.889 [ 2.000,  7.000], mean action: 54.667 [7.000, 98.000],  loss: 8.347116, mae: 2.498915, mean_q: 4.726392\n",
      "[84 95 95 42 12 95 28 76 37 97]\n",
      " 33363/50001: episode: 3707, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 14.000, mean reward:  1.556 [-10.000,  7.000], mean action: 64.111 [12.000, 97.000],  loss: 10.587339, mae: 2.460701, mean_q: 4.761683\n",
      "[43 94 97 75 13 13 88 66 94 20]\n",
      " 33372/50001: episode: 3708, duration: 0.072s, episode steps:   9, steps per second: 125, episode reward:  9.000, mean reward:  1.000 [-10.000,  5.000], mean action: 62.222 [13.000, 97.000],  loss: 7.820295, mae: 2.425542, mean_q: 4.661583\n",
      "[ 7 24 54 88 94 79 95 33  1 93]\n",
      " 33381/50001: episode: 3709, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 44.000, mean reward:  4.889 [ 3.000,  8.000], mean action: 62.333 [1.000, 95.000],  loss: 9.054752, mae: 2.375960, mean_q: 4.642570\n",
      "[77 37 62 42 13 34 88  2 72 50]\n",
      " 33390/50001: episode: 3710, duration: 0.065s, episode steps:   9, steps per second: 137, episode reward: 41.000, mean reward:  4.556 [ 1.000,  8.000], mean action: 44.444 [2.000, 88.000],  loss: 6.965246, mae: 2.383334, mean_q: 4.581450\n",
      "[29 34 11  6 84 31  1 51 79 97]\n",
      " 33399/50001: episode: 3711, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 43.778 [1.000, 97.000],  loss: 7.792786, mae: 2.343241, mean_q: 4.571343\n",
      "[87 28 92 46 68 69 27 95 12 20]\n",
      " 33408/50001: episode: 3712, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 50.778 [12.000, 95.000],  loss: 9.387146, mae: 2.346814, mean_q: 4.505501\n",
      "[77 16 67 44 99 17 13 98 89 66]\n",
      " 33417/50001: episode: 3713, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 56.556 [13.000, 99.000],  loss: 7.365126, mae: 2.410356, mean_q: 4.665657\n",
      "[36 26 12 89 96 57 37 68 97 12]\n",
      " 33426/50001: episode: 3714, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 54.889 [12.000, 97.000],  loss: 6.478088, mae: 2.417298, mean_q: 4.641344\n",
      "[81 80 18 16 35  7  7 32 70 47]\n",
      " 33435/50001: episode: 3715, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 24.000, mean reward:  2.667 [-10.000, 10.000], mean action: 34.667 [7.000, 80.000],  loss: 6.749461, mae: 2.452778, mean_q: 4.779673\n",
      "[72 46 93  1 99 75 41 54 57 32]\n",
      " 33444/50001: episode: 3716, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 43.000, mean reward:  4.778 [ 3.000,  8.000], mean action: 55.333 [1.000, 99.000],  loss: 7.839697, mae: 2.512522, mean_q: 4.795416\n",
      "[35 41 50 24 68 61 74  2 46 50]\n",
      " 33453/50001: episode: 3717, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 26.000, mean reward:  2.889 [-10.000,  6.000], mean action: 46.222 [2.000, 74.000],  loss: 6.366354, mae: 2.527089, mean_q: 4.928499\n",
      "[22 95 90 95 80 50 29  1 47 62]\n",
      " 33462/50001: episode: 3718, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 61.000 [1.000, 95.000],  loss: 6.666629, mae: 2.556992, mean_q: 4.854203\n",
      "[24 89 35 77 74 11 74 98 31 50]\n",
      " 33471/50001: episode: 3719, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 28.000, mean reward:  3.111 [-10.000,  9.000], mean action: 59.889 [11.000, 98.000],  loss: 8.408985, mae: 2.548965, mean_q: 4.895505\n",
      "[14 76  9  4 66 65 34  0 34 32]\n",
      " 33480/50001: episode: 3720, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 22.000, mean reward:  2.444 [-10.000,  7.000], mean action: 35.556 [0.000, 76.000],  loss: 7.640980, mae: 2.467334, mean_q: 4.842525\n",
      "[43 37 63 30 98 12 58 62  5 50]\n",
      " 33489/50001: episode: 3721, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 45.000, mean reward:  5.000 [ 3.000,  9.000], mean action: 46.111 [5.000, 98.000],  loss: 9.429646, mae: 2.488612, mean_q: 4.871620\n",
      "[81 50 87 98 30 75 67 99 14 28]\n",
      " 33498/50001: episode: 3722, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 60.889 [14.000, 99.000],  loss: 7.570989, mae: 2.497251, mean_q: 4.756417\n",
      "[16 28 53 12 41 31 12 32 95 97]\n",
      " 33507/50001: episode: 3723, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 29.000, mean reward:  3.222 [-10.000,  8.000], mean action: 44.556 [12.000, 97.000],  loss: 7.831888, mae: 2.476216, mean_q: 4.757734\n",
      "[56 42 48 82 93 60 33 30 71 50]\n",
      " 33516/50001: episode: 3724, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 41.000, mean reward:  4.556 [ 3.000, 10.000], mean action: 56.556 [30.000, 93.000],  loss: 7.327375, mae: 2.462642, mean_q: 4.682024\n",
      "[46 63  2  3 59 57 28 88 74 74]\n",
      " 33525/50001: episode: 3725, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 49.778 [2.000, 88.000],  loss: 7.640356, mae: 2.457881, mean_q: 4.690588\n",
      "[34 99 40 60 10 40 48 58 32 53]\n",
      " 33534/50001: episode: 3726, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 22.000, mean reward:  2.444 [-10.000,  5.000], mean action: 48.889 [10.000, 99.000],  loss: 9.564747, mae: 2.433221, mean_q: 4.626212\n",
      "[15 12 21  9 47 34 65 23 92 75]\n",
      " 33543/50001: episode: 3727, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 42.000 [9.000, 92.000],  loss: 6.178541, mae: 2.353271, mean_q: 4.498893\n",
      "[86 60 83 12 31 17 25  2 32 28]\n",
      " 33552/50001: episode: 3728, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 41.000, mean reward:  4.556 [ 2.000,  7.000], mean action: 32.222 [2.000, 83.000],  loss: 9.353331, mae: 2.400115, mean_q: 4.667265\n",
      "[51  5 76 16 63 21  9 39 48 79]\n",
      " 33561/50001: episode: 3729, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 39.000, mean reward:  4.333 [ 2.000,  8.000], mean action: 39.556 [5.000, 79.000],  loss: 8.938568, mae: 2.416760, mean_q: 4.665030\n",
      "[ 7 52 86  9 13 88 73 95 42 88]\n",
      " 33570/50001: episode: 3730, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 28.000, mean reward:  3.111 [-10.000,  9.000], mean action: 60.667 [9.000, 95.000],  loss: 7.396161, mae: 2.359369, mean_q: 4.507213\n",
      "[61 59 93 93 41 77 88  1 80 21]\n",
      " 33579/50001: episode: 3731, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 61.444 [1.000, 93.000],  loss: 6.903094, mae: 2.453422, mean_q: 4.653071\n",
      "[34 46 32 69 88 57 24 64 32 79]\n",
      " 33588/50001: episode: 3732, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 54.556 [24.000, 88.000],  loss: 6.134471, mae: 2.474672, mean_q: 4.711782\n",
      "[45 40  8 62 12 76 32 51 61  9]\n",
      " 33597/50001: episode: 3733, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 37.000, mean reward:  4.111 [ 3.000,  6.000], mean action: 39.000 [8.000, 76.000],  loss: 5.819831, mae: 2.483148, mean_q: 4.815707\n",
      "[41 66 24 88 95 88 43  1 94 50]\n",
      " 33606/50001: episode: 3734, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 28.000, mean reward:  3.111 [-10.000,  9.000], mean action: 61.000 [1.000, 95.000],  loss: 7.326925, mae: 2.502175, mean_q: 4.835459\n",
      "[78 42 52  4 37 98 28 95 59 89]\n",
      " 33615/50001: episode: 3735, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 56.000 [4.000, 98.000],  loss: 6.804036, mae: 2.532117, mean_q: 4.794335\n",
      "[53 10 37 98 78 47 64 50 24 82]\n",
      " 33624/50001: episode: 3736, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 54.444 [10.000, 98.000],  loss: 5.543005, mae: 2.491160, mean_q: 4.724802\n",
      "[46 95 98 58 28 12 52 89 63 28]\n",
      " 33633/50001: episode: 3737, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 24.000, mean reward:  2.667 [-10.000,  5.000], mean action: 58.111 [12.000, 98.000],  loss: 9.038487, mae: 2.615489, mean_q: 5.027753\n",
      "[54 31  5 73 65 21 59 83 34  4]\n",
      " 33642/50001: episode: 3738, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 40.000, mean reward:  4.444 [ 2.000, 10.000], mean action: 41.667 [4.000, 83.000],  loss: 7.786192, mae: 2.533226, mean_q: 4.849574\n",
      "[ 7 89 86 93 62  9 52 79 98 48]\n",
      " 33651/50001: episode: 3739, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 46.000, mean reward:  5.111 [ 3.000,  9.000], mean action: 68.444 [9.000, 98.000],  loss: 8.405051, mae: 2.485991, mean_q: 4.779423\n",
      "[29 50 88 46 82 14 74 52 42 42]\n",
      " 33660/50001: episode: 3740, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 23.000, mean reward:  2.556 [-10.000,  5.000], mean action: 54.444 [14.000, 88.000],  loss: 7.826090, mae: 2.462304, mean_q: 4.766287\n",
      "[ 8 43 46 77 23 56 24  8 88 89]\n",
      " 33669/50001: episode: 3741, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 50.444 [8.000, 89.000],  loss: 7.893911, mae: 2.402449, mean_q: 4.674097\n",
      "[43 42  4 41 93 31 51 48 67 81]\n",
      " 33678/50001: episode: 3742, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 50.889 [4.000, 93.000],  loss: 6.635706, mae: 2.405284, mean_q: 4.632784\n",
      "[68 28 89 98 47 37 50 99 13 85]\n",
      " 33687/50001: episode: 3743, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 60.667 [13.000, 99.000],  loss: 7.236893, mae: 2.455869, mean_q: 4.685061\n",
      "[72 30 48 26 24 66 59 63 48 82]\n",
      " 33696/50001: episode: 3744, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 49.556 [24.000, 82.000],  loss: 7.643962, mae: 2.446023, mean_q: 4.680536\n",
      "[37 52 28 99  0 54  0 50 20 23]\n",
      " 33705/50001: episode: 3745, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 36.222 [0.000, 99.000],  loss: 9.076608, mae: 2.366053, mean_q: 4.561594\n",
      "[68 16 61  9 89 18 11 17 48 46]\n",
      " 33714/50001: episode: 3746, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 38.000, mean reward:  4.222 [ 2.000,  8.000], mean action: 35.000 [9.000, 89.000],  loss: 5.680679, mae: 2.399042, mean_q: 4.640963\n",
      "[41 88 76 90 28 79 46 10 33 18]\n",
      " 33723/50001: episode: 3747, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 38.000, mean reward:  4.222 [ 3.000,  5.000], mean action: 52.000 [10.000, 90.000],  loss: 5.102807, mae: 2.429900, mean_q: 4.686948\n",
      "[73 37 47 89 93 54 10 89 42 87]\n",
      " 33732/50001: episode: 3748, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 60.889 [10.000, 93.000],  loss: 5.799696, mae: 2.546900, mean_q: 4.864447\n",
      "[57 95 80 99 24  0 32 27 83 86]\n",
      " 33741/50001: episode: 3749, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 33.000, mean reward:  3.667 [ 2.000,  5.000], mean action: 58.444 [0.000, 99.000],  loss: 7.702120, mae: 2.612145, mean_q: 5.008074\n",
      "[96 10 88  2 95 34 89  8  6 88]\n",
      " 33750/50001: episode: 3750, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 46.667 [2.000, 95.000],  loss: 9.414709, mae: 2.587953, mean_q: 4.940503\n",
      "[30 50 24 37 28 80 53 75 31 12]\n",
      " 33759/50001: episode: 3751, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 42.000, mean reward:  4.667 [ 2.000,  8.000], mean action: 43.333 [12.000, 80.000],  loss: 6.181655, mae: 2.559903, mean_q: 4.831432\n",
      "[71 52 19 89 74  1 19 46 55 31]\n",
      " 33768/50001: episode: 3752, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 42.889 [1.000, 89.000],  loss: 9.501767, mae: 2.511578, mean_q: 4.742759\n",
      "[94 23 18 86 24 96 66 68  2 14]\n",
      " 33777/50001: episode: 3753, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 37.000, mean reward:  4.111 [ 2.000,  6.000], mean action: 44.111 [2.000, 96.000],  loss: 9.513552, mae: 2.488183, mean_q: 4.789892\n",
      "[75 48 38 82 78 57 73 37 40 82]\n",
      " 33786/50001: episode: 3754, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 24.000, mean reward:  2.667 [-10.000,  8.000], mean action: 59.444 [37.000, 82.000],  loss: 7.759769, mae: 2.458054, mean_q: 4.628097\n",
      "[86 95 76 23  9 48 62 13  9 48]\n",
      " 33795/50001: episode: 3755, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 12.000, mean reward:  1.333 [-10.000,  7.000], mean action: 42.556 [9.000, 95.000],  loss: 6.294650, mae: 2.346879, mean_q: 4.525654\n",
      "[ 1 28 99 21 46 59 74 23 37 37]\n",
      " 33804/50001: episode: 3756, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 24.000, mean reward:  2.667 [-10.000,  8.000], mean action: 47.111 [21.000, 99.000],  loss: 8.306149, mae: 2.444497, mean_q: 4.700156\n",
      "[62 28 53 48 88 34 45 78 80 48]\n",
      " 33813/50001: episode: 3757, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 21.000, mean reward:  2.333 [-10.000,  6.000], mean action: 55.778 [28.000, 88.000],  loss: 7.938879, mae: 2.431865, mean_q: 4.651029\n",
      "[62 95 77 49 98 99 86 34 95 34]\n",
      " 33822/50001: episode: 3758, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 11.000, mean reward:  1.222 [-10.000,  8.000], mean action: 74.111 [34.000, 99.000],  loss: 7.398282, mae: 2.415246, mean_q: 4.546909\n",
      "[97 53 67 84 10 50 42 98 81 13]\n",
      " 33831/50001: episode: 3759, duration: 0.073s, episode steps:   9, steps per second: 124, episode reward: 41.000, mean reward:  4.556 [ 3.000,  9.000], mean action: 55.333 [10.000, 98.000],  loss: 6.318419, mae: 2.468027, mean_q: 4.671745\n",
      "[46 67 24 47 33  9 49 37 80 75]\n",
      " 33840/50001: episode: 3760, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 38.000, mean reward:  4.222 [ 3.000,  7.000], mean action: 46.778 [9.000, 80.000],  loss: 10.352336, mae: 2.481851, mean_q: 4.726638\n",
      "[93 67 14 93  1  0  8 74 34 19]\n",
      " 33849/50001: episode: 3761, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 25.000, mean reward:  2.778 [-10.000,  9.000], mean action: 34.444 [0.000, 93.000],  loss: 7.629004, mae: 2.425354, mean_q: 4.649209\n",
      "[90 56 81 12 58 14 39 43 89 74]\n",
      " 33858/50001: episode: 3762, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 34.000, mean reward:  3.778 [ 2.000,  6.000], mean action: 51.778 [12.000, 89.000],  loss: 7.734904, mae: 2.401494, mean_q: 4.609120\n",
      "[80 31  4 75 84 28 50 30 34  1]\n",
      " 33867/50001: episode: 3763, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 45.000, mean reward:  5.000 [ 3.000,  8.000], mean action: 37.444 [1.000, 84.000],  loss: 8.177630, mae: 2.414555, mean_q: 4.587710\n",
      "[94 42 47  4 23 54 57 17 20 46]\n",
      " 33876/50001: episode: 3764, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 36.000, mean reward:  4.000 [ 2.000,  7.000], mean action: 34.444 [4.000, 57.000],  loss: 8.656726, mae: 2.433141, mean_q: 4.633820\n",
      "[ 8 95 80 48 18 14 24 66 37 88]\n",
      " 33885/50001: episode: 3765, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 43.000, mean reward:  4.778 [ 2.000,  7.000], mean action: 52.222 [14.000, 95.000],  loss: 6.759933, mae: 2.353258, mean_q: 4.479741\n",
      "[42  0 93 11 98 84 95  9 12  5]\n",
      " 33894/50001: episode: 3766, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 44.000, mean reward:  4.889 [ 2.000,  8.000], mean action: 45.222 [0.000, 98.000],  loss: 6.849503, mae: 2.357842, mean_q: 4.508160\n",
      "[ 5 43 58 57 67 88 41 95 31  2]\n",
      " 33903/50001: episode: 3767, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 43.000, mean reward:  4.778 [ 2.000,  8.000], mean action: 53.556 [2.000, 95.000],  loss: 7.691544, mae: 2.451109, mean_q: 4.704991\n",
      "[41 52 81 66 88 88 52 82 28 46]\n",
      " 33912/50001: episode: 3768, duration: 0.070s, episode steps:   9, steps per second: 129, episode reward: 11.000, mean reward:  1.222 [-10.000,  6.000], mean action: 64.778 [28.000, 88.000],  loss: 9.181071, mae: 2.466771, mean_q: 4.695683\n",
      "[96 18 59 26 82 56 30 52 48 31]\n",
      " 33921/50001: episode: 3769, duration: 0.073s, episode steps:   9, steps per second: 123, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 44.667 [18.000, 82.000],  loss: 6.811729, mae: 2.359675, mean_q: 4.512049\n",
      "[ 8 31 76 88 63 79 80 67 31 66]\n",
      " 33930/50001: episode: 3770, duration: 0.079s, episode steps:   9, steps per second: 114, episode reward: 26.000, mean reward:  2.889 [-10.000,  8.000], mean action: 64.556 [31.000, 88.000],  loss: 8.312003, mae: 2.366661, mean_q: 4.492044\n",
      "[23 96 48 49 75 74 51 54 46 48]\n",
      " 33939/50001: episode: 3771, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 22.000, mean reward:  2.444 [-10.000,  7.000], mean action: 60.111 [46.000, 96.000],  loss: 6.906748, mae: 2.347489, mean_q: 4.428981\n",
      "[76 94 44 95 37 88 92 18 24 44]\n",
      " 33948/50001: episode: 3772, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 26.000, mean reward:  2.889 [-10.000,  6.000], mean action: 59.556 [18.000, 95.000],  loss: 7.490344, mae: 2.335517, mean_q: 4.491582\n",
      "[83 27 30 60 23 41 60 40 94 46]\n",
      " 33957/50001: episode: 3773, duration: 0.068s, episode steps:   9, steps per second: 133, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 46.778 [23.000, 94.000],  loss: 8.372599, mae: 2.358634, mean_q: 4.540808\n",
      "[82 52 30 69 43 87  9 95 57  8]\n",
      " 33966/50001: episode: 3774, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 39.000, mean reward:  4.333 [ 2.000,  9.000], mean action: 50.000 [8.000, 95.000],  loss: 10.609115, mae: 2.360039, mean_q: 4.583392\n",
      "[70 91 20 30 97 14 90 98 62 50]\n",
      " 33975/50001: episode: 3775, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 44.000, mean reward:  4.889 [ 3.000,  9.000], mean action: 61.333 [14.000, 98.000],  loss: 7.604794, mae: 2.300945, mean_q: 4.487969\n",
      "[63 27 66 17 99 87 92 17  5 44]\n",
      " 33984/50001: episode: 3776, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 17.000, mean reward:  1.889 [-10.000,  5.000], mean action: 50.444 [5.000, 99.000],  loss: 6.335798, mae: 2.333430, mean_q: 4.530391\n",
      "[28 97 37 54 41 83 46 71 85 66]\n",
      " 33993/50001: episode: 3777, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 39.000, mean reward:  4.333 [ 3.000,  9.000], mean action: 64.444 [37.000, 97.000],  loss: 7.735735, mae: 2.352541, mean_q: 4.489874\n",
      "[97  4 84 98 28 74 37 95  2 97]\n",
      " 34002/50001: episode: 3778, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 28.000, mean reward:  3.111 [-10.000,  7.000], mean action: 57.667 [2.000, 98.000],  loss: 7.311942, mae: 2.449256, mean_q: 4.672442\n",
      "[ 0 40 76 53 27 41 23 87 12 97]\n",
      " 34011/50001: episode: 3779, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 44.000, mean reward:  4.889 [ 3.000,  8.000], mean action: 50.667 [12.000, 97.000],  loss: 7.310219, mae: 2.429085, mean_q: 4.615561\n",
      "[64 57 27 97 40  0 10 17  0 50]\n",
      " 34020/50001: episode: 3780, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 24.000, mean reward:  2.667 [-10.000, 10.000], mean action: 33.111 [0.000, 97.000],  loss: 8.305354, mae: 2.404989, mean_q: 4.629730\n",
      "[13 28 79 59 24 46 66 76 59 98]\n",
      " 34029/50001: episode: 3781, duration: 0.070s, episode steps:   9, steps per second: 128, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 59.444 [24.000, 98.000],  loss: 6.713659, mae: 2.361946, mean_q: 4.528199\n",
      "[45 98 25 79 30 72 35 52 42 96]\n",
      " 34038/50001: episode: 3782, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 38.000, mean reward:  4.222 [ 2.000,  6.000], mean action: 58.778 [25.000, 98.000],  loss: 8.864646, mae: 2.384327, mean_q: 4.600374\n",
      "[ 3 75 60 13 42 44 54 95 24 47]\n",
      " 34047/50001: episode: 3783, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 45.000, mean reward:  5.000 [ 3.000,  9.000], mean action: 50.444 [13.000, 95.000],  loss: 8.570178, mae: 2.411565, mean_q: 4.667459\n",
      "[30 28 66 45 49 90 44 33 76  2]\n",
      " 34056/50001: episode: 3784, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 35.000, mean reward:  3.889 [ 2.000,  8.000], mean action: 48.111 [2.000, 90.000],  loss: 5.657293, mae: 2.355114, mean_q: 4.497678\n",
      "[97 60 12 58  1 23 88 15 27 55]\n",
      " 34065/50001: episode: 3785, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 35.000, mean reward:  3.889 [ 2.000,  6.000], mean action: 37.667 [1.000, 88.000],  loss: 8.852704, mae: 2.485239, mean_q: 4.717251\n",
      "[74  2 39 18 31 76 96  9  7 64]\n",
      " 34074/50001: episode: 3786, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 33.000, mean reward:  3.667 [ 2.000,  5.000], mean action: 38.000 [2.000, 96.000],  loss: 9.897210, mae: 2.485063, mean_q: 4.756578\n",
      "[90 13 16 32 88 54 53  8 52 15]\n",
      " 34083/50001: episode: 3787, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 34.000, mean reward:  3.778 [ 3.000,  5.000], mean action: 36.778 [8.000, 88.000],  loss: 8.526270, mae: 2.458151, mean_q: 4.658234\n",
      "[ 1  1 33 95  9 69 83 96 37 37]\n",
      " 34092/50001: episode: 3788, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward:  9.000, mean reward:  1.000 [-10.000,  8.000], mean action: 51.111 [1.000, 96.000],  loss: 8.160089, mae: 2.359029, mean_q: 4.583038\n",
      "[ 0 13 38  4 62 24 52 50 12 12]\n",
      " 34101/50001: episode: 3789, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 29.000, mean reward:  3.222 [-10.000,  8.000], mean action: 29.667 [4.000, 62.000],  loss: 5.547764, mae: 2.362032, mean_q: 4.585368\n",
      "[31 39 28 75 10 46 60 10 28 64]\n",
      " 34110/50001: episode: 3790, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward:  7.000, mean reward:  0.778 [-10.000,  5.000], mean action: 40.000 [10.000, 75.000],  loss: 9.371436, mae: 2.405065, mean_q: 4.628057\n",
      "[86 11 83 82 47 88 17 50  2  2]\n",
      " 34119/50001: episode: 3791, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 42.444 [2.000, 88.000],  loss: 7.671442, mae: 2.495628, mean_q: 4.713255\n",
      "[46 97 90 34 95 74 48 95 75 75]\n",
      " 34128/50001: episode: 3792, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 13.000, mean reward:  1.444 [-10.000,  6.000], mean action: 75.889 [34.000, 97.000],  loss: 5.147738, mae: 2.459168, mean_q: 4.680280\n",
      "[88 26 50 60 95 95 95 93 88  4]\n",
      " 34137/50001: episode: 3793, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: -4.000, mean reward: -0.444 [-10.000,  6.000], mean action: 67.333 [4.000, 95.000],  loss: 8.348251, mae: 2.518900, mean_q: 4.832232\n",
      "[39 50 37 86 54 18 41 34 41  3]\n",
      " 34146/50001: episode: 3794, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 24.000, mean reward:  2.667 [-10.000,  8.000], mean action: 40.444 [3.000, 86.000],  loss: 6.730811, mae: 2.534983, mean_q: 4.815329\n",
      "[96 68  7 12 66 45 90 12 34 64]\n",
      " 34155/50001: episode: 3795, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 24.000, mean reward:  2.667 [-10.000,  8.000], mean action: 44.222 [7.000, 90.000],  loss: 7.197499, mae: 2.553221, mean_q: 4.853086\n",
      "[51 52 53 88  9 59 50 28  9 97]\n",
      " 34164/50001: episode: 3796, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 49.444 [9.000, 97.000],  loss: 8.020852, mae: 2.527900, mean_q: 4.859560\n",
      "[67 69 66  1 55 83 37 98 41 89]\n",
      " 34173/50001: episode: 3797, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 59.889 [1.000, 98.000],  loss: 6.475015, mae: 2.437770, mean_q: 4.653252\n",
      "[66 37 12 93 12 94 89 35 31 26]\n",
      " 34182/50001: episode: 3798, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 19.000, mean reward:  2.111 [-10.000,  6.000], mean action: 47.667 [12.000, 94.000],  loss: 6.977297, mae: 2.511086, mean_q: 4.760806\n",
      "[71 30 33 10 54 83 95 50 37 97]\n",
      " 34191/50001: episode: 3799, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 47.000, mean reward:  5.222 [ 3.000,  8.000], mean action: 54.333 [10.000, 97.000],  loss: 6.021382, mae: 2.444463, mean_q: 4.680194\n",
      "[44 89 48 51 31 12 82 61 36  9]\n",
      " 34200/50001: episode: 3800, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 36.000, mean reward:  4.000 [ 2.000,  6.000], mean action: 46.556 [9.000, 89.000],  loss: 9.431354, mae: 2.467059, mean_q: 4.692392\n",
      "[11 24 94 32 62  5 88 23 95 50]\n",
      " 34209/50001: episode: 3801, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 46.000, mean reward:  5.111 [ 3.000,  9.000], mean action: 52.556 [5.000, 95.000],  loss: 9.537106, mae: 2.467402, mean_q: 4.755393\n",
      "[77 95 16 12 92 48 95 48 93 32]\n",
      " 34218/50001: episode: 3802, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward:  9.000, mean reward:  1.000 [-10.000,  6.000], mean action: 59.000 [12.000, 95.000],  loss: 8.294430, mae: 2.371776, mean_q: 4.584205\n",
      "[18  5 63 24  1 67 57 42 37  9]\n",
      " 34227/50001: episode: 3803, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 33.889 [1.000, 67.000],  loss: 7.032789, mae: 2.311987, mean_q: 4.481126\n",
      "[59 23 95 47 78 15 58 51 88 21]\n",
      " 34236/50001: episode: 3804, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 35.000, mean reward:  3.889 [ 2.000,  7.000], mean action: 52.889 [15.000, 95.000],  loss: 5.440081, mae: 2.394949, mean_q: 4.676691\n",
      "[39 34 36 38  1 37 39 28 42 46]\n",
      " 34245/50001: episode: 3805, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 30.000, mean reward:  3.333 [-10.000,  7.000], mean action: 33.444 [1.000, 46.000],  loss: 7.385620, mae: 2.443751, mean_q: 4.660281\n",
      "[47 44 46 34 67  4 90 32 76 18]\n",
      " 34254/50001: episode: 3806, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 37.000, mean reward:  4.111 [ 2.000,  6.000], mean action: 45.667 [4.000, 90.000],  loss: 7.493757, mae: 2.447194, mean_q: 4.717120\n",
      "[92 64 87 64 12 84 46 97 85 37]\n",
      " 34263/50001: episode: 3807, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 28.000, mean reward:  3.111 [-10.000,  9.000], mean action: 64.000 [12.000, 97.000],  loss: 6.610533, mae: 2.457191, mean_q: 4.666950\n",
      "[33 37 37 79 44 31 25 69 89 85]\n",
      " 34272/50001: episode: 3808, duration: 0.073s, episode steps:   9, steps per second: 123, episode reward: 21.000, mean reward:  2.333 [-10.000,  5.000], mean action: 55.111 [25.000, 89.000],  loss: 8.040014, mae: 2.538312, mean_q: 4.848883\n",
      "[79 48 96 29 84 28  1  9 97 47]\n",
      " 34281/50001: episode: 3809, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 48.778 [1.000, 97.000],  loss: 7.216688, mae: 2.566681, mean_q: 4.844367\n",
      "[28 34 40 85 94 57  0 57  9 88]\n",
      " 34290/50001: episode: 3810, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 24.000, mean reward:  2.667 [-10.000,  9.000], mean action: 51.556 [0.000, 94.000],  loss: 6.724170, mae: 2.455976, mean_q: 4.762556\n",
      "[60  5 39 82 47 97  8 10 68 50]\n",
      " 34299/50001: episode: 3811, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 39.000, mean reward:  4.333 [ 2.000,  9.000], mean action: 45.111 [5.000, 97.000],  loss: 6.934757, mae: 2.535962, mean_q: 4.877703\n",
      "[59 34 64 93 97 46 13 38 57  6]\n",
      " 34308/50001: episode: 3812, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 40.000, mean reward:  4.444 [ 2.000,  6.000], mean action: 49.778 [6.000, 97.000],  loss: 7.375422, mae: 2.533189, mean_q: 4.835333\n",
      "[57 79 28 32 82  8 48 48 48 74]\n",
      " 34317/50001: episode: 3813, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward:  9.000, mean reward:  1.000 [-10.000,  5.000], mean action: 49.667 [8.000, 82.000],  loss: 6.579964, mae: 2.557583, mean_q: 4.914366\n",
      "[48 28 13 37 47 89 13 67  1 28]\n",
      " 34326/50001: episode: 3814, duration: 0.072s, episode steps:   9, steps per second: 126, episode reward:  9.000, mean reward:  1.000 [-10.000,  6.000], mean action: 35.889 [1.000, 89.000],  loss: 7.693406, mae: 2.558585, mean_q: 4.872109\n",
      "[ 6 14 97 86 96  9 46 50 10 97]\n",
      " 34335/50001: episode: 3815, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 56.111 [9.000, 97.000],  loss: 5.879751, mae: 2.491843, mean_q: 4.822586\n",
      "[98 86 98 95 28 50 37 97 81 48]\n",
      " 34344/50001: episode: 3816, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 29.000, mean reward:  3.222 [-10.000,  7.000], mean action: 68.889 [28.000, 98.000],  loss: 6.614219, mae: 2.519292, mean_q: 4.854259\n",
      "[44 35 82 47 50 21 97 84 48 12]\n",
      " 34353/50001: episode: 3817, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 45.000, mean reward:  5.000 [ 3.000,  8.000], mean action: 52.889 [12.000, 97.000],  loss: 5.863235, mae: 2.544321, mean_q: 4.909404\n",
      "[ 4 28 15 16 61 77 95 83 98 79]\n",
      " 34362/50001: episode: 3818, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 38.000, mean reward:  4.222 [ 2.000,  8.000], mean action: 61.333 [15.000, 98.000],  loss: 7.838133, mae: 2.528256, mean_q: 4.877763\n",
      "[53 95 12 82 14 95 50 95 13 12]\n",
      " 34371/50001: episode: 3819, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: -3.000, mean reward: -0.333 [-10.000,  6.000], mean action: 52.000 [12.000, 95.000],  loss: 6.103777, mae: 2.541816, mean_q: 4.826521\n",
      "[70 27 47 18  1 95 34 95 41 46]\n",
      " 34380/50001: episode: 3820, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 29.000, mean reward:  3.222 [-10.000,  7.000], mean action: 44.889 [1.000, 95.000],  loss: 7.524801, mae: 2.467847, mean_q: 4.683736\n",
      "[40  5 96  3  1 95 28 33 50 96]\n",
      " 34389/50001: episode: 3821, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 45.222 [1.000, 96.000],  loss: 5.988438, mae: 2.502485, mean_q: 4.758105\n",
      "[ 7 71 85 65 28 62 13 21 28 50]\n",
      " 34398/50001: episode: 3822, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 26.000, mean reward:  2.889 [-10.000,  9.000], mean action: 47.000 [13.000, 85.000],  loss: 7.091724, mae: 2.533786, mean_q: 4.939706\n",
      "[90 27 73 41 99  3  9 31 66 33]\n",
      " 34407/50001: episode: 3823, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 42.444 [3.000, 99.000],  loss: 5.311269, mae: 2.515640, mean_q: 4.801136\n",
      "[77 48 48 35 31 34 28  8 15 79]\n",
      " 34416/50001: episode: 3824, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 36.222 [8.000, 79.000],  loss: 7.467043, mae: 2.625156, mean_q: 4.956456\n",
      "[ 2 37 79 14 27 66 46 87 88 14]\n",
      " 34425/50001: episode: 3825, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 50.889 [14.000, 88.000],  loss: 7.314711, mae: 2.621346, mean_q: 4.952614\n",
      "[30 15  6 21 34 59 15 86 69 27]\n",
      " 34434/50001: episode: 3826, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 36.889 [6.000, 86.000],  loss: 8.481416, mae: 2.623027, mean_q: 5.011737\n",
      "[99 41 78 95 96 25 94 31 23 66]\n",
      " 34443/50001: episode: 3827, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 43.000, mean reward:  4.778 [ 3.000,  8.000], mean action: 61.000 [23.000, 96.000],  loss: 6.275154, mae: 2.579879, mean_q: 4.909483\n",
      "[32 67 76 91 95 42 74  6 37  4]\n",
      " 34452/50001: episode: 3828, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 54.667 [4.000, 95.000],  loss: 6.219823, mae: 2.537546, mean_q: 4.899767\n",
      "[42 61 50 80 28  9 96 18 10 88]\n",
      " 34461/50001: episode: 3829, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 40.000, mean reward:  4.444 [ 2.000,  9.000], mean action: 48.889 [9.000, 96.000],  loss: 5.866861, mae: 2.602947, mean_q: 4.987727\n",
      "[81  2 99 73 99 46  2 87 42 23]\n",
      " 34470/50001: episode: 3830, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward:  8.000, mean reward:  0.889 [-10.000,  6.000], mean action: 52.556 [2.000, 99.000],  loss: 7.079185, mae: 2.575162, mean_q: 4.818009\n",
      "[98 42 37 14 60 95 34 60 33 27]\n",
      " 34479/50001: episode: 3831, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 27.000, mean reward:  3.000 [-10.000,  6.000], mean action: 44.667 [14.000, 95.000],  loss: 8.448421, mae: 2.559507, mean_q: 4.875241\n",
      "[53  1 23 11 31 17 41 61  0 12]\n",
      " 34488/50001: episode: 3832, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 37.000, mean reward:  4.111 [ 2.000, 10.000], mean action: 21.889 [0.000, 61.000],  loss: 5.867594, mae: 2.539681, mean_q: 4.873998\n",
      "[35 41  3 95 75  4  2 46 34 96]\n",
      " 34497/50001: episode: 3833, duration: 0.065s, episode steps:   9, steps per second: 137, episode reward: 43.000, mean reward:  4.778 [ 3.000,  8.000], mean action: 44.000 [2.000, 96.000],  loss: 7.988521, mae: 2.606405, mean_q: 4.940022\n",
      "[38 13 27 55 40 69 60 73 37 43]\n",
      " 34506/50001: episode: 3834, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 36.000, mean reward:  4.000 [ 2.000,  9.000], mean action: 46.333 [13.000, 73.000],  loss: 8.611025, mae: 2.559593, mean_q: 4.876876\n",
      "[33  9 83 66  9  2  2 55 31 95]\n",
      " 34515/50001: episode: 3835, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 15.000, mean reward:  1.667 [-10.000, 11.000], mean action: 39.111 [2.000, 95.000],  loss: 6.061039, mae: 2.544047, mean_q: 4.826539\n",
      "[ 4 41 13 92 44 52 34 32 14 52]\n",
      " 34524/50001: episode: 3836, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 41.556 [13.000, 92.000],  loss: 6.782516, mae: 2.459119, mean_q: 4.699061\n",
      "[78 34 51 13 25 45 57 55 52 94]\n",
      " 34533/50001: episode: 3837, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 35.000, mean reward:  3.889 [ 2.000,  5.000], mean action: 47.333 [13.000, 94.000],  loss: 7.028584, mae: 2.433050, mean_q: 4.645695\n",
      "[12 89 37 45 67 70 28 99  7 12]\n",
      " 34542/50001: episode: 3838, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 18.000, mean reward:  2.000 [-10.000,  6.000], mean action: 50.444 [7.000, 99.000],  loss: 7.397708, mae: 2.397048, mean_q: 4.551738\n",
      "[12 42 28 38 85 27 63 98 64 23]\n",
      " 34551/50001: episode: 3839, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 52.000 [23.000, 98.000],  loss: 7.508841, mae: 2.417184, mean_q: 4.606277\n",
      "[75  5 32 37 13 35 98 34 94 88]\n",
      " 34560/50001: episode: 3840, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 41.000, mean reward:  4.556 [ 2.000,  7.000], mean action: 48.444 [5.000, 98.000],  loss: 4.848570, mae: 2.435440, mean_q: 4.722765\n",
      "[77 80 88 34 95 24 25 92 28 79]\n",
      " 34569/50001: episode: 3841, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 40.000, mean reward:  4.444 [ 2.000,  6.000], mean action: 60.556 [24.000, 95.000],  loss: 8.225336, mae: 2.441889, mean_q: 4.622599\n",
      "[ 0 37 24 75 32 34 42 68 13 13]\n",
      " 34578/50001: episode: 3842, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 37.556 [13.000, 75.000],  loss: 6.045419, mae: 2.464225, mean_q: 4.695467\n",
      "[62 83 88  4 13 94 14 95 98 24]\n",
      " 34587/50001: episode: 3843, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 57.000 [4.000, 98.000],  loss: 7.145989, mae: 2.528873, mean_q: 4.849705\n",
      "[18 31 81 65 11 13 57 62 57 79]\n",
      " 34596/50001: episode: 3844, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 50.667 [11.000, 81.000],  loss: 6.960522, mae: 2.602176, mean_q: 4.946096\n",
      "[95 41 12 46 75  4  9  6 71 34]\n",
      " 34605/50001: episode: 3845, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 40.000, mean reward:  4.444 [ 3.000, 10.000], mean action: 33.111 [4.000, 75.000],  loss: 9.849080, mae: 2.617797, mean_q: 5.006845\n",
      "[11 60 46 37 34 60 79 48 34 66]\n",
      " 34614/50001: episode: 3846, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 14.000, mean reward:  1.556 [-10.000,  7.000], mean action: 51.556 [34.000, 79.000],  loss: 5.896365, mae: 2.476838, mean_q: 4.719289\n",
      "[54  4 77 86 42 74 34 68 83 12]\n",
      " 34623/50001: episode: 3847, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward: 41.000, mean reward:  4.556 [ 3.000,  9.000], mean action: 53.333 [4.000, 86.000],  loss: 5.356807, mae: 2.509179, mean_q: 4.761621\n",
      "[60 30  2 11 11 61 32 86 74 79]\n",
      " 34632/50001: episode: 3848, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 42.889 [2.000, 86.000],  loss: 9.479414, mae: 2.570249, mean_q: 4.893384\n",
      "[71 13 71 30 53 57 41 97 31 53]\n",
      " 34641/50001: episode: 3849, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 14.000, mean reward:  1.556 [-10.000,  7.000], mean action: 49.556 [13.000, 97.000],  loss: 8.995170, mae: 2.530345, mean_q: 4.774034\n",
      "[40 54 31 84 28 46 47 85 69 43]\n",
      " 34650/50001: episode: 3850, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 34.000, mean reward:  3.778 [ 2.000,  6.000], mean action: 54.111 [28.000, 85.000],  loss: 6.304590, mae: 2.463560, mean_q: 4.695149\n",
      "[39 99 63  4 82 35 47 28 89 51]\n",
      " 34659/50001: episode: 3851, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 55.333 [4.000, 99.000],  loss: 6.393850, mae: 2.425122, mean_q: 4.607815\n",
      "[ 4 35  3  2 82 77  2 89 75 66]\n",
      " 34668/50001: episode: 3852, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 47.889 [2.000, 89.000],  loss: 7.612055, mae: 2.492793, mean_q: 4.699941\n",
      "[78 30 36 13 37 27 88 98  8 13]\n",
      " 34677/50001: episode: 3853, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 38.889 [8.000, 98.000],  loss: 7.989982, mae: 2.451535, mean_q: 4.656374\n",
      "[51 13 42  1 42 25 69 10 34 85]\n",
      " 34686/50001: episode: 3854, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 25.000, mean reward:  2.778 [-10.000,  9.000], mean action: 35.667 [1.000, 85.000],  loss: 7.166218, mae: 2.408102, mean_q: 4.614790\n",
      "[91 37 35 73 13 60 51 64 31  2]\n",
      " 34695/50001: episode: 3855, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 39.000, mean reward:  4.333 [ 2.000,  6.000], mean action: 40.667 [2.000, 73.000],  loss: 8.087703, mae: 2.449021, mean_q: 4.625704\n",
      "[ 7  1 41 28  9 81 12 48 37 20]\n",
      " 34704/50001: episode: 3856, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 30.778 [1.000, 81.000],  loss: 10.791505, mae: 2.496555, mean_q: 4.759377\n",
      "[ 4 33 34 31 84 48  6 84 48 12]\n",
      " 34713/50001: episode: 3857, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 11.000, mean reward:  1.222 [-10.000,  8.000], mean action: 42.222 [6.000, 84.000],  loss: 6.251552, mae: 2.351716, mean_q: 4.565267\n",
      "[42 76 62 13 56 53 88 46 28 89]\n",
      " 34722/50001: episode: 3858, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 41.000, mean reward:  4.556 [ 3.000,  6.000], mean action: 56.778 [13.000, 89.000],  loss: 8.543290, mae: 2.426862, mean_q: 4.563770\n",
      "[93 91  2 54 30 92 13 24 89 50]\n",
      " 34731/50001: episode: 3859, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 44.000, mean reward:  4.889 [ 3.000,  9.000], mean action: 49.444 [2.000, 92.000],  loss: 9.145882, mae: 2.418296, mean_q: 4.572275\n",
      "[37 95  4 30 19 97 24 24 98 47]\n",
      " 34740/50001: episode: 3860, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 48.667 [4.000, 98.000],  loss: 5.977958, mae: 2.375573, mean_q: 4.607947\n",
      "[34 87 54 45 57 84  7 98 46  9]\n",
      " 34749/50001: episode: 3861, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 37.000, mean reward:  4.111 [ 2.000,  7.000], mean action: 54.111 [7.000, 98.000],  loss: 6.186076, mae: 2.430647, mean_q: 4.622984\n",
      "[84 28 48 63 79 95 11 41 70 16]\n",
      " 34758/50001: episode: 3862, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 36.000, mean reward:  4.000 [ 2.000,  6.000], mean action: 50.111 [11.000, 95.000],  loss: 8.132859, mae: 2.377542, mean_q: 4.504938\n",
      "[53 51 74 40 27 25  2 82 82  6]\n",
      " 34767/50001: episode: 3863, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 21.000, mean reward:  2.333 [-10.000,  5.000], mean action: 43.222 [2.000, 82.000],  loss: 8.838994, mae: 2.462895, mean_q: 4.692594\n",
      "[44 88 20 11 82 14 86 50 24 31]\n",
      " 34776/50001: episode: 3864, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 45.111 [11.000, 88.000],  loss: 7.017939, mae: 2.359271, mean_q: 4.528720\n",
      "[36 37 28 66 57 54 88 41 52 93]\n",
      " 34785/50001: episode: 3865, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 42.000, mean reward:  4.667 [ 3.000,  6.000], mean action: 57.333 [28.000, 93.000],  loss: 6.447263, mae: 2.400531, mean_q: 4.600348\n",
      "[51 49 31 93 32 53 75 99 98 95]\n",
      " 34794/50001: episode: 3866, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 44.000, mean reward:  4.889 [ 3.000, 10.000], mean action: 69.444 [31.000, 99.000],  loss: 8.961711, mae: 2.495666, mean_q: 4.760375\n",
      "[12 34 37 98 70 32 95 28 14 89]\n",
      " 34803/50001: episode: 3867, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 41.000, mean reward:  4.556 [ 2.000,  7.000], mean action: 55.222 [14.000, 98.000],  loss: 10.315960, mae: 2.481878, mean_q: 4.730961\n",
      "[ 4 31 29 93 76 21 98 14 37 94]\n",
      " 34812/50001: episode: 3868, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 41.000, mean reward:  4.556 [ 2.000,  8.000], mean action: 54.778 [14.000, 98.000],  loss: 7.267594, mae: 2.436357, mean_q: 4.624725\n",
      "[96 69 27 49 91 31 90 88 39 24]\n",
      " 34821/50001: episode: 3869, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 56.444 [24.000, 91.000],  loss: 5.546242, mae: 2.394661, mean_q: 4.591426\n",
      "[80  5 32 31 37 95 89  6 13  5]\n",
      " 34830/50001: episode: 3870, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 34.778 [5.000, 95.000],  loss: 7.502106, mae: 2.401440, mean_q: 4.611836\n",
      "[36 63 24 31 93 51 42 97 88 62]\n",
      " 34839/50001: episode: 3871, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 61.222 [24.000, 97.000],  loss: 9.316113, mae: 2.541059, mean_q: 4.818805\n",
      "[45 45 76 48 56 86  2  6 95 95]\n",
      " 34848/50001: episode: 3872, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 14.000, mean reward:  1.556 [-10.000, 10.000], mean action: 56.556 [2.000, 95.000],  loss: 8.300948, mae: 2.405294, mean_q: 4.549011\n",
      "[83 28 28 76 65 13 83 73 58 98]\n",
      " 34857/50001: episode: 3873, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 11.000, mean reward:  1.222 [-10.000,  8.000], mean action: 58.000 [13.000, 98.000],  loss: 6.108408, mae: 2.435107, mean_q: 4.608490\n",
      "[34 71 56 42 95 60  4 85 48 12]\n",
      " 34866/50001: episode: 3874, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 41.000, mean reward:  4.556 [ 2.000,  7.000], mean action: 52.556 [4.000, 95.000],  loss: 7.932207, mae: 2.420664, mean_q: 4.636958\n",
      "[33 74 87 35 79 27 46 92 92  0]\n",
      " 34875/50001: episode: 3875, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 20.000, mean reward:  2.222 [-10.000,  5.000], mean action: 59.111 [0.000, 92.000],  loss: 8.012207, mae: 2.377301, mean_q: 4.490563\n",
      "[24 37 94 93 46 12 94 28 66 48]\n",
      " 34884/50001: episode: 3876, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 28.000, mean reward:  3.111 [-10.000,  6.000], mean action: 57.556 [12.000, 94.000],  loss: 6.882083, mae: 2.410165, mean_q: 4.558271\n",
      "[34 73 41 10 12 90 64 74 24 86]\n",
      " 34893/50001: episode: 3877, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 36.000, mean reward:  4.000 [ 2.000,  6.000], mean action: 52.667 [10.000, 90.000],  loss: 9.633304, mae: 2.388398, mean_q: 4.616048\n",
      "[ 6 44 92 36  1 41 13 50 88 75]\n",
      " 34902/50001: episode: 3878, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 45.000, mean reward:  5.000 [ 2.000,  7.000], mean action: 48.889 [1.000, 92.000],  loss: 7.772103, mae: 2.329507, mean_q: 4.521833\n",
      "[15 42 71 41 44 30 32 14 88 80]\n",
      " 34911/50001: episode: 3879, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 41.000, mean reward:  4.556 [ 2.000,  8.000], mean action: 49.111 [14.000, 88.000],  loss: 6.118343, mae: 2.346929, mean_q: 4.488061\n",
      "[89 96 66 54 89 13 37 28 57 98]\n",
      " 34920/50001: episode: 3880, duration: 0.072s, episode steps:   9, steps per second: 125, episode reward: 28.000, mean reward:  3.111 [-10.000,  6.000], mean action: 59.778 [13.000, 98.000],  loss: 9.099799, mae: 2.375658, mean_q: 4.580591\n",
      "[99 34  6 41 63 53 82 42 34 85]\n",
      " 34929/50001: episode: 3881, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 22.000, mean reward:  2.444 [-10.000,  5.000], mean action: 48.889 [6.000, 85.000],  loss: 6.636518, mae: 2.445658, mean_q: 4.595906\n",
      "[81 24 32 31 11 46 30 93 31 78]\n",
      " 34938/50001: episode: 3882, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 23.000, mean reward:  2.556 [-10.000,  5.000], mean action: 41.778 [11.000, 93.000],  loss: 8.572925, mae: 2.419745, mean_q: 4.596415\n",
      "[92 98 77 64 97 13 31 69 23 13]\n",
      " 34947/50001: episode: 3883, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 53.889 [13.000, 98.000],  loss: 5.352629, mae: 2.461170, mean_q: 4.736955\n",
      "[ 8  8 31  4  9  4 74 52 24 88]\n",
      " 34956/50001: episode: 3884, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 13.000, mean reward:  1.444 [-10.000,  7.000], mean action: 32.667 [4.000, 88.000],  loss: 6.765571, mae: 2.481756, mean_q: 4.713491\n",
      "[45 83 77 50 31 95 51 95 28 42]\n",
      " 34965/50001: episode: 3885, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 29.000, mean reward:  3.222 [-10.000,  7.000], mean action: 61.333 [28.000, 95.000],  loss: 7.446737, mae: 2.463935, mean_q: 4.632452\n",
      "[53 12 37 96 77 66 62 42 36 31]\n",
      " 34974/50001: episode: 3886, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 36.000, mean reward:  4.000 [ 1.000,  7.000], mean action: 51.000 [12.000, 96.000],  loss: 8.772017, mae: 2.489425, mean_q: 4.724490\n",
      "[26 95 94 32 80 87 98 37 14 27]\n",
      " 34983/50001: episode: 3887, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 62.667 [14.000, 98.000],  loss: 6.854887, mae: 2.503874, mean_q: 4.713634\n",
      "[58 49 59 49  1 58 96 68  2 96]\n",
      " 34992/50001: episode: 3888, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: -5.000, mean reward: -0.556 [-10.000,  7.000], mean action: 53.111 [1.000, 96.000],  loss: 5.979897, mae: 2.488722, mean_q: 4.661437\n",
      "[63 37 93 95 64 89 99 97 34 95]\n",
      " 35001/50001: episode: 3889, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 28.000, mean reward:  3.111 [-10.000,  8.000], mean action: 78.111 [34.000, 99.000],  loss: 7.246920, mae: 2.521146, mean_q: 4.813833\n",
      "[50 74 68 42 55 18 88 77 31 92]\n",
      " 35010/50001: episode: 3890, duration: 0.073s, episode steps:   9, steps per second: 124, episode reward: 35.000, mean reward:  3.889 [ 3.000,  6.000], mean action: 60.556 [18.000, 92.000],  loss: 5.857050, mae: 2.529748, mean_q: 4.761839\n",
      "[47 34 41 37 72 51 34 95  9 77]\n",
      " 35019/50001: episode: 3891, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 50.000 [9.000, 95.000],  loss: 7.984183, mae: 2.492746, mean_q: 4.782152\n",
      "[65  5 17  9 31 95 34  3 16 95]\n",
      " 35028/50001: episode: 3892, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 33.889 [3.000, 95.000],  loss: 8.221344, mae: 2.455222, mean_q: 4.693698\n",
      "[93 42 96 98 62 27 33 88 27 66]\n",
      " 35037/50001: episode: 3893, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 29.000, mean reward:  3.222 [-10.000,  9.000], mean action: 59.889 [27.000, 98.000],  loss: 7.949296, mae: 2.506894, mean_q: 4.757367\n",
      "[40 41 37 79 31 92  1 46 34 99]\n",
      " 35046/50001: episode: 3894, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 51.111 [1.000, 99.000],  loss: 7.813688, mae: 2.487847, mean_q: 4.692330\n",
      "[22 28 13 79 35 96 46 34 86 50]\n",
      " 35055/50001: episode: 3895, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 42.000, mean reward:  4.667 [ 3.000,  9.000], mean action: 51.889 [13.000, 96.000],  loss: 6.949718, mae: 2.428699, mean_q: 4.612294\n",
      "[32 97 62 40 31 59 74 41 28 70]\n",
      " 35064/50001: episode: 3896, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 36.000, mean reward:  4.000 [ 3.000,  7.000], mean action: 55.778 [28.000, 97.000],  loss: 7.740079, mae: 2.414265, mean_q: 4.649999\n",
      "[61 34 10 27 34 49 86 31 75 42]\n",
      " 35073/50001: episode: 3897, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 26.000, mean reward:  2.889 [-10.000,  6.000], mean action: 43.111 [10.000, 86.000],  loss: 9.377110, mae: 2.413059, mean_q: 4.610974\n",
      "[28 41 30 37 16 66  8 92 84 98]\n",
      " 35082/50001: episode: 3898, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 38.000, mean reward:  4.222 [ 3.000,  7.000], mean action: 52.444 [8.000, 98.000],  loss: 8.419887, mae: 2.417797, mean_q: 4.660690\n",
      "[ 8 50 69 34 86 17 82 48 48 73]\n",
      " 35091/50001: episode: 3899, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 56.333 [17.000, 86.000],  loss: 7.285493, mae: 2.385859, mean_q: 4.577587\n",
      "[25 95 42 90 60 18 66 56 98 50]\n",
      " 35100/50001: episode: 3900, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 63.889 [18.000, 98.000],  loss: 7.458041, mae: 2.366157, mean_q: 4.573423\n",
      "[98 83  4 12 37 48  1 51 28 88]\n",
      " 35109/50001: episode: 3901, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 39.111 [1.000, 88.000],  loss: 7.971278, mae: 2.366210, mean_q: 4.468879\n",
      "[77 41 93 95 31 99 11 41 37 45]\n",
      " 35118/50001: episode: 3902, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 54.778 [11.000, 99.000],  loss: 4.618068, mae: 2.409062, mean_q: 4.592471\n",
      "[56 98 95 42 89  4 66 28 27 66]\n",
      " 35127/50001: episode: 3903, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 57.222 [4.000, 98.000],  loss: 7.862557, mae: 2.412909, mean_q: 4.594766\n",
      "[69 30 30 96 69 32 20 72  4 48]\n",
      " 35136/50001: episode: 3904, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 12.000, mean reward:  1.333 [-10.000,  8.000], mean action: 44.556 [4.000, 96.000],  loss: 8.897503, mae: 2.496552, mean_q: 4.805069\n",
      "[59 53 11 34 95 80 59 57  1 66]\n",
      " 35145/50001: episode: 3905, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 29.000, mean reward:  3.222 [-10.000,  7.000], mean action: 50.667 [1.000, 95.000],  loss: 6.684275, mae: 2.521869, mean_q: 4.812920\n",
      "[53 41 51 46 31 20 98 45 86 32]\n",
      " 35154/50001: episode: 3906, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 38.000, mean reward:  4.222 [ 2.000,  8.000], mean action: 50.000 [20.000, 98.000],  loss: 7.357430, mae: 2.499988, mean_q: 4.778997\n",
      "[68 57 10 92 88 99 95 95 76 90]\n",
      " 35163/50001: episode: 3907, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 78.000 [10.000, 99.000],  loss: 7.332723, mae: 2.454291, mean_q: 4.678080\n",
      "[26 51 98 86 85 37 32 28 93 79]\n",
      " 35172/50001: episode: 3908, duration: 0.079s, episode steps:   9, steps per second: 114, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 65.444 [28.000, 98.000],  loss: 6.143247, mae: 2.510887, mean_q: 4.745170\n",
      "[64 36  2 18 37  4 54 37 94 84]\n",
      " 35181/50001: episode: 3909, duration: 0.079s, episode steps:   9, steps per second: 114, episode reward: 21.000, mean reward:  2.333 [-10.000,  6.000], mean action: 40.667 [2.000, 94.000],  loss: 8.986382, mae: 2.469422, mean_q: 4.688487\n",
      "[64 82 66 86 28 53 42 99 34 79]\n",
      " 35190/50001: episode: 3910, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 43.000, mean reward:  4.778 [ 3.000,  9.000], mean action: 63.222 [28.000, 99.000],  loss: 6.924402, mae: 2.473338, mean_q: 4.713285\n",
      "[12 37 20 50 26 90 95 34 41 63]\n",
      " 35199/50001: episode: 3911, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 42.000, mean reward:  4.667 [ 2.000,  7.000], mean action: 50.667 [20.000, 95.000],  loss: 8.076272, mae: 2.476628, mean_q: 4.718065\n",
      "[96 56 67 79 13 21 93 43 10 89]\n",
      " 35208/50001: episode: 3912, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 35.000, mean reward:  3.889 [ 2.000,  6.000], mean action: 52.333 [10.000, 93.000],  loss: 8.212551, mae: 2.532596, mean_q: 4.874307\n",
      "[24 24 90 11 66 56 88 40 28  2]\n",
      " 35217/50001: episode: 3913, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 27.000, mean reward:  3.000 [-10.000,  6.000], mean action: 45.000 [2.000, 90.000],  loss: 8.731653, mae: 2.463329, mean_q: 4.662142\n",
      "[87 28 95 12 40 90 53 95 57  4]\n",
      " 35226/50001: episode: 3914, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 52.667 [4.000, 95.000],  loss: 8.414267, mae: 2.438089, mean_q: 4.654768\n",
      "[44 60 59 48 78 10 63 89 66  0]\n",
      " 35235/50001: episode: 3915, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 38.000, mean reward:  4.222 [ 3.000,  8.000], mean action: 52.556 [0.000, 89.000],  loss: 7.190030, mae: 2.428017, mean_q: 4.704886\n",
      "[18 44 39 15 66 28 49 12 27 30]\n",
      " 35244/50001: episode: 3916, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 42.000, mean reward:  4.667 [ 2.000,  7.000], mean action: 34.444 [12.000, 66.000],  loss: 5.591389, mae: 2.419676, mean_q: 4.635935\n",
      "[91 34 63 48 30 97 62 78 66 81]\n",
      " 35253/50001: episode: 3917, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 38.000, mean reward:  4.222 [ 3.000,  7.000], mean action: 62.111 [30.000, 97.000],  loss: 6.895277, mae: 2.525186, mean_q: 4.801808\n",
      "[52 31  2 37 16 13 27 79 98 89]\n",
      " 35262/50001: episode: 3918, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 39.000, mean reward:  4.333 [ 3.000,  5.000], mean action: 43.556 [2.000, 98.000],  loss: 7.875593, mae: 2.492114, mean_q: 4.689424\n",
      "[42 10 43 74 50 46 74 23 30 97]\n",
      " 35271/50001: episode: 3919, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 28.000, mean reward:  3.111 [-10.000,  8.000], mean action: 49.667 [10.000, 97.000],  loss: 6.766349, mae: 2.522801, mean_q: 4.809025\n",
      "[53 27 28 28 35 63 54 82 99 23]\n",
      " 35280/50001: episode: 3920, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 48.778 [23.000, 99.000],  loss: 7.535355, mae: 2.501387, mean_q: 4.773304\n",
      "[14 34 30 60 31 72 90 16 54 16]\n",
      " 35289/50001: episode: 3921, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 20.000, mean reward:  2.222 [-10.000,  5.000], mean action: 44.778 [16.000, 90.000],  loss: 4.818151, mae: 2.490757, mean_q: 4.778354\n",
      "[45 68 12 94 11  1  2 88  5 42]\n",
      " 35298/50001: episode: 3922, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 40.000, mean reward:  4.444 [ 3.000,  6.000], mean action: 35.889 [1.000, 94.000],  loss: 5.053489, mae: 2.587420, mean_q: 4.930175\n",
      "[71 41 93 53  9 88 24 87 82 66]\n",
      " 35307/50001: episode: 3923, duration: 0.085s, episode steps:   9, steps per second: 107, episode reward: 44.000, mean reward:  4.889 [ 3.000,  9.000], mean action: 60.333 [9.000, 93.000],  loss: 9.941929, mae: 2.595358, mean_q: 4.887660\n",
      "[89 30 76 53 95 25  9  6 15 64]\n",
      " 35316/50001: episode: 3924, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward: 35.000, mean reward:  3.889 [ 3.000,  6.000], mean action: 41.444 [6.000, 95.000],  loss: 6.272840, mae: 2.596069, mean_q: 4.880388\n",
      "[ 3 37 82 31 85 57 57 34 28 34]\n",
      " 35325/50001: episode: 3925, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 11.000, mean reward:  1.222 [-10.000,  7.000], mean action: 49.444 [28.000, 85.000],  loss: 7.542372, mae: 2.552907, mean_q: 4.871548\n",
      "[46  9 95 92 71 96 95  1 60 50]\n",
      " 35334/50001: episode: 3926, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 26.000, mean reward:  2.889 [-10.000,  8.000], mean action: 63.222 [1.000, 96.000],  loss: 9.060008, mae: 2.521333, mean_q: 4.746677\n",
      "[49 28 82 37  9 76 28 23 84 99]\n",
      " 35343/50001: episode: 3927, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 51.778 [9.000, 99.000],  loss: 7.119655, mae: 2.560468, mean_q: 4.901354\n",
      "[86  4 88 99 46 97 41 62 95 50]\n",
      " 35352/50001: episode: 3928, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 44.000, mean reward:  4.889 [ 3.000,  9.000], mean action: 64.667 [4.000, 99.000],  loss: 9.439144, mae: 2.453737, mean_q: 4.703157\n",
      "[86 46 76  1 60 32 37 57 11 75]\n",
      " 35361/50001: episode: 3929, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 44.000, mean reward:  4.889 [ 3.000,  7.000], mean action: 43.889 [1.000, 76.000],  loss: 5.387423, mae: 2.400752, mean_q: 4.620601\n",
      "[87 95 35  2 79  6 95 14  5 24]\n",
      " 35370/50001: episode: 3930, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 39.444 [2.000, 95.000],  loss: 9.602142, mae: 2.441200, mean_q: 4.633780\n",
      "[56  5 46 58 27 75  2 95 10 14]\n",
      " 35379/50001: episode: 3931, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 43.000, mean reward:  4.778 [ 3.000,  8.000], mean action: 36.889 [2.000, 95.000],  loss: 5.708115, mae: 2.446130, mean_q: 4.626506\n",
      "[74 88 16 35  5 32 51 25 50 84]\n",
      " 35388/50001: episode: 3932, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 38.000, mean reward:  4.222 [ 3.000,  8.000], mean action: 42.889 [5.000, 88.000],  loss: 6.599681, mae: 2.445128, mean_q: 4.643756\n",
      "[14  0 95  1 53 66  1 27 88 50]\n",
      " 35397/50001: episode: 3933, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 28.000, mean reward:  3.111 [-10.000,  8.000], mean action: 42.333 [0.000, 95.000],  loss: 7.742165, mae: 2.529712, mean_q: 4.817005\n",
      "[36 96 91 88 41 73 47 19 99 50]\n",
      " 35406/50001: episode: 3934, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 42.000, mean reward:  4.667 [ 3.000,  9.000], mean action: 67.111 [19.000, 99.000],  loss: 7.155169, mae: 2.550514, mean_q: 4.835281\n",
      "[ 7 52 77  9 87 27 63 63 52 40]\n",
      " 35415/50001: episode: 3935, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward:  9.000, mean reward:  1.000 [-10.000,  7.000], mean action: 52.222 [9.000, 87.000],  loss: 6.077562, mae: 2.484078, mean_q: 4.743560\n",
      "[28 46 66 57 95 25 34 95 54 34]\n",
      " 35424/50001: episode: 3936, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward:  9.000, mean reward:  1.000 [-10.000,  7.000], mean action: 56.222 [25.000, 95.000],  loss: 7.861158, mae: 2.538160, mean_q: 4.841630\n",
      "[ 4 37 20 93 99 47 28 67 79 15]\n",
      " 35433/50001: episode: 3937, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 35.000, mean reward:  3.889 [ 2.000,  6.000], mean action: 53.889 [15.000, 99.000],  loss: 8.998002, mae: 2.573175, mean_q: 4.920802\n",
      "[97  1 18 55 72 42 33 99 39 66]\n",
      " 35442/50001: episode: 3938, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 33.000, mean reward:  3.667 [ 2.000,  8.000], mean action: 47.222 [1.000, 99.000],  loss: 10.338561, mae: 2.443338, mean_q: 4.687477\n",
      "[96 52 33 37  2 66 74 16 28 67]\n",
      " 35451/50001: episode: 3939, duration: 0.070s, episode steps:   9, steps per second: 129, episode reward: 37.000, mean reward:  4.111 [ 3.000,  6.000], mean action: 41.667 [2.000, 74.000],  loss: 7.447829, mae: 2.415047, mean_q: 4.655919\n",
      "[62 34 42 65 54 51 89 51 41 57]\n",
      " 35460/50001: episode: 3940, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 53.778 [34.000, 89.000],  loss: 5.874468, mae: 2.406240, mean_q: 4.564476\n",
      "[61 93  8 23 81 33 88 12 88 50]\n",
      " 35469/50001: episode: 3941, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 28.000, mean reward:  3.111 [-10.000,  9.000], mean action: 52.889 [8.000, 93.000],  loss: 5.071593, mae: 2.493654, mean_q: 4.663181\n",
      "[27 46 90 95 86 12 75 66 28 35]\n",
      " 35478/50001: episode: 3942, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 41.000, mean reward:  4.556 [ 3.000,  6.000], mean action: 59.222 [12.000, 95.000],  loss: 7.169233, mae: 2.489573, mean_q: 4.745955\n",
      "[76 24 81 34 99 15 68 98 48 48]\n",
      " 35487/50001: episode: 3943, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 57.222 [15.000, 99.000],  loss: 6.095249, mae: 2.567305, mean_q: 4.830509\n",
      "[ 7 60 80 42 95 75 96 20 38 43]\n",
      " 35496/50001: episode: 3944, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 32.000, mean reward:  3.556 [ 2.000,  6.000], mean action: 61.000 [20.000, 96.000],  loss: 8.293265, mae: 2.574688, mean_q: 4.913622\n",
      "[52 67 11 24 97 41 90 11  5 24]\n",
      " 35505/50001: episode: 3945, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward:  9.000, mean reward:  1.000 [-10.000,  5.000], mean action: 41.111 [5.000, 97.000],  loss: 6.554876, mae: 2.551863, mean_q: 4.816889\n",
      "[66 66 50 28  4 88  6 64 24 13]\n",
      " 35514/50001: episode: 3946, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 28.000, mean reward:  3.111 [-10.000,  8.000], mean action: 38.111 [4.000, 88.000],  loss: 8.764045, mae: 2.504090, mean_q: 4.804101\n",
      "[55 24 93 74 93 44 23 31 48  5]\n",
      " 35523/50001: episode: 3947, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 48.333 [5.000, 93.000],  loss: 6.548754, mae: 2.468932, mean_q: 4.726675\n",
      "[66 34 63 53 65 44 90 31 14 69]\n",
      " 35532/50001: episode: 3948, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 37.000, mean reward:  4.111 [ 2.000,  6.000], mean action: 51.444 [14.000, 90.000],  loss: 11.121708, mae: 2.476934, mean_q: 4.723833\n",
      "[75 42 75 75 95 99 83 97 62 83]\n",
      " 35541/50001: episode: 3949, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: -4.000, mean reward: -0.444 [-10.000,  6.000], mean action: 79.000 [42.000, 99.000],  loss: 4.499714, mae: 2.426822, mean_q: 4.662271\n",
      "[62  9 41 48 59 72 28 93 12  2]\n",
      " 35550/50001: episode: 3950, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 42.000, mean reward:  4.667 [ 2.000,  8.000], mean action: 40.444 [2.000, 93.000],  loss: 7.471122, mae: 2.464122, mean_q: 4.692928\n",
      "[ 5 57 45 42 95 60 28  1 11 88]\n",
      " 35559/50001: episode: 3951, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 44.000, mean reward:  4.889 [ 2.000,  8.000], mean action: 47.444 [1.000, 95.000],  loss: 8.256383, mae: 2.444762, mean_q: 4.686294\n",
      "[32 42 31 66 42 62 37 28 89 34]\n",
      " 35568/50001: episode: 3952, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 29.000, mean reward:  3.222 [-10.000,  9.000], mean action: 47.889 [28.000, 89.000],  loss: 6.156754, mae: 2.493089, mean_q: 4.686117\n",
      "[94 60 37 83 50  2  4 15 42 37]\n",
      " 35577/50001: episode: 3953, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 22.000, mean reward:  2.444 [-10.000,  5.000], mean action: 36.667 [2.000, 83.000],  loss: 9.563191, mae: 2.550185, mean_q: 4.855213\n",
      "[68 34 93 93  1 33 65 32  9 40]\n",
      " 35586/50001: episode: 3954, duration: 0.069s, episode steps:   9, steps per second: 131, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 44.444 [1.000, 93.000],  loss: 7.671064, mae: 2.474427, mean_q: 4.699757\n",
      "[ 5 40 96  6 46 25 46 13 25 88]\n",
      " 35595/50001: episode: 3955, duration: 0.071s, episode steps:   9, steps per second: 126, episode reward: 11.000, mean reward:  1.222 [-10.000,  8.000], mean action: 42.778 [6.000, 96.000],  loss: 6.439654, mae: 2.458585, mean_q: 4.672212\n",
      "[34 75 66 42 14 34  8 18 24 84]\n",
      " 35604/50001: episode: 3956, duration: 0.068s, episode steps:   9, steps per second: 133, episode reward: 19.000, mean reward:  2.111 [-10.000,  6.000], mean action: 40.556 [8.000, 84.000],  loss: 6.716143, mae: 2.494868, mean_q: 4.742896\n",
      "[80 67  9 95 60 54 96 71 93 30]\n",
      " 35613/50001: episode: 3957, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 37.000, mean reward:  4.111 [ 3.000,  6.000], mean action: 63.889 [9.000, 96.000],  loss: 8.425606, mae: 2.493764, mean_q: 4.729954\n",
      "[23  6 84 66 37 13 82 99 85 69]\n",
      " 35622/50001: episode: 3958, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 36.000, mean reward:  4.000 [ 3.000,  6.000], mean action: 60.111 [6.000, 99.000],  loss: 6.805822, mae: 2.541789, mean_q: 4.846317\n",
      "[60 28 64 19 92 49 72 32 88 33]\n",
      " 35631/50001: episode: 3959, duration: 0.072s, episode steps:   9, steps per second: 125, episode reward: 36.000, mean reward:  4.000 [ 2.000,  8.000], mean action: 53.000 [19.000, 92.000],  loss: 10.202210, mae: 2.550614, mean_q: 4.869650\n",
      "[17 92 76 95 20  4 14 95 99 47]\n",
      " 35640/50001: episode: 3960, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 60.222 [4.000, 99.000],  loss: 7.094730, mae: 2.365784, mean_q: 4.544259\n",
      "[71  9 47 81 97 37 64 45 13 27]\n",
      " 35649/50001: episode: 3961, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 41.000, mean reward:  4.556 [ 2.000,  8.000], mean action: 46.667 [9.000, 97.000],  loss: 7.869560, mae: 2.313758, mean_q: 4.430010\n",
      "[69 74 50 92 49 13 62 96 27 24]\n",
      " 35658/50001: episode: 3962, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 54.111 [13.000, 96.000],  loss: 7.229417, mae: 2.410473, mean_q: 4.597061\n",
      "[ 9 41 46  6 79 95 56  1 24 46]\n",
      " 35667/50001: episode: 3963, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 28.000, mean reward:  3.111 [-10.000,  7.000], mean action: 43.778 [1.000, 95.000],  loss: 9.975894, mae: 2.448681, mean_q: 4.603266\n",
      "[30 82 34 34  9 53 67 13 89 75]\n",
      " 35676/50001: episode: 3964, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 50.667 [9.000, 89.000],  loss: 7.497197, mae: 2.416608, mean_q: 4.599245\n",
      "[46 93 31 76 82 28  1 88 93 47]\n",
      " 35685/50001: episode: 3965, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 29.000, mean reward:  3.222 [-10.000,  8.000], mean action: 59.889 [1.000, 93.000],  loss: 6.590164, mae: 2.372828, mean_q: 4.479903\n",
      "[50 23 83 81 67 90  9 23 95 63]\n",
      " 35694/50001: episode: 3966, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 24.000, mean reward:  2.667 [-10.000,  9.000], mean action: 59.333 [9.000, 95.000],  loss: 8.456343, mae: 2.416947, mean_q: 4.606081\n",
      "[92 23  6 44 88 25  4 95 74 66]\n",
      " 35703/50001: episode: 3967, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 47.222 [4.000, 95.000],  loss: 8.654222, mae: 2.361643, mean_q: 4.467837\n",
      "[87 60 34 53 37 49 59 83 31 86]\n",
      " 35712/50001: episode: 3968, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward: 36.000, mean reward:  4.000 [ 3.000,  6.000], mean action: 54.667 [31.000, 86.000],  loss: 10.611172, mae: 2.349245, mean_q: 4.533804\n",
      "[55  9 31 68 11 95 69  7 23 50]\n",
      " 35721/50001: episode: 3969, duration: 0.067s, episode steps:   9, steps per second: 133, episode reward: 40.000, mean reward:  4.444 [ 1.000,  9.000], mean action: 40.333 [7.000, 95.000],  loss: 6.527501, mae: 2.331436, mean_q: 4.472541\n",
      "[84 98 38 12 76 13  1  6 96 48]\n",
      " 35730/50001: episode: 3970, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 41.000, mean reward:  4.556 [ 2.000,  8.000], mean action: 43.111 [1.000, 98.000],  loss: 6.005247, mae: 2.387042, mean_q: 4.572092\n",
      "[27 94 31 90 74 57 35 37  1 88]\n",
      " 35739/50001: episode: 3971, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 44.000, mean reward:  4.889 [ 3.000,  8.000], mean action: 56.333 [1.000, 94.000],  loss: 6.862318, mae: 2.445798, mean_q: 4.632154\n",
      "[87 23 46 90 56 24 63 53 95 12]\n",
      " 35748/50001: episode: 3972, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 44.000, mean reward:  4.889 [ 3.000,  9.000], mean action: 51.333 [12.000, 95.000],  loss: 9.093792, mae: 2.446985, mean_q: 4.687654\n",
      "[61 60 12 38 82 97 99 95 52 31]\n",
      " 35757/50001: episode: 3973, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 41.000, mean reward:  4.556 [ 2.000,  8.000], mean action: 62.889 [12.000, 99.000],  loss: 8.570870, mae: 2.423794, mean_q: 4.610771\n",
      "[45 85 68 93 88 27 63 53 48 14]\n",
      " 35766/50001: episode: 3974, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 59.889 [14.000, 93.000],  loss: 6.568644, mae: 2.388051, mean_q: 4.547382\n",
      "[59 10 48  1 78 46  9 52 69 46]\n",
      " 35775/50001: episode: 3975, duration: 0.070s, episode steps:   9, steps per second: 129, episode reward: 21.000, mean reward:  2.333 [-10.000,  5.000], mean action: 39.889 [1.000, 78.000],  loss: 7.172608, mae: 2.460781, mean_q: 4.690856\n",
      "[45 59 26 30 16 92 80 46 42 64]\n",
      " 35784/50001: episode: 3976, duration: 0.073s, episode steps:   9, steps per second: 124, episode reward: 38.000, mean reward:  4.222 [ 2.000,  6.000], mean action: 50.556 [16.000, 92.000],  loss: 9.386354, mae: 2.525553, mean_q: 4.837386\n",
      "[79 34 46 31 27 76 78 67 64 48]\n",
      " 35793/50001: episode: 3977, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 40.000, mean reward:  4.444 [ 3.000,  9.000], mean action: 52.333 [27.000, 78.000],  loss: 6.605406, mae: 2.460679, mean_q: 4.680550\n",
      "[63 60  2 99 45 10 88 51 64 28]\n",
      " 35802/50001: episode: 3978, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 49.667 [2.000, 99.000],  loss: 8.022704, mae: 2.476967, mean_q: 4.671031\n",
      "[ 6 30 21 72 85  1  2 48 16 50]\n",
      " 35811/50001: episode: 3979, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 42.000, mean reward:  4.667 [ 2.000,  9.000], mean action: 36.111 [1.000, 85.000],  loss: 7.209348, mae: 2.542868, mean_q: 4.799524\n",
      "[ 4 37 26 10  2 62 33 95 57 13]\n",
      " 35820/50001: episode: 3980, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 37.222 [2.000, 95.000],  loss: 12.153823, mae: 2.517749, mean_q: 4.735042\n",
      "[59 50 86 41  2 63 92 90 40 64]\n",
      " 35829/50001: episode: 3981, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward: 40.000, mean reward:  4.444 [ 3.000,  6.000], mean action: 58.667 [2.000, 92.000],  loss: 10.025136, mae: 2.412311, mean_q: 4.612244\n",
      "[ 5 88 49 57 89 32 47 51 28 24]\n",
      " 35838/50001: episode: 3982, duration: 0.078s, episode steps:   9, steps per second: 116, episode reward: 41.000, mean reward:  4.556 [ 3.000,  6.000], mean action: 51.667 [24.000, 89.000],  loss: 7.363763, mae: 2.371336, mean_q: 4.569614\n",
      "[70 48 41 48 53 29 32 13 46 16]\n",
      " 35847/50001: episode: 3983, duration: 0.073s, episode steps:   9, steps per second: 123, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 36.222 [13.000, 53.000],  loss: 6.061909, mae: 2.361341, mean_q: 4.553019\n",
      "[28 35 68 85 10 13 64 57 50 12]\n",
      " 35856/50001: episode: 3984, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward: 43.000, mean reward:  4.778 [ 3.000,  8.000], mean action: 43.778 [10.000, 85.000],  loss: 7.164836, mae: 2.520340, mean_q: 4.780359\n",
      "[52 92 81 42 53 12 66 12 66 69]\n",
      " 35865/50001: episode: 3985, duration: 0.081s, episode steps:   9, steps per second: 111, episode reward:  7.000, mean reward:  0.778 [-10.000,  5.000], mean action: 54.778 [12.000, 92.000],  loss: 9.036997, mae: 2.488265, mean_q: 4.731575\n",
      "[80 55 93 92 31 51 24 88 48 83]\n",
      " 35874/50001: episode: 3986, duration: 0.088s, episode steps:   9, steps per second: 103, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 62.778 [24.000, 93.000],  loss: 9.380062, mae: 2.543277, mean_q: 4.806031\n",
      "[60 42 54 95 48 37 23 71 85 85]\n",
      " 35883/50001: episode: 3987, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 60.000 [23.000, 95.000],  loss: 7.208971, mae: 2.466717, mean_q: 4.696458\n",
      "[ 3 46 93 42 98 74 10 88 37 82]\n",
      " 35892/50001: episode: 3988, duration: 0.091s, episode steps:   9, steps per second:  99, episode reward: 45.000, mean reward:  5.000 [ 4.000,  8.000], mean action: 63.333 [10.000, 98.000],  loss: 8.184576, mae: 2.414328, mean_q: 4.540760\n",
      "[30 25 75 42 71 84 95 31  2  2]\n",
      " 35901/50001: episode: 3989, duration: 0.091s, episode steps:   9, steps per second:  99, episode reward: 26.000, mean reward:  2.889 [-10.000,  8.000], mean action: 47.444 [2.000, 95.000],  loss: 7.534525, mae: 2.457849, mean_q: 4.635302\n",
      "[27 30  5  4 33 23 15 13  6 66]\n",
      " 35910/50001: episode: 3990, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward: 38.000, mean reward:  4.222 [ 2.000,  8.000], mean action: 21.667 [4.000, 66.000],  loss: 8.391161, mae: 2.494747, mean_q: 4.736063\n",
      "[88 62 56 34 88 99  1 11  1 30]\n",
      " 35919/50001: episode: 3991, duration: 0.091s, episode steps:   9, steps per second:  99, episode reward:  8.000, mean reward:  0.889 [-10.000,  6.000], mean action: 42.444 [1.000, 99.000],  loss: 9.636818, mae: 2.531327, mean_q: 4.816009\n",
      "[35 48 95 58 37 85 50 35 50 66]\n",
      " 35928/50001: episode: 3992, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward: 12.000, mean reward:  1.333 [-10.000,  7.000], mean action: 58.222 [35.000, 95.000],  loss: 8.431471, mae: 2.492858, mean_q: 4.841196\n",
      "[45 98 28 34 81 43 66  5 23 66]\n",
      " 35937/50001: episode: 3993, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 49.333 [5.000, 98.000],  loss: 7.361207, mae: 2.439901, mean_q: 4.632642\n",
      "[21 31 34 66 64 96 83 10 50 51]\n",
      " 35946/50001: episode: 3994, duration: 0.079s, episode steps:   9, steps per second: 114, episode reward: 39.000, mean reward:  4.333 [ 3.000,  8.000], mean action: 53.889 [10.000, 96.000],  loss: 7.469130, mae: 2.475420, mean_q: 4.727403\n",
      "[79 17 11 56 82 37 38 37 28 98]\n",
      " 35955/50001: episode: 3995, duration: 0.088s, episode steps:   9, steps per second: 103, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 44.889 [11.000, 98.000],  loss: 8.304302, mae: 2.466373, mean_q: 4.690662\n",
      "[44 37 37 66  0 44 79 42 14 20]\n",
      " 35964/50001: episode: 3996, duration: 0.078s, episode steps:   9, steps per second: 115, episode reward: 11.000, mean reward:  1.222 [-10.000,  6.000], mean action: 37.667 [0.000, 79.000],  loss: 6.929619, mae: 2.397258, mean_q: 4.574130\n",
      "[70 34 17 62 31 26 96 83 66 32]\n",
      " 35973/50001: episode: 3997, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 42.000, mean reward:  4.667 [ 2.000,  8.000], mean action: 49.667 [17.000, 96.000],  loss: 5.547509, mae: 2.443554, mean_q: 4.737964\n",
      "[31  9 72 37 32 12 33 31  9 81]\n",
      " 35982/50001: episode: 3998, duration: 0.072s, episode steps:   9, steps per second: 125, episode reward:  6.000, mean reward:  0.667 [-10.000,  5.000], mean action: 35.111 [9.000, 81.000],  loss: 7.866068, mae: 2.474698, mean_q: 4.693401\n",
      "[78  5 13 93 37 42 95 99 75 98]\n",
      " 35991/50001: episode: 3999, duration: 0.066s, episode steps:   9, steps per second: 135, episode reward: 43.000, mean reward:  4.778 [ 3.000,  7.000], mean action: 61.889 [5.000, 99.000],  loss: 7.945585, mae: 2.531153, mean_q: 4.809223\n",
      "[50 71 12 97 31 23 31 84 34 14]\n",
      " 36000/50001: episode: 4000, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 44.111 [12.000, 97.000],  loss: 11.128535, mae: 2.464057, mean_q: 4.756818\n",
      "[12 12 47 72 31 24 37 20 59 50]\n",
      " 36009/50001: episode: 4001, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 26.000, mean reward:  2.889 [-10.000,  9.000], mean action: 39.111 [12.000, 72.000],  loss: 7.529150, mae: 2.441399, mean_q: 4.656143\n",
      "[23 90 30  4 10 15 28 66 31  9]\n",
      " 36018/50001: episode: 4002, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 38.000, mean reward:  4.222 [ 2.000,  6.000], mean action: 31.444 [4.000, 90.000],  loss: 8.371384, mae: 2.429536, mean_q: 4.627184\n",
      "[45 37 69 95 89 25 52 42 66 90]\n",
      " 36027/50001: episode: 4003, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 62.778 [25.000, 95.000],  loss: 8.236784, mae: 2.401495, mean_q: 4.541349\n",
      "[32 27 13 64 95 50 83 64 71 87]\n",
      " 36036/50001: episode: 4004, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 20.000, mean reward:  2.222 [-10.000,  6.000], mean action: 61.556 [13.000, 95.000],  loss: 7.835302, mae: 2.419295, mean_q: 4.591987\n",
      "[74 96 20 88 17 74 50 89 55 66]\n",
      " 36045/50001: episode: 4005, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 26.000, mean reward:  2.889 [-10.000,  8.000], mean action: 61.667 [17.000, 96.000],  loss: 9.740446, mae: 2.426002, mean_q: 4.597378\n",
      "[56 13  9 27 56 24 59 29 27 88]\n",
      " 36054/50001: episode: 4006, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward:  8.000, mean reward:  0.889 [-10.000,  8.000], mean action: 36.889 [9.000, 88.000],  loss: 6.155496, mae: 2.464104, mean_q: 4.699746\n",
      "[64 23 67 42 45 61 94 28  9 12]\n",
      " 36063/50001: episode: 4007, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 41.000, mean reward:  4.556 [ 2.000,  9.000], mean action: 42.333 [9.000, 94.000],  loss: 8.318211, mae: 2.454963, mean_q: 4.647921\n",
      "[36 32 81 61 37 63 50 42 37 13]\n",
      " 36072/50001: episode: 4008, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 28.000, mean reward:  3.111 [-10.000,  8.000], mean action: 46.222 [13.000, 81.000],  loss: 9.875969, mae: 2.470885, mean_q: 4.681921\n",
      "[13 37 76 97 34 88  1 69 69 12]\n",
      " 36081/50001: episode: 4009, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 53.667 [1.000, 97.000],  loss: 6.320488, mae: 2.386382, mean_q: 4.504260\n",
      "[10 50 10 75 12 44 98 84 10  8]\n",
      " 36090/50001: episode: 4010, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward:  7.000, mean reward:  0.778 [-10.000,  5.000], mean action: 43.444 [8.000, 98.000],  loss: 6.355206, mae: 2.438355, mean_q: 4.662207\n",
      "[60 27 52 58 41 70 64 97 89 52]\n",
      " 36099/50001: episode: 4011, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 21.000, mean reward:  2.333 [-10.000,  7.000], mean action: 61.111 [27.000, 97.000],  loss: 8.586978, mae: 2.462308, mean_q: 4.587726\n",
      "[26 27 10 33 36 64 66 87 47 99]\n",
      " 36108/50001: episode: 4012, duration: 0.072s, episode steps:   9, steps per second: 125, episode reward: 38.000, mean reward:  4.222 [ 2.000,  8.000], mean action: 52.111 [10.000, 99.000],  loss: 7.006211, mae: 2.465372, mean_q: 4.688092\n",
      "[22 95 55 92 24  8  4 83 87 13]\n",
      " 36117/50001: episode: 4013, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 37.000, mean reward:  4.111 [ 2.000,  9.000], mean action: 51.222 [4.000, 95.000],  loss: 8.943012, mae: 2.533631, mean_q: 4.830776\n",
      "[26 59 24 79 93 97 95 31 46 31]\n",
      " 36126/50001: episode: 4014, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 61.667 [24.000, 97.000],  loss: 9.249635, mae: 2.458461, mean_q: 4.742713\n",
      "[91 67 60  9 75 13  9 34 18 23]\n",
      " 36135/50001: episode: 4015, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 34.222 [9.000, 75.000],  loss: 6.160432, mae: 2.440509, mean_q: 4.652166\n",
      "[21 34 41 60 64 59 88 28 84 88]\n",
      " 36144/50001: episode: 4016, duration: 0.087s, episode steps:   9, steps per second: 104, episode reward: 26.000, mean reward:  2.889 [-10.000,  6.000], mean action: 60.667 [28.000, 88.000],  loss: 10.374342, mae: 2.448157, mean_q: 4.551921\n",
      "[45 48 50 62 95 60 28 67 79 79]\n",
      " 36153/50001: episode: 4017, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 63.111 [28.000, 95.000],  loss: 7.823230, mae: 2.360923, mean_q: 4.464607\n",
      "[64 33 89 78 28 93 13  1  4 30]\n",
      " 36162/50001: episode: 4018, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 44.000, mean reward:  4.889 [ 3.000,  7.000], mean action: 41.000 [1.000, 93.000],  loss: 6.783232, mae: 2.408419, mean_q: 4.649280\n",
      "[56 74 58  1 33 34 95 86 13  0]\n",
      " 36171/50001: episode: 4019, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 43.778 [0.000, 95.000],  loss: 9.561163, mae: 2.413289, mean_q: 4.582556\n",
      "[48 96  2  2 51 37 44 75 47 48]\n",
      " 36180/50001: episode: 4020, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward:  9.000, mean reward:  1.000 [-10.000,  6.000], mean action: 44.667 [2.000, 96.000],  loss: 6.130301, mae: 2.354301, mean_q: 4.536679\n",
      "[59 94 25 32 41 13 59 63 20 96]\n",
      " 36189/50001: episode: 4021, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 49.222 [13.000, 96.000],  loss: 7.035725, mae: 2.391193, mean_q: 4.555390\n",
      "[ 5 60 60 53 28 37 95 57 66 60]\n",
      " 36198/50001: episode: 4022, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 14.000, mean reward:  1.556 [-10.000,  6.000], mean action: 57.333 [28.000, 95.000],  loss: 7.380893, mae: 2.485965, mean_q: 4.683833\n",
      "[65 98 98 42 13 57 24 10 48 94]\n",
      " 36207/50001: episode: 4023, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 26.000, mean reward:  2.889 [-10.000,  6.000], mean action: 53.778 [10.000, 98.000],  loss: 9.958049, mae: 2.525836, mean_q: 4.763220\n",
      "[72  4 10 88 13 47 98 27 89 12]\n",
      " 36216/50001: episode: 4024, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 45.000, mean reward:  5.000 [ 3.000,  9.000], mean action: 43.111 [4.000, 98.000],  loss: 8.362827, mae: 2.445291, mean_q: 4.628968\n",
      "[60 92 82 79 12 30 88  1  1 41]\n",
      " 36225/50001: episode: 4025, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 27.000, mean reward:  3.000 [-10.000,  6.000], mean action: 47.333 [1.000, 92.000],  loss: 7.478283, mae: 2.509562, mean_q: 4.716928\n",
      "[12 97 13 37 55 63 90 37 12 54]\n",
      " 36234/50001: episode: 4026, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward:  5.000, mean reward:  0.556 [-10.000,  5.000], mean action: 50.889 [12.000, 97.000],  loss: 8.896151, mae: 2.447897, mean_q: 4.638496\n",
      "[26 41 85 12  6 12 88 57  4 82]\n",
      " 36243/50001: episode: 4027, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 27.000, mean reward:  3.000 [-10.000,  6.000], mean action: 43.000 [4.000, 88.000],  loss: 8.218441, mae: 2.447819, mean_q: 4.645894\n",
      "[65 37 97 34 37 95 98 40 48 31]\n",
      " 36252/50001: episode: 4028, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 29.000, mean reward:  3.222 [-10.000,  6.000], mean action: 57.444 [31.000, 98.000],  loss: 6.748140, mae: 2.393114, mean_q: 4.500650\n",
      "[12 93 34 99 34 81 73 46 42 42]\n",
      " 36261/50001: episode: 4029, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward:  6.000, mean reward:  0.667 [-10.000,  5.000], mean action: 60.444 [34.000, 99.000],  loss: 6.636138, mae: 2.369136, mean_q: 4.501834\n",
      "[22 88 10 30 41 30 26 82 52 64]\n",
      " 36270/50001: episode: 4030, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 47.000 [10.000, 88.000],  loss: 8.730473, mae: 2.385163, mean_q: 4.563739\n",
      "[27 16 12  0 94 67 23  1 79 46]\n",
      " 36279/50001: episode: 4031, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 39.000, mean reward:  4.333 [ 2.000,  6.000], mean action: 37.556 [0.000, 94.000],  loss: 9.169168, mae: 2.341147, mean_q: 4.446331\n",
      "[32 95  8 55 29 79 24 67 49 79]\n",
      " 36288/50001: episode: 4032, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 18.000, mean reward:  2.000 [-10.000,  5.000], mean action: 53.889 [8.000, 95.000],  loss: 6.495220, mae: 2.348599, mean_q: 4.589689\n",
      "[18  1 82 70 33 64 51 23 95 40]\n",
      " 36297/50001: episode: 4033, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 44.000, mean reward:  4.889 [ 2.000, 10.000], mean action: 51.000 [1.000, 95.000],  loss: 7.630408, mae: 2.427510, mean_q: 4.624677\n",
      "[95 34 37 16 30 68 34 13 98  2]\n",
      " 36306/50001: episode: 4034, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 36.889 [2.000, 98.000],  loss: 5.819936, mae: 2.498767, mean_q: 4.764028\n",
      "[47 51 12 34  9 97 37 96 12 93]\n",
      " 36315/50001: episode: 4035, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 49.000 [9.000, 97.000],  loss: 8.510204, mae: 2.548396, mean_q: 4.811298\n",
      "[74 88 74 83  2  4 13 60 59 66]\n",
      " 36324/50001: episode: 4036, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 28.000, mean reward:  3.111 [-10.000,  7.000], mean action: 49.889 [2.000, 88.000],  loss: 9.394950, mae: 2.501081, mean_q: 4.696555\n",
      "[67 95 58 40 40 34 59 24 75 44]\n",
      " 36333/50001: episode: 4037, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 52.111 [24.000, 95.000],  loss: 5.776786, mae: 2.475144, mean_q: 4.746046\n",
      "[32 11 24 17  9 67 75 50 44 88]\n",
      " 36342/50001: episode: 4038, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 41.000, mean reward:  4.556 [ 2.000,  9.000], mean action: 42.778 [9.000, 88.000],  loss: 7.943856, mae: 2.464170, mean_q: 4.715886\n",
      "[37 83 12 48 97 34  6 28 64 35]\n",
      " 36351/50001: episode: 4039, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 36.000, mean reward:  4.000 [ 3.000,  6.000], mean action: 45.222 [6.000, 97.000],  loss: 5.206126, mae: 2.492846, mean_q: 4.738504\n",
      "[92 51 42 34 75 31 99 31 88 75]\n",
      " 36360/50001: episode: 4040, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward:  9.000, mean reward:  1.000 [-10.000,  6.000], mean action: 58.444 [31.000, 99.000],  loss: 9.631763, mae: 2.549365, mean_q: 4.804787\n",
      "[ 0 50 77 65 89 88  2 62  7 88]\n",
      " 36369/50001: episode: 4041, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 20.000, mean reward:  2.222 [-10.000,  6.000], mean action: 58.667 [2.000, 89.000],  loss: 7.452111, mae: 2.541113, mean_q: 4.837147\n",
      "[56  5 93 37 86 57 11 15 14 96]\n",
      " 36378/50001: episode: 4042, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 35.000, mean reward:  3.889 [ 2.000,  6.000], mean action: 46.000 [5.000, 96.000],  loss: 7.306604, mae: 2.468341, mean_q: 4.589440\n",
      "[24 23 52 27 65  9 89 42 34 93]\n",
      " 36387/50001: episode: 4043, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 40.000, mean reward:  4.444 [ 2.000,  9.000], mean action: 48.222 [9.000, 93.000],  loss: 7.202334, mae: 2.481615, mean_q: 4.746073\n",
      "[31 74  3 74 85  1 78 54 98  6]\n",
      " 36396/50001: episode: 4044, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 52.556 [1.000, 98.000],  loss: 6.535026, mae: 2.502077, mean_q: 4.707978\n",
      "[12 41 94 14 13  1 95 40 34 97]\n",
      " 36405/50001: episode: 4045, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward: 45.000, mean reward:  5.000 [ 3.000,  7.000], mean action: 47.667 [1.000, 97.000],  loss: 8.239662, mae: 2.556631, mean_q: 4.836061\n",
      "[60 91  2 86 37 64 35 78 27 13]\n",
      " 36414/50001: episode: 4046, duration: 0.091s, episode steps:   9, steps per second:  99, episode reward: 40.000, mean reward:  4.444 [ 3.000,  9.000], mean action: 48.111 [2.000, 91.000],  loss: 7.964069, mae: 2.548530, mean_q: 4.874281\n",
      "[89  5 76 95 34 30 97 44 88 89]\n",
      " 36423/50001: episode: 4047, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 28.000, mean reward:  3.111 [-10.000,  7.000], mean action: 62.000 [5.000, 97.000],  loss: 5.792084, mae: 2.478255, mean_q: 4.694614\n",
      "[70  4 74 95 62 43 52 27 97 50]\n",
      " 36432/50001: episode: 4048, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 44.000, mean reward:  4.889 [ 2.000,  9.000], mean action: 56.000 [4.000, 97.000],  loss: 7.289869, mae: 2.526232, mean_q: 4.784068\n",
      "[56 85 94 14 14 74 24 80 38 57]\n",
      " 36441/50001: episode: 4049, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 21.000, mean reward:  2.333 [-10.000,  7.000], mean action: 53.333 [14.000, 94.000],  loss: 8.171572, mae: 2.543720, mean_q: 4.798882\n",
      "[45 48  2 91  2 34 62 20 77 50]\n",
      " 36450/50001: episode: 4050, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 26.000, mean reward:  2.889 [-10.000,  9.000], mean action: 42.889 [2.000, 91.000],  loss: 9.152384, mae: 2.570393, mean_q: 4.816880\n",
      "[78  7 62 94 45  3 97 66 66 66]\n",
      " 36459/50001: episode: 4051, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward:  9.000, mean reward:  1.000 [-10.000,  7.000], mean action: 56.222 [3.000, 97.000],  loss: 8.673830, mae: 2.479045, mean_q: 4.758921\n",
      "[20 68 28 90 16 13 19 96 33 37]\n",
      " 36468/50001: episode: 4052, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 39.000, mean reward:  4.333 [ 3.000,  9.000], mean action: 44.444 [13.000, 96.000],  loss: 7.253054, mae: 2.482427, mean_q: 4.690194\n",
      "[ 0 99  0 39 24  5 32 86 36 31]\n",
      " 36477/50001: episode: 4053, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 23.000, mean reward:  2.556 [-10.000,  8.000], mean action: 39.111 [0.000, 99.000],  loss: 7.472098, mae: 2.522907, mean_q: 4.847172\n",
      "[15  1 54 50 41 46 48 37 93  5]\n",
      " 36486/50001: episode: 4054, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 43.000, mean reward:  4.778 [ 3.000,  7.000], mean action: 41.667 [1.000, 93.000],  loss: 7.672039, mae: 2.505674, mean_q: 4.791370\n",
      "[ 9 28 55  6 44 88  5 13 78  9]\n",
      " 36495/50001: episode: 4055, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 36.222 [5.000, 88.000],  loss: 7.796075, mae: 2.502726, mean_q: 4.785397\n",
      "[81 42 34 93  9 74 10  3 50 51]\n",
      " 36504/50001: episode: 4056, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 40.667 [3.000, 93.000],  loss: 6.906652, mae: 2.601439, mean_q: 4.897824\n",
      "[65 46  8  6 89 95 74 43 46 16]\n",
      " 36513/50001: episode: 4057, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 47.000 [6.000, 95.000],  loss: 7.026758, mae: 2.506499, mean_q: 4.759338\n",
      "[33 34 50 25 80 32 88 14 44 50]\n",
      " 36522/50001: episode: 4058, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward: 21.000, mean reward:  2.333 [-10.000,  5.000], mean action: 46.333 [14.000, 88.000],  loss: 6.245449, mae: 2.527373, mean_q: 4.819406\n",
      "[91  2 56 52 18  1 52 53 52 33]\n",
      " 36531/50001: episode: 4059, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward:  7.000, mean reward:  0.778 [-10.000,  5.000], mean action: 35.444 [1.000, 56.000],  loss: 9.380631, mae: 2.516179, mean_q: 4.731801\n",
      "[54 37 87 66 56 84 92 40 13 23]\n",
      " 36540/50001: episode: 4060, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 55.333 [13.000, 92.000],  loss: 8.349257, mae: 2.457168, mean_q: 4.704409\n",
      "[43 92 19 30 82 51 74 31 12 41]\n",
      " 36549/50001: episode: 4061, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 48.000 [12.000, 92.000],  loss: 5.950580, mae: 2.408433, mean_q: 4.647348\n",
      "[84 34 16 13 34 37 78  4 40 23]\n",
      " 36558/50001: episode: 4062, duration: 0.071s, episode steps:   9, steps per second: 127, episode reward: 25.000, mean reward:  2.778 [-10.000,  5.000], mean action: 31.000 [4.000, 78.000],  loss: 8.946891, mae: 2.417068, mean_q: 4.558304\n",
      "[54 36 41  8  1 79 24 92  6 31]\n",
      " 36567/50001: episode: 4063, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 35.333 [1.000, 92.000],  loss: 7.377164, mae: 2.421114, mean_q: 4.653812\n",
      "[65 39 84 92 12 61 33 57 32  2]\n",
      " 36576/50001: episode: 4064, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 45.778 [2.000, 92.000],  loss: 7.825978, mae: 2.500506, mean_q: 4.696496\n",
      "[39 12 87 48 94 36 31  1 27 20]\n",
      " 36585/50001: episode: 4065, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 37.000, mean reward:  4.111 [ 2.000,  6.000], mean action: 39.556 [1.000, 94.000],  loss: 5.904990, mae: 2.464032, mean_q: 4.734277\n",
      "[63 34 15 57 35 28 14 58 64 85]\n",
      " 36594/50001: episode: 4066, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 36.000, mean reward:  4.000 [ 2.000,  5.000], mean action: 43.333 [14.000, 85.000],  loss: 8.110582, mae: 2.497843, mean_q: 4.698717\n",
      "[24 21 18 42  1  2 12 79 98 40]\n",
      " 36603/50001: episode: 4067, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 42.000, mean reward:  4.667 [ 3.000,  6.000], mean action: 34.778 [1.000, 98.000],  loss: 8.546787, mae: 2.489673, mean_q: 4.720484\n",
      "[65 37 82 34 92 33 75 60 80 48]\n",
      " 36612/50001: episode: 4068, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 60.111 [33.000, 92.000],  loss: 7.580826, mae: 2.448437, mean_q: 4.614428\n",
      "[64 82 94 12 47 46 98 57  2 14]\n",
      " 36621/50001: episode: 4069, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 41.000, mean reward:  4.556 [ 3.000,  6.000], mean action: 50.222 [2.000, 98.000],  loss: 9.371871, mae: 2.407353, mean_q: 4.581357\n",
      "[27 52 95 22 98 28  4 23 26 60]\n",
      " 36630/50001: episode: 4070, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 37.000, mean reward:  4.111 [ 2.000,  7.000], mean action: 45.333 [4.000, 98.000],  loss: 6.713507, mae: 2.361484, mean_q: 4.534798\n",
      "[31 95 19 99 74 38 47 32 34 89]\n",
      " 36639/50001: episode: 4071, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 39.000, mean reward:  4.333 [ 2.000,  8.000], mean action: 58.556 [19.000, 99.000],  loss: 7.326755, mae: 2.395474, mean_q: 4.570174\n",
      "[32 34 92  4 52 70 41  1  2 62]\n",
      " 36648/50001: episode: 4072, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 39.778 [1.000, 92.000],  loss: 6.854786, mae: 2.396392, mean_q: 4.546350\n",
      "[39 52 39 96 79 24  1 96 32 28]\n",
      " 36657/50001: episode: 4073, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 16.000, mean reward:  1.778 [-10.000,  7.000], mean action: 49.667 [1.000, 96.000],  loss: 9.026025, mae: 2.404755, mean_q: 4.602787\n",
      "[67 30 11 52 88 91  4 62 72 16]\n",
      " 36666/50001: episode: 4074, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 36.000, mean reward:  4.000 [ 2.000,  5.000], mean action: 47.333 [4.000, 91.000],  loss: 10.155437, mae: 2.393891, mean_q: 4.659426\n",
      "[38 98 90 13 37 23 78 31 94 66]\n",
      " 36675/50001: episode: 4075, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 58.889 [13.000, 98.000],  loss: 9.039118, mae: 2.415888, mean_q: 4.659786\n",
      "[40 48 60 34 40 12 31 98 84 50]\n",
      " 36684/50001: episode: 4076, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 28.000, mean reward:  3.111 [-10.000,  8.000], mean action: 50.778 [12.000, 98.000],  loss: 7.133583, mae: 2.407861, mean_q: 4.646991\n",
      "[65 97 93 41 17 99 48 89 27 66]\n",
      " 36693/50001: episode: 4077, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 44.000, mean reward:  4.889 [ 3.000,  9.000], mean action: 64.111 [17.000, 99.000],  loss: 6.645789, mae: 2.401038, mean_q: 4.582349\n",
      "[98  5 50 47 69 68 37 62 40  4]\n",
      " 36702/50001: episode: 4078, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 37.000, mean reward:  4.111 [ 3.000,  6.000], mean action: 42.444 [4.000, 69.000],  loss: 6.748505, mae: 2.498983, mean_q: 4.766371\n",
      "[ 3 76 60 99 42 40 10 74 50 24]\n",
      " 36711/50001: episode: 4079, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 43.000, mean reward:  4.778 [ 3.000,  8.000], mean action: 52.778 [10.000, 99.000],  loss: 9.019721, mae: 2.486512, mean_q: 4.751266\n",
      "[38 10 31 24 30 68 50 80 88  7]\n",
      " 36720/50001: episode: 4080, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 37.000, mean reward:  4.111 [ 2.000,  7.000], mean action: 43.111 [7.000, 88.000],  loss: 6.935919, mae: 2.446175, mean_q: 4.615696\n",
      "[65 66 12 16 60 97 63 97 45 42]\n",
      " 36729/50001: episode: 4081, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 55.333 [12.000, 97.000],  loss: 7.670228, mae: 2.543552, mean_q: 4.865852\n",
      "[ 4 74 84 54 34  7 21 59 27 33]\n",
      " 36738/50001: episode: 4082, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 34.000, mean reward:  3.778 [ 2.000,  6.000], mean action: 43.667 [7.000, 84.000],  loss: 7.379435, mae: 2.502052, mean_q: 4.734007\n",
      "[68 20  9 37 41 60 82 12 10  4]\n",
      " 36747/50001: episode: 4083, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 30.556 [4.000, 82.000],  loss: 5.555495, mae: 2.495847, mean_q: 4.759062\n",
      "[64 76 13 94 31 11 57 42 10 34]\n",
      " 36756/50001: episode: 4084, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 44.000, mean reward:  4.889 [ 3.000, 10.000], mean action: 40.889 [10.000, 94.000],  loss: 6.255911, mae: 2.566413, mean_q: 4.883241\n",
      "[21 50 76 32 31 66 81 69 88 83]\n",
      " 36765/50001: episode: 4085, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 64.000 [31.000, 88.000],  loss: 5.581553, mae: 2.550089, mean_q: 4.842814\n",
      "[11 50 99 23 31 23 21 75 47 48]\n",
      " 36774/50001: episode: 4086, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 46.333 [21.000, 99.000],  loss: 7.765917, mae: 2.620080, mean_q: 4.950943\n",
      "[38 34 11 87  2 95 52 30 11 28]\n",
      " 36783/50001: episode: 4087, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 28.000, mean reward:  3.111 [-10.000,  7.000], mean action: 38.889 [2.000, 95.000],  loss: 6.771202, mae: 2.510764, mean_q: 4.791615\n",
      "[56 80 31 60  9  2 82 60 14 97]\n",
      " 36792/50001: episode: 4088, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 48.333 [2.000, 97.000],  loss: 6.798662, mae: 2.558483, mean_q: 4.865201\n",
      "[71 14  8 86  6 32 41 43 84 12]\n",
      " 36801/50001: episode: 4089, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 38.000, mean reward:  4.222 [ 2.000, 10.000], mean action: 36.222 [6.000, 86.000],  loss: 5.985258, mae: 2.515666, mean_q: 4.765994\n",
      "[24 23  1 86 50 33 91 99 98 33]\n",
      " 36810/50001: episode: 4090, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 21.000, mean reward:  2.333 [-10.000,  6.000], mean action: 57.111 [1.000, 99.000],  loss: 10.743818, mae: 2.499067, mean_q: 4.808366\n",
      "[23 84 40 32 76 90 35 95 79 37]\n",
      " 36819/50001: episode: 4091, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 63.111 [32.000, 95.000],  loss: 8.510970, mae: 2.461755, mean_q: 4.722437\n",
      "[97 88  8 14 93  6 60 13  3 53]\n",
      " 36828/50001: episode: 4092, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 37.000, mean reward:  4.111 [ 2.000,  7.000], mean action: 37.556 [3.000, 93.000],  loss: 7.193410, mae: 2.452550, mean_q: 4.711207\n",
      "[ 3 24 37 13 76 34 37  6 50 84]\n",
      " 36837/50001: episode: 4093, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 40.111 [6.000, 84.000],  loss: 8.885443, mae: 2.463286, mean_q: 4.726961\n",
      "[ 9 28 63 40 74 44 95 31  4 42]\n",
      " 36846/50001: episode: 4094, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 46.778 [4.000, 95.000],  loss: 9.358190, mae: 2.460441, mean_q: 4.679287\n",
      "[55 28 82 28 81 34 88  2 32 28]\n",
      " 36855/50001: episode: 4095, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 11.000, mean reward:  1.222 [-10.000,  6.000], mean action: 44.778 [2.000, 88.000],  loss: 8.254634, mae: 2.456645, mean_q: 4.694806\n",
      "[22 12 24  4 69 52 88 51 32 23]\n",
      " 36864/50001: episode: 4096, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 39.444 [4.000, 88.000],  loss: 6.553843, mae: 2.425894, mean_q: 4.665002\n",
      "[41 53 37 45 96 31 98 67 34  4]\n",
      " 36873/50001: episode: 4097, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 51.667 [4.000, 98.000],  loss: 7.661546, mae: 2.415959, mean_q: 4.668276\n",
      "[ 4 10 79 16 44 95 13 41 27 28]\n",
      " 36882/50001: episode: 4098, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 39.222 [10.000, 95.000],  loss: 7.091536, mae: 2.448952, mean_q: 4.677976\n",
      "[47 28 88 95 47 37 81 46 64 81]\n",
      " 36891/50001: episode: 4099, duration: 0.071s, episode steps:   9, steps per second: 127, episode reward:  9.000, mean reward:  1.000 [-10.000,  5.000], mean action: 63.000 [28.000, 95.000],  loss: 8.009365, mae: 2.491523, mean_q: 4.757546\n",
      "[84 12 97 56 10 62 13 72 82 27]\n",
      " 36900/50001: episode: 4100, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 47.889 [10.000, 97.000],  loss: 8.590014, mae: 2.508793, mean_q: 4.805364\n",
      "[48 60 65 96 11 61 21 30 12 12]\n",
      " 36909/50001: episode: 4101, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 22.000, mean reward:  2.444 [-10.000,  9.000], mean action: 40.889 [11.000, 96.000],  loss: 8.101707, mae: 2.433086, mean_q: 4.616443\n",
      "[36 34  2 74 93 60 60 33 68 50]\n",
      " 36918/50001: episode: 4102, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 26.000, mean reward:  2.889 [-10.000,  9.000], mean action: 52.667 [2.000, 93.000],  loss: 6.651708, mae: 2.374040, mean_q: 4.591624\n",
      "[89 27  4 74 86 94 95 62 12 24]\n",
      " 36927/50001: episode: 4103, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 43.000, mean reward:  4.778 [ 3.000,  8.000], mean action: 53.111 [4.000, 95.000],  loss: 8.113042, mae: 2.475423, mean_q: 4.699964\n",
      "[37 86 60 10 53 99 27 59 92 17]\n",
      " 36936/50001: episode: 4104, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 33.000, mean reward:  3.667 [ 2.000,  5.000], mean action: 55.889 [10.000, 99.000],  loss: 7.109216, mae: 2.424004, mean_q: 4.611258\n",
      "[24 34  2 37 12 19 42 41 48  4]\n",
      " 36945/50001: episode: 4105, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 41.000, mean reward:  4.556 [ 2.000,  7.000], mean action: 26.556 [2.000, 48.000],  loss: 9.268406, mae: 2.494674, mean_q: 4.771966\n",
      "[29  5 31  6 38 24 13  3 86  6]\n",
      " 36954/50001: episode: 4106, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 20.000, mean reward:  2.222 [-10.000,  7.000], mean action: 23.556 [3.000, 86.000],  loss: 7.812891, mae: 2.428258, mean_q: 4.566591\n",
      "[80 41  1 18 72 83 13 95 56 25]\n",
      " 36963/50001: episode: 4107, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 39.000, mean reward:  4.333 [ 2.000,  8.000], mean action: 44.889 [1.000, 95.000],  loss: 5.948659, mae: 2.486798, mean_q: 4.685619\n",
      "[52 86 97 60 95  1  6 78 84 61]\n",
      " 36972/50001: episode: 4108, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 35.000, mean reward:  3.889 [ 3.000,  6.000], mean action: 63.111 [1.000, 97.000],  loss: 8.492605, mae: 2.527676, mean_q: 4.808786\n",
      "[50 32  2 39 12 56 82 75 75 75]\n",
      " 36981/50001: episode: 4109, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward:  6.000, mean reward:  0.667 [-10.000,  5.000], mean action: 49.778 [2.000, 82.000],  loss: 5.979576, mae: 2.500305, mean_q: 4.767591\n",
      "[97 28 83 35 66 88 88  4  1 53]\n",
      " 36990/50001: episode: 4110, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 49.556 [1.000, 88.000],  loss: 7.213574, mae: 2.502198, mean_q: 4.718077\n",
      "[22  9 54 30  6 27 60 77 42 46]\n",
      " 36999/50001: episode: 4111, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 39.000 [6.000, 77.000],  loss: 6.243385, mae: 2.570106, mean_q: 4.934839\n",
      "[83 38 28 46 52 88 23 17 89 70]\n",
      " 37008/50001: episode: 4112, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 37.000, mean reward:  4.111 [ 3.000,  5.000], mean action: 50.111 [17.000, 89.000],  loss: 5.603692, mae: 2.593884, mean_q: 4.827735\n",
      "[20 57 23  6 88 56 41 92  9 83]\n",
      " 37017/50001: episode: 4113, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 50.556 [6.000, 92.000],  loss: 9.132183, mae: 2.496475, mean_q: 4.669360\n",
      "[71 36 15 97  8 88 50 30 24 48]\n",
      " 37026/50001: episode: 4114, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 44.000, mean reward:  4.889 [ 2.000,  8.000], mean action: 44.000 [8.000, 97.000],  loss: 6.047586, mae: 2.485274, mean_q: 4.676419\n",
      "[20 86 58 95 45 88 23 50  2 13]\n",
      " 37035/50001: episode: 4115, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward: 47.000, mean reward:  5.222 [ 3.000,  8.000], mean action: 51.111 [2.000, 95.000],  loss: 8.142767, mae: 2.594776, mean_q: 4.819189\n",
      "[33 93 92 85  8 13 14 48 84 47]\n",
      " 37044/50001: episode: 4116, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 53.778 [8.000, 93.000],  loss: 7.653898, mae: 2.514541, mean_q: 4.781775\n",
      "[91 37  6 24 90 57 13 93 79 97]\n",
      " 37053/50001: episode: 4117, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 44.000, mean reward:  4.889 [ 3.000,  7.000], mean action: 55.111 [6.000, 97.000],  loss: 7.017841, mae: 2.487710, mean_q: 4.747105\n",
      "[34 41 12 27 35 68 97 36 40 48]\n",
      " 37062/50001: episode: 4118, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 37.000, mean reward:  4.111 [ 2.000,  8.000], mean action: 44.889 [12.000, 97.000],  loss: 10.931425, mae: 2.470509, mean_q: 4.663195\n",
      "[89 90 93 42 37 13 37 37 85 35]\n",
      " 37071/50001: episode: 4119, duration: 0.069s, episode steps:   9, steps per second: 130, episode reward:  8.000, mean reward:  0.889 [-10.000,  6.000], mean action: 52.111 [13.000, 93.000],  loss: 9.180903, mae: 2.453911, mean_q: 4.692291\n",
      "[96 95  2 28 60 79 23 78 90 12]\n",
      " 37080/50001: episode: 4120, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward: 39.000, mean reward:  4.333 [ 2.000,  8.000], mean action: 51.889 [2.000, 95.000],  loss: 7.840449, mae: 2.408971, mean_q: 4.655991\n",
      "[36  6 23 68 20 88 95 60 37  9]\n",
      " 37089/50001: episode: 4121, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 44.000, mean reward:  4.889 [ 3.000,  8.000], mean action: 45.111 [6.000, 95.000],  loss: 9.689074, mae: 2.353817, mean_q: 4.496970\n",
      "[ 8 44 31 92 53 51 52 83 21 14]\n",
      " 37098/50001: episode: 4122, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 36.000, mean reward:  4.000 [ 2.000,  7.000], mean action: 49.000 [14.000, 92.000],  loss: 6.125487, mae: 2.394283, mean_q: 4.554641\n",
      "[99  3 37 60 37  2 46 62 75 42]\n",
      " 37107/50001: episode: 4123, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 40.444 [2.000, 75.000],  loss: 7.902096, mae: 2.402696, mean_q: 4.584492\n",
      "[57 51 23 49  8 84 95 64 94 64]\n",
      " 37116/50001: episode: 4124, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 19.000, mean reward:  2.111 [-10.000,  7.000], mean action: 59.111 [8.000, 95.000],  loss: 5.278215, mae: 2.403804, mean_q: 4.594268\n",
      "[ 8 67  4 62 92 52 54 57 27 46]\n",
      " 37125/50001: episode: 4125, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 51.222 [4.000, 92.000],  loss: 6.203448, mae: 2.451860, mean_q: 4.695049\n",
      "[43 83 47 92  9 11 97 69 13  4]\n",
      " 37134/50001: episode: 4126, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 47.222 [4.000, 97.000],  loss: 6.647081, mae: 2.492286, mean_q: 4.735795\n",
      "[82 60 98 80 96 54 92  1 96 32]\n",
      " 37143/50001: episode: 4127, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 26.000, mean reward:  2.889 [-10.000,  8.000], mean action: 67.667 [1.000, 98.000],  loss: 6.575373, mae: 2.564132, mean_q: 4.863437\n",
      "[84 10 37 65 34 47 34 52  6 81]\n",
      " 37152/50001: episode: 4128, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 40.667 [6.000, 81.000],  loss: 8.521835, mae: 2.509219, mean_q: 4.793670\n",
      "[98 66 28 38 32 25 82 21 12 42]\n",
      " 37161/50001: episode: 4129, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 38.000, mean reward:  4.222 [ 2.000,  8.000], mean action: 38.444 [12.000, 82.000],  loss: 7.450334, mae: 2.545530, mean_q: 4.849376\n",
      "[15 89 79 63 46 98 50 67 28 42]\n",
      " 37170/50001: episode: 4130, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 43.000, mean reward:  4.778 [ 3.000,  7.000], mean action: 62.444 [28.000, 98.000],  loss: 8.212443, mae: 2.595313, mean_q: 4.989104\n",
      "[77 82 14 74 31 52 29  9 89 75]\n",
      " 37179/50001: episode: 4131, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 50.556 [9.000, 89.000],  loss: 7.327408, mae: 2.517663, mean_q: 4.868341\n",
      "[26 48 57 87  9 23 80 32 68 88]\n",
      " 37188/50001: episode: 4132, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 37.000, mean reward:  4.111 [ 3.000,  8.000], mean action: 54.667 [9.000, 88.000],  loss: 11.127842, mae: 2.489940, mean_q: 4.811533\n",
      "[92 95 58 93 14 37 99 10 57  4]\n",
      " 37197/50001: episode: 4133, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 51.889 [4.000, 99.000],  loss: 8.712352, mae: 2.368706, mean_q: 4.600401\n",
      "[73  9 13 34 81 85  2 35 70 71]\n",
      " 37206/50001: episode: 4134, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 33.000, mean reward:  3.667 [ 3.000,  5.000], mean action: 44.444 [2.000, 85.000],  loss: 6.821249, mae: 2.387597, mean_q: 4.580024\n",
      "[65 42 33 76 10 62 20 31 33 12]\n",
      " 37215/50001: episode: 4135, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 27.000, mean reward:  3.000 [-10.000, 10.000], mean action: 35.444 [10.000, 76.000],  loss: 8.898890, mae: 2.391454, mean_q: 4.536027\n",
      "[39 52 36 37 33 95 14 95 28 85]\n",
      " 37224/50001: episode: 4136, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 52.778 [14.000, 95.000],  loss: 7.669715, mae: 2.362527, mean_q: 4.527332\n",
      "[98 14 35 60 79 33 13  6 96 46]\n",
      " 37233/50001: episode: 4137, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 35.000, mean reward:  3.889 [ 3.000,  6.000], mean action: 42.444 [6.000, 96.000],  loss: 7.989195, mae: 2.439309, mean_q: 4.660173\n",
      "[99 95 31 95 14 21  4 71 34 40]\n",
      " 37242/50001: episode: 4138, duration: 0.070s, episode steps:   9, steps per second: 129, episode reward: 26.000, mean reward:  2.889 [-10.000,  8.000], mean action: 45.000 [4.000, 95.000],  loss: 10.040318, mae: 2.461333, mean_q: 4.636621\n",
      "[63 37 18 31 90 69 94 75 12 66]\n",
      " 37251/50001: episode: 4139, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 41.000, mean reward:  4.556 [ 2.000,  7.000], mean action: 54.667 [12.000, 94.000],  loss: 6.819711, mae: 2.397794, mean_q: 4.551504\n",
      "[ 5 13 28 69 11 60 60 57  5 39]\n",
      " 37260/50001: episode: 4140, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward:  6.000, mean reward:  0.667 [-10.000,  5.000], mean action: 38.000 [5.000, 69.000],  loss: 8.804936, mae: 2.376761, mean_q: 4.521882\n",
      "[ 9 28 44  1 95 24 29 70 74 61]\n",
      " 37269/50001: episode: 4141, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 34.000, mean reward:  3.778 [ 2.000,  6.000], mean action: 47.333 [1.000, 95.000],  loss: 9.711864, mae: 2.376867, mean_q: 4.511279\n",
      "[56 95  2 47 11 23 47 37 98 12]\n",
      " 37278/50001: episode: 4142, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 28.000, mean reward:  3.111 [-10.000,  7.000], mean action: 41.333 [2.000, 98.000],  loss: 5.441718, mae: 2.334072, mean_q: 4.568141\n",
      "[80 30 66 63 34 82 30 95  1 31]\n",
      " 37287/50001: episode: 4143, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 31.000, mean reward:  3.444 [-10.000,  8.000], mean action: 48.000 [1.000, 95.000],  loss: 8.882580, mae: 2.429947, mean_q: 4.621855\n",
      "[67 46 99 87 46  2 20 60 63 71]\n",
      " 37296/50001: episode: 4144, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 54.889 [2.000, 99.000],  loss: 6.891384, mae: 2.383509, mean_q: 4.555736\n",
      "[97 48 97  1 44 40 55 99 26 55]\n",
      " 37305/50001: episode: 4145, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward:  0.000, mean reward:  0.000 [-10.000,  4.000], mean action: 51.667 [1.000, 99.000],  loss: 8.514896, mae: 2.444712, mean_q: 4.648761\n",
      "[15  5 32 65 35 75 96 60 31 52]\n",
      " 37314/50001: episode: 4146, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 50.111 [5.000, 96.000],  loss: 8.083089, mae: 2.419316, mean_q: 4.646745\n",
      "[89 53 40 13 42 79 83 32 32 48]\n",
      " 37323/50001: episode: 4147, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 46.889 [13.000, 83.000],  loss: 7.911512, mae: 2.469528, mean_q: 4.671121\n",
      "[99 41  1 48  8 67 13  2 95 32]\n",
      " 37332/50001: episode: 4148, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 43.000, mean reward:  4.778 [ 3.000,  8.000], mean action: 34.111 [1.000, 95.000],  loss: 7.129581, mae: 2.447744, mean_q: 4.662202\n",
      "[38 34 31  2 84 45 79 72 40 50]\n",
      " 37341/50001: episode: 4149, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 40.000, mean reward:  4.444 [ 2.000,  9.000], mean action: 48.556 [2.000, 84.000],  loss: 9.899113, mae: 2.524242, mean_q: 4.797435\n",
      "[47 31 31 46 49 97 13 34 97 64]\n",
      " 37350/50001: episode: 4150, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 13.000, mean reward:  1.444 [-10.000,  7.000], mean action: 51.333 [13.000, 97.000],  loss: 6.284607, mae: 2.434035, mean_q: 4.634830\n",
      "[80 75 47 56 90 42 14 51 53 47]\n",
      " 37359/50001: episode: 4151, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 21.000, mean reward:  2.333 [-10.000,  5.000], mean action: 52.778 [14.000, 90.000],  loss: 9.454995, mae: 2.435825, mean_q: 4.671930\n",
      "[41 31 93 18 88 34 24 53 27 71]\n",
      " 37368/50001: episode: 4152, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 48.778 [18.000, 93.000],  loss: 6.224751, mae: 2.395957, mean_q: 4.534960\n",
      "[48 32 51 28 60 38 95  4 42 48]\n",
      " 37377/50001: episode: 4153, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 44.222 [4.000, 95.000],  loss: 7.803545, mae: 2.409166, mean_q: 4.629636\n",
      "[ 5 96 70 31 99 77  1 10  4 13]\n",
      " 37386/50001: episode: 4154, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 41.000, mean reward:  4.556 [ 2.000,  8.000], mean action: 44.556 [1.000, 99.000],  loss: 8.300182, mae: 2.370489, mean_q: 4.524035\n",
      "[17 66 79 27 53  9 92 50 66 80]\n",
      " 37395/50001: episode: 4155, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 58.000 [9.000, 92.000],  loss: 8.658859, mae: 2.426365, mean_q: 4.636992\n",
      "[ 4 89 40 38 30 82 28  1 41 90]\n",
      " 37404/50001: episode: 4156, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 41.000, mean reward:  4.556 [ 2.000,  7.000], mean action: 48.778 [1.000, 90.000],  loss: 7.783843, mae: 2.437332, mean_q: 4.651035\n",
      "[ 6 52 46  6 59 63 74 66 88 88]\n",
      " 37413/50001: episode: 4157, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 14.000, mean reward:  1.556 [-10.000,  7.000], mean action: 60.222 [6.000, 88.000],  loss: 6.055660, mae: 2.439604, mean_q: 4.694689\n",
      "[71 96 74  2  5 92 50 93 66 28]\n",
      " 37422/50001: episode: 4158, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward: 44.000, mean reward:  4.889 [ 3.000,  7.000], mean action: 56.222 [2.000, 96.000],  loss: 4.170217, mae: 2.476052, mean_q: 4.735363\n",
      "[54 41 86 62 12 11 62 75 31 97]\n",
      " 37431/50001: episode: 4159, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 30.000, mean reward:  3.333 [-10.000,  7.000], mean action: 53.000 [11.000, 97.000],  loss: 4.963358, mae: 2.475507, mean_q: 4.687627\n",
      "[32 37 19  4 96 24 97  4 67 79]\n",
      " 37440/50001: episode: 4160, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 47.444 [4.000, 97.000],  loss: 8.206003, mae: 2.502515, mean_q: 4.801880\n",
      "[72 52 84 63  4 11 82 60 24 48]\n",
      " 37449/50001: episode: 4161, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 44.000, mean reward:  4.889 [ 3.000,  8.000], mean action: 47.556 [4.000, 84.000],  loss: 7.633928, mae: 2.535718, mean_q: 4.846488\n",
      "[12 37 12 87 41 30 13 50 80 85]\n",
      " 37458/50001: episode: 4162, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 48.333 [12.000, 87.000],  loss: 7.610988, mae: 2.603097, mean_q: 4.919708\n",
      "[30 95 20 50 93 50 20 95 12 57]\n",
      " 37467/50001: episode: 4163, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: -2.000, mean reward: -0.222 [-10.000,  7.000], mean action: 54.667 [12.000, 95.000],  loss: 7.536535, mae: 2.503211, mean_q: 4.794798\n",
      "[44  0 28 53 66 57 51 50 88 96]\n",
      " 37476/50001: episode: 4164, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 54.333 [0.000, 96.000],  loss: 7.632320, mae: 2.553730, mean_q: 4.850153\n",
      "[33 37 93 34 95 86 69 28 82 12]\n",
      " 37485/50001: episode: 4165, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 41.000, mean reward:  4.556 [ 2.000,  8.000], mean action: 59.556 [12.000, 95.000],  loss: 6.275385, mae: 2.487520, mean_q: 4.711602\n",
      "[60 79 86 34 77 14 13 59 14 75]\n",
      " 37494/50001: episode: 4166, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 50.111 [13.000, 86.000],  loss: 8.316647, mae: 2.540637, mean_q: 4.791481\n",
      "[93 46 48 79  1 11 84 14 34 57]\n",
      " 37503/50001: episode: 4167, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 40.000, mean reward:  4.444 [ 3.000,  8.000], mean action: 41.556 [1.000, 84.000],  loss: 10.308199, mae: 2.448596, mean_q: 4.649402\n",
      "[50 57 79 14 33 31 10 42 95 32]\n",
      " 37512/50001: episode: 4168, duration: 0.070s, episode steps:   9, steps per second: 129, episode reward: 42.000, mean reward:  4.667 [ 3.000,  9.000], mean action: 43.667 [10.000, 95.000],  loss: 7.394314, mae: 2.415084, mean_q: 4.606091\n",
      "[28 13 16 82 61  1 74 50 66 48]\n",
      " 37521/50001: episode: 4169, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 45.667 [1.000, 82.000],  loss: 9.877117, mae: 2.413932, mean_q: 4.579119\n",
      "[43 23 95 34 95  5 97 66 29 96]\n",
      " 37530/50001: episode: 4170, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 20.000, mean reward:  2.222 [-10.000,  5.000], mean action: 60.000 [5.000, 97.000],  loss: 5.002687, mae: 2.363002, mean_q: 4.480811\n",
      "[ 8 95 68 69 94 95  0 64 42 12]\n",
      " 37539/50001: episode: 4171, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 25.000, mean reward:  2.778 [-10.000,  9.000], mean action: 59.889 [0.000, 95.000],  loss: 7.666510, mae: 2.439122, mean_q: 4.657652\n",
      "[31 48 94  1 58 37 63 23 31 93]\n",
      " 37548/50001: episode: 4172, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 21.000, mean reward:  2.333 [-10.000,  6.000], mean action: 49.778 [1.000, 94.000],  loss: 8.326533, mae: 2.406769, mean_q: 4.569903\n",
      "[29 13 76 77 97 60 50 97 12 46]\n",
      " 37557/50001: episode: 4173, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 29.000, mean reward:  3.222 [-10.000,  7.000], mean action: 58.667 [12.000, 97.000],  loss: 5.225647, mae: 2.426377, mean_q: 4.597760\n",
      "[89 93 75 20 46 79 71  9 63 34]\n",
      " 37566/50001: episode: 4174, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 42.000, mean reward:  4.667 [ 2.000, 10.000], mean action: 54.444 [9.000, 93.000],  loss: 7.291651, mae: 2.451989, mean_q: 4.659850\n",
      "[36 12 24 75 13 75 26 20 78 50]\n",
      " 37575/50001: episode: 4175, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 25.000, mean reward:  2.778 [-10.000, 10.000], mean action: 41.444 [12.000, 78.000],  loss: 9.077593, mae: 2.557071, mean_q: 4.913679\n",
      "[63 71  8  8 46 54 75 14 24 51]\n",
      " 37584/50001: episode: 4176, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 26.000, mean reward:  2.889 [-10.000,  6.000], mean action: 39.000 [8.000, 75.000],  loss: 8.250568, mae: 2.442549, mean_q: 4.746850\n",
      "[45 27 75  7 32 95 95 66 12 54]\n",
      " 37593/50001: episode: 4177, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 51.444 [7.000, 95.000],  loss: 5.731077, mae: 2.387623, mean_q: 4.490126\n",
      "[40 50 60 11 23 41 66 41 42 34]\n",
      " 37602/50001: episode: 4178, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 29.000, mean reward:  3.222 [-10.000, 10.000], mean action: 40.889 [11.000, 66.000],  loss: 6.802186, mae: 2.443181, mean_q: 4.591955\n",
      "[26 76 35 77 31 84 58 34 30 52]\n",
      " 37611/50001: episode: 4179, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 43.000, mean reward:  4.778 [ 3.000,  8.000], mean action: 53.000 [30.000, 84.000],  loss: 5.091699, mae: 2.496627, mean_q: 4.685271\n",
      "[16 27 89 99 36 88 46 24 14 42]\n",
      " 37620/50001: episode: 4180, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 41.000, mean reward:  4.556 [ 2.000,  6.000], mean action: 51.667 [14.000, 99.000],  loss: 6.847616, mae: 2.534490, mean_q: 4.736741\n",
      "[86 17 30 75  2 93 91 96 14 24]\n",
      " 37629/50001: episode: 4181, duration: 0.069s, episode steps:   9, steps per second: 131, episode reward: 40.000, mean reward:  4.444 [ 3.000,  6.000], mean action: 49.111 [2.000, 96.000],  loss: 8.816120, mae: 2.523461, mean_q: 4.739486\n",
      "[68 60 93 86 24 79 42  2 98 12]\n",
      " 37638/50001: episode: 4182, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 44.000, mean reward:  4.889 [ 3.000,  8.000], mean action: 55.111 [2.000, 98.000],  loss: 7.735096, mae: 2.520761, mean_q: 4.726986\n",
      "[24  1 92 95  1 53 46 42 66 66]\n",
      " 37647/50001: episode: 4183, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 12.000, mean reward:  1.333 [-10.000,  7.000], mean action: 51.333 [1.000, 95.000],  loss: 8.517192, mae: 2.488658, mean_q: 4.719697\n",
      "[29 37  2 23 78 86 95 41 79 75]\n",
      " 37656/50001: episode: 4184, duration: 0.072s, episode steps:   9, steps per second: 125, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 57.333 [2.000, 95.000],  loss: 7.274108, mae: 2.468760, mean_q: 4.608700\n",
      "[64 60 21 95 19 16 18 33 40 51]\n",
      " 37665/50001: episode: 4185, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward: 37.000, mean reward:  4.111 [ 3.000,  6.000], mean action: 39.222 [16.000, 95.000],  loss: 8.052147, mae: 2.473331, mean_q: 4.676485\n",
      "[90 34 12 37 95 95 83 97 12 66]\n",
      " 37674/50001: episode: 4186, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 12.000, mean reward:  1.333 [-10.000,  6.000], mean action: 59.000 [12.000, 97.000],  loss: 7.662130, mae: 2.540011, mean_q: 4.819941\n",
      "[86  2 31 55 52 21 91 28  5 66]\n",
      " 37683/50001: episode: 4187, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 39.000 [2.000, 91.000],  loss: 7.063952, mae: 2.501894, mean_q: 4.679124\n",
      "[84 58 31 49 81 89 68  4  2  1]\n",
      " 37692/50001: episode: 4188, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 42.556 [1.000, 89.000],  loss: 8.902546, mae: 2.454532, mean_q: 4.583184\n",
      "[29 12  2  8 84 95 94 12 52 16]\n",
      " 37701/50001: episode: 4189, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 21.000, mean reward:  2.333 [-10.000,  7.000], mean action: 41.667 [2.000, 95.000],  loss: 5.415613, mae: 2.409357, mean_q: 4.572184\n",
      "[78 94 41  2 28 75 69 74 35 52]\n",
      " 37710/50001: episode: 4190, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 39.000, mean reward:  4.333 [ 3.000,  5.000], mean action: 52.222 [2.000, 94.000],  loss: 9.052972, mae: 2.470899, mean_q: 4.720677\n",
      "[29 24 96 39 62 25 60 59 13 48]\n",
      " 37719/50001: episode: 4191, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 42.000, mean reward:  4.667 [ 2.000,  8.000], mean action: 47.333 [13.000, 96.000],  loss: 6.143963, mae: 2.501138, mean_q: 4.682291\n",
      "[31 93 74 34 82 53 74 23 51 77]\n",
      " 37728/50001: episode: 4192, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 20.000, mean reward:  2.222 [-10.000,  5.000], mean action: 62.333 [23.000, 93.000],  loss: 7.949479, mae: 2.407070, mean_q: 4.621149\n",
      "[86 96 88 50  2 28 46 13 27 63]\n",
      " 37737/50001: episode: 4193, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 41.000, mean reward:  4.556 [ 3.000,  6.000], mean action: 45.889 [2.000, 96.000],  loss: 6.010656, mae: 2.512873, mean_q: 4.729254\n",
      "[13 52 31 57 90 31 80 67 43 98]\n",
      " 37746/50001: episode: 4194, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 21.000, mean reward:  2.333 [-10.000,  8.000], mean action: 61.000 [31.000, 98.000],  loss: 8.764553, mae: 2.461863, mean_q: 4.636664\n",
      "[19 24 34 53 93 13 13 68 38 89]\n",
      " 37755/50001: episode: 4195, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 20.000, mean reward:  2.222 [-10.000,  5.000], mean action: 47.222 [13.000, 93.000],  loss: 8.305500, mae: 2.494585, mean_q: 4.736416\n",
      "[73 60 79 37 95 31 58 42 40 87]\n",
      " 37764/50001: episode: 4196, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 58.778 [31.000, 95.000],  loss: 7.136842, mae: 2.503698, mean_q: 4.726362\n",
      "[46 51 49 80 93 99 24 95 12  2]\n",
      " 37773/50001: episode: 4197, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 56.111 [2.000, 99.000],  loss: 7.624473, mae: 2.452952, mean_q: 4.584658\n",
      "[36 16 97 66 14 81 35 87 12  1]\n",
      " 37782/50001: episode: 4198, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 38.000, mean reward:  4.222 [ 3.000,  7.000], mean action: 45.444 [1.000, 97.000],  loss: 6.425718, mae: 2.539152, mean_q: 4.796943\n",
      "[64  1 40 34 53 57 67 98  4 66]\n",
      " 37791/50001: episode: 4199, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 46.667 [1.000, 98.000],  loss: 8.051411, mae: 2.575334, mean_q: 4.818652\n",
      "[82  8 75 88 31 75 13 93  2 50]\n",
      " 37800/50001: episode: 4200, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 28.000, mean reward:  3.111 [-10.000,  8.000], mean action: 48.333 [2.000, 93.000],  loss: 7.272886, mae: 2.516000, mean_q: 4.770656\n",
      "[61 34  8 93 48 79 69  5 34 28]\n",
      " 37809/50001: episode: 4201, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 44.222 [5.000, 93.000],  loss: 5.066691, mae: 2.456989, mean_q: 4.632452\n",
      "[44 69 75  3 34 88 12 79 14 32]\n",
      " 37818/50001: episode: 4202, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 44.000, mean reward:  4.889 [ 3.000,  6.000], mean action: 45.111 [3.000, 88.000],  loss: 6.916882, mae: 2.502222, mean_q: 4.706479\n",
      "[24  8 59 24 29 90 40 37  9 27]\n",
      " 37827/50001: episode: 4203, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 35.889 [8.000, 90.000],  loss: 8.146089, mae: 2.545821, mean_q: 4.844381\n",
      "[ 9 54 93 98 37 18 14 59 97 79]\n",
      " 37836/50001: episode: 4204, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 44.000, mean reward:  4.889 [ 3.000,  7.000], mean action: 61.000 [14.000, 98.000],  loss: 9.288233, mae: 2.542083, mean_q: 4.793577\n",
      "[46 13 78 56 66  8 47 66 37 48]\n",
      " 37845/50001: episode: 4205, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 46.556 [8.000, 78.000],  loss: 8.641354, mae: 2.514147, mean_q: 4.809371\n",
      "[ 9 28 36 54  2 14 21  5 90 51]\n",
      " 37854/50001: episode: 4206, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 38.000, mean reward:  4.222 [ 2.000,  6.000], mean action: 33.444 [2.000, 90.000],  loss: 7.257449, mae: 2.516145, mean_q: 4.740999\n",
      "[43 31 99 70 53 97 83 88 42  9]\n",
      " 37863/50001: episode: 4207, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 63.556 [9.000, 99.000],  loss: 5.180607, mae: 2.540290, mean_q: 4.745183\n",
      "[49 27 18 54  2 60 44 13 32 12]\n",
      " 37872/50001: episode: 4208, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 43.000, mean reward:  4.778 [ 2.000,  8.000], mean action: 29.111 [2.000, 60.000],  loss: 4.579283, mae: 2.606452, mean_q: 4.866330\n",
      "[82 50  0 63 88 53  1 74 14 86]\n",
      " 37881/50001: episode: 4209, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 38.000, mean reward:  4.222 [ 2.000,  6.000], mean action: 47.667 [0.000, 88.000],  loss: 6.601522, mae: 2.646035, mean_q: 4.949929\n",
      "[63 53 80 55 27 66 11 23  2 99]\n",
      " 37890/50001: episode: 4210, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 46.222 [2.000, 99.000],  loss: 6.618125, mae: 2.618966, mean_q: 4.892665\n",
      "[35 41 98 48 98 89 23 89 12  4]\n",
      " 37899/50001: episode: 4211, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 15.000, mean reward:  1.667 [-10.000,  8.000], mean action: 55.778 [4.000, 98.000],  loss: 9.469151, mae: 2.585390, mean_q: 4.887839\n",
      "[13 90  0 25 31 23 48 63 98 50]\n",
      " 37908/50001: episode: 4212, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 47.556 [0.000, 98.000],  loss: 6.715770, mae: 2.580272, mean_q: 4.778927\n",
      "[49 85 37 12 24 23 23 44  5 43]\n",
      " 37917/50001: episode: 4213, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 21.000, mean reward:  2.333 [-10.000,  5.000], mean action: 32.889 [5.000, 85.000],  loss: 7.176495, mae: 2.569645, mean_q: 4.785802\n",
      "[49 58 69 14 95 78  4 34 37 52]\n",
      " 37926/50001: episode: 4214, duration: 0.065s, episode steps:   9, steps per second: 137, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 49.000 [4.000, 95.000],  loss: 8.222216, mae: 2.543144, mean_q: 4.746037\n",
      "[48 59  6 80  6 62 21 30 44  6]\n",
      " 37935/50001: episode: 4215, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward:  5.000, mean reward:  0.556 [-10.000,  5.000], mean action: 34.889 [6.000, 80.000],  loss: 6.548733, mae: 2.522893, mean_q: 4.752361\n",
      "[40 21 12 30  9 14 71 55 90 52]\n",
      " 37944/50001: episode: 4216, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 37.000, mean reward:  4.111 [ 2.000,  6.000], mean action: 39.333 [9.000, 90.000],  loss: 7.496502, mae: 2.523230, mean_q: 4.796772\n",
      "[49 96 78 76 98 46 56 56 52 62]\n",
      " 37953/50001: episode: 4217, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 68.889 [46.000, 98.000],  loss: 5.418067, mae: 2.555451, mean_q: 4.854398\n",
      "[ 4 78 90 85 37 56 69 32 20 79]\n",
      " 37962/50001: episode: 4218, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 60.667 [20.000, 90.000],  loss: 7.870854, mae: 2.536530, mean_q: 4.779598\n",
      "[18 88 37 11 48 60 19 13 52 47]\n",
      " 37971/50001: episode: 4219, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 41.667 [11.000, 88.000],  loss: 9.024853, mae: 2.559760, mean_q: 4.827239\n",
      "[84 55 66 23 50 42 37 99 31 75]\n",
      " 37980/50001: episode: 4220, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 40.000, mean reward:  4.444 [ 2.000,  6.000], mean action: 53.111 [23.000, 99.000],  loss: 5.634924, mae: 2.540649, mean_q: 4.745677\n",
      "[76 28 57  6 11  1 34 27 66 64]\n",
      " 37989/50001: episode: 4221, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 43.000, mean reward:  4.778 [ 3.000,  7.000], mean action: 32.667 [1.000, 66.000],  loss: 10.869927, mae: 2.510849, mean_q: 4.775062\n",
      "[35 42 13 80 35 47 83 68 62 13]\n",
      " 37998/50001: episode: 4222, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward:  6.000, mean reward:  0.667 [-10.000,  5.000], mean action: 49.222 [13.000, 83.000],  loss: 6.113446, mae: 2.453867, mean_q: 4.632740\n",
      "[32 11 47 71 81 70 14 74 86 12]\n",
      " 38007/50001: episode: 4223, duration: 0.065s, episode steps:   9, steps per second: 137, episode reward: 38.000, mean reward:  4.222 [ 2.000, 10.000], mean action: 51.778 [11.000, 86.000],  loss: 6.766732, mae: 2.518316, mean_q: 4.724419\n",
      "[45 93 82 32 13 89 79 86 34  1]\n",
      " 38016/50001: episode: 4224, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 45.000, mean reward:  5.000 [ 3.000,  8.000], mean action: 56.556 [1.000, 93.000],  loss: 7.886433, mae: 2.556181, mean_q: 4.750365\n",
      "[99 82 98  4 12 95 79 93  9 76]\n",
      " 38025/50001: episode: 4225, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 60.889 [4.000, 98.000],  loss: 5.932661, mae: 2.557484, mean_q: 4.816028\n",
      "[58 17 94 82 32 18 68 50 94 50]\n",
      " 38034/50001: episode: 4226, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward:  9.000, mean reward:  1.000 [-10.000,  7.000], mean action: 56.111 [17.000, 94.000],  loss: 5.071795, mae: 2.494628, mean_q: 4.691531\n",
      "[62 31 37 45 30 14 99 23 24  2]\n",
      " 38043/50001: episode: 4227, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 41.000, mean reward:  4.556 [ 2.000,  6.000], mean action: 33.889 [2.000, 99.000],  loss: 8.143393, mae: 2.534320, mean_q: 4.755725\n",
      "[94 41 18  7 93 27 54 57 79 26]\n",
      " 38052/50001: episode: 4228, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 44.667 [7.000, 93.000],  loss: 7.611241, mae: 2.544183, mean_q: 4.817001\n",
      "[11 17 34 32  8 48 42 53 20 32]\n",
      " 38061/50001: episode: 4229, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 22.000, mean reward:  2.444 [-10.000,  5.000], mean action: 31.778 [8.000, 53.000],  loss: 9.672401, mae: 2.544519, mean_q: 4.731012\n",
      "[72 25 11 96 13 37 51 31 85 97]\n",
      " 38070/50001: episode: 4230, duration: 0.068s, episode steps:   9, steps per second: 133, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 49.556 [11.000, 97.000],  loss: 8.345702, mae: 2.451137, mean_q: 4.622263\n",
      "[87 40 96 98 37  8 87 79  2 88]\n",
      " 38079/50001: episode: 4231, duration: 0.066s, episode steps:   9, steps per second: 135, episode reward: 28.000, mean reward:  3.111 [-10.000,  7.000], mean action: 59.444 [2.000, 98.000],  loss: 7.848953, mae: 2.488983, mean_q: 4.736809\n",
      "[76 62 55 87 16 94  8  1 62 63]\n",
      " 38088/50001: episode: 4232, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 21.000, mean reward:  2.333 [-10.000,  7.000], mean action: 49.778 [1.000, 94.000],  loss: 7.948443, mae: 2.413476, mean_q: 4.574411\n",
      "[40 95 69 40 40 76 24 42 66 75]\n",
      " 38097/50001: episode: 4233, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 14.000, mean reward:  1.556 [-10.000,  7.000], mean action: 58.556 [24.000, 95.000],  loss: 5.920438, mae: 2.422395, mean_q: 4.623028\n",
      "[33 13 51  0 29 42  9 40  6 72]\n",
      " 38106/50001: episode: 4234, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 31.000, mean reward:  3.444 [ 2.000,  5.000], mean action: 29.111 [0.000, 72.000],  loss: 6.988446, mae: 2.434082, mean_q: 4.637427\n",
      "[70  9 77 24 27 73 56 96 57 12]\n",
      " 38115/50001: episode: 4235, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 39.000, mean reward:  4.333 [ 2.000,  8.000], mean action: 47.889 [9.000, 96.000],  loss: 8.720534, mae: 2.505597, mean_q: 4.816443\n",
      "[53 28  8  0 89 34 66 12 44 47]\n",
      " 38124/50001: episode: 4236, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 43.000, mean reward:  4.778 [ 2.000,  8.000], mean action: 36.444 [0.000, 89.000],  loss: 4.810930, mae: 2.526756, mean_q: 4.778803\n",
      "[41 54 27 40 74  4 79 83 10 32]\n",
      " 38133/50001: episode: 4237, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 44.778 [4.000, 83.000],  loss: 7.843714, mae: 2.591614, mean_q: 4.849184\n",
      "[68 27 75 95 20 45 79 88 88 97]\n",
      " 38142/50001: episode: 4238, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 68.222 [20.000, 97.000],  loss: 6.165953, mae: 2.569367, mean_q: 4.874198\n",
      "[64 14 27 37 18 59 39 94 67 28]\n",
      " 38151/50001: episode: 4239, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 36.000, mean reward:  4.000 [ 2.000,  8.000], mean action: 42.556 [14.000, 94.000],  loss: 7.803450, mae: 2.498194, mean_q: 4.759212\n",
      "[ 4 27 77 16 31 94 28 60 12 37]\n",
      " 38160/50001: episode: 4240, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 44.000, mean reward:  4.889 [ 3.000,  8.000], mean action: 42.444 [12.000, 94.000],  loss: 3.980108, mae: 2.499284, mean_q: 4.701607\n",
      "[99 37 50 53 31 85 83 50 87 51]\n",
      " 38169/50001: episode: 4241, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 19.000, mean reward:  2.111 [-10.000,  5.000], mean action: 58.556 [31.000, 87.000],  loss: 7.470371, mae: 2.585134, mean_q: 4.861493\n",
      "[65 98 97 48 59 83 37  1 98 84]\n",
      " 38178/50001: episode: 4242, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 67.222 [1.000, 98.000],  loss: 6.939991, mae: 2.560791, mean_q: 4.832979\n",
      "[97 51 32 13 52 37 95 79 52 75]\n",
      " 38187/50001: episode: 4243, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 54.000 [13.000, 95.000],  loss: 6.768099, mae: 2.595440, mean_q: 4.907282\n",
      "[27 83 47  1 63 76 62 30 37 20]\n",
      " 38196/50001: episode: 4244, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 39.000, mean reward:  4.333 [ 3.000,  8.000], mean action: 46.556 [1.000, 83.000],  loss: 4.973623, mae: 2.544385, mean_q: 4.784487\n",
      "[66 82 32 64 41 32 23 26 40 57]\n",
      " 38205/50001: episode: 4245, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 44.111 [23.000, 82.000],  loss: 8.467866, mae: 2.599622, mean_q: 4.845228\n",
      "[41 36 13 77 69 59 83 34 13 84]\n",
      " 38214/50001: episode: 4246, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 23.000, mean reward:  2.556 [-10.000,  8.000], mean action: 52.000 [13.000, 84.000],  loss: 7.242430, mae: 2.628085, mean_q: 4.910767\n",
      "[90 97 46 17 81 13 94 90 30 98]\n",
      " 38223/50001: episode: 4247, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 62.889 [13.000, 98.000],  loss: 6.912328, mae: 2.584469, mean_q: 4.809652\n",
      "[53 10 42 26 83 60 66 40 12 13]\n",
      " 38232/50001: episode: 4248, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 41.000, mean reward:  4.556 [ 2.000,  8.000], mean action: 39.111 [10.000, 83.000],  loss: 7.238844, mae: 2.540920, mean_q: 4.810915\n",
      "[22 26 95 72 18  1 63 17 81 48]\n",
      " 38241/50001: episode: 4249, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 37.000, mean reward:  4.111 [ 2.000,  8.000], mean action: 46.778 [1.000, 95.000],  loss: 7.441778, mae: 2.510957, mean_q: 4.746889\n",
      "[23 22 54 79 79 51  1 44 48 46]\n",
      " 38250/50001: episode: 4250, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 47.111 [1.000, 79.000],  loss: 9.523400, mae: 2.479643, mean_q: 4.722692\n",
      "[23 14 23 54 92 46 34 50 46 79]\n",
      " 38259/50001: episode: 4251, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 15.000, mean reward:  1.667 [-10.000,  7.000], mean action: 48.667 [14.000, 92.000],  loss: 6.177040, mae: 2.386563, mean_q: 4.498788\n",
      "[59 62 54 55 10 79 13 21 88 32]\n",
      " 38268/50001: episode: 4252, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 46.000 [10.000, 88.000],  loss: 7.721349, mae: 2.453495, mean_q: 4.667376\n",
      "[64 49 31 33 67  4  9 24 64 46]\n",
      " 38277/50001: episode: 4253, duration: 0.068s, episode steps:   9, steps per second: 133, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 36.333 [4.000, 67.000],  loss: 7.179408, mae: 2.451243, mean_q: 4.595449\n",
      "[18 43 49 54 28 34 56 46 99 48]\n",
      " 38286/50001: episode: 4254, duration: 0.068s, episode steps:   9, steps per second: 133, episode reward: 43.000, mean reward:  4.778 [ 2.000,  8.000], mean action: 50.778 [28.000, 99.000],  loss: 7.186033, mae: 2.495697, mean_q: 4.705130\n",
      "[35 37 88 17 97 12 46 19 93 14]\n",
      " 38295/50001: episode: 4255, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 40.000, mean reward:  4.444 [ 2.000,  6.000], mean action: 47.000 [12.000, 97.000],  loss: 10.718748, mae: 2.491645, mean_q: 4.678953\n",
      "[85 78 10 93 58 90 46  3 95 50]\n",
      " 38304/50001: episode: 4256, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 47.000, mean reward:  5.222 [ 3.000, 10.000], mean action: 58.111 [3.000, 95.000],  loss: 6.563561, mae: 2.434354, mean_q: 4.562138\n",
      "[ 3 37 74 49 80 75 37 96 12 48]\n",
      " 38313/50001: episode: 4257, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 56.444 [12.000, 96.000],  loss: 7.737280, mae: 2.478360, mean_q: 4.766973\n",
      "[71 39  5 32 75 48 28 84 12 94]\n",
      " 38322/50001: episode: 4258, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 46.333 [5.000, 94.000],  loss: 8.170094, mae: 2.431695, mean_q: 4.539721\n",
      "[74 20 21 93 96 87 32 13 62 50]\n",
      " 38331/50001: episode: 4259, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 43.000, mean reward:  4.778 [ 3.000,  9.000], mean action: 52.667 [13.000, 96.000],  loss: 6.032632, mae: 2.460088, mean_q: 4.669991\n",
      "[50  5 35 13 44 59 60 20 86 72]\n",
      " 38340/50001: episode: 4260, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 30.000, mean reward:  3.333 [ 2.000,  5.000], mean action: 43.778 [5.000, 86.000],  loss: 6.902361, mae: 2.534477, mean_q: 4.844601\n",
      "[45  4 82 99 50 47 67 84 32 97]\n",
      " 38349/50001: episode: 4261, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 62.444 [4.000, 99.000],  loss: 6.251765, mae: 2.509978, mean_q: 4.751639\n",
      "[36 97 31 19 78 81 48 41 53  1]\n",
      " 38358/50001: episode: 4262, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 49.889 [1.000, 97.000],  loss: 7.909032, mae: 2.492517, mean_q: 4.777161\n",
      "[41 50 41 32 40 97 50 95 27 36]\n",
      " 38367/50001: episode: 4263, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 10.000, mean reward:  1.111 [-10.000,  7.000], mean action: 52.000 [27.000, 97.000],  loss: 6.338175, mae: 2.530933, mean_q: 4.688629\n",
      "[31 95 99 12 75 99 60 20 30 34]\n",
      " 38376/50001: episode: 4264, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 26.000, mean reward:  2.889 [-10.000,  9.000], mean action: 58.222 [12.000, 99.000],  loss: 7.855905, mae: 2.528075, mean_q: 4.821194\n",
      "[62  0 85 93 88 34 13 88 12 93]\n",
      " 38385/50001: episode: 4265, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 14.000, mean reward:  1.556 [-10.000,  7.000], mean action: 56.222 [0.000, 93.000],  loss: 9.202966, mae: 2.478017, mean_q: 4.686287\n",
      "[12 42 28 15 84 34 91 54 54 84]\n",
      " 38394/50001: episode: 4266, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward:  6.000, mean reward:  0.667 [-10.000,  7.000], mean action: 54.000 [15.000, 91.000],  loss: 7.517272, mae: 2.417850, mean_q: 4.613554\n",
      "[93 96 59 41 12 77 24 94 54 95]\n",
      " 38403/50001: episode: 4267, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 41.000, mean reward:  4.556 [ 3.000, 11.000], mean action: 61.333 [12.000, 96.000],  loss: 8.977179, mae: 2.397656, mean_q: 4.575798\n",
      "[ 8 46 16 57 14 50 42 20 88 47]\n",
      " 38412/50001: episode: 4268, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 44.000, mean reward:  4.889 [ 3.000,  8.000], mean action: 42.222 [14.000, 88.000],  loss: 7.912501, mae: 2.417851, mean_q: 4.547693\n",
      "[11 27 16 62 24 95 50 13 10 91]\n",
      " 38421/50001: episode: 4269, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 43.111 [10.000, 95.000],  loss: 9.096673, mae: 2.321297, mean_q: 4.469091\n",
      "[88 34 41  1 76 54 36 62 54 23]\n",
      " 38430/50001: episode: 4270, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 22.000, mean reward:  2.444 [-10.000,  8.000], mean action: 42.333 [1.000, 76.000],  loss: 7.685472, mae: 2.361516, mean_q: 4.460847\n",
      "[83 46 16 70 63 23 66 69 98 32]\n",
      " 38439/50001: episode: 4271, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 43.000, mean reward:  4.778 [ 2.000,  8.000], mean action: 53.667 [16.000, 98.000],  loss: 9.007653, mae: 2.383512, mean_q: 4.518639\n",
      "[91 60 23  4  2 18 28 50 32 76]\n",
      " 38448/50001: episode: 4272, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 32.556 [2.000, 76.000],  loss: 9.477494, mae: 2.367132, mean_q: 4.437135\n",
      "[64  0 32  2 75 59 83 44 55 50]\n",
      " 38457/50001: episode: 4273, duration: 0.070s, episode steps:   9, steps per second: 129, episode reward: 40.000, mean reward:  4.444 [ 2.000, 11.000], mean action: 44.444 [0.000, 83.000],  loss: 11.224044, mae: 2.334993, mean_q: 4.432381\n",
      "[35 37 98 53 13 46 20 88 66 88]\n",
      " 38466/50001: episode: 4274, duration: 0.101s, episode steps:   9, steps per second:  89, episode reward: 26.000, mean reward:  2.889 [-10.000,  6.000], mean action: 56.556 [13.000, 98.000],  loss: 6.470534, mae: 2.216732, mean_q: 4.209089\n",
      "[82 31 58 23 33 75 89  5 85 75]\n",
      " 38475/50001: episode: 4275, duration: 0.083s, episode steps:   9, steps per second: 108, episode reward: 22.000, mean reward:  2.444 [-10.000,  5.000], mean action: 52.667 [5.000, 89.000],  loss: 7.710748, mae: 2.340194, mean_q: 4.361328\n",
      "[85 63 79 11 31 12 27 95 82 13]\n",
      " 38484/50001: episode: 4276, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 44.000, mean reward:  4.889 [ 3.000,  8.000], mean action: 45.889 [11.000, 95.000],  loss: 6.362823, mae: 2.411180, mean_q: 4.532373\n",
      "[83 41 99 59 64 60 56 42  5 88]\n",
      " 38493/50001: episode: 4277, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 57.111 [5.000, 99.000],  loss: 8.618541, mae: 2.382807, mean_q: 4.503411\n",
      "[ 3 42 27 96  2 32 46 37 41 16]\n",
      " 38502/50001: episode: 4278, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 37.667 [2.000, 96.000],  loss: 6.911682, mae: 2.477836, mean_q: 4.629047\n",
      "[46 93 60 13 32 34 71 74  5 14]\n",
      " 38511/50001: episode: 4279, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 39.000, mean reward:  4.333 [ 2.000,  6.000], mean action: 44.000 [5.000, 93.000],  loss: 7.957765, mae: 2.461332, mean_q: 4.605917\n",
      "[72 21 57 83 37 62 94  9 28 54]\n",
      " 38520/50001: episode: 4280, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 49.444 [9.000, 94.000],  loss: 7.261594, mae: 2.525248, mean_q: 4.697632\n",
      "[45 93 37 19 31 13  0 36 55 66]\n",
      " 38529/50001: episode: 4281, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 36.000, mean reward:  4.000 [ 2.000,  8.000], mean action: 38.889 [0.000, 93.000],  loss: 7.627041, mae: 2.453570, mean_q: 4.571371\n",
      "[92  4 50 70 33 47 55 50  2 66]\n",
      " 38538/50001: episode: 4282, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 41.889 [2.000, 70.000],  loss: 6.879681, mae: 2.510518, mean_q: 4.683344\n",
      "[20 71 81  9 96 45 88 32  9 12]\n",
      " 38547/50001: episode: 4283, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 27.000, mean reward:  3.000 [-10.000,  9.000], mean action: 49.222 [9.000, 96.000],  loss: 9.397545, mae: 2.570848, mean_q: 4.818113\n",
      "[67 44 81 42 26 71 97  6 82 12]\n",
      " 38556/50001: episode: 4284, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 40.000, mean reward:  4.444 [ 2.000, 10.000], mean action: 51.222 [6.000, 97.000],  loss: 9.766257, mae: 2.489871, mean_q: 4.725453\n",
      "[86 87 34 13 53 66 52  1 44 22]\n",
      " 38565/50001: episode: 4285, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 41.333 [1.000, 87.000],  loss: 6.552949, mae: 2.396557, mean_q: 4.592116\n",
      "[57 32 37 13 46 99 95 87 48 46]\n",
      " 38574/50001: episode: 4286, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 55.889 [13.000, 99.000],  loss: 7.635629, mae: 2.393095, mean_q: 4.517287\n",
      "[75 12 18 95 12 24  0 21 12 31]\n",
      " 38583/50001: episode: 4287, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward:  8.000, mean reward:  0.889 [-10.000,  7.000], mean action: 25.000 [0.000, 95.000],  loss: 7.100733, mae: 2.387015, mean_q: 4.535100\n",
      "[45 43 59 35 29 42 39 13 66 79]\n",
      " 38592/50001: episode: 4288, duration: 0.080s, episode steps:   9, steps per second: 112, episode reward: 41.000, mean reward:  4.556 [ 2.000,  8.000], mean action: 45.000 [13.000, 79.000],  loss: 6.699764, mae: 2.421050, mean_q: 4.527101\n",
      "[72 34 96 18 81 47 63 27  4 81]\n",
      " 38601/50001: episode: 4289, duration: 0.074s, episode steps:   9, steps per second: 122, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 50.111 [4.000, 96.000],  loss: 6.049973, mae: 2.523356, mean_q: 4.758348\n",
      "[ 9 84 37 31 84 56 66 19 67 45]\n",
      " 38610/50001: episode: 4290, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 20.000, mean reward:  2.222 [-10.000,  5.000], mean action: 54.333 [19.000, 84.000],  loss: 6.528367, mae: 2.499576, mean_q: 4.675018\n",
      "[55 32 23  2 19 27 66 73 12 23]\n",
      " 38619/50001: episode: 4291, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 22.000, mean reward:  2.444 [-10.000,  7.000], mean action: 30.778 [2.000, 73.000],  loss: 6.948919, mae: 2.499877, mean_q: 4.701839\n",
      "[56 41 37 10 42 31 36 15 89 51]\n",
      " 38628/50001: episode: 4292, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 39.111 [10.000, 89.000],  loss: 6.693258, mae: 2.477330, mean_q: 4.669182\n",
      "[66 78 98 54 53 82 35 42 95 87]\n",
      " 38637/50001: episode: 4293, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 41.000, mean reward:  4.556 [ 3.000, 10.000], mean action: 69.333 [35.000, 98.000],  loss: 6.978697, mae: 2.488471, mean_q: 4.664355\n",
      "[56 28 95 52 85 21 66 23 88 50]\n",
      " 38646/50001: episode: 4294, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 43.000, mean reward:  4.778 [ 3.000,  8.000], mean action: 56.444 [21.000, 95.000],  loss: 9.359570, mae: 2.598772, mean_q: 4.856470\n",
      "[93  1 13 11 95 84 52 66 66 96]\n",
      " 38655/50001: episode: 4295, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 53.778 [1.000, 96.000],  loss: 8.118164, mae: 2.510340, mean_q: 4.691835\n",
      "[96 50 77 33  4 97 46 78  8 83]\n",
      " 38664/50001: episode: 4296, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 36.000, mean reward:  4.000 [ 3.000,  5.000], mean action: 52.889 [4.000, 97.000],  loss: 7.799204, mae: 2.539295, mean_q: 4.703148\n",
      "[38  9 37 41 36 68 41 50 88 12]\n",
      " 38673/50001: episode: 4297, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 30.000, mean reward:  3.333 [-10.000,  9.000], mean action: 42.444 [9.000, 88.000],  loss: 6.490025, mae: 2.484074, mean_q: 4.667700\n",
      "[68 76 98 70 27 37 42 50 50 53]\n",
      " 38682/50001: episode: 4298, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 55.889 [27.000, 98.000],  loss: 8.384712, mae: 2.509274, mean_q: 4.702411\n",
      "[36 17 48 88  2 40 41 42 40 50]\n",
      " 38691/50001: episode: 4299, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 29.000, mean reward:  3.222 [-10.000,  9.000], mean action: 40.889 [2.000, 88.000],  loss: 7.124731, mae: 2.550146, mean_q: 4.713196\n",
      "[96 61 95 98 79  4 83 62  2 50]\n",
      " 38700/50001: episode: 4300, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 41.000, mean reward:  4.556 [ 2.000,  8.000], mean action: 59.333 [2.000, 98.000],  loss: 5.284737, mae: 2.519338, mean_q: 4.757619\n",
      "[96  2 37 85 88 60 13 46 31 86]\n",
      " 38709/50001: episode: 4301, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 40.000, mean reward:  4.444 [ 3.000,  6.000], mean action: 49.778 [2.000, 88.000],  loss: 6.797685, mae: 2.575223, mean_q: 4.881295\n",
      "[54  4 30  1 78 30 48 79 66 54]\n",
      " 38718/50001: episode: 4302, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 14.000, mean reward:  1.556 [-10.000,  7.000], mean action: 43.333 [1.000, 79.000],  loss: 6.005816, mae: 2.601061, mean_q: 4.881063\n",
      "[28 38 35 95 64 89 83  6 62 50]\n",
      " 38727/50001: episode: 4303, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 41.000, mean reward:  4.556 [ 3.000, 10.000], mean action: 58.000 [6.000, 95.000],  loss: 7.134236, mae: 2.599586, mean_q: 4.858230\n",
      "[85 46 73 95 47 57 95 53 95  5]\n",
      " 38736/50001: episode: 4304, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 10.000, mean reward:  1.111 [-10.000,  6.000], mean action: 62.889 [5.000, 95.000],  loss: 8.489262, mae: 2.630688, mean_q: 4.906700\n",
      "[30 13 21 97 37 86 51 92 50 12]\n",
      " 38745/50001: episode: 4305, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 42.000, mean reward:  4.667 [ 2.000,  8.000], mean action: 51.000 [12.000, 97.000],  loss: 8.195345, mae: 2.594677, mean_q: 4.863939\n",
      "[22 26 87 81 89 95 95  4 88 65]\n",
      " 38754/50001: episode: 4306, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 22.000, mean reward:  2.444 [-10.000,  7.000], mean action: 70.000 [4.000, 95.000],  loss: 7.381771, mae: 2.509979, mean_q: 4.668768\n",
      "[28 95 34 56 13 49 48 65 97 50]\n",
      " 38763/50001: episode: 4307, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 43.000, mean reward:  4.778 [ 2.000,  8.000], mean action: 56.333 [13.000, 97.000],  loss: 7.228567, mae: 2.407635, mean_q: 4.609272\n",
      "[70  5 88 34 82 88 37  7 34 64]\n",
      " 38772/50001: episode: 4308, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 10.000, mean reward:  1.111 [-10.000,  6.000], mean action: 48.778 [5.000, 88.000],  loss: 4.986797, mae: 2.453082, mean_q: 4.658363\n",
      "[92 20 25 63 74 36 98 96 31 66]\n",
      " 38781/50001: episode: 4309, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 41.000, mean reward:  4.556 [ 2.000,  8.000], mean action: 56.556 [20.000, 98.000],  loss: 7.591604, mae: 2.467097, mean_q: 4.630854\n",
      "[57 13 13  8 62 95 60 66  6 66]\n",
      " 38790/50001: episode: 4310, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward:  7.000, mean reward:  0.778 [-10.000,  6.000], mean action: 43.222 [6.000, 95.000],  loss: 7.799250, mae: 2.462954, mean_q: 4.653703\n",
      "[35 64 74 55 98 34 50  4 68 51]\n",
      " 38799/50001: episode: 4311, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 55.333 [4.000, 98.000],  loss: 6.643410, mae: 2.500171, mean_q: 4.672064\n",
      "[ 1 27 49 54 95 47 91 32 54 84]\n",
      " 38808/50001: episode: 4312, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 59.222 [27.000, 95.000],  loss: 8.028305, mae: 2.540481, mean_q: 4.777914\n",
      "[18 34 82 88 40 75 50 57 73  4]\n",
      " 38817/50001: episode: 4313, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 42.000, mean reward:  4.667 [ 2.000,  7.000], mean action: 55.889 [4.000, 88.000],  loss: 6.550241, mae: 2.517494, mean_q: 4.711915\n",
      "[20 83 97 25 98 44 24 34 85 50]\n",
      " 38826/50001: episode: 4314, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 44.000, mean reward:  4.889 [ 3.000,  9.000], mean action: 60.000 [24.000, 98.000],  loss: 9.516953, mae: 2.487011, mean_q: 4.706571\n",
      "[38 14 98 28 84 82 93  2 40  9]\n",
      " 38835/50001: episode: 4315, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 40.000, mean reward:  4.444 [ 3.000,  5.000], mean action: 50.000 [2.000, 98.000],  loss: 8.715911, mae: 2.490672, mean_q: 4.700666\n",
      "[80 98 66 82 30 53 88 40 88 48]\n",
      " 38844/50001: episode: 4316, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 28.000, mean reward:  3.111 [-10.000,  8.000], mean action: 65.889 [30.000, 98.000],  loss: 5.808394, mae: 2.518668, mean_q: 4.746412\n",
      "[98 34 88 52 30 13 87 95 64 24]\n",
      " 38853/50001: episode: 4317, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 54.111 [13.000, 95.000],  loss: 5.055356, mae: 2.431500, mean_q: 4.634297\n",
      "[72 60 77 27 89 21 58  4 31 75]\n",
      " 38862/50001: episode: 4318, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 49.111 [4.000, 89.000],  loss: 7.935844, mae: 2.523769, mean_q: 4.688117\n",
      "[53 12 28 90 32 16 97 34 41 64]\n",
      " 38871/50001: episode: 4319, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 46.000 [12.000, 97.000],  loss: 6.470308, mae: 2.565017, mean_q: 4.840557\n",
      "[35 32 42 86 59 60 60 42 27 41]\n",
      " 38880/50001: episode: 4320, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward:  9.000, mean reward:  1.000 [-10.000,  6.000], mean action: 49.889 [27.000, 86.000],  loss: 6.168688, mae: 2.524019, mean_q: 4.699162\n",
      "[ 9 32 79 66 89 48 74 34 10 28]\n",
      " 38889/50001: episode: 4321, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 51.111 [10.000, 89.000],  loss: 7.178371, mae: 2.499692, mean_q: 4.759905\n",
      "[40 66 11 28 82 75  9 27 67 83]\n",
      " 38898/50001: episode: 4322, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 36.000, mean reward:  4.000 [ 3.000,  6.000], mean action: 49.778 [9.000, 83.000],  loss: 5.037848, mae: 2.554354, mean_q: 4.835790\n",
      "[13 83 63 42 58 34 74 66 32 13]\n",
      " 38907/50001: episode: 4323, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 51.667 [13.000, 83.000],  loss: 7.682745, mae: 2.583749, mean_q: 4.856510\n",
      "[24 63 48 49 89 56 76 32 83 35]\n",
      " 38916/50001: episode: 4324, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 36.000, mean reward:  4.000 [ 3.000,  6.000], mean action: 59.000 [32.000, 89.000],  loss: 7.278446, mae: 2.508850, mean_q: 4.727430\n",
      "[41 49 80 39 76 57  0 35 75 75]\n",
      " 38925/50001: episode: 4325, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 19.000, mean reward:  2.111 [-10.000,  7.000], mean action: 54.000 [0.000, 80.000],  loss: 6.402368, mae: 2.502668, mean_q: 4.674502\n",
      "[37 24 34 69 42 47 42 14 86 50]\n",
      " 38934/50001: episode: 4326, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 28.000, mean reward:  3.111 [-10.000,  9.000], mean action: 45.333 [14.000, 86.000],  loss: 7.444232, mae: 2.567415, mean_q: 4.843241\n",
      "[75  0 37 49 12 66 36 31 92 24]\n",
      " 38943/50001: episode: 4327, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 37.000, mean reward:  4.111 [ 2.000,  6.000], mean action: 38.556 [0.000, 92.000],  loss: 6.573897, mae: 2.516994, mean_q: 4.717650\n",
      "[78 11 64 89 69 80 74 52 10 97]\n",
      " 38952/50001: episode: 4328, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 40.000, mean reward:  4.444 [ 3.000,  8.000], mean action: 60.667 [10.000, 97.000],  loss: 8.209607, mae: 2.479909, mean_q: 4.674010\n",
      "[13 31  8 24 63 52 85 24 73 83]\n",
      " 38961/50001: episode: 4329, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 17.000, mean reward:  1.889 [-10.000,  4.000], mean action: 49.222 [8.000, 85.000],  loss: 8.734147, mae: 2.504148, mean_q: 4.813996\n",
      "[42 33 58 34 27 88 80  8 50 50]\n",
      " 38970/50001: episode: 4330, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 47.556 [8.000, 88.000],  loss: 9.099014, mae: 2.426347, mean_q: 4.585711\n",
      "[52 78 35 41  4 77 34  1 97 69]\n",
      " 38979/50001: episode: 4331, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 48.444 [1.000, 97.000],  loss: 8.523386, mae: 2.396355, mean_q: 4.570278\n",
      "[39 14 65 11  4 12 16 13 79 18]\n",
      " 38988/50001: episode: 4332, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 25.778 [4.000, 79.000],  loss: 6.136069, mae: 2.346359, mean_q: 4.448611\n",
      "[69 44 47 58 13 37 42 92 75 48]\n",
      " 38997/50001: episode: 4333, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 41.000, mean reward:  4.556 [ 2.000,  7.000], mean action: 50.667 [13.000, 92.000],  loss: 6.972520, mae: 2.412565, mean_q: 4.587969\n",
      "[24 95  4 19 90 69 21  1  9 97]\n",
      " 39006/50001: episode: 4334, duration: 0.065s, episode steps:   9, steps per second: 140, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 45.000 [1.000, 97.000],  loss: 10.169409, mae: 2.484000, mean_q: 4.738333\n",
      "[ 8 74 90 34 28 14 74 46 34 90]\n",
      " 39015/50001: episode: 4335, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: -3.000, mean reward: -0.333 [-10.000,  6.000], mean action: 53.778 [14.000, 90.000],  loss: 8.015118, mae: 2.459793, mean_q: 4.691796\n",
      "[59 91 21 95 25 12 41 72 13 48]\n",
      " 39024/50001: episode: 4336, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 44.000, mean reward:  4.889 [ 2.000,  8.000], mean action: 46.444 [12.000, 95.000],  loss: 6.582344, mae: 2.407792, mean_q: 4.630240\n",
      "[97 41 18 48 56 73 37 50 27 69]\n",
      " 39033/50001: episode: 4337, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 46.556 [18.000, 73.000],  loss: 6.399074, mae: 2.486950, mean_q: 4.622366\n",
      "[59 95 37 62 76 93 42 41  1  2]\n",
      " 39042/50001: episode: 4338, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 49.889 [1.000, 95.000],  loss: 8.438374, mae: 2.588616, mean_q: 4.807108\n",
      "[11 95 47 90 34 23 42 47 27  4]\n",
      " 39051/50001: episode: 4339, duration: 0.068s, episode steps:   9, steps per second: 133, episode reward: 25.000, mean reward:  2.778 [-10.000,  5.000], mean action: 45.444 [4.000, 95.000],  loss: 7.585159, mae: 2.488219, mean_q: 4.672635\n",
      "[73  4 98 92 97 35 74 67 24 40]\n",
      " 39060/50001: episode: 4340, duration: 0.075s, episode steps:   9, steps per second: 120, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 59.000 [4.000, 98.000],  loss: 8.057829, mae: 2.496061, mean_q: 4.660200\n",
      "[18 46 75 95 11 37 33 55 66 79]\n",
      " 39069/50001: episode: 4341, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward: 43.000, mean reward:  4.778 [ 3.000,  7.000], mean action: 55.222 [11.000, 95.000],  loss: 6.267558, mae: 2.534348, mean_q: 4.729815\n",
      "[26  6 33 53 88 34 63 12 55 50]\n",
      " 39078/50001: episode: 4342, duration: 0.068s, episode steps:   9, steps per second: 133, episode reward: 46.000, mean reward:  5.111 [ 3.000, 10.000], mean action: 43.778 [6.000, 88.000],  loss: 8.436188, mae: 2.564302, mean_q: 4.795210\n",
      "[ 4 55 60 52 86 34 23 57 34 40]\n",
      " 39087/50001: episode: 4343, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 49.000 [23.000, 86.000],  loss: 6.660505, mae: 2.541635, mean_q: 4.723270\n",
      "[14 34  4 24 17 51 87 48 48 62]\n",
      " 39096/50001: episode: 4344, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 41.667 [4.000, 87.000],  loss: 6.790260, mae: 2.572292, mean_q: 4.756418\n",
      "[72 73 98 95 95 28 23 13 17 12]\n",
      " 39105/50001: episode: 4345, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 27.000, mean reward:  3.000 [-10.000,  8.000], mean action: 50.444 [12.000, 98.000],  loss: 7.722744, mae: 2.567621, mean_q: 4.792295\n",
      "[38 83 31 13 31 18 52 27  4 54]\n",
      " 39114/50001: episode: 4346, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 34.778 [4.000, 83.000],  loss: 5.904913, mae: 2.520700, mean_q: 4.684703\n",
      "[90 49 47 20 42 57 60 67 94 61]\n",
      " 39123/50001: episode: 4347, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 34.000, mean reward:  3.778 [ 3.000,  5.000], mean action: 55.222 [20.000, 94.000],  loss: 7.297079, mae: 2.593411, mean_q: 4.873011\n",
      "[94 57 23 97 74 54 74 92 12  5]\n",
      " 39132/50001: episode: 4348, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 23.000, mean reward:  2.556 [-10.000,  8.000], mean action: 54.222 [5.000, 97.000],  loss: 9.116761, mae: 2.533691, mean_q: 4.762641\n",
      "[67  2 47 37 78 35 67 40 88 21]\n",
      " 39141/50001: episode: 4349, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 46.111 [2.000, 88.000],  loss: 5.958677, mae: 2.522173, mean_q: 4.708394\n",
      "[31 34 32 41 50 47  2 99 12 88]\n",
      " 39150/50001: episode: 4350, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 42.000, mean reward:  4.667 [ 3.000,  6.000], mean action: 45.000 [2.000, 99.000],  loss: 6.705091, mae: 2.593423, mean_q: 4.837106\n",
      "[63 66 60 99 53 66 27 20  5 13]\n",
      " 39159/50001: episode: 4351, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 25.000, mean reward:  2.778 [-10.000,  9.000], mean action: 45.444 [5.000, 99.000],  loss: 7.438130, mae: 2.549159, mean_q: 4.778002\n",
      "[51  1 83 62 45 13 61 95 95 82]\n",
      " 39168/50001: episode: 4352, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 24.000, mean reward:  2.667 [-10.000,  8.000], mean action: 59.667 [1.000, 95.000],  loss: 9.230370, mae: 2.494208, mean_q: 4.726930\n",
      "[67 60 24 95 84 50 95 22 34 68]\n",
      " 39177/50001: episode: 4353, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 59.111 [22.000, 95.000],  loss: 7.418666, mae: 2.503260, mean_q: 4.780147\n",
      "[64  4 42 21 16 91 95 66 48 36]\n",
      " 39186/50001: episode: 4354, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward: 38.000, mean reward:  4.222 [ 1.000,  8.000], mean action: 46.556 [4.000, 95.000],  loss: 7.365526, mae: 2.447203, mean_q: 4.689959\n",
      "[76 93 60 28 58 66 23  4 42 46]\n",
      " 39195/50001: episode: 4355, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 46.667 [4.000, 93.000],  loss: 9.691334, mae: 2.496019, mean_q: 4.673442\n",
      "[34 52 33 48 82 90 74 92 79 17]\n",
      " 39204/50001: episode: 4356, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 34.000, mean reward:  3.778 [ 2.000,  6.000], mean action: 63.000 [17.000, 92.000],  loss: 8.509989, mae: 2.363369, mean_q: 4.470581\n",
      "[48 13  8 93  9  6 56 96 31 40]\n",
      " 39213/50001: episode: 4357, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 35.000, mean reward:  3.889 [ 2.000,  6.000], mean action: 39.111 [6.000, 96.000],  loss: 9.714253, mae: 2.425914, mean_q: 4.547755\n",
      "[ 3 87 45 87 82  9 54 44 84 47]\n",
      " 39222/50001: episode: 4358, duration: 0.068s, episode steps:   9, steps per second: 133, episode reward: 27.000, mean reward:  3.000 [-10.000, 10.000], mean action: 59.889 [9.000, 87.000],  loss: 10.640769, mae: 2.426293, mean_q: 4.571340\n",
      "[39 13 95 82 97 75 92  2 88 12]\n",
      " 39231/50001: episode: 4359, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 44.000, mean reward:  4.889 [ 3.000,  8.000], mean action: 61.778 [2.000, 97.000],  loss: 8.362782, mae: 2.328470, mean_q: 4.375519\n",
      "[47 34 69 30 90 82 14 28 62 83]\n",
      " 39240/50001: episode: 4360, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 38.000, mean reward:  4.222 [ 3.000,  5.000], mean action: 54.667 [14.000, 90.000],  loss: 8.611783, mae: 2.335110, mean_q: 4.450333\n",
      "[57 28 21  2 46 68 51 50 60 96]\n",
      " 39249/50001: episode: 4361, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 38.000, mean reward:  4.222 [ 3.000,  7.000], mean action: 46.889 [2.000, 96.000],  loss: 7.844364, mae: 2.398386, mean_q: 4.455017\n",
      "[83  9 78 42 95 95 88 82 66  2]\n",
      " 39258/50001: episode: 4362, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 29.000, mean reward:  3.222 [-10.000,  7.000], mean action: 61.889 [2.000, 95.000],  loss: 6.297621, mae: 2.412437, mean_q: 4.596146\n",
      "[26 32  6 32  8 95 48 61 41 46]\n",
      " 39267/50001: episode: 4363, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 41.000 [6.000, 95.000],  loss: 6.656568, mae: 2.493160, mean_q: 4.673534\n",
      "[69  3 80  2 88 93 35 50 79 31]\n",
      " 39276/50001: episode: 4364, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 41.000, mean reward:  4.556 [ 2.000,  7.000], mean action: 51.222 [2.000, 93.000],  loss: 8.974658, mae: 2.508423, mean_q: 4.646691\n",
      "[ 0 75 75 41 54 47 48 79  3 31]\n",
      " 39285/50001: episode: 4365, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 27.000, mean reward:  3.000 [-10.000,  8.000], mean action: 50.333 [3.000, 79.000],  loss: 6.918224, mae: 2.507220, mean_q: 4.696836\n",
      "[97  2 56  4 26 96 95 12 40 97]\n",
      " 39294/50001: episode: 4366, duration: 0.070s, episode steps:   9, steps per second: 128, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 47.556 [2.000, 97.000],  loss: 5.535938, mae: 2.501042, mean_q: 4.589583\n",
      "[67 85 77 56 81  2 53 88 27 99]\n",
      " 39303/50001: episode: 4367, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 63.111 [2.000, 99.000],  loss: 6.998299, mae: 2.568878, mean_q: 4.809990\n",
      "[ 3 14 34 50 99 46 59 74 98 12]\n",
      " 39312/50001: episode: 4368, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward: 43.000, mean reward:  4.778 [ 3.000,  8.000], mean action: 54.000 [12.000, 99.000],  loss: 7.674189, mae: 2.584859, mean_q: 4.835031\n",
      "[21 28 79 70 95 37 34 57 82 79]\n",
      " 39321/50001: episode: 4369, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 26.000, mean reward:  2.889 [-10.000,  6.000], mean action: 62.333 [28.000, 95.000],  loss: 5.403784, mae: 2.617656, mean_q: 4.809902\n",
      "[58 34 48  2 52 99  2 94 57 47]\n",
      " 39330/50001: episode: 4370, duration: 0.068s, episode steps:   9, steps per second: 133, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 48.333 [2.000, 99.000],  loss: 7.976618, mae: 2.649330, mean_q: 4.934813\n",
      "[95 95 36 96  2 60 88 56 40 18]\n",
      " 39339/50001: episode: 4371, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 54.556 [2.000, 96.000],  loss: 7.178849, mae: 2.623701, mean_q: 4.958259\n",
      "[34 23  8  2 62 73  1 51 16 16]\n",
      " 39348/50001: episode: 4372, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 18.000, mean reward:  2.000 [-10.000,  6.000], mean action: 28.000 [1.000, 73.000],  loss: 6.786215, mae: 2.601290, mean_q: 4.843095\n",
      "[96  2 16 30 44 60 49 69 66 12]\n",
      " 39357/50001: episode: 4373, duration: 0.072s, episode steps:   9, steps per second: 125, episode reward: 43.000, mean reward:  4.778 [ 3.000,  9.000], mean action: 38.667 [2.000, 69.000],  loss: 9.513344, mae: 2.610934, mean_q: 4.891153\n",
      "[73 95 17 11 31 93 86 30  1 34]\n",
      " 39366/50001: episode: 4374, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 47.000, mean reward:  5.222 [ 3.000, 10.000], mean action: 44.222 [1.000, 95.000],  loss: 5.533631, mae: 2.537608, mean_q: 4.748205\n",
      "[68 82 49 12 97 28 88 32 67 53]\n",
      " 39375/50001: episode: 4375, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 41.000, mean reward:  4.556 [ 3.000,  6.000], mean action: 56.444 [12.000, 97.000],  loss: 9.992218, mae: 2.531356, mean_q: 4.697157\n",
      "[93 34 88 42 89 57 56 66 84 12]\n",
      " 39384/50001: episode: 4376, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 58.667 [12.000, 89.000],  loss: 7.137454, mae: 2.539747, mean_q: 4.735048\n",
      "[77 58 50 21  2 63 69 89 50 59]\n",
      " 39393/50001: episode: 4377, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 22.000, mean reward:  2.444 [-10.000,  5.000], mean action: 51.222 [2.000, 89.000],  loss: 5.896817, mae: 2.535544, mean_q: 4.700671\n",
      "[20 38  4 34 72 66 14 37 40 57]\n",
      " 39402/50001: episode: 4378, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 43.000, mean reward:  4.778 [ 2.000,  7.000], mean action: 40.222 [4.000, 72.000],  loss: 8.063396, mae: 2.562157, mean_q: 4.827432\n",
      "[29 27 53 21 47 67  9 37 86 50]\n",
      " 39411/50001: episode: 4379, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 41.000, mean reward:  4.556 [ 3.000,  9.000], mean action: 44.111 [9.000, 86.000],  loss: 7.846078, mae: 2.511654, mean_q: 4.663112\n",
      "[54 30 46 21 53 60 95 54 40 51]\n",
      " 39420/50001: episode: 4380, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 28.000, mean reward:  3.111 [-10.000,  7.000], mean action: 50.000 [21.000, 95.000],  loss: 7.055103, mae: 2.486245, mean_q: 4.637834\n",
      "[40 86 48 98  2 49 95 60 85 88]\n",
      " 39429/50001: episode: 4381, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 44.000, mean reward:  4.889 [ 3.000,  8.000], mean action: 67.889 [2.000, 98.000],  loss: 6.546462, mae: 2.427896, mean_q: 4.586759\n",
      "[24 91 51 19 51 54 48 78 27 31]\n",
      " 39438/50001: episode: 4382, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 50.000 [19.000, 91.000],  loss: 7.233564, mae: 2.529640, mean_q: 4.742189\n",
      "[ 0 78 69  4 23 46 37 13 10 48]\n",
      " 39447/50001: episode: 4383, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 45.000, mean reward:  5.000 [ 3.000,  7.000], mean action: 36.444 [4.000, 78.000],  loss: 11.020678, mae: 2.502680, mean_q: 4.641058\n",
      "[46 41 93  2 31  1 88 32 32 42]\n",
      " 39456/50001: episode: 4384, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 26.000, mean reward:  2.889 [-10.000,  5.000], mean action: 40.222 [1.000, 93.000],  loss: 6.096700, mae: 2.495573, mean_q: 4.729706\n",
      "[76 37 93 46 60 50 95 12 88 48]\n",
      " 39465/50001: episode: 4385, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 47.000, mean reward:  5.222 [ 3.000,  7.000], mean action: 58.778 [12.000, 95.000],  loss: 6.759499, mae: 2.526790, mean_q: 4.766028\n",
      "[ 6 28  1  2 57 34 11 89 52 64]\n",
      " 39474/50001: episode: 4386, duration: 0.082s, episode steps:   9, steps per second: 109, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 37.556 [1.000, 89.000],  loss: 9.284801, mae: 2.466387, mean_q: 4.623216\n",
      "[43 57 66 50 58 34 20  1 66 46]\n",
      " 39483/50001: episode: 4387, duration: 0.087s, episode steps:   9, steps per second: 103, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 44.222 [1.000, 66.000],  loss: 6.971716, mae: 2.449458, mean_q: 4.590620\n",
      "[35  1 35 31 36 37 44 46 48 76]\n",
      " 39492/50001: episode: 4388, duration: 0.065s, episode steps:   9, steps per second: 137, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 39.333 [1.000, 76.000],  loss: 7.462859, mae: 2.494622, mean_q: 4.641231\n",
      "[30 26 77 97 73 60 64 33 66 12]\n",
      " 39501/50001: episode: 4389, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 41.000, mean reward:  4.556 [ 2.000,  9.000], mean action: 56.444 [12.000, 97.000],  loss: 7.540026, mae: 2.426039, mean_q: 4.547350\n",
      "[70 76 99  1 95 30 16 68 83 70]\n",
      " 39510/50001: episode: 4390, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 59.778 [1.000, 99.000],  loss: 8.519417, mae: 2.487318, mean_q: 4.670561\n",
      "[77 22 95 50 81  1 52 34 90 33]\n",
      " 39519/50001: episode: 4391, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 50.889 [1.000, 95.000],  loss: 7.140612, mae: 2.414300, mean_q: 4.591347\n",
      "[52 74 28 30 97 76 71 64 48 97]\n",
      " 39528/50001: episode: 4392, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 22.000, mean reward:  2.444 [-10.000,  7.000], mean action: 65.000 [28.000, 97.000],  loss: 6.034190, mae: 2.441654, mean_q: 4.630455\n",
      "[93 54  4 46 79 69 34 50 64  3]\n",
      " 39537/50001: episode: 4393, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 36.000, mean reward:  4.000 [ 2.000,  7.000], mean action: 44.778 [3.000, 79.000],  loss: 7.692642, mae: 2.594688, mean_q: 4.875462\n",
      "[92 40 76 81 23 61 13 37 50  1]\n",
      " 39546/50001: episode: 4394, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 42.000, mean reward:  4.667 [ 2.000,  7.000], mean action: 42.444 [1.000, 81.000],  loss: 8.253177, mae: 2.586028, mean_q: 4.812397\n",
      "[44 59 41  2 42 12 21  4 87 75]\n",
      " 39555/50001: episode: 4395, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 38.111 [2.000, 87.000],  loss: 5.022478, mae: 2.588923, mean_q: 4.883131\n",
      "[28 99 41 74 37 65 45 21 97  9]\n",
      " 39564/50001: episode: 4396, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 37.000, mean reward:  4.111 [ 2.000,  8.000], mean action: 54.222 [9.000, 99.000],  loss: 8.847045, mae: 2.546917, mean_q: 4.794592\n",
      "[19 95 36 85 87 95 28 33 68 83]\n",
      " 39573/50001: episode: 4397, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 18.000, mean reward:  2.000 [-10.000,  5.000], mean action: 67.778 [28.000, 95.000],  loss: 8.970055, mae: 2.555561, mean_q: 4.810757\n",
      "[81 89 82  4  0 95 50 48 88 86]\n",
      " 39582/50001: episode: 4398, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 41.000, mean reward:  4.556 [ 2.000,  7.000], mean action: 60.222 [0.000, 95.000],  loss: 9.193760, mae: 2.562352, mean_q: 4.779572\n",
      "[ 5 13  9 92 77 76 37  5 99 34]\n",
      " 39591/50001: episode: 4399, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 26.000, mean reward:  2.889 [-10.000,  9.000], mean action: 49.111 [5.000, 99.000],  loss: 6.841530, mae: 2.479743, mean_q: 4.651468\n",
      "[26 44 88 88 27 50 82 88 12 40]\n",
      " 39600/50001: episode: 4400, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 14.000, mean reward:  1.556 [-10.000,  8.000], mean action: 57.667 [12.000, 88.000],  loss: 9.141968, mae: 2.449846, mean_q: 4.640599\n",
      "[47 39 64 12 97 49  1 89 99 66]\n",
      " 39609/50001: episode: 4401, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 57.333 [1.000, 99.000],  loss: 8.677965, mae: 2.425379, mean_q: 4.582868\n",
      "[73 34 95 91 82 32 33 48 12 66]\n",
      " 39618/50001: episode: 4402, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 45.000, mean reward:  5.000 [ 3.000,  7.000], mean action: 54.778 [12.000, 95.000],  loss: 7.171621, mae: 2.411979, mean_q: 4.508644\n",
      "[99 62 29 83 78 42 99 82 13 42]\n",
      " 39627/50001: episode: 4403, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 10.000, mean reward:  1.111 [-10.000,  8.000], mean action: 58.889 [13.000, 99.000],  loss: 6.503146, mae: 2.436151, mean_q: 4.638506\n",
      "[37 88 30 34 76 97 57 16 27  2]\n",
      " 39636/50001: episode: 4404, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 47.444 [2.000, 97.000],  loss: 7.387358, mae: 2.457924, mean_q: 4.603464\n",
      "[34 12 98 64 66 79 88  2  6 85]\n",
      " 39645/50001: episode: 4405, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 35.000, mean reward:  3.889 [ 3.000,  6.000], mean action: 55.556 [2.000, 98.000],  loss: 7.388698, mae: 2.517720, mean_q: 4.810385\n",
      "[14 24 25 16 85 31 46 69 52 79]\n",
      " 39654/50001: episode: 4406, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 47.444 [16.000, 85.000],  loss: 10.318495, mae: 2.560960, mean_q: 4.841655\n",
      "[64 52 82 28 94 57 82 90 12 27]\n",
      " 39663/50001: episode: 4407, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 58.222 [12.000, 94.000],  loss: 5.881551, mae: 2.498582, mean_q: 4.669991\n",
      "[39 82 76 52 82 44 24 27 53 28]\n",
      " 39672/50001: episode: 4408, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 29.000, mean reward:  3.222 [-10.000,  8.000], mean action: 52.000 [24.000, 82.000],  loss: 8.175646, mae: 2.527801, mean_q: 4.780855\n",
      "[36 24 31 30  8 50 64 30 40 23]\n",
      " 39681/50001: episode: 4409, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 27.000, mean reward:  3.000 [-10.000,  6.000], mean action: 33.333 [8.000, 64.000],  loss: 6.532577, mae: 2.448697, mean_q: 4.596168\n",
      "[ 0  9 37  4 37 32 13 46 76 79]\n",
      " 39690/50001: episode: 4410, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 37.000 [4.000, 79.000],  loss: 6.111347, mae: 2.459798, mean_q: 4.679717\n",
      "[53 42 77 70 44 31 14 89 60 24]\n",
      " 39699/50001: episode: 4411, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 41.000, mean reward:  4.556 [ 2.000,  7.000], mean action: 50.111 [14.000, 89.000],  loss: 9.485010, mae: 2.443550, mean_q: 4.650482\n",
      "[83 34  4 32 31 88 14 74 45 66]\n",
      " 39708/50001: episode: 4412, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 43.111 [4.000, 88.000],  loss: 6.662302, mae: 2.432751, mean_q: 4.581581\n",
      "[56 86  2 25 11 93 35 33 16 87]\n",
      " 39717/50001: episode: 4413, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 35.000, mean reward:  3.889 [ 3.000,  5.000], mean action: 43.111 [2.000, 93.000],  loss: 7.632121, mae: 2.429430, mean_q: 4.600508\n",
      "[64 34 42 52 37 28 12 96 30 31]\n",
      " 39726/50001: episode: 4414, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 40.000, mean reward:  4.444 [ 3.000,  6.000], mean action: 40.222 [12.000, 96.000],  loss: 7.568101, mae: 2.508327, mean_q: 4.732873\n",
      "[37 50 26 86 23 97 49 27 66 50]\n",
      " 39735/50001: episode: 4415, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 52.667 [23.000, 97.000],  loss: 7.676912, mae: 2.546296, mean_q: 4.756798\n",
      "[72 84 98 53 80 27 94 34  4  2]\n",
      " 39744/50001: episode: 4416, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 52.889 [2.000, 98.000],  loss: 9.790561, mae: 2.476860, mean_q: 4.657725\n",
      "[95 23 86 32 77 76 46  9 36 34]\n",
      " 39753/50001: episode: 4417, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 38.000, mean reward:  4.222 [ 2.000, 11.000], mean action: 46.556 [9.000, 86.000],  loss: 4.512597, mae: 2.386934, mean_q: 4.450745\n",
      "[34 98 82 41  2  1 75 84 60 53]\n",
      " 39762/50001: episode: 4418, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 55.111 [1.000, 98.000],  loss: 5.387905, mae: 2.473980, mean_q: 4.628950\n",
      "[42 64 68 76 95 31 28 76 88 31]\n",
      " 39771/50001: episode: 4419, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 12.000, mean reward:  1.333 [-10.000,  7.000], mean action: 61.889 [28.000, 95.000],  loss: 5.601729, mae: 2.538303, mean_q: 4.765677\n",
      "[98 94 11 28  4  4 60 98 10 92]\n",
      " 39780/50001: episode: 4420, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward:  7.000, mean reward:  0.778 [-10.000,  5.000], mean action: 44.556 [4.000, 98.000],  loss: 6.585792, mae: 2.593463, mean_q: 4.863529\n",
      "[61 28 13 71 37 30 74 93 82 32]\n",
      " 39789/50001: episode: 4421, duration: 0.065s, episode steps:   9, steps per second: 137, episode reward: 42.000, mean reward:  4.667 [ 2.000,  7.000], mean action: 51.111 [13.000, 93.000],  loss: 9.191294, mae: 2.586651, mean_q: 4.795087\n",
      "[74  4 54 95 47 82 37 96 31 81]\n",
      " 39798/50001: episode: 4422, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 40.000, mean reward:  4.444 [ 3.000,  6.000], mean action: 58.556 [4.000, 96.000],  loss: 5.813737, mae: 2.491292, mean_q: 4.689814\n",
      "[46 95 33 98 79 83 10 10 10 60]\n",
      " 39807/50001: episode: 4423, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward:  9.000, mean reward:  1.000 [-10.000,  6.000], mean action: 53.111 [10.000, 98.000],  loss: 8.185366, mae: 2.480682, mean_q: 4.737690\n",
      "[22 60 23 28 41 14 82 23 98  7]\n",
      " 39816/50001: episode: 4424, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 20.000, mean reward:  2.222 [-10.000,  6.000], mean action: 41.778 [7.000, 98.000],  loss: 6.961195, mae: 2.536759, mean_q: 4.757796\n",
      "[54 37 43 75 95 60 35 52 69 79]\n",
      " 39825/50001: episode: 4425, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 60.556 [35.000, 95.000],  loss: 9.505377, mae: 2.501101, mean_q: 4.827570\n",
      "[23 13 95 51 88 37 57  4 66 48]\n",
      " 39834/50001: episode: 4426, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 40.000, mean reward:  4.444 [ 3.000,  6.000], mean action: 51.000 [4.000, 95.000],  loss: 6.542387, mae: 2.478452, mean_q: 4.658857\n",
      "[78 42 83  1 34 83 95 78 34 53]\n",
      " 39843/50001: episode: 4427, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: -2.000, mean reward: -0.222 [-10.000,  7.000], mean action: 55.889 [1.000, 95.000],  loss: 6.709365, mae: 2.482995, mean_q: 4.679533\n",
      "[70 67 28 97 28 14 52  1 37 91]\n",
      " 39852/50001: episode: 4428, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 28.000, mean reward:  3.111 [-10.000,  7.000], mean action: 46.111 [1.000, 97.000],  loss: 9.261263, mae: 2.430415, mean_q: 4.624903\n",
      "[93 13 86 38 32 61 95 88 12 66]\n",
      " 39861/50001: episode: 4429, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 43.000, mean reward:  4.778 [ 2.000,  8.000], mean action: 54.556 [12.000, 95.000],  loss: 8.132704, mae: 2.505829, mean_q: 4.794474\n",
      "[66 62 66 38 72 82 11 97 37 57]\n",
      " 39870/50001: episode: 4430, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 27.000, mean reward:  3.000 [-10.000,  8.000], mean action: 58.000 [11.000, 97.000],  loss: 6.658430, mae: 2.479349, mean_q: 4.689873\n",
      "[70 42 13 80 52 68 14 74 89 12]\n",
      " 39879/50001: episode: 4431, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 42.000, mean reward:  4.667 [ 3.000, 10.000], mean action: 49.333 [12.000, 89.000],  loss: 6.161523, mae: 2.506260, mean_q: 4.718814\n",
      "[74 23  5 37 28 52 59 35 98 82]\n",
      " 39888/50001: episode: 4432, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 36.000, mean reward:  4.000 [ 3.000,  6.000], mean action: 46.556 [5.000, 98.000],  loss: 6.289949, mae: 2.492306, mean_q: 4.724730\n",
      "[84 34 27 26 77  8 46 55 40 98]\n",
      " 39897/50001: episode: 4433, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 36.000, mean reward:  4.000 [ 2.000,  7.000], mean action: 45.667 [8.000, 98.000],  loss: 8.548168, mae: 2.562234, mean_q: 4.854800\n",
      "[23 88 90 48 28 14 91 60  5  4]\n",
      " 39906/50001: episode: 4434, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 40.000, mean reward:  4.444 [ 3.000,  6.000], mean action: 47.556 [4.000, 91.000],  loss: 7.560609, mae: 2.563125, mean_q: 4.885009\n",
      "[46 96 20 80 14 17 13 66 82 52]\n",
      " 39915/50001: episode: 4435, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 48.889 [13.000, 96.000],  loss: 6.713572, mae: 2.537769, mean_q: 4.812575\n",
      "[68 57 24 37  6  8 47 69 19 45]\n",
      " 39924/50001: episode: 4436, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 34.000, mean reward:  3.778 [ 3.000,  6.000], mean action: 34.667 [6.000, 69.000],  loss: 6.357053, mae: 2.546422, mean_q: 4.837992\n",
      "[49 68 37 27 27 31 86 57 98 84]\n",
      " 39933/50001: episode: 4437, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 57.222 [27.000, 98.000],  loss: 7.158778, mae: 2.569823, mean_q: 4.950764\n",
      "[71 31 37 59 52 53 41 13  6 99]\n",
      " 39942/50001: episode: 4438, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 43.444 [6.000, 99.000],  loss: 8.949529, mae: 2.532850, mean_q: 4.773621\n",
      "[38 52  8 93 95 27 32 37 89 79]\n",
      " 39951/50001: episode: 4439, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 43.000, mean reward:  4.778 [ 3.000,  7.000], mean action: 56.889 [8.000, 95.000],  loss: 7.782915, mae: 2.535427, mean_q: 4.788624\n",
      "[94 96 47 77 23  1 34 88 66 50]\n",
      " 39960/50001: episode: 4440, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 46.000, mean reward:  5.111 [ 3.000,  8.000], mean action: 53.556 [1.000, 96.000],  loss: 7.036814, mae: 2.515064, mean_q: 4.752586\n",
      "[42 41 83 31 17 34 95 50  2 27]\n",
      " 39969/50001: episode: 4441, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 45.000, mean reward:  5.000 [ 3.000,  7.000], mean action: 42.222 [2.000, 95.000],  loss: 7.168540, mae: 2.447223, mean_q: 4.555704\n",
      "[66 32 92 66 11 31 97 95 48 96]\n",
      " 39978/50001: episode: 4442, duration: 0.070s, episode steps:   9, steps per second: 128, episode reward: 27.000, mean reward:  3.000 [-10.000,  8.000], mean action: 63.111 [11.000, 97.000],  loss: 8.587046, mae: 2.477892, mean_q: 4.718941\n",
      "[79 82 81 37 44 13 65  2 27  0]\n",
      " 39987/50001: episode: 4443, duration: 0.073s, episode steps:   9, steps per second: 124, episode reward: 34.000, mean reward:  3.778 [ 2.000,  6.000], mean action: 39.000 [0.000, 82.000],  loss: 6.995100, mae: 2.477891, mean_q: 4.630036\n",
      "[66 17 95 44 11 95 62  1 19 96]\n",
      " 39996/50001: episode: 4444, duration: 0.069s, episode steps:   9, steps per second: 131, episode reward: 21.000, mean reward:  2.333 [-10.000,  6.000], mean action: 48.889 [1.000, 96.000],  loss: 7.544378, mae: 2.476384, mean_q: 4.705877\n",
      "[30 13 96 74 89 33 82 66 55 56]\n",
      " 40005/50001: episode: 4445, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 37.000, mean reward:  4.111 [ 3.000,  6.000], mean action: 62.667 [13.000, 96.000],  loss: 8.181606, mae: 2.500095, mean_q: 4.764931\n",
      "[51 95 34 66 37 13  1 74 40 31]\n",
      " 40014/50001: episode: 4446, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 39.000, mean reward:  4.333 [ 3.000,  5.000], mean action: 43.444 [1.000, 95.000],  loss: 8.596980, mae: 2.453137, mean_q: 4.679885\n",
      "[16 74 35  2 41 46 66  1 30 24]\n",
      " 40023/50001: episode: 4447, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 45.000, mean reward:  5.000 [ 3.000,  7.000], mean action: 35.444 [1.000, 74.000],  loss: 5.964055, mae: 2.487794, mean_q: 4.746637\n",
      "[84 13 89 27  2  4 83 88 94 74]\n",
      " 40032/50001: episode: 4448, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 37.000, mean reward:  4.111 [ 3.000,  6.000], mean action: 52.667 [2.000, 94.000],  loss: 7.360876, mae: 2.472499, mean_q: 4.660767\n",
      "[90 85 58 35 68 37 83 20 69  4]\n",
      " 40041/50001: episode: 4449, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 37.000, mean reward:  4.111 [ 3.000,  7.000], mean action: 51.000 [4.000, 85.000],  loss: 7.862671, mae: 2.458986, mean_q: 4.737816\n",
      "[23 78 98 75 84  4 98 60 28 88]\n",
      " 40050/50001: episode: 4450, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 28.000, mean reward:  3.111 [-10.000,  8.000], mean action: 68.111 [4.000, 98.000],  loss: 6.038528, mae: 2.529301, mean_q: 4.833910\n",
      "[52 28 28 16 13 40 34 94 42 28]\n",
      " 40059/50001: episode: 4451, duration: 0.073s, episode steps:   9, steps per second: 123, episode reward:  9.000, mean reward:  1.000 [-10.000,  6.000], mean action: 35.889 [13.000, 94.000],  loss: 5.674523, mae: 2.522663, mean_q: 4.798824\n",
      "[76 30 13  2  2 31 90 35 64 71]\n",
      " 40068/50001: episode: 4452, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 20.000, mean reward:  2.222 [-10.000,  5.000], mean action: 37.556 [2.000, 90.000],  loss: 6.923569, mae: 2.496385, mean_q: 4.723886\n",
      "[ 4 14 65 67 42 24 92 95 12  2]\n",
      " 40077/50001: episode: 4453, duration: 0.073s, episode steps:   9, steps per second: 123, episode reward: 42.000, mean reward:  4.667 [ 2.000,  8.000], mean action: 45.889 [2.000, 95.000],  loss: 5.391612, mae: 2.523710, mean_q: 4.772232\n",
      "[ 7 34  8 88  2 79 95 54 74 85]\n",
      " 40086/50001: episode: 4454, duration: 0.068s, episode steps:   9, steps per second: 133, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 57.667 [2.000, 95.000],  loss: 7.441336, mae: 2.548266, mean_q: 4.880634\n",
      "[40 98 31 76 23 48 74 14 93 47]\n",
      " 40095/50001: episode: 4455, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 56.000 [14.000, 98.000],  loss: 7.122045, mae: 2.528466, mean_q: 4.814711\n",
      "[45 86 79 98 28 10 99 12 80 55]\n",
      " 40104/50001: episode: 4456, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 36.000, mean reward:  4.000 [ 3.000,  6.000], mean action: 60.778 [10.000, 99.000],  loss: 8.043050, mae: 2.585315, mean_q: 4.980907\n",
      "[25 83 17 59 22 14 48 90 82 77]\n",
      " 40113/50001: episode: 4457, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 38.000, mean reward:  4.222 [ 3.000,  5.000], mean action: 54.667 [14.000, 90.000],  loss: 6.047028, mae: 2.570204, mean_q: 4.880059\n",
      "[15 17 40 46 85 30 10  6 74  6]\n",
      " 40122/50001: episode: 4458, duration: 0.069s, episode steps:   9, steps per second: 130, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 34.889 [6.000, 85.000],  loss: 10.797995, mae: 2.566993, mean_q: 4.841599\n",
      "[95  2 17 32 37 30 96 10 88 63]\n",
      " 40131/50001: episode: 4459, duration: 0.068s, episode steps:   9, steps per second: 133, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 41.667 [2.000, 96.000],  loss: 7.874422, mae: 2.480897, mean_q: 4.704416\n",
      "[14 43 13 97  6  6  2 48 13 48]\n",
      " 40140/50001: episode: 4460, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: -5.000, mean reward: -0.556 [-10.000,  6.000], mean action: 30.667 [2.000, 97.000],  loss: 6.200655, mae: 2.312125, mean_q: 4.455682\n",
      "[83 75 41 23  1 89 50 92 79 27]\n",
      " 40149/50001: episode: 4461, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 42.000, mean reward:  4.667 [ 3.000,  6.000], mean action: 53.000 [1.000, 92.000],  loss: 7.605328, mae: 2.469003, mean_q: 4.591264\n",
      "[44 50 33 14 82 69 20 66 46 33]\n",
      " 40158/50001: episode: 4462, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 45.889 [14.000, 82.000],  loss: 5.781296, mae: 2.553645, mean_q: 4.882094\n",
      "[72 71 27  1  2 28 10 11 63 51]\n",
      " 40167/50001: episode: 4463, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 40.000, mean reward:  4.444 [ 2.000,  6.000], mean action: 29.333 [1.000, 71.000],  loss: 7.785955, mae: 2.549917, mean_q: 4.828413\n",
      "[16 53 94 95 23 20 47 47 46  1]\n",
      " 40176/50001: episode: 4464, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 47.333 [1.000, 95.000],  loss: 8.941106, mae: 2.522118, mean_q: 4.754492\n",
      "[28 13 87  1 98 46 33 49 26 46]\n",
      " 40185/50001: episode: 4465, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 17.000, mean reward:  1.889 [-10.000,  4.000], mean action: 44.333 [1.000, 98.000],  loss: 9.648163, mae: 2.478890, mean_q: 4.735001\n",
      "[59 82 82  4 79 63 50 73 94 64]\n",
      " 40194/50001: episode: 4466, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 65.667 [4.000, 94.000],  loss: 6.982470, mae: 2.433997, mean_q: 4.624051\n",
      "[72 97 98 98 14 28 64  1 48 79]\n",
      " 40203/50001: episode: 4467, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 28.000, mean reward:  3.111 [-10.000,  6.000], mean action: 58.556 [1.000, 98.000],  loss: 7.006164, mae: 2.448368, mean_q: 4.593687\n",
      "[88 68 28 12 27 52 28 72 55 79]\n",
      " 40212/50001: episode: 4468, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 20.000, mean reward:  2.222 [-10.000,  7.000], mean action: 46.778 [12.000, 79.000],  loss: 7.805027, mae: 2.408289, mean_q: 4.577383\n",
      "[ 6 30 50 97 58 13 68 21 30 97]\n",
      " 40221/50001: episode: 4469, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward:  8.000, mean reward:  0.889 [-10.000,  6.000], mean action: 51.556 [13.000, 97.000],  loss: 8.399891, mae: 2.481660, mean_q: 4.693545\n",
      "[60 13 69 12 56 75 95  2 12 53]\n",
      " 40230/50001: episode: 4470, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 43.000 [2.000, 95.000],  loss: 5.165926, mae: 2.458414, mean_q: 4.678713\n",
      "[ 9 23 41 23 59 95 46 92 79 57]\n",
      " 40239/50001: episode: 4471, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 28.000, mean reward:  3.111 [-10.000,  7.000], mean action: 57.222 [23.000, 95.000],  loss: 2.692435, mae: 2.505296, mean_q: 4.833018\n",
      "[48 95 20 21 37 13  2  1 46 63]\n",
      " 40248/50001: episode: 4472, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 38.000, mean reward:  4.222 [ 2.000,  5.000], mean action: 33.111 [1.000, 95.000],  loss: 6.578482, mae: 2.612241, mean_q: 4.971920\n",
      "[22 34 68 79  8 59 49 48 92 12]\n",
      " 40257/50001: episode: 4473, duration: 0.071s, episode steps:   9, steps per second: 127, episode reward: 39.000, mean reward:  4.333 [ 3.000,  8.000], mean action: 49.889 [8.000, 92.000],  loss: 5.669280, mae: 2.635971, mean_q: 4.937697\n",
      "[95 97 71 69 69 62 88 66 32 13]\n",
      " 40266/50001: episode: 4474, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 29.000, mean reward:  3.222 [-10.000,  8.000], mean action: 63.000 [13.000, 97.000],  loss: 8.032000, mae: 2.644580, mean_q: 5.024994\n",
      "[59 24 31 49 37 88 50  3 93 50]\n",
      " 40275/50001: episode: 4475, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 47.222 [3.000, 93.000],  loss: 7.495303, mae: 2.666007, mean_q: 5.063788\n",
      "[ 8 45 40 51 66 34  9  1 73 47]\n",
      " 40284/50001: episode: 4476, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 41.000, mean reward:  4.556 [ 2.000,  8.000], mean action: 40.667 [1.000, 73.000],  loss: 6.061505, mae: 2.592928, mean_q: 4.939157\n",
      "[77 40 41 40 74 46 78 88 34 34]\n",
      " 40293/50001: episode: 4477, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 15.000, mean reward:  1.667 [-10.000,  9.000], mean action: 52.778 [34.000, 88.000],  loss: 7.293825, mae: 2.613471, mean_q: 5.027219\n",
      "[77 57 30 82 30 53 50 24 64  4]\n",
      " 40302/50001: episode: 4478, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 28.000, mean reward:  3.111 [-10.000,  7.000], mean action: 43.778 [4.000, 82.000],  loss: 7.460505, mae: 2.604922, mean_q: 4.969461\n",
      "[77 69 28 35 32 66  4 75 95 63]\n",
      " 40311/50001: episode: 4479, duration: 0.068s, episode steps:   9, steps per second: 133, episode reward: 43.000, mean reward:  4.778 [ 3.000,  9.000], mean action: 51.889 [4.000, 95.000],  loss: 4.738297, mae: 2.580794, mean_q: 4.909666\n",
      "[50 71 58 34 54 68 41  6 94 38]\n",
      " 40320/50001: episode: 4480, duration: 0.070s, episode steps:   9, steps per second: 129, episode reward: 32.000, mean reward:  3.556 [ 2.000,  5.000], mean action: 51.556 [6.000, 94.000],  loss: 5.238760, mae: 2.644979, mean_q: 5.042973\n",
      "[17 34  4 64 50 97  3 21 76 33]\n",
      " 40329/50001: episode: 4481, duration: 0.068s, episode steps:   9, steps per second: 133, episode reward: 35.000, mean reward:  3.889 [ 2.000,  5.000], mean action: 42.444 [3.000, 97.000],  loss: 7.370934, mae: 2.689908, mean_q: 5.068739\n",
      "[ 4 13 76 44 16 47 87 83 21 48]\n",
      " 40338/50001: episode: 4482, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 35.000, mean reward:  3.889 [ 2.000,  9.000], mean action: 48.333 [13.000, 87.000],  loss: 8.208920, mae: 2.713238, mean_q: 5.152285\n",
      "[70 75 10 65 27 80 42 96  9 48]\n",
      " 40347/50001: episode: 4483, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 39.000, mean reward:  4.333 [ 3.000,  8.000], mean action: 50.222 [9.000, 96.000],  loss: 8.137745, mae: 2.610023, mean_q: 5.017477\n",
      "[34 67 98 53 74 95  1 63 89 34]\n",
      " 40356/50001: episode: 4484, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 63.778 [1.000, 98.000],  loss: 8.771442, mae: 2.555406, mean_q: 4.891111\n",
      "[ 2 82 65 14 37 60 48 47 37 92]\n",
      " 40365/50001: episode: 4485, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 22.000, mean reward:  2.444 [-10.000,  5.000], mean action: 53.556 [14.000, 92.000],  loss: 7.973401, mae: 2.471888, mean_q: 4.664672\n",
      "[89  4 62 37 82 24 88 66 34 66]\n",
      " 40374/50001: episode: 4486, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 30.000, mean reward:  3.333 [-10.000,  8.000], mean action: 51.444 [4.000, 88.000],  loss: 7.397439, mae: 2.412468, mean_q: 4.543413\n",
      "[71  8 13 82 24 17 86 50 49 37]\n",
      " 40383/50001: episode: 4487, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 45.000, mean reward:  5.000 [ 3.000, 10.000], mean action: 40.667 [8.000, 86.000],  loss: 7.890396, mae: 2.475538, mean_q: 4.690825\n",
      "[95 98 62 62 53 82 59  1 95 50]\n",
      " 40392/50001: episode: 4488, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 13.000, mean reward:  1.444 [-10.000,  8.000], mean action: 62.444 [1.000, 98.000],  loss: 6.860342, mae: 2.430714, mean_q: 4.597846\n",
      "[26 82 90 37  4 95 14 50 74 46]\n",
      " 40401/50001: episode: 4489, duration: 0.065s, episode steps:   9, steps per second: 140, episode reward: 43.000, mean reward:  4.778 [ 4.000,  6.000], mean action: 54.667 [4.000, 95.000],  loss: 6.331379, mae: 2.551482, mean_q: 4.826709\n",
      "[59 43 86 28 87  5 95 54 18 31]\n",
      " 40410/50001: episode: 4490, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 49.667 [5.000, 95.000],  loss: 6.883115, mae: 2.544490, mean_q: 4.782235\n",
      "[67  8 68 12 30 97 24 44 88 12]\n",
      " 40419/50001: episode: 4491, duration: 0.069s, episode steps:   9, steps per second: 130, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 42.556 [8.000, 97.000],  loss: 6.714138, mae: 2.580571, mean_q: 4.862631\n",
      "[76  4 30 52 34  4 88 24 37 37]\n",
      " 40428/50001: episode: 4492, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 14.000, mean reward:  1.556 [-10.000,  7.000], mean action: 34.444 [4.000, 88.000],  loss: 6.092954, mae: 2.530695, mean_q: 4.810318\n",
      "[89  9 31 84 88 82  1 95 12 88]\n",
      " 40437/50001: episode: 4493, duration: 0.065s, episode steps:   9, steps per second: 140, episode reward: 30.000, mean reward:  3.333 [-10.000,  8.000], mean action: 54.444 [1.000, 95.000],  loss: 5.845438, mae: 2.581342, mean_q: 4.873096\n",
      "[27 50 44 36 56 28 53  1 26 53]\n",
      " 40446/50001: episode: 4494, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 19.000, mean reward:  2.111 [-10.000,  6.000], mean action: 38.556 [1.000, 56.000],  loss: 8.387774, mae: 2.619716, mean_q: 5.004112\n",
      "[10  8 24 41 38  4 50 75 89 93]\n",
      " 40455/50001: episode: 4495, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 46.889 [4.000, 93.000],  loss: 6.845843, mae: 2.537406, mean_q: 4.825789\n",
      "[67 28 28 92 76 69 20 41 75 46]\n",
      " 40464/50001: episode: 4496, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 52.778 [20.000, 92.000],  loss: 8.336467, mae: 2.455980, mean_q: 4.640757\n",
      "[19 50 47 56 21 50 47 34 79 79]\n",
      " 40473/50001: episode: 4497, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: -4.000, mean reward: -0.444 [-10.000,  7.000], mean action: 51.444 [21.000, 79.000],  loss: 6.251102, mae: 2.513911, mean_q: 4.737669\n",
      "[40 97 53 34  9 82 69 67 95 10]\n",
      " 40482/50001: episode: 4498, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 39.000, mean reward:  4.333 [ 3.000,  9.000], mean action: 57.333 [9.000, 97.000],  loss: 5.033049, mae: 2.554121, mean_q: 4.791616\n",
      "[43 74 82 98 12  8 24 79 74 97]\n",
      " 40491/50001: episode: 4499, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 28.000, mean reward:  3.111 [-10.000,  7.000], mean action: 60.889 [8.000, 98.000],  loss: 8.073188, mae: 2.600503, mean_q: 4.884458\n",
      "[31 50  2 95 73 12  2 58 67 32]\n",
      " 40500/50001: episode: 4500, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 43.444 [2.000, 95.000],  loss: 7.425186, mae: 2.538411, mean_q: 4.760050\n",
      "[21 12 46 23 83 24 14 67 79 31]\n",
      " 40509/50001: episode: 4501, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 42.111 [12.000, 83.000],  loss: 7.022316, mae: 2.580143, mean_q: 4.875587\n",
      "[63  5  9 98 76 15 37 63 30 54]\n",
      " 40518/50001: episode: 4502, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 22.000, mean reward:  2.444 [-10.000,  7.000], mean action: 43.000 [5.000, 98.000],  loss: 6.812074, mae: 2.590837, mean_q: 4.855477\n",
      "[16 82 62 95  1 28 20 48 28 27]\n",
      " 40527/50001: episode: 4503, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 27.000, mean reward:  3.000 [-10.000,  6.000], mean action: 43.444 [1.000, 95.000],  loss: 7.929670, mae: 2.558604, mean_q: 4.858931\n",
      "[84  8 50  2 95 30  1 95 79 56]\n",
      " 40536/50001: episode: 4504, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 27.000, mean reward:  3.000 [-10.000,  6.000], mean action: 46.222 [1.000, 95.000],  loss: 5.808168, mae: 2.546042, mean_q: 4.787985\n",
      "[95  4 83 12 95 17 44  6 44 44]\n",
      " 40545/50001: episode: 4505, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: -11.000, mean reward: -1.222 [-10.000,  4.000], mean action: 38.778 [4.000, 95.000],  loss: 5.952040, mae: 2.560464, mean_q: 4.863634\n",
      "[15 13 61 89 32 99 50 82 14 27]\n",
      " 40554/50001: episode: 4506, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 42.000, mean reward:  4.667 [ 2.000,  6.000], mean action: 51.889 [13.000, 99.000],  loss: 5.981497, mae: 2.621631, mean_q: 4.935381\n",
      "[18 23 62 61 79 34 13 52 99 79]\n",
      " 40563/50001: episode: 4507, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 55.778 [13.000, 99.000],  loss: 4.395986, mae: 2.602903, mean_q: 4.899469\n",
      "[75 88 95 95 95 82 88  1 99 23]\n",
      " 40572/50001: episode: 4508, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: -3.000, mean reward: -0.333 [-10.000,  5.000], mean action: 74.000 [1.000, 99.000],  loss: 9.047456, mae: 2.679987, mean_q: 5.007802\n",
      "[55  0 75 34 66 37 33  4 71 54]\n",
      " 40581/50001: episode: 4509, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 36.000, mean reward:  4.000 [ 2.000,  6.000], mean action: 41.556 [0.000, 75.000],  loss: 6.537106, mae: 2.675938, mean_q: 4.987361\n",
      "[ 1 33 52 81 37 34 95 86 23 88]\n",
      " 40590/50001: episode: 4510, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 42.000, mean reward:  4.667 [ 2.000,  7.000], mean action: 58.778 [23.000, 95.000],  loss: 6.255499, mae: 2.593086, mean_q: 4.925497\n",
      "[ 2 96 35 33 83 89  8 46 50 44]\n",
      " 40599/50001: episode: 4511, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 39.000, mean reward:  4.333 [ 3.000,  8.000], mean action: 53.778 [8.000, 96.000],  loss: 7.500753, mae: 2.581652, mean_q: 4.868110\n",
      "[56 98 91 41 34  1 33 59 60 27]\n",
      " 40608/50001: episode: 4512, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 43.000, mean reward:  4.778 [ 3.000,  7.000], mean action: 49.333 [1.000, 98.000],  loss: 8.028528, mae: 2.518557, mean_q: 4.740332\n",
      "[44 34 13 62 20 69 57 19 83 46]\n",
      " 40617/50001: episode: 4513, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 44.778 [13.000, 83.000],  loss: 7.056747, mae: 2.501527, mean_q: 4.722407\n",
      "[36 28 55 81 47 73  1 31 89 79]\n",
      " 40626/50001: episode: 4514, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 41.000, mean reward:  4.556 [ 2.000,  7.000], mean action: 53.778 [1.000, 89.000],  loss: 6.037397, mae: 2.537073, mean_q: 4.754909\n",
      "[51 84  9 87 37  3 34 31 41 12]\n",
      " 40635/50001: episode: 4515, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 43.000, mean reward:  4.778 [ 2.000,  9.000], mean action: 37.556 [3.000, 87.000],  loss: 7.045408, mae: 2.622602, mean_q: 4.874241\n",
      "[39 71 87  7 57 64 24 23 34 71]\n",
      " 40644/50001: episode: 4516, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 26.000, mean reward:  2.889 [-10.000,  9.000], mean action: 48.667 [7.000, 87.000],  loss: 8.139011, mae: 2.628074, mean_q: 4.913299\n",
      "[28 93 73  1 21 94 48 50 95 50]\n",
      " 40653/50001: episode: 4517, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 27.000, mean reward:  3.000 [-10.000,  9.000], mean action: 58.333 [1.000, 95.000],  loss: 6.064744, mae: 2.622759, mean_q: 4.950532\n",
      "[12 28 98 82 88 88 50 89 23 87]\n",
      " 40662/50001: episode: 4518, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 70.333 [23.000, 98.000],  loss: 6.911695, mae: 2.665632, mean_q: 4.977292\n",
      "[59 41  9  2 67 95 66 46 74 52]\n",
      " 40671/50001: episode: 4519, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 50.222 [2.000, 95.000],  loss: 7.871688, mae: 2.559180, mean_q: 4.806346\n",
      "[88  9 50 98 50 30 50 67 64  9]\n",
      " 40680/50001: episode: 4520, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: -9.000, mean reward: -1.000 [-10.000,  4.000], mean action: 47.444 [9.000, 98.000],  loss: 8.253326, mae: 2.548531, mean_q: 4.823942\n",
      "[ 0 34 56 93 28 32 44 97 30  4]\n",
      " 40689/50001: episode: 4521, duration: 0.065s, episode steps:   9, steps per second: 137, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 46.444 [4.000, 97.000],  loss: 6.534353, mae: 2.465163, mean_q: 4.676268\n",
      "[47 14 74  9 50 18 32 14 70 21]\n",
      " 40698/50001: episode: 4522, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 19.000, mean reward:  2.111 [-10.000,  5.000], mean action: 33.556 [9.000, 74.000],  loss: 5.879277, mae: 2.482218, mean_q: 4.710083\n",
      "[53 37 71  9 74 57 83 79 32 69]\n",
      " 40707/50001: episode: 4523, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 36.000, mean reward:  4.000 [ 2.000,  6.000], mean action: 56.778 [9.000, 83.000],  loss: 8.229884, mae: 2.464064, mean_q: 4.608673\n",
      "[85  6 85 84 64 97 73 27 12 88]\n",
      " 40716/50001: episode: 4524, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 30.000, mean reward:  3.333 [-10.000,  9.000], mean action: 59.556 [6.000, 97.000],  loss: 8.178814, mae: 2.445388, mean_q: 4.583157\n",
      "[63 46 13 29 68 14 60 24  0 62]\n",
      " 40725/50001: episode: 4525, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 35.000, mean reward:  3.889 [ 2.000,  5.000], mean action: 35.111 [0.000, 68.000],  loss: 9.785488, mae: 2.366405, mean_q: 4.500235\n",
      "[10  1 85 53 73 21 51 16 68 79]\n",
      " 40734/50001: episode: 4526, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 35.000, mean reward:  3.889 [ 2.000,  7.000], mean action: 49.667 [1.000, 85.000],  loss: 6.415365, mae: 2.381180, mean_q: 4.523376\n",
      "[88 92 92 93 34 58 12 32 69 46]\n",
      " 40743/50001: episode: 4527, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 58.667 [12.000, 93.000],  loss: 7.349588, mae: 2.406201, mean_q: 4.540144\n",
      "[50  8 98 65 74 78 37 27 31 55]\n",
      " 40752/50001: episode: 4528, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 52.556 [8.000, 98.000],  loss: 6.497234, mae: 2.490629, mean_q: 4.651914\n",
      "[22 91 30 31 81  6 90 43 98 50]\n",
      " 40761/50001: episode: 4529, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 42.000, mean reward:  4.667 [ 3.000,  9.000], mean action: 57.778 [6.000, 98.000],  loss: 5.434812, mae: 2.599910, mean_q: 4.945455\n",
      "[55 34 93 76  2 27 47 52  6 89]\n",
      " 40770/50001: episode: 4530, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 47.333 [2.000, 93.000],  loss: 10.341590, mae: 2.586675, mean_q: 4.874755\n",
      "[94 41 60 18 11 63 34 13 34 79]\n",
      " 40779/50001: episode: 4531, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 28.000, mean reward:  3.111 [-10.000,  7.000], mean action: 39.222 [11.000, 79.000],  loss: 8.998469, mae: 2.522329, mean_q: 4.754005\n",
      "[13 30 56 82 28 47 65 17 34 28]\n",
      " 40788/50001: episode: 4532, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 24.000, mean reward:  2.667 [-10.000,  9.000], mean action: 43.000 [17.000, 82.000],  loss: 8.752386, mae: 2.423048, mean_q: 4.613000\n",
      "[93 23 74 77 48 79 50 92 66 48]\n",
      " 40797/50001: episode: 4533, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 61.889 [23.000, 92.000],  loss: 7.882156, mae: 2.402397, mean_q: 4.592016\n",
      "[65 88 28 66  6 93 13 79 33 83]\n",
      " 40806/50001: episode: 4534, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 54.333 [6.000, 93.000],  loss: 5.756289, mae: 2.409183, mean_q: 4.572294\n",
      "[61 56 77 28 97  4 45 33 89 46]\n",
      " 40815/50001: episode: 4535, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 52.778 [4.000, 97.000],  loss: 8.838047, mae: 2.504640, mean_q: 4.711901\n",
      "[ 3 48 79 95 34 36 62 47 99 16]\n",
      " 40824/50001: episode: 4536, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 39.000, mean reward:  4.333 [ 2.000,  6.000], mean action: 57.333 [16.000, 99.000],  loss: 10.171467, mae: 2.418291, mean_q: 4.716794\n",
      "[60 70 37 71 90 77 52 83  2 48]\n",
      " 40833/50001: episode: 4537, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 38.000, mean reward:  4.222 [ 2.000,  8.000], mean action: 58.889 [2.000, 90.000],  loss: 8.299855, mae: 2.389337, mean_q: 4.603663\n",
      "[ 0 53 36 76  8 25 79 63 34 64]\n",
      " 40842/50001: episode: 4538, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 39.000, mean reward:  4.333 [ 2.000,  9.000], mean action: 48.667 [8.000, 79.000],  loss: 4.820722, mae: 2.367557, mean_q: 4.602968\n",
      "[23 31  1 19 44 48 95 83 24 30]\n",
      " 40851/50001: episode: 4539, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 41.667 [1.000, 95.000],  loss: 5.363587, mae: 2.446769, mean_q: 4.636431\n",
      "[12  5 25 62 28 31 20 64 27 32]\n",
      " 40860/50001: episode: 4540, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 32.667 [5.000, 64.000],  loss: 8.076270, mae: 2.546693, mean_q: 4.782740\n",
      "[67 98 85 11 79 74 59 52 12 12]\n",
      " 40869/50001: episode: 4541, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 26.000, mean reward:  2.889 [-10.000,  8.000], mean action: 53.556 [11.000, 98.000],  loss: 8.164467, mae: 2.606219, mean_q: 4.978182\n",
      "[38 27 87 24 77 41 86 96  6 50]\n",
      " 40878/50001: episode: 4542, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 38.000, mean reward:  4.222 [ 3.000, 10.000], mean action: 54.889 [6.000, 96.000],  loss: 5.166749, mae: 2.599246, mean_q: 4.882410\n",
      "[60 75 91 16 44  9 62 88 63 14]\n",
      " 40887/50001: episode: 4543, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 51.333 [9.000, 91.000],  loss: 5.470595, mae: 2.580250, mean_q: 4.839099\n",
      "[14 35 37 22  4 27 74 52 87 37]\n",
      " 40896/50001: episode: 4544, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 20.000, mean reward:  2.222 [-10.000,  5.000], mean action: 41.667 [4.000, 87.000],  loss: 6.136303, mae: 2.532654, mean_q: 4.760599\n",
      "[24 34 30 16 31 28 30 38 58 50]\n",
      " 40905/50001: episode: 4545, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 26.000, mean reward:  2.889 [-10.000,  9.000], mean action: 35.000 [16.000, 58.000],  loss: 7.664458, mae: 2.574238, mean_q: 4.898948\n",
      "[87 87 50 66 64 49 31  1  4 53]\n",
      " 40914/50001: episode: 4546, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 27.000, mean reward:  3.000 [-10.000,  6.000], mean action: 45.000 [1.000, 87.000],  loss: 7.208203, mae: 2.585899, mean_q: 4.791108\n",
      "[82 41 63 69 83 71 98 27 87 79]\n",
      " 40923/50001: episode: 4547, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 39.000, mean reward:  4.333 [ 2.000,  8.000], mean action: 68.667 [27.000, 98.000],  loss: 6.981171, mae: 2.651772, mean_q: 4.982649\n",
      "[62 34 82 54 30 47 42 48 84  4]\n",
      " 40932/50001: episode: 4548, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 42.000, mean reward:  4.667 [ 3.000,  6.000], mean action: 47.222 [4.000, 84.000],  loss: 8.528275, mae: 2.594803, mean_q: 4.810552\n",
      "[45  8 59 52 32 95  1 89 88 12]\n",
      " 40941/50001: episode: 4549, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 48.000, mean reward:  5.333 [ 3.000,  9.000], mean action: 48.444 [1.000, 95.000],  loss: 8.306680, mae: 2.495376, mean_q: 4.741308\n",
      "[79 24 27  4 23 53 34  8 20 95]\n",
      " 40950/50001: episode: 4550, duration: 0.065s, episode steps:   9, steps per second: 137, episode reward: 40.000, mean reward:  4.444 [ 3.000, 10.000], mean action: 32.000 [4.000, 95.000],  loss: 5.406057, mae: 2.474461, mean_q: 4.682528\n",
      "[33 52 94 75 98 84 42 33  1 88]\n",
      " 40959/50001: episode: 4551, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 29.000, mean reward:  3.222 [-10.000,  8.000], mean action: 63.000 [1.000, 98.000],  loss: 8.093058, mae: 2.456282, mean_q: 4.636532\n",
      "[ 2 41 13 73 36  4 89 84 24 32]\n",
      " 40968/50001: episode: 4552, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 44.000 [4.000, 89.000],  loss: 8.496622, mae: 2.486583, mean_q: 4.736856\n",
      "[56 13 34 75  3 75 95 34 69 13]\n",
      " 40977/50001: episode: 4553, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: -6.000, mean reward: -0.667 [-10.000,  7.000], mean action: 45.667 [3.000, 95.000],  loss: 6.850045, mae: 2.448489, mean_q: 4.673865\n",
      "[79 63 28 63 31  8 59 23 30 49]\n",
      " 40986/50001: episode: 4554, duration: 0.065s, episode steps:   9, steps per second: 140, episode reward: 21.000, mean reward:  2.333 [-10.000,  5.000], mean action: 39.333 [8.000, 63.000],  loss: 6.845140, mae: 2.448424, mean_q: 4.689251\n",
      "[ 3 62 94 46 24 14 64 48 60 44]\n",
      " 40995/50001: episode: 4555, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 50.667 [14.000, 94.000],  loss: 5.143037, mae: 2.501485, mean_q: 4.737612\n",
      "[28 51 17 30 69 20  2 35 47 75]\n",
      " 41004/50001: episode: 4556, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 38.000, mean reward:  4.222 [ 3.000,  7.000], mean action: 38.444 [2.000, 75.000],  loss: 9.509726, mae: 2.618306, mean_q: 4.903097\n",
      "[71 66 37 17 79  4 66 56 93 67]\n",
      " 41013/50001: episode: 4557, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 53.889 [4.000, 93.000],  loss: 6.037181, mae: 2.564335, mean_q: 4.843574\n",
      "[53  8 77 27 62 37 41 72 62 79]\n",
      " 41022/50001: episode: 4558, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 24.000, mean reward:  2.667 [-10.000,  8.000], mean action: 51.667 [8.000, 79.000],  loss: 7.126326, mae: 2.609220, mean_q: 4.853454\n",
      "[56 78 81 48 86  1 89 42 82  6]\n",
      " 41031/50001: episode: 4559, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 57.000 [1.000, 89.000],  loss: 10.159648, mae: 2.609466, mean_q: 4.877002\n",
      "[ 7 28 72 16 14 95 59 42  6 93]\n",
      " 41040/50001: episode: 4560, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 47.222 [6.000, 95.000],  loss: 5.797552, mae: 2.542855, mean_q: 4.827159\n",
      "[13 48 11 38 14 31 55 54 34 12]\n",
      " 41049/50001: episode: 4561, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 39.000, mean reward:  4.333 [ 2.000,  9.000], mean action: 33.000 [11.000, 55.000],  loss: 8.784508, mae: 2.526717, mean_q: 4.756533\n",
      "[58 15 66 37 89 25 49 14 14  1]\n",
      " 41058/50001: episode: 4562, duration: 0.065s, episode steps:   9, steps per second: 137, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 34.444 [1.000, 89.000],  loss: 7.416271, mae: 2.505466, mean_q: 4.714506\n",
      "[58  4 95 97 24 46 50 30 82 88]\n",
      " 41067/50001: episode: 4563, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 42.000, mean reward:  4.667 [ 4.000,  7.000], mean action: 57.333 [4.000, 97.000],  loss: 7.422522, mae: 2.494622, mean_q: 4.711694\n",
      "[18  3 98 37  1  4 88 42 11 73]\n",
      " 41076/50001: episode: 4564, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 39.667 [1.000, 98.000],  loss: 6.230451, mae: 2.512732, mean_q: 4.712634\n",
      "[33  2 63 81 97 30 89 62 97 31]\n",
      " 41085/50001: episode: 4565, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 61.333 [2.000, 97.000],  loss: 6.195012, mae: 2.536946, mean_q: 4.772617\n",
      "[ 3 92 90 57 95 33 28 69 14 27]\n",
      " 41094/50001: episode: 4566, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 40.000, mean reward:  4.444 [ 3.000,  6.000], mean action: 56.111 [14.000, 95.000],  loss: 9.277003, mae: 2.578514, mean_q: 4.819021\n",
      "[80 46 58  2 79  4 57 49 50 98]\n",
      " 41103/50001: episode: 4567, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 49.222 [2.000, 98.000],  loss: 5.660908, mae: 2.499643, mean_q: 4.718033\n",
      "[28 94 34 38 85 32 37 95 50 66]\n",
      " 41112/50001: episode: 4568, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 45.000, mean reward:  5.000 [ 2.000,  8.000], mean action: 59.000 [32.000, 95.000],  loss: 4.835838, mae: 2.559431, mean_q: 4.804029\n",
      "[ 6 14 21  2 13 76  9 38 16 12]\n",
      " 41121/50001: episode: 4569, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 38.000, mean reward:  4.222 [ 2.000, 10.000], mean action: 22.333 [2.000, 76.000],  loss: 7.846815, mae: 2.602262, mean_q: 4.891277\n",
      "[38 34 74 88  8 17 88 42 76 67]\n",
      " 41130/50001: episode: 4570, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 21.000, mean reward:  2.333 [-10.000,  5.000], mean action: 54.889 [8.000, 88.000],  loss: 7.897125, mae: 2.535769, mean_q: 4.747330\n",
      "[38 26 82 84 47 64 24 69 42 31]\n",
      " 41139/50001: episode: 4571, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 52.111 [24.000, 84.000],  loss: 10.458117, mae: 2.561032, mean_q: 4.818552\n",
      "[44 33 63 37 93 91 94 12 31 50]\n",
      " 41148/50001: episode: 4572, duration: 0.069s, episode steps:   9, steps per second: 130, episode reward: 47.000, mean reward:  5.222 [ 3.000,  9.000], mean action: 56.000 [12.000, 94.000],  loss: 7.872583, mae: 2.433647, mean_q: 4.567002\n",
      "[51 50 83 95 81 75 34 91 76  4]\n",
      " 41157/50001: episode: 4573, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 65.444 [4.000, 95.000],  loss: 6.949251, mae: 2.398928, mean_q: 4.521900\n",
      "[67 28 59 66 81 65 62 52 62 26]\n",
      " 41166/50001: episode: 4574, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 17.000, mean reward:  1.889 [-10.000,  5.000], mean action: 55.667 [26.000, 81.000],  loss: 6.293406, mae: 2.442514, mean_q: 4.606239\n",
      "[ 6 95 45 90 38 51 47 47 37 49]\n",
      " 41175/50001: episode: 4575, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 55.444 [37.000, 95.000],  loss: 7.871138, mae: 2.521020, mean_q: 4.802645\n",
      "[79 82 32 95  8 95 74 88 52 14]\n",
      " 41184/50001: episode: 4576, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 60.000 [8.000, 95.000],  loss: 8.091987, mae: 2.596496, mean_q: 4.845304\n",
      "[39 96 47 46 41 63 88 84 34 88]\n",
      " 41193/50001: episode: 4577, duration: 0.066s, episode steps:   9, steps per second: 135, episode reward: 28.000, mean reward:  3.111 [-10.000,  9.000], mean action: 65.222 [34.000, 96.000],  loss: 9.186555, mae: 2.555147, mean_q: 4.785196\n",
      "[67 46 28 57 82 45 50 40 93 74]\n",
      " 41202/50001: episode: 4578, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 41.000, mean reward:  4.556 [ 2.000,  6.000], mean action: 57.222 [28.000, 93.000],  loss: 9.442102, mae: 2.478119, mean_q: 4.635168\n",
      "[74 34 14  6 80 56 30 30 57 50]\n",
      " 41211/50001: episode: 4579, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 27.000, mean reward:  3.000 [-10.000,  9.000], mean action: 39.667 [6.000, 80.000],  loss: 8.601743, mae: 2.445216, mean_q: 4.553296\n",
      "[73 37  1 67 16 71 66 59 98 34]\n",
      " 41220/50001: episode: 4580, duration: 0.068s, episode steps:   9, steps per second: 133, episode reward: 43.000, mean reward:  4.778 [ 2.000, 10.000], mean action: 49.889 [1.000, 98.000],  loss: 8.786598, mae: 2.408280, mean_q: 4.469213\n",
      "[ 8 28  8 72 56 49 98 34 31  2]\n",
      " 41229/50001: episode: 4581, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 30.000, mean reward:  3.333 [-10.000,  8.000], mean action: 42.000 [2.000, 98.000],  loss: 5.805597, mae: 2.443874, mean_q: 4.553611\n",
      "[52 79 37  3 26 46 48 41 16 28]\n",
      " 41238/50001: episode: 4582, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 36.000 [3.000, 79.000],  loss: 9.219379, mae: 2.475550, mean_q: 4.629385\n",
      "[12 96 46 69 41 50 30  6 33 13]\n",
      " 41247/50001: episode: 4583, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 40.000, mean reward:  4.444 [ 3.000,  9.000], mean action: 42.667 [6.000, 96.000],  loss: 6.389655, mae: 2.478235, mean_q: 4.634664\n",
      "[83 96 66 95  2 67 28 27 24 47]\n",
      " 41256/50001: episode: 4584, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 50.222 [2.000, 96.000],  loss: 8.350421, mae: 2.509078, mean_q: 4.711149\n",
      "[35 87 96 97 81 11 34 31 88 31]\n",
      " 41265/50001: episode: 4585, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 61.778 [11.000, 97.000],  loss: 6.155628, mae: 2.544757, mean_q: 4.837228\n",
      "[97 63 46 54 68 34 41  1 76 23]\n",
      " 41274/50001: episode: 4586, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 45.111 [1.000, 76.000],  loss: 10.227157, mae: 2.548153, mean_q: 4.715476\n",
      "[83 17 99 46 95 57 60 97 82 40]\n",
      " 41283/50001: episode: 4587, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 42.000, mean reward:  4.667 [ 3.000,  6.000], mean action: 65.889 [17.000, 99.000],  loss: 6.485879, mae: 2.530620, mean_q: 4.680666\n",
      "[48 37 62 32 98 40 80 51 81  6]\n",
      " 41292/50001: episode: 4588, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 33.000, mean reward:  3.667 [ 3.000,  4.000], mean action: 54.111 [6.000, 98.000],  loss: 6.225888, mae: 2.455640, mean_q: 4.684650\n",
      "[ 9 37 58 92 77 24 47 60 34 41]\n",
      " 41301/50001: episode: 4589, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 52.222 [24.000, 92.000],  loss: 7.080878, mae: 2.501937, mean_q: 4.716558\n",
      "[59  5 16 32 69 48 62 50  8 40]\n",
      " 41310/50001: episode: 4590, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 38.000, mean reward:  4.222 [ 3.000,  7.000], mean action: 36.667 [5.000, 69.000],  loss: 6.150961, mae: 2.555195, mean_q: 4.794047\n",
      "[92 53  1 37 74 32 11 30 12 88]\n",
      " 41319/50001: episode: 4591, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 37.556 [1.000, 88.000],  loss: 6.900830, mae: 2.601008, mean_q: 4.823651\n",
      "[86 37 14  2 10 34 98 34 32 88]\n",
      " 41328/50001: episode: 4592, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 29.000, mean reward:  3.222 [-10.000,  6.000], mean action: 38.778 [2.000, 98.000],  loss: 6.580018, mae: 2.571793, mean_q: 4.813815\n",
      "[22  8 89 46 93 42 30 84 79 57]\n",
      " 41337/50001: episode: 4593, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 58.667 [8.000, 93.000],  loss: 7.146043, mae: 2.581176, mean_q: 4.829566\n",
      "[82 69 64 13 52 28 32 11 52 82]\n",
      " 41346/50001: episode: 4594, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward:  9.000, mean reward:  1.000 [-10.000,  5.000], mean action: 44.778 [11.000, 82.000],  loss: 8.149415, mae: 2.604038, mean_q: 4.801721\n",
      "[72 13  5 51 46 48 53 13 37 33]\n",
      " 41355/50001: episode: 4595, duration: 0.069s, episode steps:   9, steps per second: 131, episode reward: 22.000, mean reward:  2.444 [-10.000,  7.000], mean action: 33.222 [5.000, 53.000],  loss: 10.037214, mae: 2.562132, mean_q: 4.838134\n",
      "[46 62 28  2 22 98 57 51  4 12]\n",
      " 41364/50001: episode: 4596, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 37.333 [2.000, 98.000],  loss: 7.813616, mae: 2.530298, mean_q: 4.718690\n",
      "[28 34 87 79 14 21 47  1 63  6]\n",
      " 41373/50001: episode: 4597, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 39.111 [1.000, 87.000],  loss: 6.711487, mae: 2.525725, mean_q: 4.843339\n",
      "[33 10 95 44 39 56 87 85 37 14]\n",
      " 41382/50001: episode: 4598, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 39.000, mean reward:  4.333 [ 2.000,  9.000], mean action: 51.889 [10.000, 95.000],  loss: 5.066564, mae: 2.471951, mean_q: 4.695630\n",
      "[36 63 82 16 95  5 52 28 73 74]\n",
      " 41391/50001: episode: 4599, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 54.222 [5.000, 95.000],  loss: 5.811045, mae: 2.539819, mean_q: 4.702306\n",
      "[49 46 34 93 29 73 46 13 13 95]\n",
      " 41400/50001: episode: 4600, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 15.000, mean reward:  1.667 [-10.000, 10.000], mean action: 49.111 [13.000, 95.000],  loss: 7.646600, mae: 2.586051, mean_q: 4.847294\n",
      "[73 48 24 46 95 50 88 88 40 32]\n",
      " 41409/50001: episode: 4601, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 28.000, mean reward:  3.111 [-10.000,  6.000], mean action: 56.778 [24.000, 95.000],  loss: 4.538516, mae: 2.528955, mean_q: 4.728467\n",
      "[ 1 13  1 25 91 46 50 50 88  0]\n",
      " 41418/50001: episode: 4602, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward:  8.000, mean reward:  0.889 [-10.000,  6.000], mean action: 40.444 [0.000, 91.000],  loss: 6.102007, mae: 2.659498, mean_q: 4.982155\n",
      "[80 34 34 30 80 19 83 62 32 64]\n",
      " 41427/50001: episode: 4603, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward:  9.000, mean reward:  1.000 [-10.000,  6.000], mean action: 48.667 [19.000, 83.000],  loss: 8.727828, mae: 2.700979, mean_q: 5.001066\n",
      "[25 89 87 41 87 89 69 35 31 75]\n",
      " 41436/50001: episode: 4604, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 13.000, mean reward:  1.444 [-10.000,  7.000], mean action: 67.000 [31.000, 89.000],  loss: 8.702682, mae: 2.613109, mean_q: 4.895071\n",
      "[67 62 85 46 95 11 88 41 10 20]\n",
      " 41445/50001: episode: 4605, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 50.889 [10.000, 95.000],  loss: 6.689837, mae: 2.572497, mean_q: 4.779434\n",
      "[93 50 48 90 43  1 57 54 48 98]\n",
      " 41454/50001: episode: 4606, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 54.333 [1.000, 98.000],  loss: 10.294134, mae: 2.560410, mean_q: 4.796778\n",
      "[88 27 75  9 16 66  4 30 92 39]\n",
      " 41463/50001: episode: 4607, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 34.000, mean reward:  3.778 [ 2.000,  5.000], mean action: 39.778 [4.000, 92.000],  loss: 5.686813, mae: 2.476083, mean_q: 4.655375\n",
      "[76 57 41  8 25 46 59 32 95 75]\n",
      " 41472/50001: episode: 4608, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 44.000, mean reward:  4.889 [ 3.000,  9.000], mean action: 48.667 [8.000, 95.000],  loss: 7.759048, mae: 2.520397, mean_q: 4.740048\n",
      "[97 13 42  2 96 41 27 44 97 79]\n",
      " 41481/50001: episode: 4609, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 49.000 [2.000, 97.000],  loss: 6.964849, mae: 2.498415, mean_q: 4.659326\n",
      "[18 95  4 62 97 88 95 74 50 79]\n",
      " 41490/50001: episode: 4610, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 29.000, mean reward:  3.222 [-10.000,  7.000], mean action: 71.556 [4.000, 97.000],  loss: 7.629339, mae: 2.537140, mean_q: 4.848244\n",
      "[86 60 42 95  4  9 21 10 13 50]\n",
      " 41499/50001: episode: 4611, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 44.000, mean reward:  4.889 [ 3.000,  8.000], mean action: 33.778 [4.000, 95.000],  loss: 8.799078, mae: 2.554813, mean_q: 4.768993\n",
      "[10 20 32 50  2 62 13 34 52 29]\n",
      " 41508/50001: episode: 4612, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 32.667 [2.000, 62.000],  loss: 7.492308, mae: 2.514964, mean_q: 4.655649\n",
      "[21 13 67 62 52 47 62 53  8 81]\n",
      " 41517/50001: episode: 4613, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 21.000, mean reward:  2.333 [-10.000,  5.000], mean action: 49.444 [8.000, 81.000],  loss: 8.092541, mae: 2.505733, mean_q: 4.734245\n",
      "[37 28 93  9 82 66 37 42 82 83]\n",
      " 41526/50001: episode: 4614, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward:  7.000, mean reward:  0.778 [-10.000,  5.000], mean action: 58.000 [9.000, 93.000],  loss: 7.507585, mae: 2.428841, mean_q: 4.615683\n",
      "[44 53  9  2 51 74 44 83 56  8]\n",
      " 41535/50001: episode: 4615, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 42.222 [2.000, 83.000],  loss: 7.562949, mae: 2.457067, mean_q: 4.585527\n",
      "[26 31 32  4 13 63 20 37 67 95]\n",
      " 41544/50001: episode: 4616, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 43.000, mean reward:  4.778 [ 3.000, 10.000], mean action: 40.222 [4.000, 95.000],  loss: 6.826902, mae: 2.490073, mean_q: 4.655129\n",
      "[28 37 25 59 97 41 95  2 26 28]\n",
      " 41553/50001: episode: 4617, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 22.000, mean reward:  2.444 [-10.000,  7.000], mean action: 45.556 [2.000, 97.000],  loss: 9.249916, mae: 2.413698, mean_q: 4.557070\n",
      "[22 42 32 38 30 53 24 10  2 84]\n",
      " 41562/50001: episode: 4618, duration: 0.065s, episode steps:   9, steps per second: 140, episode reward: 36.000, mean reward:  4.000 [ 2.000,  6.000], mean action: 35.000 [2.000, 84.000],  loss: 5.735459, mae: 2.446772, mean_q: 4.560123\n",
      "[82 58 27 19  6 63 48 83 24 98]\n",
      " 41571/50001: episode: 4619, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 47.333 [6.000, 98.000],  loss: 6.877841, mae: 2.431453, mean_q: 4.534747\n",
      "[86 93 24 98 82  1 23 32 98 74]\n",
      " 41580/50001: episode: 4620, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 27.000, mean reward:  3.000 [-10.000,  6.000], mean action: 58.333 [1.000, 98.000],  loss: 6.647676, mae: 2.459217, mean_q: 4.631401\n",
      "[67 13 33 30 95 24 60 93 98 39]\n",
      " 41589/50001: episode: 4621, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 53.889 [13.000, 98.000],  loss: 8.770226, mae: 2.536962, mean_q: 4.789899\n",
      "[ 4 88 42  6  2 42 92 10 85 10]\n",
      " 41598/50001: episode: 4622, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward:  5.000, mean reward:  0.556 [-10.000,  4.000], mean action: 41.889 [2.000, 92.000],  loss: 6.102126, mae: 2.514956, mean_q: 4.707114\n",
      "[57 46 95 92 41 52 97  2 78 46]\n",
      " 41607/50001: episode: 4623, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 23.000, mean reward:  2.556 [-10.000,  5.000], mean action: 61.000 [2.000, 97.000],  loss: 4.486609, mae: 2.496717, mean_q: 4.658681\n",
      "[23 34  2 91 51  4 85 30 13 28]\n",
      " 41616/50001: episode: 4624, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 37.556 [2.000, 91.000],  loss: 7.741691, mae: 2.622790, mean_q: 4.862559\n",
      "[75 90 90  9 34 47 37 84 14 32]\n",
      " 41625/50001: episode: 4625, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 28.000, mean reward:  3.111 [-10.000,  7.000], mean action: 48.556 [9.000, 90.000],  loss: 10.964330, mae: 2.610967, mean_q: 4.796122\n",
      "[39 42 21 22 97 14 52 12 69 35]\n",
      " 41634/50001: episode: 4626, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 40.444 [12.000, 97.000],  loss: 9.989428, mae: 2.512897, mean_q: 4.675202\n",
      "[47 37 42 41 63 85 57 37 60 60]\n",
      " 41643/50001: episode: 4627, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward:  8.000, mean reward:  0.889 [-10.000,  6.000], mean action: 53.556 [37.000, 85.000],  loss: 7.997227, mae: 2.437436, mean_q: 4.515433\n",
      "[74 52 15 97 82 42 58 37 28 90]\n",
      " 41652/50001: episode: 4628, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 55.667 [15.000, 97.000],  loss: 9.493320, mae: 2.447800, mean_q: 4.538880\n",
      "[74 23  2 57 14 25 80 72 42 29]\n",
      " 41661/50001: episode: 4629, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 31.000, mean reward:  3.444 [ 2.000,  6.000], mean action: 38.222 [2.000, 80.000],  loss: 7.321333, mae: 2.368382, mean_q: 4.338459\n",
      "[89 83  5  1 82 49 34 97 50 79]\n",
      " 41670/50001: episode: 4630, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 43.000, mean reward:  4.778 [ 3.000,  7.000], mean action: 53.333 [1.000, 97.000],  loss: 8.462330, mae: 2.347172, mean_q: 4.385321\n",
      "[56 42 37 92 12  4 93 27 38 28]\n",
      " 41679/50001: episode: 4631, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 37.000, mean reward:  4.111 [ 2.000,  7.000], mean action: 41.444 [4.000, 93.000],  loss: 8.217138, mae: 2.398491, mean_q: 4.481253\n",
      "[68 41 38 71 88 19 15 27  1 33]\n",
      " 41688/50001: episode: 4632, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 37.000 [1.000, 88.000],  loss: 7.633992, mae: 2.457129, mean_q: 4.541500\n",
      "[35 60 37 98 93  2 95 30 24 34]\n",
      " 41697/50001: episode: 4633, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 45.000, mean reward:  5.000 [ 4.000,  8.000], mean action: 52.556 [2.000, 98.000],  loss: 6.208019, mae: 2.428064, mean_q: 4.496499\n",
      "[72 27 54 60 74 78 37 76 12 34]\n",
      " 41706/50001: episode: 4634, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 47.000, mean reward:  5.222 [ 3.000, 10.000], mean action: 50.222 [12.000, 78.000],  loss: 7.159913, mae: 2.532152, mean_q: 4.691580\n",
      "[68 31 34 77 48 99 41 76 12 23]\n",
      " 41715/50001: episode: 4635, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 40.000, mean reward:  4.444 [ 3.000,  8.000], mean action: 49.000 [12.000, 99.000],  loss: 7.753491, mae: 2.562917, mean_q: 4.757719\n",
      "[40 34 50 99 28 82  1 38 46 42]\n",
      " 41724/50001: episode: 4636, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 39.000, mean reward:  4.333 [ 2.000,  6.000], mean action: 46.667 [1.000, 99.000],  loss: 10.414997, mae: 2.530250, mean_q: 4.687077\n",
      "[36 37 34 20 12 97 86 96 12 14]\n",
      " 41733/50001: episode: 4637, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 45.333 [12.000, 97.000],  loss: 5.358008, mae: 2.460932, mean_q: 4.574523\n",
      "[ 9 18 13 41 44 63  4 59 85 50]\n",
      " 41742/50001: episode: 4638, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 41.000, mean reward:  4.556 [ 2.000, 10.000], mean action: 41.889 [4.000, 85.000],  loss: 8.652658, mae: 2.474772, mean_q: 4.549749\n",
      "[84 20 46 52 13 21 76 88 12 81]\n",
      " 41751/50001: episode: 4639, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 45.444 [12.000, 88.000],  loss: 5.605906, mae: 2.474067, mean_q: 4.612487\n",
      "[73 89 46 45  9 99 46 76 53 23]\n",
      " 41760/50001: episode: 4640, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 54.000 [9.000, 99.000],  loss: 6.842619, mae: 2.534080, mean_q: 4.731143\n",
      "[29 20 49 10 82 23 29 64 31 52]\n",
      " 41769/50001: episode: 4641, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 40.000 [10.000, 82.000],  loss: 5.250906, mae: 2.511990, mean_q: 4.624249\n",
      "[16 78 92 10 32 68 21 48 13 41]\n",
      " 41778/50001: episode: 4642, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 40.000, mean reward:  4.444 [ 3.000,  8.000], mean action: 44.778 [10.000, 92.000],  loss: 9.163838, mae: 2.584851, mean_q: 4.781246\n",
      "[50 50 26 97 48 48 40 65 13 28]\n",
      " 41787/50001: episode: 4643, duration: 0.069s, episode steps:   9, steps per second: 130, episode reward:  8.000, mean reward:  0.889 [-10.000,  7.000], mean action: 46.111 [13.000, 97.000],  loss: 6.582613, mae: 2.574837, mean_q: 4.715886\n",
      "[82  3 95 66 61 18 76 93  4  4]\n",
      " 41796/50001: episode: 4644, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 46.667 [3.000, 95.000],  loss: 6.676606, mae: 2.564181, mean_q: 4.754377\n",
      "[95 95 57 93 56 23 42 27 54 10]\n",
      " 41805/50001: episode: 4645, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 20.000, mean reward:  2.222 [-10.000,  5.000], mean action: 50.778 [10.000, 95.000],  loss: 7.517477, mae: 2.612806, mean_q: 4.850521\n",
      "[26 30 41 22 60 75 27 94 50 12]\n",
      " 41814/50001: episode: 4646, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 46.000, mean reward:  5.111 [ 4.000,  8.000], mean action: 45.667 [12.000, 94.000],  loss: 7.209350, mae: 2.530584, mean_q: 4.720553\n",
      "[45 84 50 57 99 94 98 71  9  4]\n",
      " 41823/50001: episode: 4647, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 62.889 [4.000, 99.000],  loss: 6.432169, mae: 2.618317, mean_q: 4.847219\n",
      "[11 84 49 90 84 12 98 32 76 30]\n",
      " 41832/50001: episode: 4648, duration: 0.070s, episode steps:   9, steps per second: 129, episode reward: 27.000, mean reward:  3.000 [-10.000,  6.000], mean action: 61.667 [12.000, 98.000],  loss: 7.239136, mae: 2.564518, mean_q: 4.758304\n",
      "[18 54 82 79 16 79 56 96 23  4]\n",
      " 41841/50001: episode: 4649, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 54.333 [4.000, 96.000],  loss: 6.677850, mae: 2.642208, mean_q: 4.892178\n",
      "[29 43 85 34 89 77 11 50 28 37]\n",
      " 41850/50001: episode: 4650, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 44.000, mean reward:  4.889 [ 2.000,  8.000], mean action: 50.444 [11.000, 89.000],  loss: 5.673700, mae: 2.654517, mean_q: 4.931818\n",
      "[80  6 95 59 97 60  9 59 48 12]\n",
      " 41859/50001: episode: 4651, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 30.000, mean reward:  3.333 [-10.000,  8.000], mean action: 49.444 [6.000, 97.000],  loss: 7.275574, mae: 2.631261, mean_q: 4.865716\n",
      "[81 95 37 74  2 88 27 46 28 13]\n",
      " 41868/50001: episode: 4652, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 44.000, mean reward:  4.889 [ 3.000,  7.000], mean action: 45.556 [2.000, 95.000],  loss: 7.146808, mae: 2.659285, mean_q: 4.910875\n",
      "[ 6 13 74 33 95 31 86 32 44 50]\n",
      " 41877/50001: episode: 4653, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 42.000, mean reward:  4.667 [ 3.000,  9.000], mean action: 50.889 [13.000, 95.000],  loss: 8.768827, mae: 2.579270, mean_q: 4.711068\n",
      "[87 31 88 92 31  4 48 97 60 81]\n",
      " 41886/50001: episode: 4654, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 24.000, mean reward:  2.667 [-10.000,  5.000], mean action: 59.111 [4.000, 97.000],  loss: 7.524887, mae: 2.527227, mean_q: 4.675700\n",
      "[25 12 52 24 95 31 88 12 88 79]\n",
      " 41895/50001: episode: 4655, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 12.000, mean reward:  1.333 [-10.000,  6.000], mean action: 53.444 [12.000, 95.000],  loss: 6.753820, mae: 2.510525, mean_q: 4.668482\n",
      "[95 10 34 48 23 31 95 88 34 21]\n",
      " 41904/50001: episode: 4656, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward:  6.000, mean reward:  0.667 [-10.000,  5.000], mean action: 42.667 [10.000, 95.000],  loss: 5.617519, mae: 2.587149, mean_q: 4.787765\n",
      "[54 32 49 27 44 93 33 41 13 31]\n",
      " 41913/50001: episode: 4657, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 43.000, mean reward:  4.778 [ 3.000,  8.000], mean action: 40.333 [13.000, 93.000],  loss: 7.578701, mae: 2.587593, mean_q: 4.786717\n",
      "[35 34 99 88 46 82 48 49  1  4]\n",
      " 41922/50001: episode: 4658, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 50.111 [1.000, 99.000],  loss: 6.261827, mae: 2.564746, mean_q: 4.760290\n",
      "[91 41 89  1  1 66 95  6 97 32]\n",
      " 41931/50001: episode: 4659, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 30.000, mean reward:  3.333 [-10.000,  7.000], mean action: 47.556 [1.000, 97.000],  loss: 7.766971, mae: 2.553047, mean_q: 4.750921\n",
      "[92 35 75  1 57 73 86 97 32 41]\n",
      " 41940/50001: episode: 4660, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 55.222 [1.000, 97.000],  loss: 7.418058, mae: 2.553824, mean_q: 4.793370\n",
      "[76 32 99 82 69 46 14 64 66  4]\n",
      " 41949/50001: episode: 4661, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 43.000, mean reward:  4.778 [ 3.000,  7.000], mean action: 52.889 [4.000, 99.000],  loss: 6.915788, mae: 2.576064, mean_q: 4.779070\n",
      "[91 43 27 47 86 44 66 98  2 54]\n",
      " 41958/50001: episode: 4662, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 51.889 [2.000, 98.000],  loss: 5.644938, mae: 2.576737, mean_q: 4.888508\n",
      "[62 70 24 79 19 24 35 70 34 98]\n",
      " 41967/50001: episode: 4663, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 12.000, mean reward:  1.333 [-10.000,  9.000], mean action: 50.333 [19.000, 98.000],  loss: 5.276852, mae: 2.674575, mean_q: 5.005774\n",
      "[45  2 15  6 11 27 62 97  8 97]\n",
      " 41976/50001: episode: 4664, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 36.111 [2.000, 97.000],  loss: 7.596497, mae: 2.728419, mean_q: 5.030253\n",
      "[80 89 44  5 12 30  0 30  6 97]\n",
      " 41985/50001: episode: 4665, duration: 0.068s, episode steps:   9, steps per second: 133, episode reward: 24.000, mean reward:  2.667 [-10.000,  9.000], mean action: 34.778 [0.000, 97.000],  loss: 9.432625, mae: 2.721723, mean_q: 5.055664\n",
      "[65 46 73 50 53 10 66 36 50 79]\n",
      " 41994/50001: episode: 4666, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 51.444 [10.000, 79.000],  loss: 7.179708, mae: 2.564507, mean_q: 4.772183\n",
      "[36 34 13 34 11  4 57 10 76 50]\n",
      " 42003/50001: episode: 4667, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 26.000, mean reward:  2.889 [-10.000,  9.000], mean action: 32.111 [4.000, 76.000],  loss: 4.687475, mae: 2.536669, mean_q: 4.752645\n",
      "[ 6 65 79 88 37 52 12 42 65 32]\n",
      " 42012/50001: episode: 4668, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 52.444 [12.000, 88.000],  loss: 6.815643, mae: 2.500243, mean_q: 4.692341\n",
      "[48 35 10 46  9 64  1 95 47 64]\n",
      " 42021/50001: episode: 4669, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 27.000, mean reward:  3.000 [-10.000,  8.000], mean action: 41.222 [1.000, 95.000],  loss: 5.328749, mae: 2.583658, mean_q: 4.854099\n",
      "[23 32 72 16 66 78 37 31 14 62]\n",
      " 42030/50001: episode: 4670, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 37.000, mean reward:  4.111 [ 2.000,  7.000], mean action: 45.333 [14.000, 78.000],  loss: 6.565271, mae: 2.607154, mean_q: 4.847224\n",
      "[23 31 90 34  4 81 42 69  4 99]\n",
      " 42039/50001: episode: 4671, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 23.000, mean reward:  2.556 [-10.000,  5.000], mean action: 50.444 [4.000, 99.000],  loss: 6.814107, mae: 2.618730, mean_q: 4.905152\n",
      "[17 41 28 41 24 40 39 50 91 20]\n",
      " 42048/50001: episode: 4672, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 41.556 [20.000, 91.000],  loss: 7.423960, mae: 2.593798, mean_q: 4.847689\n",
      "[19 14 37  8 28 57 64 63 83  4]\n",
      " 42057/50001: episode: 4673, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 38.000, mean reward:  4.222 [ 3.000,  7.000], mean action: 39.778 [4.000, 83.000],  loss: 9.576013, mae: 2.519826, mean_q: 4.785324\n",
      "[46 88 93 40 40 14 63  6 34 97]\n",
      " 42066/50001: episode: 4674, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 28.000, mean reward:  3.111 [-10.000,  9.000], mean action: 52.778 [6.000, 97.000],  loss: 4.511393, mae: 2.494814, mean_q: 4.740301\n",
      "[69 63 30 31 90  1 42 34  6 83]\n",
      " 42075/50001: episode: 4675, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 42.222 [1.000, 90.000],  loss: 7.512805, mae: 2.553382, mean_q: 4.822567\n",
      "[82 31 30 84 34 30  0  1 88 75]\n",
      " 42084/50001: episode: 4676, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 30.000, mean reward:  3.333 [-10.000,  7.000], mean action: 41.444 [0.000, 88.000],  loss: 6.871377, mae: 2.558009, mean_q: 4.787621\n",
      "[32  1 69 66 37 12 52 88 93 20]\n",
      " 42093/50001: episode: 4677, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 37.000, mean reward:  4.111 [ 3.000,  5.000], mean action: 48.667 [1.000, 93.000],  loss: 7.707095, mae: 2.517211, mean_q: 4.730421\n",
      "[57 37 66 75 85 97 80 13 50 34]\n",
      " 42102/50001: episode: 4678, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward: 41.000, mean reward:  4.556 [ 2.000,  8.000], mean action: 59.667 [13.000, 97.000],  loss: 6.255765, mae: 2.591069, mean_q: 4.866428\n",
      "[48  4 55 74 47 27 83 75 39 24]\n",
      " 42111/50001: episode: 4679, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 35.000, mean reward:  3.889 [ 2.000,  7.000], mean action: 47.556 [4.000, 83.000],  loss: 6.354068, mae: 2.522177, mean_q: 4.824614\n",
      "[23 28 62 11  5 37 57 56 12 53]\n",
      " 42120/50001: episode: 4680, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 35.667 [5.000, 62.000],  loss: 9.502534, mae: 2.603486, mean_q: 4.879389\n",
      "[83 42 89 62 93 12  2 12 24 82]\n",
      " 42129/50001: episode: 4681, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 46.444 [2.000, 93.000],  loss: 6.744566, mae: 2.551582, mean_q: 4.837430\n",
      "[36  4 95 95 84 82 97 50 57 12]\n",
      " 42138/50001: episode: 4682, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 31.000, mean reward:  3.444 [-10.000,  8.000], mean action: 64.000 [4.000, 97.000],  loss: 5.931873, mae: 2.559184, mean_q: 4.829873\n",
      "[88 63 50 66 75 60 52 58 40 28]\n",
      " 42147/50001: episode: 4683, duration: 0.072s, episode steps:   9, steps per second: 125, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 54.667 [28.000, 75.000],  loss: 6.093696, mae: 2.546513, mean_q: 4.712980\n",
      "[21 84 98 66 13 12 59 51 82 65]\n",
      " 42156/50001: episode: 4684, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 36.000, mean reward:  4.000 [ 3.000,  5.000], mean action: 58.889 [12.000, 98.000],  loss: 8.745778, mae: 2.556835, mean_q: 4.876883\n",
      "[46  1  6 95 82 34 51 93 29 10]\n",
      " 42165/50001: episode: 4685, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 38.000, mean reward:  4.222 [ 2.000,  6.000], mean action: 44.556 [1.000, 95.000],  loss: 6.231094, mae: 2.468408, mean_q: 4.676886\n",
      "[82 80 32 97 84 42 49 18 42 37]\n",
      " 42174/50001: episode: 4686, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 25.000, mean reward:  2.778 [-10.000, 10.000], mean action: 53.444 [18.000, 97.000],  loss: 8.660472, mae: 2.482885, mean_q: 4.698760\n",
      "[24  9 27 41 97 76 56 50 40 12]\n",
      " 42183/50001: episode: 4687, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 45.333 [9.000, 97.000],  loss: 7.895037, mae: 2.525182, mean_q: 4.749236\n",
      "[57 91 80 12 95  4 74 23 53 50]\n",
      " 42192/50001: episode: 4688, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 39.000, mean reward:  4.333 [ 2.000,  8.000], mean action: 53.556 [4.000, 95.000],  loss: 8.263098, mae: 2.509657, mean_q: 4.657105\n",
      "[26 93 99 63 34 11 69 82 89 88]\n",
      " 42201/50001: episode: 4689, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 40.000, mean reward:  4.444 [ 3.000,  8.000], mean action: 69.778 [11.000, 99.000],  loss: 6.794401, mae: 2.517083, mean_q: 4.684232\n",
      "[60 52 66 56 98  9 76 99 50 52]\n",
      " 42210/50001: episode: 4690, duration: 0.069s, episode steps:   9, steps per second: 131, episode reward: 24.000, mean reward:  2.667 [-10.000,  8.000], mean action: 62.000 [9.000, 99.000],  loss: 8.026544, mae: 2.437726, mean_q: 4.551527\n",
      "[20 20  1 80 81 97 28 13 50 25]\n",
      " 42219/50001: episode: 4691, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 43.889 [1.000, 97.000],  loss: 6.577835, mae: 2.453967, mean_q: 4.571940\n",
      "[26 34 30 40 38 59 14 95 96 16]\n",
      " 42228/50001: episode: 4692, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 40.000, mean reward:  4.444 [ 2.000,  9.000], mean action: 46.889 [14.000, 96.000],  loss: 5.679712, mae: 2.521656, mean_q: 4.717998\n",
      "[ 6 52 24 19 13 83 75 64 37 88]\n",
      " 42237/50001: episode: 4693, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 43.000, mean reward:  4.778 [ 3.000,  8.000], mean action: 50.556 [13.000, 88.000],  loss: 7.341293, mae: 2.574798, mean_q: 4.752026\n",
      "[ 8 35 90 89 20 75 48 88 64 24]\n",
      " 42246/50001: episode: 4694, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 59.222 [20.000, 90.000],  loss: 8.865731, mae: 2.578834, mean_q: 4.801390\n",
      "[90 88 50 87 46 49  2 50  4 89]\n",
      " 42255/50001: episode: 4695, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 22.000, mean reward:  2.444 [-10.000,  5.000], mean action: 51.667 [2.000, 89.000],  loss: 9.738802, mae: 2.424435, mean_q: 4.547271\n",
      "[81 34 11 11 31 49 95 32 79 28]\n",
      " 42264/50001: episode: 4696, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 28.000, mean reward:  3.111 [-10.000,  7.000], mean action: 41.111 [11.000, 95.000],  loss: 7.028782, mae: 2.482707, mean_q: 4.670813\n",
      "[97 94 34 68 56 13 90 98 50 50]\n",
      " 42273/50001: episode: 4697, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 61.444 [13.000, 98.000],  loss: 7.371696, mae: 2.439045, mean_q: 4.579185\n",
      "[23 12  9 34 56 87 30  9 83 57]\n",
      " 42282/50001: episode: 4698, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 41.889 [9.000, 87.000],  loss: 8.760955, mae: 2.504369, mean_q: 4.609324\n",
      "[86 15 17 94 50 57 95 27  3 48]\n",
      " 42291/50001: episode: 4699, duration: 0.070s, episode steps:   9, steps per second: 129, episode reward: 42.000, mean reward:  4.667 [ 2.000,  8.000], mean action: 45.111 [3.000, 95.000],  loss: 9.685890, mae: 2.451210, mean_q: 4.568762\n",
      "[70 87 77 28 97  4 11 57 14 27]\n",
      " 42300/50001: episode: 4700, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 42.000, mean reward:  4.667 [ 3.000,  6.000], mean action: 44.667 [4.000, 97.000],  loss: 6.191881, mae: 2.474309, mean_q: 4.579183\n",
      "[66 88  2 82 40 69 12 62 69 50]\n",
      " 42309/50001: episode: 4701, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 52.667 [2.000, 88.000],  loss: 5.943955, mae: 2.463860, mean_q: 4.515699\n",
      "[51 32 84  4 30 32 59 42 44 48]\n",
      " 42318/50001: episode: 4702, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 22.000, mean reward:  2.444 [-10.000,  8.000], mean action: 41.667 [4.000, 84.000],  loss: 6.052607, mae: 2.528376, mean_q: 4.698066\n",
      "[49 16 51  0 34 79 13 30  3 89]\n",
      " 42327/50001: episode: 4703, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 35.000 [0.000, 89.000],  loss: 5.812319, mae: 2.622785, mean_q: 4.888494\n",
      "[28  4 41 86 34 18  9 11 50  5]\n",
      " 42336/50001: episode: 4704, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 28.667 [4.000, 86.000],  loss: 8.058326, mae: 2.622441, mean_q: 4.851097\n",
      "[31 24  6 32 53 84 46 50  1 48]\n",
      " 42345/50001: episode: 4705, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 44.000, mean reward:  4.889 [ 3.000,  7.000], mean action: 38.222 [1.000, 84.000],  loss: 7.790854, mae: 2.617545, mean_q: 4.852121\n",
      "[84 58 51 88 49  1 24 17 34 32]\n",
      " 42354/50001: episode: 4706, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 41.000, mean reward:  4.556 [ 2.000,  8.000], mean action: 39.333 [1.000, 88.000],  loss: 6.224729, mae: 2.585034, mean_q: 4.774506\n",
      "[62 44 67 21 77 88 13 95 66 79]\n",
      " 42363/50001: episode: 4707, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 44.000, mean reward:  4.889 [ 2.000,  8.000], mean action: 61.111 [13.000, 95.000],  loss: 8.973774, mae: 2.572581, mean_q: 4.832222\n",
      "[ 5 34 93 85 92 44 10 74 93 46]\n",
      " 42372/50001: episode: 4708, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 24.000, mean reward:  2.667 [-10.000,  8.000], mean action: 63.444 [10.000, 93.000],  loss: 8.280857, mae: 2.598752, mean_q: 4.855330\n",
      "[20 34 54 98 34 35 79 14 82 16]\n",
      " 42381/50001: episode: 4709, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 24.000, mean reward:  2.667 [-10.000,  5.000], mean action: 49.556 [14.000, 98.000],  loss: 9.025382, mae: 2.509297, mean_q: 4.690168\n",
      "[44 86 48 48 92 52 84 28 69 32]\n",
      " 42390/50001: episode: 4710, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 59.889 [28.000, 92.000],  loss: 7.218869, mae: 2.495826, mean_q: 4.680952\n",
      "[50 23 75  4 90 60  1 27 34 90]\n",
      " 42399/50001: episode: 4711, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 27.000, mean reward:  3.000 [-10.000,  8.000], mean action: 44.889 [1.000, 90.000],  loss: 6.227101, mae: 2.479020, mean_q: 4.787750\n",
      "[21 41  2 37 85 37 33 98 40 53]\n",
      " 42408/50001: episode: 4712, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 24.000, mean reward:  2.667 [-10.000,  5.000], mean action: 47.333 [2.000, 98.000],  loss: 6.822855, mae: 2.474053, mean_q: 4.676759\n",
      "[30 23 51 37 27 65 37 37 57 90]\n",
      " 42417/50001: episode: 4713, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward:  7.000, mean reward:  0.778 [-10.000,  5.000], mean action: 47.111 [23.000, 90.000],  loss: 6.746539, mae: 2.496917, mean_q: 4.685353\n",
      "[74  6  0 80 96 37 38 22 54 19]\n",
      " 42426/50001: episode: 4714, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 34.000, mean reward:  3.778 [ 2.000,  7.000], mean action: 39.111 [0.000, 96.000],  loss: 9.099687, mae: 2.449558, mean_q: 4.592058\n",
      "[60 34 54 53 47 60 50 60 99 32]\n",
      " 42435/50001: episode: 4715, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 12.000, mean reward:  1.333 [-10.000,  6.000], mean action: 54.333 [32.000, 99.000],  loss: 8.367470, mae: 2.454139, mean_q: 4.604663\n",
      "[51 68 99 95 95 23 63  5 98 48]\n",
      " 42444/50001: episode: 4716, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 66.000 [5.000, 99.000],  loss: 7.683902, mae: 2.478577, mean_q: 4.610358\n",
      "[22 50 16 56  2 13 26 24 89 50]\n",
      " 42453/50001: episode: 4717, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 21.000, mean reward:  2.333 [-10.000,  5.000], mean action: 36.222 [2.000, 89.000],  loss: 4.317759, mae: 2.541441, mean_q: 4.752123\n",
      "[ 3 26 98 41 27 92 50 24 37 66]\n",
      " 42462/50001: episode: 4718, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 46.000, mean reward:  5.111 [ 2.000,  8.000], mean action: 51.222 [24.000, 98.000],  loss: 5.828774, mae: 2.617333, mean_q: 4.820891\n",
      "[86 48 28 93 27 32 24  0 92 46]\n",
      " 42471/50001: episode: 4719, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 37.000, mean reward:  4.111 [ 2.000,  7.000], mean action: 43.333 [0.000, 93.000],  loss: 7.015397, mae: 2.673230, mean_q: 4.983603\n",
      "[ 6 28 76 95 60 74 13 50 10 37]\n",
      " 42480/50001: episode: 4720, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 44.000, mean reward:  4.889 [ 3.000,  8.000], mean action: 49.222 [10.000, 95.000],  loss: 10.032749, mae: 2.713298, mean_q: 5.042195\n",
      "[39 34  4  2 62  3 90 98 50 50]\n",
      " 42489/50001: episode: 4721, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 43.667 [2.000, 98.000],  loss: 8.229659, mae: 2.537173, mean_q: 4.718017\n",
      "[ 1 97 37 94 31 30 79 15 57 50]\n",
      " 42498/50001: episode: 4722, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 41.000, mean reward:  4.556 [ 2.000,  9.000], mean action: 54.444 [15.000, 97.000],  loss: 6.302221, mae: 2.542354, mean_q: 4.673598\n",
      "[11 48 55 73 95  1 20 60 67 50]\n",
      " 42507/50001: episode: 4723, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 38.000, mean reward:  4.222 [ 2.000,  9.000], mean action: 52.111 [1.000, 95.000],  loss: 6.831903, mae: 2.570113, mean_q: 4.827201\n",
      "[17 94  8 77 28 27  2 39 76 50]\n",
      " 42516/50001: episode: 4724, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 40.000, mean reward:  4.444 [ 2.000, 10.000], mean action: 44.556 [2.000, 94.000],  loss: 7.785589, mae: 2.603511, mean_q: 4.843939\n",
      "[62  6 21 93 88 34 13 31 68  5]\n",
      " 42525/50001: episode: 4725, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 39.889 [5.000, 93.000],  loss: 7.618200, mae: 2.515438, mean_q: 4.690032\n",
      "[54 59 89 28 29 92 88 79  2  1]\n",
      " 42534/50001: episode: 4726, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 44.000, mean reward:  4.889 [ 2.000,  8.000], mean action: 51.889 [1.000, 92.000],  loss: 8.730350, mae: 2.550412, mean_q: 4.661095\n",
      "[16 98 69 48 41 12 56 75 13 66]\n",
      " 42543/50001: episode: 4727, duration: 0.075s, episode steps:   9, steps per second: 120, episode reward: 43.000, mean reward:  4.778 [ 3.000,  7.000], mean action: 53.111 [12.000, 98.000],  loss: 6.383471, mae: 2.494650, mean_q: 4.617162\n",
      "[19 76 56  6 10 42 96 66 67 66]\n",
      " 42552/50001: episode: 4728, duration: 0.079s, episode steps:   9, steps per second: 114, episode reward: 21.000, mean reward:  2.333 [-10.000,  6.000], mean action: 53.889 [6.000, 96.000],  loss: 6.906258, mae: 2.523288, mean_q: 4.680155\n",
      "[ 9 89 68 34 33 83 59 31 42  2]\n",
      " 42561/50001: episode: 4729, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 49.000 [2.000, 89.000],  loss: 9.997867, mae: 2.531606, mean_q: 4.646518\n",
      "[74 76 63 97 50 54 93 90 88 32]\n",
      " 42570/50001: episode: 4730, duration: 0.071s, episode steps:   9, steps per second: 126, episode reward: 44.000, mean reward:  4.889 [ 3.000,  7.000], mean action: 71.444 [32.000, 97.000],  loss: 8.342139, mae: 2.472488, mean_q: 4.607913\n",
      "[73 48  7 95 62 97 44 68 83 28]\n",
      " 42579/50001: episode: 4731, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 39.000, mean reward:  4.333 [ 2.000,  8.000], mean action: 59.111 [7.000, 97.000],  loss: 7.581984, mae: 2.481327, mean_q: 4.607009\n",
      "[25 48 83 14 86 57 61 62 12 49]\n",
      " 42588/50001: episode: 4732, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 38.000, mean reward:  4.222 [ 3.000,  8.000], mean action: 52.444 [12.000, 86.000],  loss: 10.125353, mae: 2.481557, mean_q: 4.644817\n",
      "[40 92 34 75 87 49 97 93 27 31]\n",
      " 42597/50001: episode: 4733, duration: 0.087s, episode steps:   9, steps per second: 103, episode reward: 40.000, mean reward:  4.444 [ 3.000,  6.000], mean action: 65.000 [27.000, 97.000],  loss: 8.556706, mae: 2.471135, mean_q: 4.576501\n",
      "[23 67 99 57 48  2 67 59 62 31]\n",
      " 42606/50001: episode: 4734, duration: 0.087s, episode steps:   9, steps per second: 103, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 54.667 [2.000, 99.000],  loss: 7.981126, mae: 2.394673, mean_q: 4.446062\n",
      "[57 34 12 14 95 74 24 88 57 79]\n",
      " 42615/50001: episode: 4735, duration: 0.091s, episode steps:   9, steps per second:  99, episode reward: 27.000, mean reward:  3.000 [-10.000,  6.000], mean action: 53.000 [12.000, 95.000],  loss: 7.405906, mae: 2.385660, mean_q: 4.495873\n",
      "[74 37 76 76 25 75 13 88  8 98]\n",
      " 42624/50001: episode: 4736, duration: 0.088s, episode steps:   9, steps per second: 103, episode reward: 28.000, mean reward:  3.111 [-10.000,  7.000], mean action: 55.111 [8.000, 98.000],  loss: 5.864901, mae: 2.395050, mean_q: 4.529656\n",
      "[67 24 16  1 97 60 62 74 79 69]\n",
      " 42633/50001: episode: 4737, duration: 0.084s, episode steps:   9, steps per second: 108, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 53.556 [1.000, 97.000],  loss: 4.476219, mae: 2.526151, mean_q: 4.752789\n",
      "[32 74 82 26 75 24 50 31 59 51]\n",
      " 42642/50001: episode: 4738, duration: 0.071s, episode steps:   9, steps per second: 127, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 52.444 [24.000, 82.000],  loss: 9.200385, mae: 2.557923, mean_q: 4.773756\n",
      "[84 88 89 42 37 75 14 86  5 28]\n",
      " 42651/50001: episode: 4739, duration: 0.075s, episode steps:   9, steps per second: 119, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 51.556 [5.000, 89.000],  loss: 6.925655, mae: 2.524793, mean_q: 4.757030\n",
      "[90 25 59 24 59 26 62 88 13 64]\n",
      " 42660/50001: episode: 4740, duration: 0.070s, episode steps:   9, steps per second: 128, episode reward: 26.000, mean reward:  2.889 [-10.000,  8.000], mean action: 46.667 [13.000, 88.000],  loss: 4.746619, mae: 2.508538, mean_q: 4.726991\n",
      "[30  4 34 95 88 91 67 46 40 12]\n",
      " 42669/50001: episode: 4741, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 53.000 [4.000, 95.000],  loss: 8.192112, mae: 2.603314, mean_q: 4.858412\n",
      "[73 97 82 21 13 47 89  2 12 41]\n",
      " 42678/50001: episode: 4742, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 43.000, mean reward:  4.778 [ 3.000,  7.000], mean action: 44.889 [2.000, 97.000],  loss: 7.875139, mae: 2.631992, mean_q: 4.889655\n",
      "[16  4 84 34 83 24 36 13 97 32]\n",
      " 42687/50001: episode: 4743, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 42.000, mean reward:  4.667 [ 2.000,  7.000], mean action: 45.222 [4.000, 97.000],  loss: 8.464191, mae: 2.570175, mean_q: 4.795524\n",
      "[48 14 13 24 66 23 89 37 98 93]\n",
      " 42696/50001: episode: 4744, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 50.778 [13.000, 98.000],  loss: 5.204786, mae: 2.544242, mean_q: 4.777044\n",
      "[33 88 58 82 11 21 50 66 14  2]\n",
      " 42705/50001: episode: 4745, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 43.000, mean reward:  4.778 [ 3.000,  7.000], mean action: 43.556 [2.000, 88.000],  loss: 6.648710, mae: 2.650626, mean_q: 4.883378\n",
      "[42 34 98 84  6 63 20 37 96 50]\n",
      " 42714/50001: episode: 4746, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 43.000, mean reward:  4.778 [ 3.000,  9.000], mean action: 54.222 [6.000, 98.000],  loss: 5.615385, mae: 2.656976, mean_q: 4.867099\n",
      "[ 5 93 97 95 37 44 95 74  4 89]\n",
      " 42723/50001: episode: 4747, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 21.000, mean reward:  2.333 [-10.000,  5.000], mean action: 69.778 [4.000, 97.000],  loss: 5.393396, mae: 2.695194, mean_q: 5.003892\n",
      "[34 74 94 75 65 68 24 46 31 34]\n",
      " 42732/50001: episode: 4748, duration: 0.077s, episode steps:   9, steps per second: 116, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 56.778 [24.000, 94.000],  loss: 7.890379, mae: 2.609520, mean_q: 4.908776\n",
      "[13 97 88 40 98 27 31 62 20 57]\n",
      " 42741/50001: episode: 4749, duration: 0.086s, episode steps:   9, steps per second: 104, episode reward: 38.000, mean reward:  4.222 [ 3.000,  7.000], mean action: 57.778 [20.000, 98.000],  loss: 6.194391, mae: 2.666908, mean_q: 4.948321\n",
      "[42 30 95 75 61 27 31 44  4  2]\n",
      " 42750/50001: episode: 4750, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 41.000 [2.000, 95.000],  loss: 9.152882, mae: 2.638423, mean_q: 4.900952\n",
      "[19 50 32 95 63 25 10  4 34 89]\n",
      " 42759/50001: episode: 4751, duration: 0.074s, episode steps:   9, steps per second: 122, episode reward: 40.000, mean reward:  4.444 [ 3.000,  8.000], mean action: 44.667 [4.000, 95.000],  loss: 6.413448, mae: 2.532294, mean_q: 4.681984\n",
      "[93 33 46  2 52 60 34 68 75 79]\n",
      " 42768/50001: episode: 4752, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 49.889 [2.000, 79.000],  loss: 8.924393, mae: 2.520501, mean_q: 4.745909\n",
      "[54 34 54 49 82  2 40 20 34 55]\n",
      " 42777/50001: episode: 4753, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward:  9.000, mean reward:  1.000 [-10.000,  5.000], mean action: 41.111 [2.000, 82.000],  loss: 8.012318, mae: 2.465503, mean_q: 4.563693\n",
      "[86 28 63 68 24 97 40 62 66 50]\n",
      " 42786/50001: episode: 4754, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward: 44.000, mean reward:  4.889 [ 3.000,  9.000], mean action: 55.333 [24.000, 97.000],  loss: 6.670372, mae: 2.448883, mean_q: 4.555547\n",
      "[72 62 34 97 69 20 63  4 95 50]\n",
      " 42795/50001: episode: 4755, duration: 0.084s, episode steps:   9, steps per second: 108, episode reward: 46.000, mean reward:  5.111 [ 3.000,  9.000], mean action: 54.889 [4.000, 97.000],  loss: 4.930757, mae: 2.492709, mean_q: 4.682145\n",
      "[58 96 13 28 10 97  9 44 24 34]\n",
      " 42804/50001: episode: 4756, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 42.000, mean reward:  4.667 [ 3.000,  9.000], mean action: 39.444 [9.000, 97.000],  loss: 7.245551, mae: 2.542033, mean_q: 4.733221\n",
      "[18 44 89  4 28 42 47 66 61 86]\n",
      " 42813/50001: episode: 4757, duration: 0.071s, episode steps:   9, steps per second: 126, episode reward: 36.000, mean reward:  4.000 [ 2.000,  6.000], mean action: 51.889 [4.000, 89.000],  loss: 6.587852, mae: 2.585756, mean_q: 4.775464\n",
      "[81 41 27 13 82 79 97 97 40 50]\n",
      " 42822/50001: episode: 4758, duration: 0.077s, episode steps:   9, steps per second: 116, episode reward: 29.000, mean reward:  3.222 [-10.000,  8.000], mean action: 58.444 [13.000, 97.000],  loss: 5.942428, mae: 2.655493, mean_q: 4.887292\n",
      "[11 66  9 48 81 98 33  2 24 86]\n",
      " 42831/50001: episode: 4759, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 36.000, mean reward:  4.000 [ 3.000,  6.000], mean action: 49.667 [2.000, 98.000],  loss: 6.313868, mae: 2.716823, mean_q: 5.083286\n",
      "[ 5 37 37  4 23 79 51 88 95 50]\n",
      " 42840/50001: episode: 4760, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 29.000, mean reward:  3.222 [-10.000,  8.000], mean action: 51.556 [4.000, 95.000],  loss: 8.524327, mae: 2.704313, mean_q: 5.043149\n",
      "[58 95 97 14 78 14  4  1 34  4]\n",
      " 42849/50001: episode: 4761, duration: 0.081s, episode steps:   9, steps per second: 112, episode reward: 13.000, mean reward:  1.444 [-10.000,  7.000], mean action: 37.889 [1.000, 97.000],  loss: 6.236047, mae: 2.649258, mean_q: 4.901271\n",
      "[61 37 63 90 34 66 65  4 12 28]\n",
      " 42858/50001: episode: 4762, duration: 0.075s, episode steps:   9, steps per second: 120, episode reward: 43.000, mean reward:  4.778 [ 2.000,  7.000], mean action: 44.333 [4.000, 90.000],  loss: 5.715826, mae: 2.583886, mean_q: 4.792736\n",
      "[ 9  4 29 17 88 46 83 13 92 51]\n",
      " 42867/50001: episode: 4763, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 47.000 [4.000, 92.000],  loss: 8.499289, mae: 2.670347, mean_q: 4.992925\n",
      "[81  9 28 21 54 46 71 82 34 12]\n",
      " 42876/50001: episode: 4764, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 44.000, mean reward:  4.889 [ 3.000,  9.000], mean action: 39.667 [9.000, 82.000],  loss: 8.417236, mae: 2.604142, mean_q: 4.873408\n",
      "[67 49 99 56 27 44 66 75 31 68]\n",
      " 42885/50001: episode: 4765, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 57.222 [27.000, 99.000],  loss: 7.923382, mae: 2.568044, mean_q: 4.774889\n",
      "[ 6 12 54 37  3 82 74 84 34 14]\n",
      " 42894/50001: episode: 4766, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 42.000, mean reward:  4.667 [ 2.000,  9.000], mean action: 43.778 [3.000, 84.000],  loss: 4.365333, mae: 2.576049, mean_q: 4.866334\n",
      "[24  1 56 40 99 33 23 66 57 75]\n",
      " 42903/50001: episode: 4767, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 36.000, mean reward:  4.000 [ 3.000,  6.000], mean action: 50.000 [1.000, 99.000],  loss: 9.120600, mae: 2.608045, mean_q: 4.849532\n",
      "[50 24  2 15 62 73 74 97 88 56]\n",
      " 42912/50001: episode: 4768, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 54.556 [2.000, 97.000],  loss: 6.371406, mae: 2.564623, mean_q: 4.788506\n",
      "[18 31 34 24 48 89 95 96 79 96]\n",
      " 42921/50001: episode: 4769, duration: 0.078s, episode steps:   9, steps per second: 116, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 65.778 [24.000, 96.000],  loss: 3.942727, mae: 2.500086, mean_q: 4.666271\n",
      "[45 98 41  2 32  5 47 13 17  6]\n",
      " 42930/50001: episode: 4770, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward: 40.000, mean reward:  4.444 [ 2.000,  6.000], mean action: 29.000 [2.000, 98.000],  loss: 6.969005, mae: 2.565832, mean_q: 4.863137\n",
      "[13 66 34  1  0 99 50 62 93 88]\n",
      " 42939/50001: episode: 4771, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 54.778 [0.000, 99.000],  loss: 7.026636, mae: 2.644204, mean_q: 4.921259\n",
      "[90  4  4 35 12 56 88 66 34 76]\n",
      " 42948/50001: episode: 4772, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 28.000, mean reward:  3.111 [-10.000,  8.000], mean action: 41.667 [4.000, 88.000],  loss: 6.473092, mae: 2.636945, mean_q: 4.937389\n",
      "[77 12 44 73 41 18 11 90 34 40]\n",
      " 42957/50001: episode: 4773, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 41.000, mean reward:  4.556 [ 2.000,  9.000], mean action: 40.333 [11.000, 90.000],  loss: 6.818847, mae: 2.618897, mean_q: 4.903204\n",
      "[31 37 54 69 12 60 66 73 49 24]\n",
      " 42966/50001: episode: 4774, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 37.000, mean reward:  4.111 [ 2.000,  7.000], mean action: 49.333 [12.000, 73.000],  loss: 7.762034, mae: 2.649972, mean_q: 4.915809\n",
      "[95 95 66 95 58 32 63 40 40 13]\n",
      " 42975/50001: episode: 4775, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: -5.000, mean reward: -0.556 [-10.000,  7.000], mean action: 55.778 [13.000, 95.000],  loss: 5.072351, mae: 2.654353, mean_q: 5.066002\n",
      "[22 22 66 36  5 87 96 50 14 13]\n",
      " 42984/50001: episode: 4776, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 26.000, mean reward:  2.889 [-10.000,  9.000], mean action: 43.222 [5.000, 96.000],  loss: 6.895821, mae: 2.672765, mean_q: 4.975604\n",
      "[16 34 28 80 95 97 65 72 37 96]\n",
      " 42993/50001: episode: 4777, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 38.000, mean reward:  4.222 [ 2.000,  8.000], mean action: 67.111 [28.000, 97.000],  loss: 6.392214, mae: 2.658298, mean_q: 4.980229\n",
      "[54 20 54 24 50 67 24  3 26 13]\n",
      " 43002/50001: episode: 4778, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 13.000, mean reward:  1.444 [-10.000, 10.000], mean action: 31.222 [3.000, 67.000],  loss: 7.586641, mae: 2.617728, mean_q: 4.874724\n",
      "[81 51 11  1 74 46 27 33 93 89]\n",
      " 43011/50001: episode: 4779, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 37.000, mean reward:  4.111 [ 3.000,  6.000], mean action: 47.222 [1.000, 93.000],  loss: 8.821293, mae: 2.549764, mean_q: 4.781340\n",
      "[20 34 62 42 97 18 23 46 40 16]\n",
      " 43020/50001: episode: 4780, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 36.000, mean reward:  4.000 [ 3.000,  5.000], mean action: 42.000 [16.000, 97.000],  loss: 7.407046, mae: 2.519552, mean_q: 4.793533\n",
      "[95 28 99 30 93 42 13 21 12 19]\n",
      " 43029/50001: episode: 4781, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 36.000, mean reward:  4.000 [ 2.000,  7.000], mean action: 39.667 [12.000, 99.000],  loss: 8.580627, mae: 2.456501, mean_q: 4.697955\n",
      "[18 21  9 84  3 37 35 82 88 31]\n",
      " 43038/50001: episode: 4782, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 43.000, mean reward:  4.778 [ 3.000,  8.000], mean action: 43.333 [3.000, 88.000],  loss: 7.865162, mae: 2.477536, mean_q: 4.692417\n",
      "[51 51 31 28 41 41 95 50 24 99]\n",
      " 43047/50001: episode: 4783, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 15.000, mean reward:  1.667 [-10.000,  7.000], mean action: 51.111 [24.000, 99.000],  loss: 7.178835, mae: 2.459685, mean_q: 4.607027\n",
      "[38  6 41 79 82 60 37  5 50 50]\n",
      " 43056/50001: episode: 4784, duration: 0.081s, episode steps:   9, steps per second: 111, episode reward: 30.000, mean reward:  3.333 [-10.000,  7.000], mean action: 45.556 [5.000, 82.000],  loss: 8.429546, mae: 2.471382, mean_q: 4.684882\n",
      "[55 87 58 11 88 89 99  8  5 52]\n",
      " 43065/50001: episode: 4785, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 36.000, mean reward:  4.000 [ 3.000,  5.000], mean action: 55.222 [5.000, 99.000],  loss: 7.315187, mae: 2.522789, mean_q: 4.712519\n",
      "[87 97 23 34  2 13  1 35  5 19]\n",
      " 43074/50001: episode: 4786, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 36.000, mean reward:  4.000 [ 3.000,  5.000], mean action: 25.444 [1.000, 97.000],  loss: 7.358744, mae: 2.509897, mean_q: 4.674658\n",
      "[75 91 13 48 30 98 60 88 41 34]\n",
      " 43083/50001: episode: 4787, duration: 0.074s, episode steps:   9, steps per second: 122, episode reward: 46.000, mean reward:  5.111 [ 3.000,  9.000], mean action: 55.889 [13.000, 98.000],  loss: 9.066509, mae: 2.507885, mean_q: 4.721585\n",
      "[96 11 69 27 11 68 28 95 90 12]\n",
      " 43092/50001: episode: 4788, duration: 0.097s, episode steps:   9, steps per second:  93, episode reward: 29.000, mean reward:  3.222 [-10.000,  8.000], mean action: 45.667 [11.000, 95.000],  loss: 6.676189, mae: 2.462085, mean_q: 4.565192\n",
      "[20 14 30 14 37 31  5 95 85 41]\n",
      " 43101/50001: episode: 4789, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 39.111 [5.000, 95.000],  loss: 5.398695, mae: 2.465377, mean_q: 4.665830\n",
      "[47 24  9 31 37 82  1 83 52 35]\n",
      " 43110/50001: episode: 4790, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 35.000, mean reward:  3.889 [ 3.000,  5.000], mean action: 39.333 [1.000, 83.000],  loss: 8.367264, mae: 2.543547, mean_q: 4.827319\n",
      "[51 34 24 49 77 44 11 24 58 75]\n",
      " 43119/50001: episode: 4791, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 44.000 [11.000, 77.000],  loss: 8.309051, mae: 2.538377, mean_q: 4.780882\n",
      "[92 28 35 98 16 24 24 67 50 12]\n",
      " 43128/50001: episode: 4792, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 28.000, mean reward:  3.111 [-10.000,  8.000], mean action: 39.333 [12.000, 98.000],  loss: 7.801888, mae: 2.569848, mean_q: 4.795218\n",
      "[19 74 97 66 12 95 50 79 13 10]\n",
      " 43137/50001: episode: 4793, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 41.000, mean reward:  4.556 [ 3.000,  6.000], mean action: 55.111 [10.000, 97.000],  loss: 7.874880, mae: 2.470390, mean_q: 4.630681\n",
      "[98  8 24 32 88 91 30 37 66 46]\n",
      " 43146/50001: episode: 4794, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 44.000, mean reward:  4.889 [ 3.000,  7.000], mean action: 46.889 [8.000, 91.000],  loss: 8.067890, mae: 2.487592, mean_q: 4.690619\n",
      "[68  2 32 69 37 85 23 24 95 75]\n",
      " 43155/50001: episode: 4795, duration: 0.079s, episode steps:   9, steps per second: 114, episode reward: 43.000, mean reward:  4.778 [ 3.000,  9.000], mean action: 49.111 [2.000, 95.000],  loss: 6.918550, mae: 2.504641, mean_q: 4.667655\n",
      "[90 12 10 37 95 24 95 50 52 24]\n",
      " 43164/50001: episode: 4796, duration: 0.075s, episode steps:   9, steps per second: 119, episode reward: 11.000, mean reward:  1.222 [-10.000,  6.000], mean action: 44.333 [10.000, 95.000],  loss: 7.285773, mae: 2.456778, mean_q: 4.598222\n",
      "[22 84 90 23 34 21 14 47 32 66]\n",
      " 43173/50001: episode: 4797, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 42.000, mean reward:  4.667 [ 2.000,  7.000], mean action: 45.667 [14.000, 90.000],  loss: 6.187441, mae: 2.532235, mean_q: 4.658138\n",
      "[71 36 75 82 90 60 80 93 13  9]\n",
      " 43182/50001: episode: 4798, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 43.000, mean reward:  4.778 [ 2.000,  8.000], mean action: 59.778 [9.000, 93.000],  loss: 6.849881, mae: 2.580443, mean_q: 4.821761\n",
      "[45 31 13 67  2  9 81  4 96 12]\n",
      " 43191/50001: episode: 4799, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 40.000, mean reward:  4.444 [ 3.000,  8.000], mean action: 35.000 [2.000, 96.000],  loss: 4.675114, mae: 2.598655, mean_q: 4.833010\n",
      "[74 30 88 82 75 87 27  6 34 26]\n",
      " 43200/50001: episode: 4800, duration: 0.068s, episode steps:   9, steps per second: 133, episode reward: 38.000, mean reward:  4.222 [ 2.000,  9.000], mean action: 50.556 [6.000, 88.000],  loss: 8.313831, mae: 2.549991, mean_q: 4.820035\n",
      "[97 13 31 80 37 88 88 63 50 32]\n",
      " 43209/50001: episode: 4801, duration: 0.072s, episode steps:   9, steps per second: 125, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 53.556 [13.000, 88.000],  loss: 6.872447, mae: 2.643865, mean_q: 4.862807\n",
      "[47 14 55 41 33 73 72  9 97 97]\n",
      " 43218/50001: episode: 4802, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward: 19.000, mean reward:  2.111 [-10.000,  8.000], mean action: 54.556 [9.000, 97.000],  loss: 7.031404, mae: 2.603705, mean_q: 4.866768\n",
      "[ 0 68 66  2 99 66 34 83 12 92]\n",
      " 43227/50001: episode: 4803, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 58.000 [2.000, 99.000],  loss: 5.973345, mae: 2.646181, mean_q: 4.882492\n",
      "[43  2 60 27 96 59 11 50 31 79]\n",
      " 43236/50001: episode: 4804, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 43.000, mean reward:  4.778 [ 3.000,  7.000], mean action: 46.111 [2.000, 96.000],  loss: 6.982461, mae: 2.625964, mean_q: 4.897269\n",
      "[16 88  2 64 54 50 37 43 92 47]\n",
      " 43245/50001: episode: 4805, duration: 0.082s, episode steps:   9, steps per second: 110, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 53.000 [2.000, 92.000],  loss: 7.054198, mae: 2.643185, mean_q: 4.943343\n",
      "[13 51 37 30  1 98 53 50 27 92]\n",
      " 43254/50001: episode: 4806, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 37.000, mean reward:  4.111 [ 3.000,  6.000], mean action: 48.778 [1.000, 98.000],  loss: 8.077450, mae: 2.589529, mean_q: 4.762190\n",
      "[84 85 95  1 34  8 24 85  2  4]\n",
      " 43263/50001: episode: 4807, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 27.000, mean reward:  3.000 [-10.000,  6.000], mean action: 37.556 [1.000, 95.000],  loss: 6.816493, mae: 2.589247, mean_q: 4.868590\n",
      "[87 51 13 14 32 66  4 46 84 79]\n",
      " 43272/50001: episode: 4808, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 43.222 [4.000, 84.000],  loss: 5.946546, mae: 2.635758, mean_q: 4.854122\n",
      "[89 96 31 25 29 82  8 18 82 57]\n",
      " 43281/50001: episode: 4809, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 20.000, mean reward:  2.222 [-10.000,  7.000], mean action: 47.556 [8.000, 96.000],  loss: 8.152877, mae: 2.647843, mean_q: 4.967310\n",
      "[59 89 60 12 82 28 63 46 31 40]\n",
      " 43290/50001: episode: 4810, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 42.000, mean reward:  4.667 [ 3.000,  6.000], mean action: 50.111 [12.000, 89.000],  loss: 7.158200, mae: 2.616017, mean_q: 4.827852\n",
      "[31 11 84 86  2 62 97 57 97 97]\n",
      " 43299/50001: episode: 4811, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward:  9.000, mean reward:  1.000 [-10.000,  6.000], mean action: 65.889 [2.000, 97.000],  loss: 8.176863, mae: 2.561624, mean_q: 4.816943\n",
      "[75 14 97 93 37 80 37 67  9 13]\n",
      " 43308/50001: episode: 4812, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 24.000, mean reward:  2.667 [-10.000,  8.000], mean action: 49.667 [9.000, 97.000],  loss: 8.109690, mae: 2.470943, mean_q: 4.671952\n",
      "[88 60 39 53 90  6  2  2 56  9]\n",
      " 43317/50001: episode: 4813, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 35.222 [2.000, 90.000],  loss: 6.280544, mae: 2.483824, mean_q: 4.690054\n",
      "[31 35 53 23 78 95 30  2 37 60]\n",
      " 43326/50001: episode: 4814, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 45.889 [2.000, 95.000],  loss: 8.175307, mae: 2.493537, mean_q: 4.647880\n",
      "[80  2 13 47 89  1 59 13 28 31]\n",
      " 43335/50001: episode: 4815, duration: 0.085s, episode steps:   9, steps per second: 106, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 31.444 [1.000, 89.000],  loss: 8.259476, mae: 2.503842, mean_q: 4.695121\n",
      "[76 68 75 15 23 52 50 22 60 14]\n",
      " 43344/50001: episode: 4816, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 42.111 [14.000, 75.000],  loss: 7.462715, mae: 2.534638, mean_q: 4.796163\n",
      "[78 70 31 34 34 27 77 88 76 38]\n",
      " 43353/50001: episode: 4817, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 52.778 [27.000, 88.000],  loss: 4.491220, mae: 2.534325, mean_q: 4.783711\n",
      "[54 75 64 89  5 95 13 13 24 93]\n",
      " 43362/50001: episode: 4818, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 52.333 [5.000, 95.000],  loss: 7.878464, mae: 2.580245, mean_q: 4.905107\n",
      "[64 31 73 95 32 97 66 96 20 20]\n",
      " 43371/50001: episode: 4819, duration: 0.078s, episode steps:   9, steps per second: 115, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 58.889 [20.000, 97.000],  loss: 5.993387, mae: 2.535578, mean_q: 4.759866\n",
      "[95  2 28 13 62 95 95  3 86 93]\n",
      " 43380/50001: episode: 4820, duration: 0.070s, episode steps:   9, steps per second: 129, episode reward:  6.000, mean reward:  0.667 [-10.000,  6.000], mean action: 53.000 [2.000, 95.000],  loss: 7.260089, mae: 2.545533, mean_q: 4.761304\n",
      "[32 50 14 73 46  2 12 42 73 36]\n",
      " 43389/50001: episode: 4821, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 19.000, mean reward:  2.111 [-10.000,  5.000], mean action: 38.667 [2.000, 73.000],  loss: 6.325178, mae: 2.567576, mean_q: 4.782593\n",
      "[ 2 98 11 63 64 96 48 37  9 27]\n",
      " 43398/50001: episode: 4822, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 38.000, mean reward:  4.222 [ 3.000,  7.000], mean action: 50.333 [9.000, 98.000],  loss: 7.621999, mae: 2.669713, mean_q: 5.024254\n",
      "[81  8 75 95 20 97 83 52 93 64]\n",
      " 43407/50001: episode: 4823, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 37.000, mean reward:  4.111 [ 3.000,  6.000], mean action: 65.222 [8.000, 97.000],  loss: 7.204316, mae: 2.572790, mean_q: 4.877810\n",
      "[ 2 51 82 76 79 96 95  9 34 63]\n",
      " 43416/50001: episode: 4824, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 65.000 [9.000, 96.000],  loss: 8.089468, mae: 2.546071, mean_q: 4.769911\n",
      "[26 95 34 99 92  4  2  2  1 32]\n",
      " 43425/50001: episode: 4825, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 27.000, mean reward:  3.000 [-10.000,  6.000], mean action: 40.111 [1.000, 99.000],  loss: 6.171444, mae: 2.585966, mean_q: 4.817686\n",
      "[17 98 24 44 10 99 95 78 94 32]\n",
      " 43434/50001: episode: 4826, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 63.778 [10.000, 99.000],  loss: 9.185587, mae: 2.484916, mean_q: 4.684064\n",
      "[17 32 85 41 41 99 95 60 27 72]\n",
      " 43443/50001: episode: 4827, duration: 0.069s, episode steps:   9, steps per second: 130, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 61.333 [27.000, 99.000],  loss: 7.417274, mae: 2.424086, mean_q: 4.552691\n",
      "[26 68  8 47 28 88  1 31 53 51]\n",
      " 43452/50001: episode: 4828, duration: 0.069s, episode steps:   9, steps per second: 130, episode reward: 40.000, mean reward:  4.444 [ 3.000,  5.000], mean action: 41.667 [1.000, 88.000],  loss: 7.379724, mae: 2.429733, mean_q: 4.582351\n",
      "[94 90 10 98 88 95 52 31 95 50]\n",
      " 43461/50001: episode: 4829, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 28.000, mean reward:  3.111 [-10.000,  7.000], mean action: 67.667 [10.000, 98.000],  loss: 6.085425, mae: 2.443458, mean_q: 4.592204\n",
      "[56 26 85 95 40 76 24 97 24 98]\n",
      " 43470/50001: episode: 4830, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 62.778 [24.000, 98.000],  loss: 6.729894, mae: 2.492885, mean_q: 4.659501\n",
      "[32 75 37 37 54 13 65 95  5 34]\n",
      " 43479/50001: episode: 4831, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 30.000, mean reward:  3.333 [-10.000,  9.000], mean action: 46.111 [5.000, 95.000],  loss: 7.725306, mae: 2.538030, mean_q: 4.825406\n",
      "[99 28 88  2 98 31  1 59 53 79]\n",
      " 43488/50001: episode: 4832, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 48.778 [1.000, 98.000],  loss: 8.068215, mae: 2.583826, mean_q: 4.820031\n",
      "[58  0 24 32 95 76 28 66 47 17]\n",
      " 43497/50001: episode: 4833, duration: 0.067s, episode steps:   9, steps per second: 133, episode reward: 38.000, mean reward:  4.222 [ 2.000,  6.000], mean action: 42.778 [0.000, 95.000],  loss: 4.771970, mae: 2.531533, mean_q: 4.695580\n",
      "[82  1 98 92 46 74 96 14  9 64]\n",
      " 43506/50001: episode: 4834, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 36.000, mean reward:  4.000 [ 3.000,  5.000], mean action: 54.889 [1.000, 98.000],  loss: 6.975685, mae: 2.558785, mean_q: 4.819145\n",
      "[72 27 32 50 33  8 13 42 81 79]\n",
      " 43515/50001: episode: 4835, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 40.556 [8.000, 81.000],  loss: 7.032310, mae: 2.619281, mean_q: 4.902335\n",
      "[ 6 91 93 57 83 92 83 95 82 87]\n",
      " 43524/50001: episode: 4836, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 84.778 [57.000, 95.000],  loss: 6.134492, mae: 2.578443, mean_q: 4.833929\n",
      "[31 21 44 37 97 59 37  4 12  4]\n",
      " 43533/50001: episode: 4837, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 11.000, mean reward:  1.222 [-10.000,  7.000], mean action: 35.000 [4.000, 97.000],  loss: 7.012667, mae: 2.590705, mean_q: 4.825956\n",
      "[47 51 27 32 26 44 50 31  2 13]\n",
      " 43542/50001: episode: 4838, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 42.000, mean reward:  4.667 [ 2.000,  8.000], mean action: 30.667 [2.000, 51.000],  loss: 8.571553, mae: 2.628437, mean_q: 4.894480\n",
      "[94 78  1 53 74 60 63 37 24 26]\n",
      " 43551/50001: episode: 4839, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 46.222 [1.000, 78.000],  loss: 5.036086, mae: 2.631230, mean_q: 4.828442\n",
      "[14 45 11 37 53 13 23 28 88 97]\n",
      " 43560/50001: episode: 4840, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 41.000, mean reward:  4.556 [ 2.000,  7.000], mean action: 43.889 [11.000, 97.000],  loss: 7.049985, mae: 2.656412, mean_q: 4.957975\n",
      "[97 19 64 57 88 48 41 51 85 32]\n",
      " 43569/50001: episode: 4841, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 53.889 [19.000, 88.000],  loss: 6.779799, mae: 2.669893, mean_q: 4.981528\n",
      "[88 60 95 95 93 88 53  5  1 83]\n",
      " 43578/50001: episode: 4842, duration: 0.072s, episode steps:   9, steps per second: 126, episode reward:  9.000, mean reward:  1.000 [-10.000,  6.000], mean action: 63.667 [1.000, 95.000],  loss: 7.352170, mae: 2.587126, mean_q: 4.856430\n",
      "[ 7 52 60 21 96 95 86 63 17  5]\n",
      " 43587/50001: episode: 4843, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 37.000, mean reward:  4.111 [ 3.000,  7.000], mean action: 55.000 [5.000, 96.000],  loss: 6.621443, mae: 2.603003, mean_q: 4.851254\n",
      "[94 34 32 48 16 62 28 13 64 33]\n",
      " 43596/50001: episode: 4844, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 37.000, mean reward:  4.111 [ 3.000,  6.000], mean action: 36.667 [13.000, 64.000],  loss: 7.738707, mae: 2.579992, mean_q: 4.936392\n",
      "[35 13 17 95  2 97 62 13 13 25]\n",
      " 43605/50001: episode: 4845, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward:  6.000, mean reward:  0.667 [-10.000,  6.000], mean action: 37.444 [2.000, 97.000],  loss: 10.768631, mae: 2.541080, mean_q: 4.695830\n",
      "[16 53 42 34  5 32 84 83 79 37]\n",
      " 43614/50001: episode: 4846, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 42.000, mean reward:  4.667 [ 3.000,  9.000], mean action: 49.889 [5.000, 84.000],  loss: 9.565303, mae: 2.501735, mean_q: 4.661897\n",
      "[41  0 31  8 76 37 90 82 31 34]\n",
      " 43623/50001: episode: 4847, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 27.000, mean reward:  3.000 [-10.000,  9.000], mean action: 43.222 [0.000, 90.000],  loss: 6.382165, mae: 2.402172, mean_q: 4.595356\n",
      "[59  5 51 66  0 99 57 13  9  9]\n",
      " 43632/50001: episode: 4848, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 21.000, mean reward:  2.333 [-10.000,  7.000], mean action: 34.333 [0.000, 99.000],  loss: 7.405800, mae: 2.438638, mean_q: 4.605177\n",
      "[43 13 64 46 52 97 37 87 97 12]\n",
      " 43641/50001: episode: 4849, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 27.000, mean reward:  3.000 [-10.000,  8.000], mean action: 56.111 [12.000, 97.000],  loss: 7.884176, mae: 2.432636, mean_q: 4.497370\n",
      "[59 24 34 96 34 37 51 27 30  2]\n",
      " 43650/50001: episode: 4850, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 27.000, mean reward:  3.000 [-10.000,  6.000], mean action: 37.222 [2.000, 96.000],  loss: 7.115459, mae: 2.419357, mean_q: 4.576507\n",
      "[64 93 28 87 23 91 37  9 78 51]\n",
      " 43659/50001: episode: 4851, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 38.000, mean reward:  4.222 [ 3.000,  7.000], mean action: 55.222 [9.000, 93.000],  loss: 8.243732, mae: 2.436042, mean_q: 4.585762\n",
      "[72 19 28 53 85 96 24 73 34 12]\n",
      " 43668/50001: episode: 4852, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 41.000, mean reward:  4.556 [ 2.000,  9.000], mean action: 47.111 [12.000, 96.000],  loss: 7.730025, mae: 2.392197, mean_q: 4.482989\n",
      "[61 34 99 11 69 52 48  2 27 75]\n",
      " 43677/50001: episode: 4853, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 46.333 [2.000, 99.000],  loss: 9.068105, mae: 2.479366, mean_q: 4.702948\n",
      "[19  9 44 22 14 72 86 34 10 88]\n",
      " 43686/50001: episode: 4854, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 41.000, mean reward:  4.556 [ 2.000,  9.000], mean action: 42.111 [9.000, 88.000],  loss: 8.148517, mae: 2.465340, mean_q: 4.606866\n",
      "[ 4 10 34 43 37 14 29 12 29 93]\n",
      " 43695/50001: episode: 4855, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 33.444 [10.000, 93.000],  loss: 8.536250, mae: 2.493758, mean_q: 4.681901\n",
      "[35 53 88 66 95 68 34 60 79 86]\n",
      " 43704/50001: episode: 4856, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 69.889 [34.000, 95.000],  loss: 10.399504, mae: 2.464862, mean_q: 4.674221\n",
      "[58 15 60  9 93 32 34 46 50 79]\n",
      " 43713/50001: episode: 4857, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 44.000, mean reward:  4.889 [ 2.000,  7.000], mean action: 46.444 [9.000, 93.000],  loss: 8.097660, mae: 2.453711, mean_q: 4.624649\n",
      "[39 48  2 32 87 34  4 32 12 16]\n",
      " 43722/50001: episode: 4858, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 29.667 [2.000, 87.000],  loss: 7.051210, mae: 2.448860, mean_q: 4.650963\n",
      "[78 30 86 10 34 29 44 59 48 24]\n",
      " 43731/50001: episode: 4859, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 40.444 [10.000, 86.000],  loss: 7.864695, mae: 2.457895, mean_q: 4.670547\n",
      "[99 31 94 98 50 59 88  2  8 88]\n",
      " 43740/50001: episode: 4860, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 22.000, mean reward:  2.444 [-10.000,  5.000], mean action: 57.556 [2.000, 98.000],  loss: 8.754145, mae: 2.504596, mean_q: 4.736692\n",
      "[65 34 84 78  2 50 42 96 44 12]\n",
      " 43749/50001: episode: 4861, duration: 0.068s, episode steps:   9, steps per second: 133, episode reward: 44.000, mean reward:  4.889 [ 3.000,  9.000], mean action: 49.111 [2.000, 96.000],  loss: 9.291439, mae: 2.469837, mean_q: 4.617581\n",
      "[70 50 13 50  7 59 74 67  2 54]\n",
      " 43758/50001: episode: 4862, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 41.778 [2.000, 74.000],  loss: 7.196797, mae: 2.452313, mean_q: 4.576389\n",
      "[55 32 21 13 77 96 49  5 99 75]\n",
      " 43767/50001: episode: 4863, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 37.000, mean reward:  4.111 [ 3.000,  6.000], mean action: 51.889 [5.000, 99.000],  loss: 9.339776, mae: 2.402770, mean_q: 4.540090\n",
      "[45 20 14 11  6 83 24 33 95 93]\n",
      " 43776/50001: episode: 4864, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 42.000, mean reward:  4.667 [ 3.000, 10.000], mean action: 42.111 [6.000, 95.000],  loss: 8.761227, mae: 2.420451, mean_q: 4.564203\n",
      "[17 60 48 52 45 95 58 24 27 97]\n",
      " 43785/50001: episode: 4865, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 44.000, mean reward:  4.889 [ 2.000,  8.000], mean action: 56.222 [24.000, 97.000],  loss: 7.595749, mae: 2.443264, mean_q: 4.655496\n",
      "[49 19 16 19 31 28 53 92 34 44]\n",
      " 43794/50001: episode: 4866, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 26.000, mean reward:  2.889 [-10.000,  8.000], mean action: 37.333 [16.000, 92.000],  loss: 5.278673, mae: 2.439037, mean_q: 4.584970\n",
      "[37 50 78 78 89 63  2 33 79 57]\n",
      " 43803/50001: episode: 4867, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 26.000, mean reward:  2.889 [-10.000,  6.000], mean action: 58.778 [2.000, 89.000],  loss: 6.103129, mae: 2.518907, mean_q: 4.784444\n",
      "[36 24 41 25 79 13 92 28 41 64]\n",
      " 43812/50001: episode: 4868, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 27.000, mean reward:  3.000 [-10.000,  6.000], mean action: 45.222 [13.000, 92.000],  loss: 7.908191, mae: 2.546304, mean_q: 4.814895\n",
      "[61 88 41 53 84 53  2  1 62 99]\n",
      " 43821/50001: episode: 4869, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 53.667 [1.000, 99.000],  loss: 7.666781, mae: 2.564731, mean_q: 4.753132\n",
      "[21  4 88 98 47 20 50 75 79 13]\n",
      " 43830/50001: episode: 4870, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward: 44.000, mean reward:  4.889 [ 3.000,  8.000], mean action: 52.667 [4.000, 98.000],  loss: 9.753763, mae: 2.596539, mean_q: 4.866210\n",
      "[32 60 26 34 90 34 28 46 90 90]\n",
      " 43839/50001: episode: 4871, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: -4.000, mean reward: -0.444 [-10.000,  6.000], mean action: 55.333 [26.000, 90.000],  loss: 10.141171, mae: 2.437168, mean_q: 4.603835\n",
      "[87 34 84 66 62 99 92 31 37 57]\n",
      " 43848/50001: episode: 4872, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 62.444 [31.000, 99.000],  loss: 9.934167, mae: 2.400487, mean_q: 4.552916\n",
      "[56 37 47 93 50  6 76 34 13 93]\n",
      " 43857/50001: episode: 4873, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 29.000, mean reward:  3.222 [-10.000,  8.000], mean action: 49.889 [6.000, 93.000],  loss: 5.595278, mae: 2.352632, mean_q: 4.433188\n",
      "[27 83 58 54 98 48 97 71  6 12]\n",
      " 43866/50001: episode: 4874, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 40.000, mean reward:  4.444 [ 2.000,  9.000], mean action: 58.556 [6.000, 98.000],  loss: 9.929596, mae: 2.417646, mean_q: 4.492856\n",
      "[32 24 21 25 59 16 27 95  2 96]\n",
      " 43875/50001: episode: 4875, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 40.000, mean reward:  4.444 [ 3.000,  9.000], mean action: 40.556 [2.000, 96.000],  loss: 9.129663, mae: 2.395210, mean_q: 4.478632\n",
      "[52  2 73 47 26 99 87 50  5 64]\n",
      " 43884/50001: episode: 4876, duration: 0.065s, episode steps:   9, steps per second: 137, episode reward: 36.000, mean reward:  4.000 [ 2.000,  7.000], mean action: 50.333 [2.000, 99.000],  loss: 7.853046, mae: 2.429219, mean_q: 4.549892\n",
      "[63 28 28 35 89 46 83 55 98 87]\n",
      " 43893/50001: episode: 4877, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 61.000 [28.000, 98.000],  loss: 5.528961, mae: 2.419770, mean_q: 4.548542\n",
      "[66 41 72 89 25 66 34 86 99 32]\n",
      " 43902/50001: episode: 4878, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 60.444 [25.000, 99.000],  loss: 5.071378, mae: 2.547953, mean_q: 4.749826\n",
      "[13 41 44 12 66 91 23 42 91 57]\n",
      " 43911/50001: episode: 4879, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 51.889 [12.000, 91.000],  loss: 7.250566, mae: 2.554590, mean_q: 4.730001\n",
      "[51 82 37 35 76 86 86 15 99 63]\n",
      " 43920/50001: episode: 4880, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 64.333 [15.000, 99.000],  loss: 5.659435, mae: 2.635554, mean_q: 4.851468\n",
      "[50 41 90 95 37 46 47 58 66  4]\n",
      " 43929/50001: episode: 4881, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 42.000, mean reward:  4.667 [ 4.000,  6.000], mean action: 53.778 [4.000, 95.000],  loss: 7.641440, mae: 2.667754, mean_q: 4.910824\n",
      "[ 4 95 13 31 92 96  6 75 74 64]\n",
      " 43938/50001: episode: 4882, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 37.000, mean reward:  4.111 [ 3.000,  6.000], mean action: 60.667 [6.000, 96.000],  loss: 5.844763, mae: 2.597395, mean_q: 4.767007\n",
      "[27 37 60  1 78 31 66 89 10  4]\n",
      " 43947/50001: episode: 4883, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 40.000, mean reward:  4.444 [ 3.000,  6.000], mean action: 41.778 [1.000, 89.000],  loss: 6.290738, mae: 2.660049, mean_q: 4.962168\n",
      "[92 69 57 27  2 40 60 13  4 27]\n",
      " 43956/50001: episode: 4884, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 33.222 [2.000, 69.000],  loss: 5.231357, mae: 2.675145, mean_q: 5.024122\n",
      "[34 88 12 31 64 86 73 82 34 50]\n",
      " 43965/50001: episode: 4885, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 23.000, mean reward:  2.556 [-10.000,  8.000], mean action: 57.778 [12.000, 88.000],  loss: 5.683680, mae: 2.696085, mean_q: 5.006681\n",
      "[53  8 94 13 79  1 37 71 32 76]\n",
      " 43974/50001: episode: 4886, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 37.000, mean reward:  4.111 [ 1.000,  6.000], mean action: 45.667 [1.000, 94.000],  loss: 7.587137, mae: 2.696814, mean_q: 5.025445\n",
      "[31 95 32 97 24 88 24 32 42 84]\n",
      " 43983/50001: episode: 4887, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward:  9.000, mean reward:  1.000 [-10.000,  5.000], mean action: 57.556 [24.000, 97.000],  loss: 7.750536, mae: 2.689673, mean_q: 5.021312\n",
      "[12 62 79 56 98 37 49 14  4 87]\n",
      " 43992/50001: episode: 4888, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward: 37.000, mean reward:  4.111 [ 3.000,  6.000], mean action: 54.000 [4.000, 98.000],  loss: 7.863790, mae: 2.593091, mean_q: 4.815765\n",
      "[10  1 12 18 60 60 55  3 50 46]\n",
      " 44001/50001: episode: 4889, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 22.000, mean reward:  2.444 [-10.000,  8.000], mean action: 33.889 [1.000, 60.000],  loss: 8.910963, mae: 2.450627, mean_q: 4.572113\n",
      "[29 57 28 65 86 46 48 52 40 64]\n",
      " 44010/50001: episode: 4890, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 38.000, mean reward:  4.222 [ 2.000,  6.000], mean action: 54.000 [28.000, 86.000],  loss: 5.260050, mae: 2.464098, mean_q: 4.614414\n",
      "[65 54 13 76  1 95 13 30 50 85]\n",
      " 44019/50001: episode: 4891, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 28.000, mean reward:  3.111 [-10.000,  7.000], mean action: 46.333 [1.000, 95.000],  loss: 7.212224, mae: 2.427220, mean_q: 4.575290\n",
      "[95 12 49 21 79 77 84 64 40 28]\n",
      " 44028/50001: episode: 4892, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 35.000, mean reward:  3.889 [ 2.000,  8.000], mean action: 50.444 [12.000, 84.000],  loss: 7.075361, mae: 2.533494, mean_q: 4.725557\n",
      "[50  1 32 95 56 49 50 37 40 12]\n",
      " 44037/50001: episode: 4893, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 41.333 [1.000, 95.000],  loss: 5.646292, mae: 2.544566, mean_q: 4.775625\n",
      "[81  1 66 77 88 97 50 50 37 50]\n",
      " 44046/50001: episode: 4894, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 12.000, mean reward:  1.333 [-10.000,  6.000], mean action: 57.333 [1.000, 97.000],  loss: 7.852357, mae: 2.582278, mean_q: 4.833531\n",
      "[67  5 44 97 62 13 37 93 68 54]\n",
      " 44055/50001: episode: 4895, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 37.000, mean reward:  4.111 [ 2.000,  6.000], mean action: 52.556 [5.000, 97.000],  loss: 7.009479, mae: 2.620252, mean_q: 4.924965\n",
      "[35 24 31 90 93 46  1 86 88 97]\n",
      " 44064/50001: episode: 4896, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 44.000, mean reward:  4.889 [ 3.000,  7.000], mean action: 61.778 [1.000, 97.000],  loss: 6.303299, mae: 2.596454, mean_q: 4.909042\n",
      "[95 50 42 97 74 96 78 96 14 12]\n",
      " 44073/50001: episode: 4897, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward: 25.000, mean reward:  2.778 [-10.000,  9.000], mean action: 62.111 [12.000, 97.000],  loss: 6.263005, mae: 2.549452, mean_q: 4.717887\n",
      "[46 98 98 84 89 19 99 88 34  4]\n",
      " 44082/50001: episode: 4898, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 30.000, mean reward:  3.333 [-10.000,  8.000], mean action: 68.111 [4.000, 99.000],  loss: 6.299467, mae: 2.609893, mean_q: 4.926276\n",
      "[80 45 76 46 95 99 37  3 93 46]\n",
      " 44091/50001: episode: 4899, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 60.000 [3.000, 99.000],  loss: 8.099220, mae: 2.626158, mean_q: 4.883036\n",
      "[95 37 83 80 88 63 53  2 12  5]\n",
      " 44100/50001: episode: 4900, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 47.000 [2.000, 88.000],  loss: 7.293798, mae: 2.575857, mean_q: 4.835853\n",
      "[ 2 67 62 67 88 95 86 40 28 66]\n",
      " 44109/50001: episode: 4901, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 31.000, mean reward:  3.444 [-10.000,  8.000], mean action: 66.556 [28.000, 95.000],  loss: 7.816871, mae: 2.595359, mean_q: 4.792897\n",
      "[45 41 32  2 56 95 49 10 50  4]\n",
      " 44118/50001: episode: 4902, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 44.000, mean reward:  4.889 [ 3.000,  8.000], mean action: 37.667 [2.000, 95.000],  loss: 8.338459, mae: 2.577941, mean_q: 4.914012\n",
      "[ 5 90 76 69 14 31 94 88 60 48]\n",
      " 44127/50001: episode: 4903, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward: 43.000, mean reward:  4.778 [ 3.000,  8.000], mean action: 63.333 [14.000, 94.000],  loss: 7.545283, mae: 2.582377, mean_q: 4.873988\n",
      "[93 50  1 57 27 67 37 95 81 54]\n",
      " 44136/50001: episode: 4904, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 52.111 [1.000, 95.000],  loss: 9.261292, mae: 2.521152, mean_q: 4.656007\n",
      "[ 9 50 36 42 34 24 48 42 73 64]\n",
      " 44145/50001: episode: 4905, duration: 0.069s, episode steps:   9, steps per second: 130, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 45.889 [24.000, 73.000],  loss: 3.955849, mae: 2.438946, mean_q: 4.512999\n",
      "[40 23 66 13 86 51 94 30  3 30]\n",
      " 44154/50001: episode: 4906, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 18.000, mean reward:  2.000 [-10.000,  5.000], mean action: 44.000 [3.000, 94.000],  loss: 8.546302, mae: 2.512720, mean_q: 4.671486\n",
      "[72 98 60 44 90 42 76  2 96 32]\n",
      " 44163/50001: episode: 4907, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 60.000 [2.000, 98.000],  loss: 6.325335, mae: 2.539948, mean_q: 4.713900\n",
      "[18  4 34 55 61 97 74 73 30 88]\n",
      " 44172/50001: episode: 4908, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 40.000, mean reward:  4.444 [ 2.000,  9.000], mean action: 57.333 [4.000, 97.000],  loss: 8.946277, mae: 2.495938, mean_q: 4.624651\n",
      "[70 46 85 85  1 95 32 97 32 23]\n",
      " 44181/50001: episode: 4909, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 15.000, mean reward:  1.667 [-10.000,  7.000], mean action: 55.111 [1.000, 97.000],  loss: 7.147657, mae: 2.546069, mean_q: 4.724349\n",
      "[ 6 36 17 75 13 14 76 21 65 48]\n",
      " 44190/50001: episode: 4910, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 39.000, mean reward:  4.333 [ 2.000,  9.000], mean action: 40.556 [13.000, 76.000],  loss: 7.610792, mae: 2.608390, mean_q: 4.823348\n",
      "[53 32 24 13 68 94 73 28 31 75]\n",
      " 44199/50001: episode: 4911, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 39.000, mean reward:  4.333 [ 2.000,  6.000], mean action: 48.667 [13.000, 94.000],  loss: 9.456759, mae: 2.536416, mean_q: 4.713042\n",
      "[ 4 96 76 84  4 28 95 88 37 41]\n",
      " 44208/50001: episode: 4912, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 28.000, mean reward:  3.111 [-10.000,  7.000], mean action: 61.000 [4.000, 96.000],  loss: 9.926762, mae: 2.556645, mean_q: 4.738025\n",
      "[ 7 46 37  6 18 41 48 73 31 16]\n",
      " 44217/50001: episode: 4913, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 35.111 [6.000, 73.000],  loss: 7.594094, mae: 2.398572, mean_q: 4.493596\n",
      "[98 27 28 97 27 97 94 37 14 27]\n",
      " 44226/50001: episode: 4914, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: -4.000, mean reward: -0.444 [-10.000,  7.000], mean action: 49.778 [14.000, 97.000],  loss: 7.637562, mae: 2.432954, mean_q: 4.548124\n",
      "[76  2 37 89 50 30 51 50  1 12]\n",
      " 44235/50001: episode: 4915, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 29.000, mean reward:  3.222 [-10.000,  8.000], mean action: 35.778 [1.000, 89.000],  loss: 6.796404, mae: 2.516053, mean_q: 4.653465\n",
      "[91 46 18 40 28 88 50 63 31 24]\n",
      " 44244/50001: episode: 4916, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 43.000, mean reward:  4.778 [ 2.000,  6.000], mean action: 43.111 [18.000, 88.000],  loss: 7.188505, mae: 2.560495, mean_q: 4.821596\n",
      "[12 98 95 28 88 24 82 88 40 52]\n",
      " 44253/50001: episode: 4917, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 25.000, mean reward:  2.778 [-10.000,  5.000], mean action: 66.111 [24.000, 98.000],  loss: 7.325451, mae: 2.550702, mean_q: 4.760683\n",
      "[94 34 54 13 57 67 82  1 95 83]\n",
      " 44262/50001: episode: 4918, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 43.000, mean reward:  4.778 [ 3.000,  9.000], mean action: 54.000 [1.000, 95.000],  loss: 5.894034, mae: 2.563162, mean_q: 4.744886\n",
      "[74 10 44 99 47 48 95 64 34 89]\n",
      " 44271/50001: episode: 4919, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 58.889 [10.000, 99.000],  loss: 9.334388, mae: 2.505691, mean_q: 4.615084\n",
      "[34 27 30 59 93 70 37 73 79 50]\n",
      " 44280/50001: episode: 4920, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 41.000, mean reward:  4.556 [ 2.000,  9.000], mean action: 57.556 [27.000, 93.000],  loss: 5.751281, mae: 2.456346, mean_q: 4.566984\n",
      "[54  5 88 34 87 75  1 97 57 79]\n",
      " 44289/50001: episode: 4921, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 41.000, mean reward:  4.556 [ 3.000,  6.000], mean action: 58.111 [1.000, 97.000],  loss: 8.164718, mae: 2.534693, mean_q: 4.730895\n",
      "[61 41 60 53 30 68 24 74 79 12]\n",
      " 44298/50001: episode: 4922, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 46.000, mean reward:  5.111 [ 4.000,  9.000], mean action: 49.000 [12.000, 79.000],  loss: 7.937283, mae: 2.554241, mean_q: 4.816147\n",
      "[82 37 46 97 50 69 74 62 27 55]\n",
      " 44307/50001: episode: 4923, duration: 0.065s, episode steps:   9, steps per second: 137, episode reward: 36.000, mean reward:  4.000 [ 3.000,  6.000], mean action: 57.444 [27.000, 97.000],  loss: 6.211121, mae: 2.523415, mean_q: 4.678345\n",
      "[98 91 77  8 96 14 14 41 56 47]\n",
      " 44316/50001: episode: 4924, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 49.333 [8.000, 96.000],  loss: 7.164042, mae: 2.541881, mean_q: 4.723626\n",
      "[38 28 46 28 92 82 97 97 99 50]\n",
      " 44325/50001: episode: 4925, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 14.000, mean reward:  1.556 [-10.000,  8.000], mean action: 68.778 [28.000, 99.000],  loss: 6.272842, mae: 2.595376, mean_q: 4.778348\n",
      "[42 37 52  6 52 14 74 39 13 49]\n",
      " 44334/50001: episode: 4926, duration: 0.073s, episode steps:   9, steps per second: 123, episode reward: 23.000, mean reward:  2.556 [-10.000,  8.000], mean action: 37.333 [6.000, 74.000],  loss: 7.358112, mae: 2.591987, mean_q: 4.839650\n",
      "[47  0 93 95 83 82 50 76 27 31]\n",
      " 44343/50001: episode: 4927, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 59.667 [0.000, 95.000],  loss: 7.157645, mae: 2.609746, mean_q: 4.841349\n",
      "[19 83 66 85 82 56 66 10 69 74]\n",
      " 44352/50001: episode: 4928, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 65.667 [10.000, 85.000],  loss: 7.298360, mae: 2.589511, mean_q: 4.767263\n",
      "[69 51 19  8 95 23  4 16 46 33]\n",
      " 44361/50001: episode: 4929, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 37.000, mean reward:  4.111 [ 3.000,  6.000], mean action: 32.778 [4.000, 95.000],  loss: 7.550741, mae: 2.514957, mean_q: 4.707877\n",
      "[64 34 80  8 53 59 95  9 88 50]\n",
      " 44370/50001: episode: 4930, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 44.000, mean reward:  4.889 [ 2.000,  8.000], mean action: 52.889 [8.000, 95.000],  loss: 7.263689, mae: 2.529650, mean_q: 4.689119\n",
      "[85 52 10 95 32 56 42 50 35 66]\n",
      " 44379/50001: episode: 4931, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 48.667 [10.000, 95.000],  loss: 8.846447, mae: 2.600286, mean_q: 4.822566\n",
      "[62 10 37 26 32 34 13 54 32 31]\n",
      " 44388/50001: episode: 4932, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 29.889 [10.000, 54.000],  loss: 8.575630, mae: 2.566083, mean_q: 4.758403\n",
      "[ 7 97 37 73 95 99 49 56 37 48]\n",
      " 44397/50001: episode: 4933, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 65.667 [37.000, 99.000],  loss: 8.521414, mae: 2.460439, mean_q: 4.552339\n",
      "[45 67 28 66 44 53 61 64 24 31]\n",
      " 44406/50001: episode: 4934, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 40.000, mean reward:  4.444 [ 3.000,  8.000], mean action: 48.667 [24.000, 67.000],  loss: 7.798346, mae: 2.477555, mean_q: 4.594431\n",
      "[32 34 34 66 11 53 73 13 56 37]\n",
      " 44415/50001: episode: 4935, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 41.889 [11.000, 73.000],  loss: 7.429669, mae: 2.498315, mean_q: 4.709719\n",
      "[44  9 84 84 41 51 99 98 55 28]\n",
      " 44424/50001: episode: 4936, duration: 0.110s, episode steps:   9, steps per second:  82, episode reward: 26.000, mean reward:  2.889 [-10.000,  8.000], mean action: 61.000 [9.000, 99.000],  loss: 6.377974, mae: 2.529407, mean_q: 4.731659\n",
      "[ 7 24 75 40 74 10 47 98 14 24]\n",
      " 44433/50001: episode: 4937, duration: 0.078s, episode steps:   9, steps per second: 116, episode reward: 27.000, mean reward:  3.000 [-10.000,  6.000], mean action: 45.111 [10.000, 98.000],  loss: 7.690843, mae: 2.565952, mean_q: 4.780294\n",
      "[30 34 93 64 58 57 24  1 30 86]\n",
      " 44442/50001: episode: 4938, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 49.667 [1.000, 93.000],  loss: 6.484625, mae: 2.558436, mean_q: 4.757512\n",
      "[74 24 50 13 98 90 89 66 75 80]\n",
      " 44451/50001: episode: 4939, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 65.000 [13.000, 98.000],  loss: 8.002578, mae: 2.622986, mean_q: 4.870056\n",
      "[69 13 88 75 63 97 13 51 97 75]\n",
      " 44460/50001: episode: 4940, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: -6.000, mean reward: -0.667 [-10.000,  5.000], mean action: 63.556 [13.000, 97.000],  loss: 8.522175, mae: 2.623866, mean_q: 4.863362\n",
      "[74 24 24 99 97 60 41 46 89  8]\n",
      " 44469/50001: episode: 4941, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 23.000, mean reward:  2.556 [-10.000,  5.000], mean action: 54.222 [8.000, 99.000],  loss: 6.437563, mae: 2.544660, mean_q: 4.670896\n",
      "[36 95 50 87 86 97 33 76 64 60]\n",
      " 44478/50001: episode: 4942, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 40.000, mean reward:  4.444 [ 3.000,  8.000], mean action: 72.000 [33.000, 97.000],  loss: 5.604020, mae: 2.540488, mean_q: 4.729039\n",
      "[ 0 95 97 41 74 79 73 74 94 13]\n",
      " 44487/50001: episode: 4943, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 27.000, mean reward:  3.000 [-10.000,  9.000], mean action: 71.111 [13.000, 97.000],  loss: 7.845669, mae: 2.610343, mean_q: 4.845740\n",
      "[58 77 53  1 23 69 12 75 98 47]\n",
      " 44496/50001: episode: 4944, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 50.556 [1.000, 98.000],  loss: 5.261370, mae: 2.582536, mean_q: 4.801311\n",
      "[48 52 23 27 75 96 10 78 57  5]\n",
      " 44505/50001: episode: 4945, duration: 0.103s, episode steps:   9, steps per second:  88, episode reward: 34.000, mean reward:  3.778 [ 3.000,  6.000], mean action: 47.000 [5.000, 96.000],  loss: 7.960532, mae: 2.614208, mean_q: 4.890897\n",
      "[64 23 25 58 41 39  7 74 79 79]\n",
      " 44514/50001: episode: 4946, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 21.000, mean reward:  2.333 [-10.000,  8.000], mean action: 47.222 [7.000, 79.000],  loss: 6.746118, mae: 2.643401, mean_q: 4.906917\n",
      "[74 66 63 31 77 27 27 90 89 50]\n",
      " 44523/50001: episode: 4947, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 27.000, mean reward:  3.000 [-10.000,  9.000], mean action: 57.778 [27.000, 90.000],  loss: 6.898979, mae: 2.658674, mean_q: 4.973251\n",
      "[53 62 46 21 14 63 28 95 28 89]\n",
      " 44532/50001: episode: 4948, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 26.000, mean reward:  2.889 [-10.000,  8.000], mean action: 49.556 [14.000, 95.000],  loss: 5.940996, mae: 2.588231, mean_q: 4.860206\n",
      "[89 91 41 65 27 11  8 63  5 84]\n",
      " 44541/50001: episode: 4949, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 37.000, mean reward:  4.111 [ 2.000,  6.000], mean action: 43.889 [5.000, 91.000],  loss: 10.960438, mae: 2.521550, mean_q: 4.715152\n",
      "[97 96 51  4 28 77 13 49  0 59]\n",
      " 44550/50001: episode: 4950, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 34.000, mean reward:  3.778 [ 2.000,  6.000], mean action: 41.889 [0.000, 96.000],  loss: 6.563335, mae: 2.486981, mean_q: 4.669650\n",
      "[ 3 30  2 38  2 82  9 73 62 82]\n",
      " 44559/50001: episode: 4951, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward:  6.000, mean reward:  0.667 [-10.000,  5.000], mean action: 42.222 [2.000, 82.000],  loss: 10.483935, mae: 2.462088, mean_q: 4.717006\n",
      "[14 51 11 24 58 92 16 67 52 52]\n",
      " 44568/50001: episode: 4952, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 19.000, mean reward:  2.111 [-10.000,  5.000], mean action: 47.000 [11.000, 92.000],  loss: 8.510925, mae: 2.433897, mean_q: 4.546686\n",
      "[ 2  1 59 14 50 29 50 54 96 87]\n",
      " 44577/50001: episode: 4953, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 18.000, mean reward:  2.000 [-10.000,  5.000], mean action: 48.889 [1.000, 96.000],  loss: 6.687364, mae: 2.425892, mean_q: 4.597353\n",
      "[33 37 35 33 13  9 88 32 30  5]\n",
      " 44586/50001: episode: 4954, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 29.000, mean reward:  3.222 [-10.000,  6.000], mean action: 31.333 [5.000, 88.000],  loss: 7.489576, mae: 2.396423, mean_q: 4.466665\n",
      "[30 30 88 42 74 28 49 84 88 34]\n",
      " 44595/50001: episode: 4955, duration: 0.077s, episode steps:   9, steps per second: 117, episode reward: 14.000, mean reward:  1.556 [-10.000, 10.000], mean action: 57.444 [28.000, 88.000],  loss: 6.422168, mae: 2.455404, mean_q: 4.659560\n",
      "[ 1 14 47 27 33 97 34 76 12 12]\n",
      " 44604/50001: episode: 4956, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 39.111 [12.000, 97.000],  loss: 7.829181, mae: 2.519403, mean_q: 4.709015\n",
      "[44 13 41 28 74 56 28 52 87 19]\n",
      " 44613/50001: episode: 4957, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 21.000, mean reward:  2.333 [-10.000,  5.000], mean action: 44.222 [13.000, 87.000],  loss: 5.724269, mae: 2.554133, mean_q: 4.787769\n",
      "[71 98 85 12 34 28 14 19 75 55]\n",
      " 44622/50001: episode: 4958, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 37.000, mean reward:  4.111 [ 2.000,  6.000], mean action: 46.667 [12.000, 98.000],  loss: 8.557941, mae: 2.488535, mean_q: 4.676282\n",
      "[ 5 20  2  2 69 11 24  9 79  6]\n",
      " 44631/50001: episode: 4959, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 24.667 [2.000, 79.000],  loss: 8.667390, mae: 2.468852, mean_q: 4.623217\n",
      "[51 43 40  1 93  9 30 98 75 48]\n",
      " 44640/50001: episode: 4960, duration: 0.071s, episode steps:   9, steps per second: 127, episode reward: 42.000, mean reward:  4.667 [ 2.000,  8.000], mean action: 48.556 [1.000, 98.000],  loss: 6.991349, mae: 2.453287, mean_q: 4.648581\n",
      "[16 60 63 87 21 69 45 58 98 41]\n",
      " 44649/50001: episode: 4961, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 38.000, mean reward:  4.222 [ 3.000,  7.000], mean action: 60.222 [21.000, 98.000],  loss: 6.395185, mae: 2.499807, mean_q: 4.724878\n",
      "[85 50 56 12 87 82 23 56  1 52]\n",
      " 44658/50001: episode: 4962, duration: 0.110s, episode steps:   9, steps per second:  82, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 46.556 [1.000, 87.000],  loss: 8.213065, mae: 2.530663, mean_q: 4.760272\n",
      "[49 60  6 32 97 34 60 37 67 47]\n",
      " 44667/50001: episode: 4963, duration: 0.073s, episode steps:   9, steps per second: 124, episode reward: 29.000, mean reward:  3.222 [-10.000,  7.000], mean action: 48.889 [6.000, 97.000],  loss: 9.681328, mae: 2.475830, mean_q: 4.630579\n",
      "[44 60 96 90 79  9  9 40 13  1]\n",
      " 44676/50001: episode: 4964, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 30.000, mean reward:  3.333 [-10.000,  8.000], mean action: 44.111 [1.000, 96.000],  loss: 8.290815, mae: 2.406018, mean_q: 4.547457\n",
      "[63 27 44  3 38 34 60 42 23 57]\n",
      " 44685/50001: episode: 4965, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 42.000, mean reward:  4.667 [ 2.000,  7.000], mean action: 36.444 [3.000, 60.000],  loss: 6.607336, mae: 2.449658, mean_q: 4.624578\n",
      "[61 49 16  5 11 49 74 64 16 67]\n",
      " 44694/50001: episode: 4966, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward:  9.000, mean reward:  1.000 [-10.000,  5.000], mean action: 39.000 [5.000, 74.000],  loss: 9.032416, mae: 2.438210, mean_q: 4.584486\n",
      "[57 17 37  8 80  9 31 31 40 40]\n",
      " 44703/50001: episode: 4967, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward:  8.000, mean reward:  0.889 [-10.000,  5.000], mean action: 32.556 [8.000, 80.000],  loss: 7.473722, mae: 2.500310, mean_q: 4.628312\n",
      "[ 8 13 77 40 15 14  1 88 50 65]\n",
      " 44712/50001: episode: 4968, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 37.000, mean reward:  4.111 [ 2.000,  7.000], mean action: 40.333 [1.000, 88.000],  loss: 7.356791, mae: 2.473625, mean_q: 4.613264\n",
      "[45 23 28 33  6  2 54 37 98 64]\n",
      " 44721/50001: episode: 4969, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 39.000, mean reward:  4.333 [ 3.000,  8.000], mean action: 38.333 [2.000, 98.000],  loss: 7.687620, mae: 2.477125, mean_q: 4.646329\n",
      "[71  2 27  6 98 28 34 80 79 79]\n",
      " 44730/50001: episode: 4970, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 48.111 [2.000, 98.000],  loss: 7.918177, mae: 2.545531, mean_q: 4.804354\n",
      "[95 31 68 97 31  2 78 99 13 31]\n",
      " 44739/50001: episode: 4971, duration: 0.068s, episode steps:   9, steps per second: 133, episode reward:  9.000, mean reward:  1.000 [-10.000,  7.000], mean action: 50.000 [2.000, 99.000],  loss: 7.159385, mae: 2.482653, mean_q: 4.681176\n",
      "[56 75  4 13 48 80 31 83  8 47]\n",
      " 44748/50001: episode: 4972, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 36.000, mean reward:  4.000 [ 2.000,  7.000], mean action: 43.222 [4.000, 83.000],  loss: 5.916378, mae: 2.489149, mean_q: 4.729509\n",
      "[24 95  1 89 98 32 59 45 98  5]\n",
      " 44757/50001: episode: 4973, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 58.000 [1.000, 98.000],  loss: 8.272868, mae: 2.562011, mean_q: 4.788005\n",
      "[ 0 70 90 60 73 68 16 96 37 27]\n",
      " 44766/50001: episode: 4974, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 41.000, mean reward:  4.556 [ 2.000,  9.000], mean action: 59.667 [16.000, 96.000],  loss: 7.337502, mae: 2.538935, mean_q: 4.732858\n",
      "[53 30 50 95 99 34 48 52  2 62]\n",
      " 44775/50001: episode: 4975, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 40.000, mean reward:  4.444 [ 3.000,  6.000], mean action: 52.444 [2.000, 99.000],  loss: 5.846486, mae: 2.604128, mean_q: 4.778530\n",
      "[69  9 64 34 69 13 95 76 30 89]\n",
      " 44784/50001: episode: 4976, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 53.222 [9.000, 95.000],  loss: 7.766759, mae: 2.614116, mean_q: 4.854459\n",
      "[41 28 67 34  2 90 28 88 37  9]\n",
      " 44793/50001: episode: 4977, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 42.556 [2.000, 90.000],  loss: 6.729393, mae: 2.608711, mean_q: 4.920436\n",
      "[39 56 41  2 48 37 77 85 52 96]\n",
      " 44802/50001: episode: 4978, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 37.000, mean reward:  4.111 [ 3.000,  6.000], mean action: 54.889 [2.000, 96.000],  loss: 7.952012, mae: 2.512161, mean_q: 4.648427\n",
      "[62 41 47 27 33 73 28 36 28 98]\n",
      " 44811/50001: episode: 4979, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward: 22.000, mean reward:  2.444 [-10.000,  7.000], mean action: 45.667 [27.000, 98.000],  loss: 8.206294, mae: 2.542689, mean_q: 4.721623\n",
      "[89 59 47 95 41 60 88 13 88 51]\n",
      " 44820/50001: episode: 4980, duration: 0.092s, episode steps:   9, steps per second:  97, episode reward: 26.000, mean reward:  2.889 [-10.000,  6.000], mean action: 60.222 [13.000, 95.000],  loss: 6.349225, mae: 2.479242, mean_q: 4.732611\n",
      "[36 13 16 98 93  5 79 85  4 76]\n",
      " 44829/50001: episode: 4981, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 52.111 [4.000, 98.000],  loss: 7.645792, mae: 2.522799, mean_q: 4.722185\n",
      "[71 60 62 24 25 92 50 82  5 40]\n",
      " 44838/50001: episode: 4982, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 40.000, mean reward:  4.444 [ 3.000,  6.000], mean action: 48.889 [5.000, 92.000],  loss: 7.137099, mae: 2.567662, mean_q: 4.834042\n",
      "[16 51 81 82 82 13 98 88 55 31]\n",
      " 44847/50001: episode: 4983, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 64.556 [13.000, 98.000],  loss: 6.129661, mae: 2.538712, mean_q: 4.779665\n",
      "[17 21 16 82 96 60 44 68 38  8]\n",
      " 44856/50001: episode: 4984, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 34.000, mean reward:  3.778 [ 3.000,  5.000], mean action: 48.111 [8.000, 96.000],  loss: 6.292715, mae: 2.617420, mean_q: 4.922355\n",
      "[97 32 86 16 69 18 31 97 98 46]\n",
      " 44865/50001: episode: 4985, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 54.778 [16.000, 98.000],  loss: 4.733892, mae: 2.600156, mean_q: 4.841412\n",
      "[47 58 13  2 25 94 30 69 98 46]\n",
      " 44874/50001: episode: 4986, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 37.000, mean reward:  4.111 [ 2.000,  6.000], mean action: 48.333 [2.000, 98.000],  loss: 8.052387, mae: 2.648931, mean_q: 4.956883\n",
      "[60 95 66 49  9 98 74  1 96 14]\n",
      " 44883/50001: episode: 4987, duration: 0.066s, episode steps:   9, steps per second: 135, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 55.778 [1.000, 98.000],  loss: 7.678696, mae: 2.623830, mean_q: 4.946592\n",
      "[35 26 11 69 41 16 57 57  5  4]\n",
      " 44892/50001: episode: 4988, duration: 0.075s, episode steps:   9, steps per second: 121, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 31.778 [4.000, 69.000],  loss: 4.897252, mae: 2.656533, mean_q: 4.922354\n",
      "[14 16 93 32 31 87 30 66 56 83]\n",
      " 44901/50001: episode: 4989, duration: 0.075s, episode steps:   9, steps per second: 121, episode reward: 37.000, mean reward:  4.111 [ 3.000,  6.000], mean action: 54.889 [16.000, 93.000],  loss: 7.250307, mae: 2.585889, mean_q: 4.828573\n",
      "[63 41  6 28 56  4 49 50 32 46]\n",
      " 44910/50001: episode: 4990, duration: 0.067s, episode steps:   9, steps per second: 133, episode reward: 44.000, mean reward:  4.889 [ 3.000,  7.000], mean action: 34.667 [4.000, 56.000],  loss: 7.980203, mae: 2.529536, mean_q: 4.724176\n",
      "[89 48 69 11 55 66 34 12 31 49]\n",
      " 44919/50001: episode: 4991, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 38.000, mean reward:  4.222 [ 3.000,  7.000], mean action: 41.667 [11.000, 69.000],  loss: 6.389063, mae: 2.551256, mean_q: 4.753536\n",
      "[10 37 13 34 50 53 83 31 46 57]\n",
      " 44928/50001: episode: 4992, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 41.000, mean reward:  4.556 [ 3.000,  6.000], mean action: 44.889 [13.000, 83.000],  loss: 9.857709, mae: 2.548736, mean_q: 4.708461\n",
      "[69 40 36  4 86 49  6 53 88 85]\n",
      " 44937/50001: episode: 4993, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 37.000, mean reward:  4.111 [ 2.000,  8.000], mean action: 49.667 [4.000, 88.000],  loss: 7.369654, mae: 2.509808, mean_q: 4.693907\n",
      "[60 64 79 25 88 27 64 32 40 19]\n",
      " 44946/50001: episode: 4994, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 48.667 [19.000, 88.000],  loss: 6.495774, mae: 2.600881, mean_q: 4.837012\n",
      "[38  6  2 64 26 16  8  7 97 46]\n",
      " 44955/50001: episode: 4995, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 37.000, mean reward:  4.111 [ 2.000,  8.000], mean action: 30.222 [2.000, 97.000],  loss: 6.934362, mae: 2.608617, mean_q: 4.816029\n",
      "[80  9 66 18 24 26 38  8 53 85]\n",
      " 44964/50001: episode: 4996, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 34.000, mean reward:  3.778 [ 2.000,  6.000], mean action: 36.333 [8.000, 85.000],  loss: 7.251812, mae: 2.594927, mean_q: 4.810785\n",
      "[ 0 30 53 14 94 27 40  6 48 46]\n",
      " 44973/50001: episode: 4997, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 39.778 [6.000, 94.000],  loss: 8.226493, mae: 2.596355, mean_q: 4.866544\n",
      "[96 27 52 62 41 94 53 82  8 14]\n",
      " 44982/50001: episode: 4998, duration: 0.068s, episode steps:   9, steps per second: 133, episode reward: 36.000, mean reward:  4.000 [ 3.000,  7.000], mean action: 48.111 [8.000, 94.000],  loss: 7.408800, mae: 2.508111, mean_q: 4.652527\n",
      "[29 41 14 55 82  5 31 97 41 50]\n",
      " 44991/50001: episode: 4999, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 29.000, mean reward:  3.222 [-10.000,  9.000], mean action: 46.222 [5.000, 97.000],  loss: 6.743875, mae: 2.534569, mean_q: 4.675287\n",
      "[23 24 73 16 37 63 10 48  4 78]\n",
      " 45000/50001: episode: 5000, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 38.000, mean reward:  4.222 [ 2.000,  6.000], mean action: 39.222 [4.000, 78.000],  loss: 6.858603, mae: 2.581985, mean_q: 4.852810\n",
      "[52 47 55 34  1 57 21 48 89 16]\n",
      " 45009/50001: episode: 5001, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 38.000, mean reward:  4.222 [ 2.000,  6.000], mean action: 40.889 [1.000, 89.000],  loss: 8.486309, mae: 2.655518, mean_q: 4.988913\n",
      "[ 1 10 91 82 53 30 20 45 16 66]\n",
      " 45018/50001: episode: 5002, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 40.000, mean reward:  4.444 [ 3.000,  9.000], mean action: 45.889 [10.000, 91.000],  loss: 8.339454, mae: 2.617925, mean_q: 4.924755\n",
      "[50  8 93 98 74 88 24 50  5 44]\n",
      " 45027/50001: episode: 5003, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 23.000, mean reward:  2.556 [-10.000,  5.000], mean action: 53.778 [5.000, 98.000],  loss: 4.531556, mae: 2.615715, mean_q: 4.827866\n",
      "[60 98 84 82 67 61 20 50 34 89]\n",
      " 45036/50001: episode: 5004, duration: 0.073s, episode steps:   9, steps per second: 123, episode reward: 42.000, mean reward:  4.667 [ 3.000,  9.000], mean action: 65.000 [20.000, 98.000],  loss: 10.067726, mae: 2.668294, mean_q: 4.950465\n",
      "[36 77 64 50 93 13 73 90 34 49]\n",
      " 45045/50001: episode: 5005, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 41.000, mean reward:  4.556 [ 2.000,  9.000], mean action: 60.333 [13.000, 93.000],  loss: 6.234268, mae: 2.624507, mean_q: 4.874762\n",
      "[59 13 93 31 91 44  1 34 66 50]\n",
      " 45054/50001: episode: 5006, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 44.000, mean reward:  4.889 [ 3.000,  8.000], mean action: 47.000 [1.000, 93.000],  loss: 6.102465, mae: 2.590837, mean_q: 4.851923\n",
      "[44 16  4 88 28 65 74 67 31 41]\n",
      " 45063/50001: episode: 5007, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 46.000 [4.000, 88.000],  loss: 7.462912, mae: 2.603343, mean_q: 4.807915\n",
      "[44 54 55  4 24 93 92 50 48 50]\n",
      " 45072/50001: episode: 5008, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 30.000, mean reward:  3.333 [-10.000,  8.000], mean action: 52.222 [4.000, 93.000],  loss: 9.799281, mae: 2.606810, mean_q: 4.814603\n",
      "[53 30 80 46 60 96 50 31 38 23]\n",
      " 45081/50001: episode: 5009, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 50.444 [23.000, 96.000],  loss: 5.933802, mae: 2.551976, mean_q: 4.696635\n",
      "[ 4 40 53 53 61 27 10 53 68 32]\n",
      " 45090/50001: episode: 5010, duration: 0.065s, episode steps:   9, steps per second: 137, episode reward:  8.000, mean reward:  0.889 [-10.000,  8.000], mean action: 44.111 [10.000, 68.000],  loss: 8.847878, mae: 2.591778, mean_q: 4.800038\n",
      "[20 41  4 82 27 32 97 30 34 98]\n",
      " 45099/50001: episode: 5011, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 46.000, mean reward:  5.111 [ 4.000,  8.000], mean action: 49.444 [4.000, 98.000],  loss: 5.908503, mae: 2.548349, mean_q: 4.754031\n",
      "[38 98 22 22 48 50 13 47 57 98]\n",
      " 45108/50001: episode: 5012, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 13.000, mean reward:  1.444 [-10.000,  6.000], mean action: 50.556 [13.000, 98.000],  loss: 5.859396, mae: 2.557908, mean_q: 4.762072\n",
      "[70  2 34 95 10 50 46 24 13 31]\n",
      " 45117/50001: episode: 5013, duration: 0.074s, episode steps:   9, steps per second: 122, episode reward: 43.000, mean reward:  4.778 [ 3.000,  7.000], mean action: 33.889 [2.000, 95.000],  loss: 5.508056, mae: 2.681823, mean_q: 5.023658\n",
      "[94 37 93 39 98 60 26 34 34 12]\n",
      " 45126/50001: episode: 5014, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 28.000, mean reward:  3.111 [-10.000,  8.000], mean action: 48.111 [12.000, 98.000],  loss: 5.656679, mae: 2.704041, mean_q: 5.007989\n",
      "[21 59 84 57 95 58 57 66  1 54]\n",
      " 45135/50001: episode: 5015, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 59.000 [1.000, 95.000],  loss: 7.327967, mae: 2.664657, mean_q: 4.965425\n",
      "[94 46 13  9 90 95 34 57 79 46]\n",
      " 45144/50001: episode: 5016, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 29.000, mean reward:  3.222 [-10.000,  7.000], mean action: 52.111 [9.000, 95.000],  loss: 7.464463, mae: 2.638293, mean_q: 4.974902\n",
      "[20 55 81 83 98 94 40 52 12 12]\n",
      " 45153/50001: episode: 5017, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 24.000, mean reward:  2.667 [-10.000,  8.000], mean action: 58.556 [12.000, 98.000],  loss: 6.287031, mae: 2.601043, mean_q: 4.913877\n",
      "[88 37 60 93 51 52 88 37 13 13]\n",
      " 45162/50001: episode: 5018, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: -7.000, mean reward: -0.778 [-10.000,  6.000], mean action: 49.333 [13.000, 93.000],  loss: 8.389064, mae: 2.539337, mean_q: 4.801226\n",
      "[16 23  2 38 73 59 34 41 56 52]\n",
      " 45171/50001: episode: 5019, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 37.000, mean reward:  4.111 [ 2.000,  7.000], mean action: 42.000 [2.000, 73.000],  loss: 6.820479, mae: 2.528428, mean_q: 4.738628\n",
      "[25 69 47 57 37 37 74 27 89 47]\n",
      " 45180/50001: episode: 5020, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward:  8.000, mean reward:  0.889 [-10.000,  5.000], mean action: 53.778 [27.000, 89.000],  loss: 7.550994, mae: 2.519414, mean_q: 4.682711\n",
      "[17 68 88 12 16 10 18 27 47 33]\n",
      " 45189/50001: episode: 5021, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 38.000, mean reward:  4.222 [ 3.000,  8.000], mean action: 35.444 [10.000, 88.000],  loss: 7.294558, mae: 2.569782, mean_q: 4.823901\n",
      "[71 91 58 30 68 84 13 31 24  1]\n",
      " 45198/50001: episode: 5022, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 44.000, mean reward:  4.889 [ 3.000,  8.000], mean action: 44.444 [1.000, 91.000],  loss: 9.012309, mae: 2.562713, mean_q: 4.815086\n",
      "[34 49 54 37 45 95 88 31 34 14]\n",
      " 45207/50001: episode: 5023, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 49.667 [14.000, 95.000],  loss: 7.502095, mae: 2.536581, mean_q: 4.723835\n",
      "[94 35 46 42 88 60 96 89 13 54]\n",
      " 45216/50001: episode: 5024, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 58.111 [13.000, 96.000],  loss: 6.487069, mae: 2.508513, mean_q: 4.675112\n",
      "[14 24 34 16 93 46 27 23 53  6]\n",
      " 45225/50001: episode: 5025, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 38.000, mean reward:  4.222 [ 3.000,  5.000], mean action: 35.778 [6.000, 93.000],  loss: 6.307510, mae: 2.545852, mean_q: 4.701928\n",
      "[91 60 79 50  5 63 10 55 78 26]\n",
      " 45234/50001: episode: 5026, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 30.000, mean reward:  3.333 [ 2.000,  5.000], mean action: 47.333 [5.000, 79.000],  loss: 7.350171, mae: 2.539934, mean_q: 4.701825\n",
      "[89 98 80 33 93 14 42 14 60 47]\n",
      " 45243/50001: episode: 5027, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 28.000, mean reward:  3.111 [-10.000,  8.000], mean action: 53.444 [14.000, 98.000],  loss: 6.608020, mae: 2.531791, mean_q: 4.751601\n",
      "[27 71 29 82 66  8 42 12 99 34]\n",
      " 45252/50001: episode: 5028, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 43.000, mean reward:  4.778 [ 2.000, 10.000], mean action: 49.222 [8.000, 99.000],  loss: 9.334332, mae: 2.595041, mean_q: 4.879921\n",
      "[38 12 51 73 68 52 28 93  5  1]\n",
      " 45261/50001: episode: 5029, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 42.556 [1.000, 93.000],  loss: 7.754596, mae: 2.514800, mean_q: 4.723052\n",
      "[60 48 51 49 43  6 93 50 48 31]\n",
      " 45270/50001: episode: 5030, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 24.000, mean reward:  2.667 [-10.000,  8.000], mean action: 46.556 [6.000, 93.000],  loss: 7.343058, mae: 2.420916, mean_q: 4.599720\n",
      "[61 50 77 84 24 72 49 37 34 68]\n",
      " 45279/50001: episode: 5031, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 41.000, mean reward:  4.556 [ 2.000,  9.000], mean action: 55.000 [24.000, 84.000],  loss: 10.564177, mae: 2.397399, mean_q: 4.485604\n",
      "[30  9 88 46 26 27 95 42 46 50]\n",
      " 45288/50001: episode: 5032, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 30.000, mean reward:  3.333 [-10.000,  9.000], mean action: 47.667 [9.000, 95.000],  loss: 8.396929, mae: 2.359535, mean_q: 4.442413\n",
      "[46 13 47 89 88 32 88  9 50 52]\n",
      " 45297/50001: episode: 5033, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 52.000 [9.000, 89.000],  loss: 6.252078, mae: 2.417434, mean_q: 4.512620\n",
      "[46 13 38 95 50 62 98 37 41 53]\n",
      " 45306/50001: episode: 5034, duration: 0.065s, episode steps:   9, steps per second: 137, episode reward: 40.000, mean reward:  4.444 [ 2.000,  6.000], mean action: 54.111 [13.000, 98.000],  loss: 5.236427, mae: 2.447246, mean_q: 4.589707\n",
      "[80 87 19 47 82 53 13 95 40 33]\n",
      " 45315/50001: episode: 5035, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 40.000, mean reward:  4.444 [ 3.000,  8.000], mean action: 52.111 [13.000, 95.000],  loss: 5.998622, mae: 2.524208, mean_q: 4.695334\n",
      "[27  5 74  3 32 53 12 41 37 48]\n",
      " 45324/50001: episode: 5036, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 44.000, mean reward:  4.889 [ 3.000,  8.000], mean action: 33.889 [3.000, 74.000],  loss: 7.695602, mae: 2.523593, mean_q: 4.647631\n",
      "[88  4 64 98 75 46 63  9 88 28]\n",
      " 45333/50001: episode: 5037, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 52.778 [4.000, 98.000],  loss: 8.786991, mae: 2.532574, mean_q: 4.734723\n",
      "[40 97 36 87 23 88 62  3 10 49]\n",
      " 45342/50001: episode: 5038, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 33.000, mean reward:  3.667 [ 2.000,  5.000], mean action: 50.556 [3.000, 97.000],  loss: 5.566040, mae: 2.507781, mean_q: 4.743842\n",
      "[23 52 92 95 55 10 90 82 66 10]\n",
      " 45351/50001: episode: 5039, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 61.333 [10.000, 95.000],  loss: 8.402318, mae: 2.556378, mean_q: 4.729155\n",
      "[ 7 35 37 24 66 72 53 16 66  5]\n",
      " 45360/50001: episode: 5040, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 41.556 [5.000, 72.000],  loss: 4.580419, mae: 2.588313, mean_q: 4.805470\n",
      "[ 3 36 41 52 68 57 90 25 31 34]\n",
      " 45369/50001: episode: 5041, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 44.000, mean reward:  4.889 [ 2.000, 10.000], mean action: 48.222 [25.000, 90.000],  loss: 9.021882, mae: 2.570091, mean_q: 4.804296\n",
      "[40 12 35 65 36 79 11 28  1  1]\n",
      " 45378/50001: episode: 5042, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 29.778 [1.000, 79.000],  loss: 9.058316, mae: 2.593696, mean_q: 4.773377\n",
      "[83 14 77 84 56 80 31 52 66 98]\n",
      " 45387/50001: episode: 5043, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 62.000 [14.000, 98.000],  loss: 5.888391, mae: 2.490499, mean_q: 4.624234\n",
      "[68 32 58 64 95 60 74  9 20 42]\n",
      " 45396/50001: episode: 5044, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 50.444 [9.000, 95.000],  loss: 2.437415, mae: 2.590135, mean_q: 4.807280\n",
      "[ 8 50 79 24 31 13 73 88 34 48]\n",
      " 45405/50001: episode: 5045, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 44.000, mean reward:  4.889 [ 2.000,  8.000], mean action: 48.889 [13.000, 88.000],  loss: 7.629229, mae: 2.704468, mean_q: 4.990031\n",
      "[49 34 33 28 97 27 21 42 74 46]\n",
      " 45414/50001: episode: 5046, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 44.667 [21.000, 97.000],  loss: 7.249341, mae: 2.722102, mean_q: 5.052380\n",
      "[71 96 11 69 42 46  1 92 12 89]\n",
      " 45423/50001: episode: 5047, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 50.889 [1.000, 96.000],  loss: 6.876346, mae: 2.668591, mean_q: 4.936720\n",
      "[56 49 33 95 97 97 51 13 83 20]\n",
      " 45432/50001: episode: 5048, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 59.778 [13.000, 97.000],  loss: 6.409140, mae: 2.684770, mean_q: 4.926703\n",
      "[15 25 53 82 31 47 19 97 64 82]\n",
      " 45441/50001: episode: 5049, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 55.556 [19.000, 97.000],  loss: 6.713357, mae: 2.606160, mean_q: 4.900998\n",
      "[39 23 95 93 47 88  1 68 66 37]\n",
      " 45450/50001: episode: 5050, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 43.000, mean reward:  4.778 [ 3.000,  8.000], mean action: 57.556 [1.000, 95.000],  loss: 9.208050, mae: 2.650377, mean_q: 4.891837\n",
      "[74 31 10 37 37 86 82 37 14 89]\n",
      " 45459/50001: episode: 5051, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward:  7.000, mean reward:  0.778 [-10.000,  5.000], mean action: 47.000 [10.000, 89.000],  loss: 8.345412, mae: 2.603893, mean_q: 4.851371\n",
      "[47  1 14  9 92 17 13 79 85 40]\n",
      " 45468/50001: episode: 5052, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 33.000, mean reward:  3.667 [ 2.000,  6.000], mean action: 38.889 [1.000, 92.000],  loss: 7.397919, mae: 2.538872, mean_q: 4.666116\n",
      "[90 24 47  8  9 65 95 16 48 12]\n",
      " 45477/50001: episode: 5053, duration: 0.070s, episode steps:   9, steps per second: 129, episode reward: 43.000, mean reward:  4.778 [ 2.000,  8.000], mean action: 36.000 [8.000, 95.000],  loss: 7.461356, mae: 2.531958, mean_q: 4.677098\n",
      "[73  5 10 79 55 12 24 42  3 89]\n",
      " 45486/50001: episode: 5054, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 35.000, mean reward:  3.889 [ 2.000,  6.000], mean action: 35.444 [3.000, 89.000],  loss: 6.366181, mae: 2.485549, mean_q: 4.619219\n",
      "[11  9 53 12 39 51 48 32 40 31]\n",
      " 45495/50001: episode: 5055, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 36.000, mean reward:  4.000 [ 2.000,  6.000], mean action: 35.000 [9.000, 53.000],  loss: 5.222853, mae: 2.462108, mean_q: 4.571371\n",
      "[72 46 61 34 64  6 93 48 39 50]\n",
      " 45504/50001: episode: 5056, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 44.000, mean reward:  4.889 [ 2.000, 10.000], mean action: 49.000 [6.000, 93.000],  loss: 6.821708, mae: 2.491523, mean_q: 4.612346\n",
      "[51 50 37 59 77 87  4 28 89 79]\n",
      " 45513/50001: episode: 5057, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 38.000, mean reward:  4.222 [ 3.000,  7.000], mean action: 56.667 [4.000, 89.000],  loss: 4.759571, mae: 2.520659, mean_q: 4.691875\n",
      "[21 12  8  2 77 90 28 34 15 16]\n",
      " 45522/50001: episode: 5058, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 36.000, mean reward:  4.000 [ 2.000,  7.000], mean action: 31.333 [2.000, 90.000],  loss: 8.481907, mae: 2.658931, mean_q: 4.934435\n",
      "[61 34 54 57 96 34 14 66 88 12]\n",
      " 45531/50001: episode: 5059, duration: 0.065s, episode steps:   9, steps per second: 140, episode reward: 31.000, mean reward:  3.444 [-10.000,  8.000], mean action: 50.556 [12.000, 96.000],  loss: 8.279378, mae: 2.698411, mean_q: 4.967631\n",
      "[39 60 45 34 34 14 47 75 89 28]\n",
      " 45540/50001: episode: 5060, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 28.000, mean reward:  3.111 [-10.000,  7.000], mean action: 47.333 [14.000, 89.000],  loss: 7.315847, mae: 2.633110, mean_q: 4.885543\n",
      "[28 89 88 34  7 78 20 50 66 88]\n",
      " 45549/50001: episode: 5061, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 57.778 [7.000, 89.000],  loss: 7.357840, mae: 2.649791, mean_q: 4.886049\n",
      "[95 78 58  2 48 62 21 64 57 46]\n",
      " 45558/50001: episode: 5062, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 37.000, mean reward:  4.111 [ 3.000,  7.000], mean action: 48.444 [2.000, 78.000],  loss: 10.390865, mae: 2.580060, mean_q: 4.854637\n",
      "[30 44 93 31 50 90 69 59 34 82]\n",
      " 45567/50001: episode: 5063, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 42.000, mean reward:  4.667 [ 2.000,  8.000], mean action: 61.333 [31.000, 93.000],  loss: 5.707467, mae: 2.513225, mean_q: 4.685205\n",
      "[82  8 93  9 52 37 48  4 87 75]\n",
      " 45576/50001: episode: 5064, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 45.889 [4.000, 93.000],  loss: 7.710905, mae: 2.515235, mean_q: 4.730594\n",
      "[77 37 75 24 25 72 62  5 88 75]\n",
      " 45585/50001: episode: 5065, duration: 0.072s, episode steps:   9, steps per second: 124, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 51.444 [5.000, 88.000],  loss: 8.117024, mae: 2.497970, mean_q: 4.720742\n",
      "[14 99 47 42  2 92 50 24 50 55]\n",
      " 45594/50001: episode: 5066, duration: 0.068s, episode steps:   9, steps per second: 133, episode reward: 21.000, mean reward:  2.333 [-10.000,  6.000], mean action: 51.222 [2.000, 99.000],  loss: 6.904633, mae: 2.474621, mean_q: 4.666469\n",
      "[51 95 28  9 77 50 37 37 88 32]\n",
      " 45603/50001: episode: 5067, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 27.000, mean reward:  3.000 [-10.000,  6.000], mean action: 50.333 [9.000, 95.000],  loss: 9.020250, mae: 2.538220, mean_q: 4.757782\n",
      "[17 28 48 41  6 13 33 50 26 12]\n",
      " 45612/50001: episode: 5068, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 41.000, mean reward:  4.556 [ 2.000,  8.000], mean action: 28.556 [6.000, 50.000],  loss: 6.447368, mae: 2.536383, mean_q: 4.786954\n",
      "[ 2 30 37 24 88 41 91 32 37 87]\n",
      " 45621/50001: episode: 5069, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 22.000, mean reward:  2.444 [-10.000,  5.000], mean action: 51.889 [24.000, 91.000],  loss: 4.190095, mae: 2.513288, mean_q: 4.732229\n",
      "[72 31 66 76 48 37 67 27  1 12]\n",
      " 45630/50001: episode: 5070, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 43.000, mean reward:  4.778 [ 3.000,  8.000], mean action: 40.556 [1.000, 76.000],  loss: 4.686165, mae: 2.636766, mean_q: 4.936220\n",
      "[80 16 32 97 50 28 99 13  5 24]\n",
      " 45639/50001: episode: 5071, duration: 0.069s, episode steps:   9, steps per second: 131, episode reward: 41.000, mean reward:  4.556 [ 3.000,  6.000], mean action: 40.444 [5.000, 99.000],  loss: 9.260563, mae: 2.735622, mean_q: 5.122872\n",
      "[92 68 52 68 31 55 78  1 37 12]\n",
      " 45648/50001: episode: 5072, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward: 29.000, mean reward:  3.222 [-10.000,  8.000], mean action: 44.667 [1.000, 78.000],  loss: 6.738065, mae: 2.678990, mean_q: 4.975705\n",
      "[53 10 88 50 88 70 14 82 38 43]\n",
      " 45657/50001: episode: 5073, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 18.000, mean reward:  2.000 [-10.000,  5.000], mean action: 53.667 [10.000, 88.000],  loss: 6.904436, mae: 2.659565, mean_q: 4.902098\n",
      "[28 13 85 34  3 80 78 27  2  9]\n",
      " 45666/50001: episode: 5074, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 37.000, mean reward:  4.111 [ 2.000,  6.000], mean action: 36.778 [2.000, 85.000],  loss: 10.044004, mae: 2.553427, mean_q: 4.754459\n",
      "[65 82 53  4  4 46 47 47 42  2]\n",
      " 45675/50001: episode: 5075, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 14.000, mean reward:  1.556 [-10.000,  6.000], mean action: 36.333 [2.000, 82.000],  loss: 7.552801, mae: 2.516410, mean_q: 4.712560\n",
      "[12 34 79 21 32 24 50 27 59 47]\n",
      " 45684/50001: episode: 5076, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 43.000, mean reward:  4.778 [ 3.000,  8.000], mean action: 41.444 [21.000, 79.000],  loss: 7.149558, mae: 2.502604, mean_q: 4.685051\n",
      "[73 13 10 84 51 66 60 37 42 12]\n",
      " 45693/50001: episode: 5077, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 43.000, mean reward:  4.778 [ 3.000,  8.000], mean action: 41.667 [10.000, 84.000],  loss: 7.083628, mae: 2.554343, mean_q: 4.745412\n",
      "[44 13 88 37 24 37 46 23  1  1]\n",
      " 45702/50001: episode: 5078, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 11.000, mean reward:  1.222 [-10.000,  6.000], mean action: 30.000 [1.000, 88.000],  loss: 9.270397, mae: 2.530506, mean_q: 4.756681\n",
      "[51 74 97 28 12 22 11 37 80 79]\n",
      " 45711/50001: episode: 5079, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 37.000, mean reward:  4.111 [ 2.000,  7.000], mean action: 48.889 [11.000, 97.000],  loss: 7.024442, mae: 2.559373, mean_q: 4.753046\n",
      "[41 42 16 50 53 99 11 96 31 14]\n",
      " 45720/50001: episode: 5080, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 45.778 [11.000, 99.000],  loss: 6.266750, mae: 2.502791, mean_q: 4.706504\n",
      "[55 28 47 84 68 57 48 30 27 12]\n",
      " 45729/50001: episode: 5081, duration: 0.068s, episode steps:   9, steps per second: 133, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 44.556 [12.000, 84.000],  loss: 6.898727, mae: 2.495817, mean_q: 4.680168\n",
      "[44 53 43 66 32 23 42  6 84 75]\n",
      " 45738/50001: episode: 5082, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 37.000, mean reward:  4.111 [ 2.000,  7.000], mean action: 47.111 [6.000, 84.000],  loss: 7.627965, mae: 2.594020, mean_q: 4.881675\n",
      "[71 74 46 72 87 62 62  3  2 82]\n",
      " 45747/50001: episode: 5083, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 54.444 [2.000, 87.000],  loss: 6.553478, mae: 2.534952, mean_q: 4.698751\n",
      "[47 98 25 97 96 37 24 14 75 66]\n",
      " 45756/50001: episode: 5084, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 41.000, mean reward:  4.556 [ 2.000,  7.000], mean action: 59.111 [14.000, 98.000],  loss: 5.463687, mae: 2.536561, mean_q: 4.725549\n",
      "[77 52 84 66 61 13 12 60 34 65]\n",
      " 45765/50001: episode: 5085, duration: 0.066s, episode steps:   9, steps per second: 135, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 49.667 [12.000, 84.000],  loss: 5.901927, mae: 2.543489, mean_q: 4.716808\n",
      "[93 50 94 17 88 88 55  2 96  4]\n",
      " 45774/50001: episode: 5086, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 54.889 [2.000, 96.000],  loss: 7.242153, mae: 2.624362, mean_q: 4.889803\n",
      "[85 93 32 77 46 60 62 71 94 51]\n",
      " 45783/50001: episode: 5087, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 36.000, mean reward:  4.000 [ 3.000,  6.000], mean action: 65.111 [32.000, 94.000],  loss: 7.114395, mae: 2.624824, mean_q: 4.892563\n",
      "[73 26 10 20 53 11 73 51 28 14]\n",
      " 45792/50001: episode: 5088, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 31.778 [10.000, 73.000],  loss: 6.127581, mae: 2.598837, mean_q: 4.868771\n",
      "[56  2 95 52 77 59  0 13 50 46]\n",
      " 45801/50001: episode: 5089, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 41.000, mean reward:  4.556 [ 2.000,  8.000], mean action: 43.778 [0.000, 95.000],  loss: 7.558850, mae: 2.658316, mean_q: 4.922357\n",
      "[39 67 79 51 74 53 40 20 44 23]\n",
      " 45810/50001: episode: 5090, duration: 0.072s, episode steps:   9, steps per second: 125, episode reward: 37.000, mean reward:  4.111 [ 3.000,  6.000], mean action: 50.111 [20.000, 79.000],  loss: 7.842474, mae: 2.607620, mean_q: 4.773011\n",
      "[ 7 68 98 79 47 35 66 75 13 88]\n",
      " 45819/50001: episode: 5091, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 44.000, mean reward:  4.889 [ 3.000,  8.000], mean action: 63.222 [13.000, 98.000],  loss: 8.836225, mae: 2.651371, mean_q: 4.924749\n",
      "[12 92 63 98 13 42 48 31 16 88]\n",
      " 45828/50001: episode: 5092, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 54.556 [13.000, 98.000],  loss: 4.902460, mae: 2.592001, mean_q: 4.812657\n",
      "[30 39 61 19 59 57 57  6 12 66]\n",
      " 45837/50001: episode: 5093, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 26.000, mean reward:  2.889 [-10.000,  9.000], mean action: 41.778 [6.000, 66.000],  loss: 8.678508, mae: 2.572616, mean_q: 4.796422\n",
      "[53 63 97 92 95 62 38 95 14 54]\n",
      " 45846/50001: episode: 5094, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 20.000, mean reward:  2.222 [-10.000,  6.000], mean action: 67.778 [14.000, 97.000],  loss: 6.632712, mae: 2.629706, mean_q: 4.850223\n",
      "[50 68 64 57 58 60 89 28 21 31]\n",
      " 45855/50001: episode: 5095, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 52.889 [21.000, 89.000],  loss: 8.070268, mae: 2.544473, mean_q: 4.709456\n",
      "[ 0 98 34 56 33 33  1 46 18 96]\n",
      " 45864/50001: episode: 5096, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 46.111 [1.000, 98.000],  loss: 8.002995, mae: 2.547950, mean_q: 4.733747\n",
      "[26 48 47 95 95 32 95 40 30  5]\n",
      " 45873/50001: episode: 5097, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward:  9.000, mean reward:  1.000 [-10.000,  5.000], mean action: 54.111 [5.000, 95.000],  loss: 6.248133, mae: 2.548441, mean_q: 4.710972\n",
      "[21 94 11 41 81 12 10 44 61 47]\n",
      " 45882/50001: episode: 5098, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 39.000, mean reward:  4.333 [ 3.000,  9.000], mean action: 44.556 [10.000, 94.000],  loss: 8.837896, mae: 2.548746, mean_q: 4.696283\n",
      "[39  6  9 16 30 78 93 95 79 57]\n",
      " 45891/50001: episode: 5099, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 43.000, mean reward:  4.778 [ 3.000,  9.000], mean action: 51.444 [6.000, 95.000],  loss: 6.848938, mae: 2.557662, mean_q: 4.697528\n",
      "[69 60 82 59  2 79 75 78 98 24]\n",
      " 45900/50001: episode: 5100, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 39.000, mean reward:  4.333 [ 2.000,  6.000], mean action: 61.889 [2.000, 98.000],  loss: 8.605818, mae: 2.556996, mean_q: 4.696080\n",
      "[17 13 95 78 97 57 68 37 37  3]\n",
      " 45909/50001: episode: 5101, duration: 0.065s, episode steps:   9, steps per second: 140, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 53.889 [3.000, 97.000],  loss: 7.296330, mae: 2.551469, mean_q: 4.772435\n",
      "[63 13 48 67 97 51 37  0 34 28]\n",
      " 45918/50001: episode: 5102, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 41.667 [0.000, 97.000],  loss: 6.508602, mae: 2.563819, mean_q: 4.716195\n",
      "[28 11  8 31 54 57  2 84  2 48]\n",
      " 45927/50001: episode: 5103, duration: 0.071s, episode steps:   9, steps per second: 126, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 33.000 [2.000, 84.000],  loss: 7.282496, mae: 2.631482, mean_q: 4.841144\n",
      "[94 98 97 79 90 95 97 23 41 58]\n",
      " 45936/50001: episode: 5104, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 26.000, mean reward:  2.889 [-10.000,  6.000], mean action: 75.333 [23.000, 98.000],  loss: 8.702837, mae: 2.573432, mean_q: 4.741773\n",
      "[58 60 98  0 37 34 50 98 29 62]\n",
      " 45945/50001: episode: 5105, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 52.000 [0.000, 98.000],  loss: 7.343941, mae: 2.517656, mean_q: 4.625618\n",
      "[99 13  8 45 45 64 13 56 24 88]\n",
      " 45954/50001: episode: 5106, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 11.000, mean reward:  1.222 [-10.000,  8.000], mean action: 39.556 [8.000, 88.000],  loss: 7.539850, mae: 2.585123, mean_q: 4.768802\n",
      "[72 21 90 47 66  1 82 50 66 13]\n",
      " 45963/50001: episode: 5107, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 30.000, mean reward:  3.333 [-10.000,  8.000], mean action: 48.444 [1.000, 90.000],  loss: 5.348140, mae: 2.515381, mean_q: 4.598493\n",
      "[97 50 47 13  5 75 50 74 34 35]\n",
      " 45972/50001: episode: 5108, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 42.556 [5.000, 75.000],  loss: 7.509998, mae: 2.680352, mean_q: 4.938973\n",
      "[68  2 13  9 50 28 79 81 12 55]\n",
      " 45981/50001: episode: 5109, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 36.000, mean reward:  4.000 [ 2.000,  6.000], mean action: 36.556 [2.000, 81.000],  loss: 7.031342, mae: 2.658686, mean_q: 4.825839\n",
      "[62 66 14 58  4 46 99 85 31 32]\n",
      " 45990/50001: episode: 5110, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 48.333 [4.000, 99.000],  loss: 6.412438, mae: 2.704978, mean_q: 4.960244\n",
      "[37  4 59 92 47 12 59 13 53 50]\n",
      " 45999/50001: episode: 5111, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 27.000, mean reward:  3.000 [-10.000,  8.000], mean action: 43.222 [4.000, 92.000],  loss: 8.293870, mae: 2.703827, mean_q: 5.017700\n",
      "[67 66 79 88  1 13 10 45 81 20]\n",
      " 46008/50001: episode: 5112, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 34.000, mean reward:  3.778 [ 2.000,  5.000], mean action: 44.778 [1.000, 88.000],  loss: 6.855526, mae: 2.660153, mean_q: 4.893161\n",
      "[34 37 34 97  2 93 67 97 48 14]\n",
      " 46017/50001: episode: 5113, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 10.000, mean reward:  1.111 [-10.000,  6.000], mean action: 54.333 [2.000, 97.000],  loss: 6.836572, mae: 2.664021, mean_q: 4.937152\n",
      "[ 2 34 24 42 51 70 54 34  2 16]\n",
      " 46026/50001: episode: 5114, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward:  4.000, mean reward:  0.444 [-10.000,  5.000], mean action: 36.333 [2.000, 70.000],  loss: 9.182750, mae: 2.597337, mean_q: 4.797597\n",
      "[97 87 37 26 86 64 56 57 57 79]\n",
      " 46035/50001: episode: 5115, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 61.000 [26.000, 87.000],  loss: 6.957551, mae: 2.563522, mean_q: 4.739038\n",
      "[43 74 37 92 10 83 24 48  9 93]\n",
      " 46044/50001: episode: 5116, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 52.222 [9.000, 93.000],  loss: 11.125459, mae: 2.481001, mean_q: 4.645609\n",
      "[23 68  8 98 81  1 88 99 23 34]\n",
      " 46053/50001: episode: 5117, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 28.000, mean reward:  3.111 [-10.000,  9.000], mean action: 55.556 [1.000, 99.000],  loss: 8.151329, mae: 2.410081, mean_q: 4.493233\n",
      "[57 36 89 89 51 45 89  1 19 55]\n",
      " 46062/50001: episode: 5118, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward:  8.000, mean reward:  0.889 [-10.000,  7.000], mean action: 52.667 [1.000, 89.000],  loss: 6.836127, mae: 2.390948, mean_q: 4.484813\n",
      "[35  5 34 19 82 74 95 66 13 54]\n",
      " 46071/50001: episode: 5119, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 49.111 [5.000, 95.000],  loss: 7.874871, mae: 2.503887, mean_q: 4.676777\n",
      "[99 41 48 37 13 25 97 48 41 98]\n",
      " 46080/50001: episode: 5120, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 11.000, mean reward:  1.222 [-10.000,  6.000], mean action: 49.778 [13.000, 98.000],  loss: 6.021964, mae: 2.547955, mean_q: 4.810826\n",
      "[27 37 80 12  3 76 50 66 95 75]\n",
      " 46089/50001: episode: 5121, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 43.000, mean reward:  4.778 [ 2.000,  9.000], mean action: 54.889 [3.000, 95.000],  loss: 6.357299, mae: 2.638087, mean_q: 4.886210\n",
      "[58 59 27 98 14 28 46 16 86  4]\n",
      " 46098/50001: episode: 5122, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 42.000 [4.000, 98.000],  loss: 7.409572, mae: 2.657866, mean_q: 4.961176\n",
      "[86 31 27 37 62 57 97 95 54 56]\n",
      " 46107/50001: episode: 5123, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 40.000, mean reward:  4.444 [ 3.000,  8.000], mean action: 57.333 [27.000, 97.000],  loss: 7.543046, mae: 2.665554, mean_q: 4.942851\n",
      "[ 6 49 19 41 31  1 91 13 75 46]\n",
      " 46116/50001: episode: 5124, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 43.000, mean reward:  4.778 [ 3.000,  7.000], mean action: 40.667 [1.000, 91.000],  loss: 6.608925, mae: 2.637532, mean_q: 4.882836\n",
      "[56 46 13 23 93 48 31  5  1 46]\n",
      " 46125/50001: episode: 5125, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 34.000 [1.000, 93.000],  loss: 6.395057, mae: 2.577989, mean_q: 4.834685\n",
      "[54 37 45 82 78 55 66 13 93  9]\n",
      " 46134/50001: episode: 5126, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 43.000, mean reward:  4.778 [ 2.000,  8.000], mean action: 53.111 [9.000, 93.000],  loss: 10.454437, mae: 2.614735, mean_q: 4.849390\n",
      "[83 17 48 29 41 90 88 88 31 37]\n",
      " 46143/50001: episode: 5127, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 28.000, mean reward:  3.111 [-10.000,  8.000], mean action: 52.111 [17.000, 90.000],  loss: 7.101213, mae: 2.542179, mean_q: 4.753821\n",
      "[22 64 59 23 28 90 24 39 13 32]\n",
      " 46152/50001: episode: 5128, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 41.333 [13.000, 90.000],  loss: 10.778419, mae: 2.450473, mean_q: 4.644029\n",
      "[30 96 79  0 42 37 37 28 51 97]\n",
      " 46161/50001: episode: 5129, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 51.889 [0.000, 97.000],  loss: 6.568258, mae: 2.419100, mean_q: 4.520476\n",
      "[36 48 62 10 96 13 99 69 48 30]\n",
      " 46170/50001: episode: 5130, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 52.778 [10.000, 99.000],  loss: 5.181084, mae: 2.439512, mean_q: 4.573246\n",
      "[73 89 53 46 24 77 74 46  2  2]\n",
      " 46179/50001: episode: 5131, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 11.000, mean reward:  1.222 [-10.000,  6.000], mean action: 45.889 [2.000, 89.000],  loss: 6.680161, mae: 2.553389, mean_q: 4.711287\n",
      "[21 35 56  5 66 14 30 45 10 13]\n",
      " 46188/50001: episode: 5132, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 41.000, mean reward:  4.556 [ 3.000, 10.000], mean action: 30.444 [5.000, 66.000],  loss: 8.381799, mae: 2.580816, mean_q: 4.754016\n",
      "[42 40 58 11 37 50 90 13 64 18]\n",
      " 46197/50001: episode: 5133, duration: 0.066s, episode steps:   9, steps per second: 135, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 42.333 [11.000, 90.000],  loss: 8.681629, mae: 2.549864, mean_q: 4.721193\n",
      "[35 13 92 77 30 23 37 66 81  1]\n",
      " 46206/50001: episode: 5134, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 46.667 [1.000, 92.000],  loss: 8.594763, mae: 2.516520, mean_q: 4.735603\n",
      "[36 26 30  4 95 24 10 46 47 79]\n",
      " 46215/50001: episode: 5135, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 45.000, mean reward:  5.000 [ 2.000,  7.000], mean action: 40.111 [4.000, 95.000],  loss: 7.595276, mae: 2.424611, mean_q: 4.542119\n",
      "[42 37 79 37 79  1 57 66 13 99]\n",
      " 46224/50001: episode: 5136, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 12.000, mean reward:  1.333 [-10.000,  6.000], mean action: 52.000 [1.000, 99.000],  loss: 6.102956, mae: 2.437627, mean_q: 4.531140\n",
      "[85  4 34 45 50 42 36 89 66 32]\n",
      " 46233/50001: episode: 5137, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 42.000, mean reward:  4.667 [ 2.000,  8.000], mean action: 44.222 [4.000, 89.000],  loss: 6.939550, mae: 2.500703, mean_q: 4.598932\n",
      "[ 2 34  2 28 97 60 90 74  0  8]\n",
      " 46242/50001: episode: 5138, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 20.000, mean reward:  2.222 [-10.000,  5.000], mean action: 43.667 [0.000, 97.000],  loss: 5.045599, mae: 2.510882, mean_q: 4.633601\n",
      "[66 12 75 40 33 47  2 50 90 79]\n",
      " 46251/50001: episode: 5139, duration: 0.066s, episode steps:   9, steps per second: 135, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 47.556 [2.000, 90.000],  loss: 7.029853, mae: 2.633465, mean_q: 4.912830\n",
      "[17 94  7 93 62 37 32 42 56 79]\n",
      " 46260/50001: episode: 5140, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 42.000, mean reward:  4.667 [ 2.000,  7.000], mean action: 55.778 [7.000, 94.000],  loss: 6.981385, mae: 2.570278, mean_q: 4.737665\n",
      "[92 90 82 99 97 50 21 13 95 32]\n",
      " 46269/50001: episode: 5141, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 47.000, mean reward:  5.222 [ 3.000,  8.000], mean action: 64.333 [13.000, 99.000],  loss: 5.528662, mae: 2.644065, mean_q: 4.916100\n",
      "[56 35  1 63 12 47 97 50 31 13]\n",
      " 46278/50001: episode: 5142, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 44.000, mean reward:  4.889 [ 3.000,  7.000], mean action: 38.778 [1.000, 97.000],  loss: 6.300275, mae: 2.638512, mean_q: 4.907282\n",
      "[44 13 38 64 13 79 88 69 31 24]\n",
      " 46287/50001: episode: 5143, duration: 0.065s, episode steps:   9, steps per second: 137, episode reward: 26.000, mean reward:  2.889 [-10.000,  6.000], mean action: 46.556 [13.000, 88.000],  loss: 7.010783, mae: 2.687517, mean_q: 4.958503\n",
      "[80 24 66 12 53 31 52 35 43 32]\n",
      " 46296/50001: episode: 5144, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 36.000, mean reward:  4.000 [ 2.000,  7.000], mean action: 38.667 [12.000, 66.000],  loss: 8.654576, mae: 2.668994, mean_q: 4.963344\n",
      "[39 95 96 19 88 56 66 50 41 14]\n",
      " 46305/50001: episode: 5145, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 44.000, mean reward:  4.889 [ 3.000,  7.000], mean action: 58.333 [14.000, 96.000],  loss: 8.578365, mae: 2.629998, mean_q: 4.862729\n",
      "[47  2 27 54 31 97 37 13 71 63]\n",
      " 46314/50001: episode: 5146, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 39.000, mean reward:  4.333 [ 2.000,  6.000], mean action: 43.889 [2.000, 97.000],  loss: 7.750628, mae: 2.531116, mean_q: 4.640119\n",
      "[ 6 33 99 82 37 21 47 88 89 48]\n",
      " 46323/50001: episode: 5147, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 43.000, mean reward:  4.778 [ 3.000,  8.000], mean action: 60.444 [21.000, 99.000],  loss: 7.185571, mae: 2.460016, mean_q: 4.509918\n",
      "[60 41 95 75  9 84 66 66  1 52]\n",
      " 46332/50001: episode: 5148, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 54.333 [1.000, 95.000],  loss: 8.988869, mae: 2.393203, mean_q: 4.431585\n",
      "[94 11 34  2  9 97 28 34 34 64]\n",
      " 46341/50001: episode: 5149, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward:  9.000, mean reward:  1.000 [-10.000,  5.000], mean action: 34.778 [2.000, 97.000],  loss: 8.214852, mae: 2.371201, mean_q: 4.448785\n",
      "[34  5 73  1 88 47 50 16 31 24]\n",
      " 46350/50001: episode: 5150, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 39.000, mean reward:  4.333 [ 2.000,  6.000], mean action: 37.222 [1.000, 88.000],  loss: 7.526883, mae: 2.426069, mean_q: 4.544675\n",
      "[61 34 59 41 42 89 22 99 66 40]\n",
      " 46359/50001: episode: 5151, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 43.000, mean reward:  4.778 [ 3.000,  7.000], mean action: 54.667 [22.000, 99.000],  loss: 7.132242, mae: 2.411576, mean_q: 4.470222\n",
      "[42 30 55 20 92 53 70 57 40 84]\n",
      " 46368/50001: episode: 5152, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 37.000, mean reward:  4.111 [ 3.000,  6.000], mean action: 55.667 [20.000, 92.000],  loss: 7.678255, mae: 2.453383, mean_q: 4.572671\n",
      "[ 6 20 13 33 30 32 35 63 58 11]\n",
      " 46377/50001: episode: 5153, duration: 0.075s, episode steps:   9, steps per second: 119, episode reward: 38.000, mean reward:  4.222 [ 3.000,  5.000], mean action: 32.778 [11.000, 63.000],  loss: 7.361036, mae: 2.486268, mean_q: 4.576360\n",
      "[87 95 69 13 58 82  6 50 97 47]\n",
      " 46386/50001: episode: 5154, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 43.000, mean reward:  4.778 [ 3.000,  7.000], mean action: 57.444 [6.000, 97.000],  loss: 7.726661, mae: 2.446921, mean_q: 4.608178\n",
      "[64 31 84 90 77 31 22 87 13 14]\n",
      " 46395/50001: episode: 5155, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 49.889 [13.000, 90.000],  loss: 8.819205, mae: 2.476314, mean_q: 4.625668\n",
      "[55 50 44 60 93 77 39 82 57 15]\n",
      " 46404/50001: episode: 5156, duration: 0.065s, episode steps:   9, steps per second: 140, episode reward: 33.000, mean reward:  3.667 [ 2.000,  6.000], mean action: 57.444 [15.000, 93.000],  loss: 8.267134, mae: 2.518682, mean_q: 4.686060\n",
      "[54  4 47 97 97 46 57 67 42 31]\n",
      " 46413/50001: episode: 5157, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 54.222 [4.000, 97.000],  loss: 5.326676, mae: 2.518470, mean_q: 4.649289\n",
      "[53 94 97 63 67 66 46 88 28 82]\n",
      " 46422/50001: episode: 5158, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 40.000, mean reward:  4.444 [ 3.000,  6.000], mean action: 70.111 [28.000, 97.000],  loss: 6.687316, mae: 2.617127, mean_q: 4.851323\n",
      "[90 35 13 93 50 74 27 20 30 28]\n",
      " 46431/50001: episode: 5159, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 41.111 [13.000, 93.000],  loss: 7.915436, mae: 2.566102, mean_q: 4.756632\n",
      "[14 53 77 91 72 41 48 37 37 48]\n",
      " 46440/50001: episode: 5160, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 10.000, mean reward:  1.111 [-10.000,  8.000], mean action: 56.000 [37.000, 91.000],  loss: 7.093413, mae: 2.602056, mean_q: 4.821825\n",
      "[45  2  8 32 27 19 11 91 64 71]\n",
      " 46449/50001: episode: 5161, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 33.000, mean reward:  3.667 [ 3.000,  4.000], mean action: 36.111 [2.000, 91.000],  loss: 9.196411, mae: 2.507058, mean_q: 4.636171\n",
      "[79  4 30 91 34 10 24 40  1 12]\n",
      " 46458/50001: episode: 5162, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 43.000, mean reward:  4.778 [ 3.000,  8.000], mean action: 27.333 [1.000, 91.000],  loss: 8.535696, mae: 2.547973, mean_q: 4.701957\n",
      "[63 14 32 32 89  8 43 79  9 61]\n",
      " 46467/50001: episode: 5163, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 21.000, mean reward:  2.333 [-10.000,  6.000], mean action: 40.778 [8.000, 89.000],  loss: 7.084294, mae: 2.520083, mean_q: 4.660352\n",
      "[81 34 13 23 21 32 24 82 78 27]\n",
      " 46476/50001: episode: 5164, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 38.000, mean reward:  4.222 [ 3.000,  7.000], mean action: 37.111 [13.000, 82.000],  loss: 7.992550, mae: 2.537307, mean_q: 4.650594\n",
      "[67 13  6 44  4 30 47 17 21 24]\n",
      " 46485/50001: episode: 5165, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 38.000, mean reward:  4.222 [ 3.000,  7.000], mean action: 22.889 [4.000, 47.000],  loss: 6.716179, mae: 2.512204, mean_q: 4.607738\n",
      "[56 31 88 14 31  9 46 54 88 66]\n",
      " 46494/50001: episode: 5166, duration: 0.072s, episode steps:   9, steps per second: 125, episode reward: 12.000, mean reward:  1.333 [-10.000,  8.000], mean action: 47.444 [9.000, 88.000],  loss: 7.679413, mae: 2.566169, mean_q: 4.736048\n",
      "[64 57 77 68 23 97 12 20 12 34]\n",
      " 46503/50001: episode: 5167, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 26.000, mean reward:  2.889 [-10.000,  9.000], mean action: 44.444 [12.000, 97.000],  loss: 6.903039, mae: 2.587340, mean_q: 4.764408\n",
      "[69 53 50 32 31 66 98  4  6 28]\n",
      " 46512/50001: episode: 5168, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 40.889 [4.000, 98.000],  loss: 9.808314, mae: 2.581618, mean_q: 4.758748\n",
      "[79 40 32 34 95 84 46  1 27 75]\n",
      " 46521/50001: episode: 5169, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 42.000, mean reward:  4.667 [ 3.000,  6.000], mean action: 48.222 [1.000, 95.000],  loss: 7.074104, mae: 2.491109, mean_q: 4.575932\n",
      "[32 49 71 27 41 48 90 23 20  2]\n",
      " 46530/50001: episode: 5170, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 41.222 [2.000, 90.000],  loss: 7.932993, mae: 2.471118, mean_q: 4.561110\n",
      "[ 1 78 97 63 53 97 88 35 14  6]\n",
      " 46539/50001: episode: 5171, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 59.000 [6.000, 97.000],  loss: 8.382865, mae: 2.429523, mean_q: 4.494333\n",
      "[34 56 53 50  4 28 12 88 37 93]\n",
      " 46548/50001: episode: 5172, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 41.000, mean reward:  4.556 [ 3.000,  6.000], mean action: 46.778 [4.000, 93.000],  loss: 9.246797, mae: 2.419285, mean_q: 4.486195\n",
      "[54 66 72 11 52 17 66 36 32 34]\n",
      " 46557/50001: episode: 5173, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 26.000, mean reward:  2.889 [-10.000, 11.000], mean action: 42.889 [11.000, 72.000],  loss: 7.522100, mae: 2.413226, mean_q: 4.488634\n",
      "[72 95 97 66 28 57 98  1 75 56]\n",
      " 46566/50001: episode: 5174, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 42.000, mean reward:  4.667 [ 3.000,  6.000], mean action: 63.667 [1.000, 98.000],  loss: 8.323610, mae: 2.417167, mean_q: 4.458960\n",
      "[41  6 85 40 55 97 95 66  8 27]\n",
      " 46575/50001: episode: 5175, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 43.000, mean reward:  4.778 [ 3.000,  8.000], mean action: 53.222 [6.000, 97.000],  loss: 7.027998, mae: 2.473057, mean_q: 4.543931\n",
      "[59 56 87 89 34 25 75  2  2 88]\n",
      " 46584/50001: episode: 5176, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 50.889 [2.000, 89.000],  loss: 6.710945, mae: 2.486307, mean_q: 4.599246\n",
      "[74 10 24 60 54  2 11  6 71  6]\n",
      " 46593/50001: episode: 5177, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 20.000, mean reward:  2.222 [-10.000,  5.000], mean action: 27.111 [2.000, 71.000],  loss: 4.323328, mae: 2.570481, mean_q: 4.691922\n",
      "[44 14 62 99 12 54 50  4 37 67]\n",
      " 46602/50001: episode: 5178, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 44.333 [4.000, 99.000],  loss: 6.027671, mae: 2.622812, mean_q: 4.768717\n",
      "[22 13 88 37 32 37 84 38 78 23]\n",
      " 46611/50001: episode: 5179, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 22.000, mean reward:  2.444 [-10.000,  7.000], mean action: 47.778 [13.000, 88.000],  loss: 6.415099, mae: 2.651428, mean_q: 4.917980\n",
      "[83 60  1 98 88 57 95  5  9 94]\n",
      " 46620/50001: episode: 5180, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 41.000, mean reward:  4.556 [ 4.000,  7.000], mean action: 56.333 [1.000, 98.000],  loss: 10.145999, mae: 2.658362, mean_q: 4.885036\n",
      "[32 28 86  0 47 98 30  6 95  4]\n",
      " 46629/50001: episode: 5181, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 43.000, mean reward:  4.778 [ 2.000,  9.000], mean action: 43.778 [0.000, 98.000],  loss: 4.815285, mae: 2.643232, mean_q: 4.869905\n",
      "[12 46 33 97 29 72 78 21 99 63]\n",
      " 46638/50001: episode: 5182, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 34.000, mean reward:  3.778 [ 2.000,  6.000], mean action: 59.778 [21.000, 99.000],  loss: 6.739909, mae: 2.594191, mean_q: 4.796913\n",
      "[27 12 11 54  1 82 66 95 79 48]\n",
      " 46647/50001: episode: 5183, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 44.000, mean reward:  4.889 [ 3.000,  8.000], mean action: 49.778 [1.000, 95.000],  loss: 7.074969, mae: 2.688257, mean_q: 4.912279\n",
      "[50 24 59 64 86 21 57 13 64 38]\n",
      " 46656/50001: episode: 5184, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 20.000, mean reward:  2.222 [-10.000,  7.000], mean action: 47.333 [13.000, 86.000],  loss: 8.744403, mae: 2.685222, mean_q: 4.883543\n",
      "[73 90 37 12  6 20 33 68 60 53]\n",
      " 46665/50001: episode: 5185, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 42.111 [6.000, 90.000],  loss: 9.360833, mae: 2.555881, mean_q: 4.765606\n",
      "[52 28 64 95 77 60  1 74 37 81]\n",
      " 46674/50001: episode: 5186, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 57.444 [1.000, 95.000],  loss: 7.718326, mae: 2.486426, mean_q: 4.620170\n",
      "[72  9 57 63 87 88 95 93 31 57]\n",
      " 46683/50001: episode: 5187, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 28.000, mean reward:  3.111 [-10.000,  8.000], mean action: 64.444 [9.000, 95.000],  loss: 6.662773, mae: 2.441092, mean_q: 4.584284\n",
      "[21 13 96 10 82 92 11  1 90 75]\n",
      " 46692/50001: episode: 5188, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 52.222 [1.000, 96.000],  loss: 5.540602, mae: 2.508117, mean_q: 4.596753\n",
      "[59  5 35 38 79 78 67 71 12  4]\n",
      " 46701/50001: episode: 5189, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 39.000, mean reward:  4.333 [ 2.000,  9.000], mean action: 43.222 [4.000, 79.000],  loss: 9.734172, mae: 2.549481, mean_q: 4.738925\n",
      "[60 48 97 34 27  1  4 89 94 17]\n",
      " 46710/50001: episode: 5190, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 34.000, mean reward:  3.778 [ 2.000,  5.000], mean action: 45.667 [1.000, 97.000],  loss: 6.530395, mae: 2.598441, mean_q: 4.768499\n",
      "[ 3  4 26  4 72 47 56 16 82 69]\n",
      " 46719/50001: episode: 5191, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 41.778 [4.000, 82.000],  loss: 9.175719, mae: 2.513072, mean_q: 4.671169\n",
      "[ 7 37  8 90 20  2 29 31 27 37]\n",
      " 46728/50001: episode: 5192, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 31.222 [2.000, 90.000],  loss: 4.580157, mae: 2.573153, mean_q: 4.724642\n",
      "[58 66 66 75 28 53 88 21 23  2]\n",
      " 46737/50001: episode: 5193, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 26.000, mean reward:  2.889 [-10.000,  6.000], mean action: 46.889 [2.000, 88.000],  loss: 7.770726, mae: 2.623765, mean_q: 4.758430\n",
      "[75 31 88 57 89 57 87 57 12 82]\n",
      " 46746/50001: episode: 5194, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 11.000, mean reward:  1.222 [-10.000,  7.000], mean action: 62.222 [12.000, 89.000],  loss: 5.338985, mae: 2.578040, mean_q: 4.749797\n",
      "[65 13 28 42 37 92 37 10 41 74]\n",
      " 46755/50001: episode: 5195, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 22.000, mean reward:  2.444 [-10.000,  5.000], mean action: 41.556 [10.000, 92.000],  loss: 6.829199, mae: 2.632012, mean_q: 4.819623\n",
      "[43 56 55 37 77  5 41 66 50 51]\n",
      " 46764/50001: episode: 5196, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 43.000, mean reward:  4.778 [ 3.000,  8.000], mean action: 48.667 [5.000, 77.000],  loss: 5.443331, mae: 2.633364, mean_q: 4.854953\n",
      "[61 27 15 53 53 90 83  2 40 28]\n",
      " 46773/50001: episode: 5197, duration: 0.082s, episode steps:   9, steps per second: 109, episode reward: 26.000, mean reward:  2.889 [-10.000,  8.000], mean action: 43.444 [2.000, 90.000],  loss: 9.016355, mae: 2.652772, mean_q: 4.931206\n",
      "[ 1 19 41 50 30 96 66 45 66 97]\n",
      " 46782/50001: episode: 5198, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 56.667 [19.000, 97.000],  loss: 7.108397, mae: 2.657060, mean_q: 4.874131\n",
      "[62  6 82 67 92 42  1 37  9 12]\n",
      " 46791/50001: episode: 5199, duration: 0.076s, episode steps:   9, steps per second: 119, episode reward: 46.000, mean reward:  5.111 [ 3.000,  9.000], mean action: 38.667 [1.000, 92.000],  loss: 6.563299, mae: 2.591908, mean_q: 4.797517\n",
      "[59  2 28 45 92 32 11 87 98  8]\n",
      " 46800/50001: episode: 5200, duration: 0.069s, episode steps:   9, steps per second: 131, episode reward: 36.000, mean reward:  4.000 [ 2.000,  6.000], mean action: 44.778 [2.000, 98.000],  loss: 6.742299, mae: 2.608191, mean_q: 4.871100\n",
      "[60 37 53 95 88 42 42 83  4 12]\n",
      " 46809/50001: episode: 5201, duration: 0.089s, episode steps:   9, steps per second: 102, episode reward: 27.000, mean reward:  3.000 [-10.000,  8.000], mean action: 50.667 [4.000, 95.000],  loss: 7.313548, mae: 2.583254, mean_q: 4.796348\n",
      "[11 66 53 30 46 60 68 52  5 31]\n",
      " 46818/50001: episode: 5202, duration: 0.091s, episode steps:   9, steps per second:  99, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 45.667 [5.000, 68.000],  loss: 9.467638, mae: 2.573232, mean_q: 4.814792\n",
      "[38 74 79 92 81 10 48 76 14 57]\n",
      " 46827/50001: episode: 5203, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 59.000 [10.000, 92.000],  loss: 6.253739, mae: 2.540306, mean_q: 4.744320\n",
      "[36 88  2 47 44 10 97 27 42 14]\n",
      " 46836/50001: episode: 5204, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 41.222 [2.000, 97.000],  loss: 6.587653, mae: 2.503737, mean_q: 4.634164\n",
      "[10 14 84  2 87 93 39 66 37  7]\n",
      " 46845/50001: episode: 5205, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward: 34.000, mean reward:  3.778 [ 1.000,  8.000], mean action: 47.667 [2.000, 93.000],  loss: 6.996103, mae: 2.570908, mean_q: 4.724922\n",
      "[30 34 13 68 85 41 78  1 27 41]\n",
      " 46854/50001: episode: 5206, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 43.111 [1.000, 85.000],  loss: 5.834569, mae: 2.660055, mean_q: 4.847173\n",
      "[46 34 80  1 77 33 15 27 40 15]\n",
      " 46863/50001: episode: 5207, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 20.000, mean reward:  2.222 [-10.000,  6.000], mean action: 35.778 [1.000, 80.000],  loss: 5.770440, mae: 2.669678, mean_q: 4.941263\n",
      "[59 52 56  1 26 27 13 84 26 88]\n",
      " 46872/50001: episode: 5208, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 26.000, mean reward:  2.889 [-10.000,  9.000], mean action: 41.444 [1.000, 88.000],  loss: 6.013108, mae: 2.759584, mean_q: 5.088433\n",
      "[62 68 34 31 96 89  2 52 97 52]\n",
      " 46881/50001: episode: 5209, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 57.889 [2.000, 97.000],  loss: 8.101746, mae: 2.696840, mean_q: 4.930587\n",
      "[47 95 21 27 60 11 74 90 69 98]\n",
      " 46890/50001: episode: 5210, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 60.556 [11.000, 98.000],  loss: 8.478819, mae: 2.657496, mean_q: 4.907492\n",
      "[12 48  2  2 90 75 79 31 57 17]\n",
      " 46899/50001: episode: 5211, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 21.000, mean reward:  2.333 [-10.000,  5.000], mean action: 44.556 [2.000, 90.000],  loss: 8.541378, mae: 2.581908, mean_q: 4.761624\n",
      "[ 5 94 97 46  7 13 16 46 25 27]\n",
      " 46908/50001: episode: 5212, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 21.000, mean reward:  2.333 [-10.000,  7.000], mean action: 41.222 [7.000, 97.000],  loss: 8.659318, mae: 2.500431, mean_q: 4.611913\n",
      "[71 51 95 12 70 21 25 21  4 99]\n",
      " 46917/50001: episode: 5213, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 44.222 [4.000, 99.000],  loss: 8.422684, mae: 2.408193, mean_q: 4.519938\n",
      "[ 3 27 90 53 37 14 62 98 15 34]\n",
      " 46926/50001: episode: 5214, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 43.000, mean reward:  4.778 [ 3.000, 10.000], mean action: 47.778 [14.000, 98.000],  loss: 6.483722, mae: 2.430620, mean_q: 4.535662\n",
      "[25 28 95 16  1 24 26 74 48 40]\n",
      " 46935/50001: episode: 5215, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 39.000, mean reward:  4.333 [ 2.000,  6.000], mean action: 39.111 [1.000, 95.000],  loss: 4.284327, mae: 2.495235, mean_q: 4.612486\n",
      "[61 32 95 89 63 48 86 93 13 94]\n",
      " 46944/50001: episode: 5216, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 68.111 [13.000, 95.000],  loss: 8.310189, mae: 2.610399, mean_q: 4.739986\n",
      "[71 68  4 16 47 28 95 12 76 18]\n",
      " 46953/50001: episode: 5217, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 40.444 [4.000, 95.000],  loss: 6.526568, mae: 2.615531, mean_q: 4.836667\n",
      "[94 37 23 97  9 34 48 34 34 43]\n",
      " 46962/50001: episode: 5218, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward:  8.000, mean reward:  0.889 [-10.000,  6.000], mean action: 39.889 [9.000, 97.000],  loss: 7.937234, mae: 2.615417, mean_q: 4.787185\n",
      "[10  2  1 12 45 54 56 31 40 35]\n",
      " 46971/50001: episode: 5219, duration: 0.069s, episode steps:   9, steps per second: 130, episode reward: 36.000, mean reward:  4.000 [ 2.000,  6.000], mean action: 30.667 [1.000, 56.000],  loss: 8.695372, mae: 2.525287, mean_q: 4.643278\n",
      "[13 88 90 66 23  1 89  1 34  1]\n",
      " 46980/50001: episode: 5220, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward:  9.000, mean reward:  1.000 [-10.000,  7.000], mean action: 43.667 [1.000, 90.000],  loss: 8.240825, mae: 2.512181, mean_q: 4.602063\n",
      "[69 14 24  3 87 40 74 54 12 74]\n",
      " 46989/50001: episode: 5221, duration: 0.071s, episode steps:   9, steps per second: 127, episode reward: 23.000, mean reward:  2.556 [-10.000,  8.000], mean action: 42.444 [3.000, 87.000],  loss: 6.960526, mae: 2.507517, mean_q: 4.651067\n",
      "[16 87 82 77 95 17 47 41 56 95]\n",
      " 46998/50001: episode: 5222, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 66.333 [17.000, 95.000],  loss: 7.244654, mae: 2.500430, mean_q: 4.634947\n",
      "[71  1 28 66 71 46 13 62 19 48]\n",
      " 47007/50001: episode: 5223, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 29.000, mean reward:  3.222 [-10.000,  8.000], mean action: 39.333 [1.000, 71.000],  loss: 6.844642, mae: 2.479190, mean_q: 4.608747\n",
      "[55 54 18 91 62 20  1 90 98 75]\n",
      " 47016/50001: episode: 5224, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 56.556 [1.000, 98.000],  loss: 8.694197, mae: 2.546235, mean_q: 4.669417\n",
      "[88 84 76 49 32 56 49 63 24 64]\n",
      " 47025/50001: episode: 5225, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 55.222 [24.000, 84.000],  loss: 7.672776, mae: 2.503613, mean_q: 4.620656\n",
      "[31 99 22 44 24 42 47 65 31 75]\n",
      " 47034/50001: episode: 5226, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 49.889 [22.000, 99.000],  loss: 7.260842, mae: 2.520041, mean_q: 4.623278\n",
      "[32 48 74  9 78 17 90 95 73 61]\n",
      " 47043/50001: episode: 5227, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 36.000, mean reward:  4.000 [ 3.000,  9.000], mean action: 60.556 [9.000, 95.000],  loss: 9.171567, mae: 2.581132, mean_q: 4.771917\n",
      "[ 8 88 74 83 50 57 24 58 52 31]\n",
      " 47052/50001: episode: 5228, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 57.444 [24.000, 88.000],  loss: 6.381582, mae: 2.562677, mean_q: 4.696280\n",
      "[48 88 40 96 67 23 65 37 24 50]\n",
      " 47061/50001: episode: 5229, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 54.444 [23.000, 96.000],  loss: 8.324359, mae: 2.552039, mean_q: 4.701845\n",
      "[ 5  1 19 38 77 24 67 50 79 41]\n",
      " 47070/50001: episode: 5230, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 44.000 [1.000, 79.000],  loss: 5.419188, mae: 2.554389, mean_q: 4.686882\n",
      "[83 50 48 46 45 90 95 26 32 98]\n",
      " 47079/50001: episode: 5231, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 58.889 [26.000, 98.000],  loss: 7.236168, mae: 2.508263, mean_q: 4.648411\n",
      "[70 71 98 63 81 13 13  1 40  1]\n",
      " 47088/50001: episode: 5232, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward: 10.000, mean reward:  1.111 [-10.000,  6.000], mean action: 42.333 [1.000, 98.000],  loss: 8.237949, mae: 2.460931, mean_q: 4.558187\n",
      "[63 98 63 40 82 62  2  1 46 97]\n",
      " 47097/50001: episode: 5233, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 29.000, mean reward:  3.222 [-10.000,  7.000], mean action: 54.556 [1.000, 98.000],  loss: 5.721100, mae: 2.537550, mean_q: 4.711405\n",
      "[72 10 28 88 50 37 21 50 40 29]\n",
      " 47106/50001: episode: 5234, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 39.222 [10.000, 88.000],  loss: 7.259054, mae: 2.590148, mean_q: 4.761120\n",
      "[42 64 86 81 37  1 42 63 24 75]\n",
      " 47115/50001: episode: 5235, duration: 0.069s, episode steps:   9, steps per second: 130, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 52.556 [1.000, 86.000],  loss: 6.309295, mae: 2.635637, mean_q: 4.821588\n",
      "[ 6 95 94 88 37  1 18 88 48 37]\n",
      " 47124/50001: episode: 5236, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 12.000, mean reward:  1.333 [-10.000,  6.000], mean action: 56.222 [1.000, 95.000],  loss: 8.433565, mae: 2.576779, mean_q: 4.804373\n",
      "[16 19 30 47 34  8 75 93 32 41]\n",
      " 47133/50001: episode: 5237, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 42.000, mean reward:  4.667 [ 3.000,  6.000], mean action: 42.111 [8.000, 93.000],  loss: 6.551894, mae: 2.518790, mean_q: 4.695440\n",
      "[99 40 21 13 16 25 27 63 24 54]\n",
      " 47142/50001: episode: 5238, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 37.000, mean reward:  4.111 [ 3.000,  6.000], mean action: 31.444 [13.000, 63.000],  loss: 6.479043, mae: 2.528393, mean_q: 4.686533\n",
      "[46 40 24 98 88  2 60 38 82 34]\n",
      " 47151/50001: episode: 5239, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 42.000, mean reward:  4.667 [ 2.000,  9.000], mean action: 51.778 [2.000, 98.000],  loss: 6.530874, mae: 2.528599, mean_q: 4.660559\n",
      "[12 40 77 82 44 75 62  4 56 27]\n",
      " 47160/50001: episode: 5240, duration: 0.078s, episode steps:   9, steps per second: 116, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 51.889 [4.000, 82.000],  loss: 7.928924, mae: 2.544716, mean_q: 4.726330\n",
      "[60  5 64 81 68 40 42 64  4 12]\n",
      " 47169/50001: episode: 5241, duration: 0.082s, episode steps:   9, steps per second: 110, episode reward: 25.000, mean reward:  2.778 [-10.000,  8.000], mean action: 42.222 [4.000, 81.000],  loss: 6.212912, mae: 2.557938, mean_q: 4.817509\n",
      "[42 91 74 93 51 49  4 97 75 12]\n",
      " 47178/50001: episode: 5242, duration: 0.078s, episode steps:   9, steps per second: 116, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 60.667 [4.000, 97.000],  loss: 8.257236, mae: 2.601429, mean_q: 4.861887\n",
      "[81 75 24 93  8  4 47 57  5 82]\n",
      " 47187/50001: episode: 5243, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 40.000, mean reward:  4.444 [ 3.000,  6.000], mean action: 43.889 [4.000, 93.000],  loss: 5.643448, mae: 2.641643, mean_q: 4.890811\n",
      "[ 2 83 95  2 70 37 48 79 30 88]\n",
      " 47196/50001: episode: 5244, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 29.000, mean reward:  3.222 [-10.000,  8.000], mean action: 59.111 [2.000, 95.000],  loss: 7.053997, mae: 2.660996, mean_q: 4.962319\n",
      "[16 60 53 47 80 59 46 41  2 57]\n",
      " 47205/50001: episode: 5245, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 49.444 [2.000, 80.000],  loss: 4.272753, mae: 2.631139, mean_q: 4.882406\n",
      "[91 37  4 91 28 95  2 95 12 37]\n",
      " 47214/50001: episode: 5246, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: -1.000, mean reward: -0.111 [-10.000,  6.000], mean action: 44.556 [2.000, 95.000],  loss: 6.305340, mae: 2.672617, mean_q: 4.938756\n",
      "[95  1 50 23 16 13 98 48 50 35]\n",
      " 47223/50001: episode: 5247, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 22.000, mean reward:  2.444 [-10.000,  5.000], mean action: 37.111 [1.000, 98.000],  loss: 7.564283, mae: 2.759679, mean_q: 5.113050\n",
      "[83 89 15 61  1  4 10  6 63 57]\n",
      " 47232/50001: episode: 5248, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 37.000, mean reward:  4.111 [ 2.000,  7.000], mean action: 34.000 [1.000, 89.000],  loss: 9.504545, mae: 2.641734, mean_q: 4.900843\n",
      "[75 12 91 30 96 57 50 40 93 50]\n",
      " 47241/50001: episode: 5249, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 57.667 [12.000, 96.000],  loss: 7.177155, mae: 2.566546, mean_q: 4.734761\n",
      "[85 32 50  1 41  9 59 13 95 31]\n",
      " 47250/50001: episode: 5250, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 44.000, mean reward:  4.889 [ 3.000,  9.000], mean action: 36.778 [1.000, 95.000],  loss: 7.240360, mae: 2.521424, mean_q: 4.663985\n",
      "[46 60 93 28 41 75 77 41  1 17]\n",
      " 47259/50001: episode: 5251, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 23.000, mean reward:  2.556 [-10.000,  8.000], mean action: 48.111 [1.000, 93.000],  loss: 8.931865, mae: 2.513340, mean_q: 4.609903\n",
      "[ 3 93 69 12 44 24 78 37 20 44]\n",
      " 47268/50001: episode: 5252, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 24.000, mean reward:  2.667 [-10.000,  8.000], mean action: 46.778 [12.000, 93.000],  loss: 7.549531, mae: 2.562416, mean_q: 4.733892\n",
      "[41 63 31  3 72 50 91 26 48 64]\n",
      " 47277/50001: episode: 5253, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 37.000, mean reward:  4.111 [ 2.000,  8.000], mean action: 49.778 [3.000, 91.000],  loss: 6.462606, mae: 2.470179, mean_q: 4.585869\n",
      "[59 95 66 93 12 31 63 75 28 12]\n",
      " 47286/50001: episode: 5254, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 52.778 [12.000, 95.000],  loss: 7.584443, mae: 2.570931, mean_q: 4.774885\n",
      "[64 95 46 56 48 23 50 34 34 93]\n",
      " 47295/50001: episode: 5255, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 28.000, mean reward:  3.111 [-10.000,  7.000], mean action: 53.222 [23.000, 95.000],  loss: 7.521879, mae: 2.545856, mean_q: 4.656347\n",
      "[71 30 40 23  6 31 75 52 50 18]\n",
      " 47304/50001: episode: 5256, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 39.000, mean reward:  4.333 [ 3.000,  8.000], mean action: 36.111 [6.000, 75.000],  loss: 6.845168, mae: 2.544454, mean_q: 4.778222\n",
      "[95 82  7 97 37 62 91 34 24 62]\n",
      " 47313/50001: episode: 5257, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 55.111 [7.000, 97.000],  loss: 6.271545, mae: 2.602745, mean_q: 4.812952\n",
      "[88 90 66 90 37 63 65 91 31 31]\n",
      " 47322/50001: episode: 5258, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward:  7.000, mean reward:  0.778 [-10.000,  6.000], mean action: 62.667 [31.000, 91.000],  loss: 5.800364, mae: 2.605100, mean_q: 4.823583\n",
      "[34 12 82 98 37 27 89 57 93 12]\n",
      " 47331/50001: episode: 5259, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 56.333 [12.000, 98.000],  loss: 7.580098, mae: 2.653283, mean_q: 4.881440\n",
      "[59 95 63 68  1 70 47 65 85 66]\n",
      " 47340/50001: episode: 5260, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 40.000, mean reward:  4.444 [ 2.000,  9.000], mean action: 62.222 [1.000, 95.000],  loss: 8.318936, mae: 2.655024, mean_q: 4.871952\n",
      "[36 21 94 94 47 51 88  4 82 23]\n",
      " 47349/50001: episode: 5261, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 27.000, mean reward:  3.000 [-10.000,  6.000], mean action: 56.000 [4.000, 94.000],  loss: 6.525246, mae: 2.615110, mean_q: 4.863791\n",
      "[65 76 60 30 14 47 13  1 46 76]\n",
      " 47358/50001: episode: 5262, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 29.000, mean reward:  3.222 [-10.000,  6.000], mean action: 40.333 [1.000, 76.000],  loss: 8.013561, mae: 2.587344, mean_q: 4.799431\n",
      "[81 37 37 53  2  9 88 10 30 87]\n",
      " 47367/50001: episode: 5263, duration: 0.072s, episode steps:   9, steps per second: 125, episode reward: 21.000, mean reward:  2.333 [-10.000,  5.000], mean action: 39.222 [2.000, 88.000],  loss: 6.299640, mae: 2.569108, mean_q: 4.831327\n",
      "[64  5 92 28 42 89 94  4 12 66]\n",
      " 47376/50001: episode: 5264, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 43.000, mean reward:  4.778 [ 3.000,  7.000], mean action: 48.000 [4.000, 94.000],  loss: 5.327248, mae: 2.572610, mean_q: 4.825491\n",
      "[ 8 21 46 82 71 52  4 58 41 12]\n",
      " 47385/50001: episode: 5265, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 44.000, mean reward:  4.889 [ 2.000, 10.000], mean action: 43.000 [4.000, 82.000],  loss: 4.948859, mae: 2.652673, mean_q: 4.945219\n",
      "[18 89 28 50 43 76 46 88 57 13]\n",
      " 47394/50001: episode: 5266, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 45.000, mean reward:  5.000 [ 2.000,  9.000], mean action: 54.444 [13.000, 89.000],  loss: 7.777572, mae: 2.677200, mean_q: 4.879177\n",
      "[41 87 88 94 74 13  4 78 89 12]\n",
      " 47403/50001: episode: 5267, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 41.000, mean reward:  4.556 [ 3.000,  9.000], mean action: 59.889 [4.000, 94.000],  loss: 5.056972, mae: 2.625752, mean_q: 4.902796\n",
      "[90 14 93 50 27 13 64 97 12 12]\n",
      " 47412/50001: episode: 5268, duration: 0.068s, episode steps:   9, steps per second: 133, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 42.444 [12.000, 97.000],  loss: 5.762065, mae: 2.609700, mean_q: 4.851777\n",
      "[32 34  2 82 40 15 38 91 14 98]\n",
      " 47421/50001: episode: 5269, duration: 0.068s, episode steps:   9, steps per second: 133, episode reward: 36.000, mean reward:  4.000 [ 2.000,  7.000], mean action: 46.000 [2.000, 98.000],  loss: 7.595304, mae: 2.689742, mean_q: 5.031180\n",
      "[19 94 47 10 20 60 13 96 31 33]\n",
      " 47430/50001: episode: 5270, duration: 0.068s, episode steps:   9, steps per second: 133, episode reward: 37.000, mean reward:  4.111 [ 3.000,  6.000], mean action: 44.889 [10.000, 96.000],  loss: 7.177639, mae: 2.681325, mean_q: 5.043645\n",
      "[44 49 42 30  4 97 57  1  9 84]\n",
      " 47439/50001: episode: 5271, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 42.000, mean reward:  4.667 [ 3.000,  6.000], mean action: 41.444 [1.000, 97.000],  loss: 6.225557, mae: 2.723813, mean_q: 5.099444\n",
      "[69 87 48 95 95 85 11 37 23 34]\n",
      " 47448/50001: episode: 5272, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 28.000, mean reward:  3.111 [-10.000,  8.000], mean action: 57.222 [11.000, 95.000],  loss: 6.189965, mae: 2.669464, mean_q: 4.931920\n",
      "[ 7 12 19  2 53 87 76 41 62  4]\n",
      " 47457/50001: episode: 5273, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 39.556 [2.000, 87.000],  loss: 5.812296, mae: 2.616913, mean_q: 4.777669\n",
      "[65 54 12 34 93  9  1 33 46 47]\n",
      " 47466/50001: episode: 5274, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 45.000, mean reward:  5.000 [ 3.000,  9.000], mean action: 36.556 [1.000, 93.000],  loss: 9.963329, mae: 2.585962, mean_q: 4.746241\n",
      "[85 24 28 35 14 52 50 78 40 69]\n",
      " 47475/50001: episode: 5275, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 43.333 [14.000, 78.000],  loss: 8.109408, mae: 2.555090, mean_q: 4.726316\n",
      "[29 32 64 91 37 75 24 93 37 85]\n",
      " 47484/50001: episode: 5276, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 59.778 [24.000, 93.000],  loss: 6.981352, mae: 2.468783, mean_q: 4.609632\n",
      "[97 89 95 31 95 97 12 58 94 36]\n",
      " 47493/50001: episode: 5277, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward:  4.000, mean reward:  0.444 [-10.000,  5.000], mean action: 67.444 [12.000, 97.000],  loss: 6.581909, mae: 2.569439, mean_q: 4.783514\n",
      "[83 51 93 89 72 60 17 95 34 32]\n",
      " 47502/50001: episode: 5278, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 46.000, mean reward:  5.111 [ 2.000,  9.000], mean action: 60.333 [17.000, 95.000],  loss: 8.504022, mae: 2.558737, mean_q: 4.725132\n",
      "[ 0 27 62 98 84 46 95 89 36 27]\n",
      " 47511/50001: episode: 5279, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 24.000, mean reward:  2.667 [-10.000,  8.000], mean action: 62.667 [27.000, 98.000],  loss: 5.388298, mae: 2.580894, mean_q: 4.759685\n",
      "[53 30 11 63  2 97 42 36 10 64]\n",
      " 47520/50001: episode: 5280, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 35.000, mean reward:  3.889 [ 1.000,  6.000], mean action: 39.444 [2.000, 97.000],  loss: 7.363764, mae: 2.577507, mean_q: 4.762643\n",
      "[94 95 44 42 13 75 79 88 31 54]\n",
      " 47529/50001: episode: 5281, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 39.000, mean reward:  4.333 [ 2.000,  6.000], mean action: 57.889 [13.000, 95.000],  loss: 8.480892, mae: 2.530198, mean_q: 4.662934\n",
      "[78 28 54  4 94 30 10 28 61 97]\n",
      " 47538/50001: episode: 5282, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 23.000, mean reward:  2.556 [-10.000,  8.000], mean action: 45.111 [4.000, 97.000],  loss: 8.506413, mae: 2.517543, mean_q: 4.654144\n",
      "[72 98 60 50 94 40  8 84 53 47]\n",
      " 47547/50001: episode: 5283, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 39.000, mean reward:  4.333 [ 3.000,  8.000], mean action: 59.333 [8.000, 98.000],  loss: 4.494310, mae: 2.496888, mean_q: 4.633700\n",
      "[87 19 51 16 46 12  4 88 57 79]\n",
      " 47556/50001: episode: 5284, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 40.000, mean reward:  4.444 [ 3.000,  6.000], mean action: 41.333 [4.000, 88.000],  loss: 7.277842, mae: 2.606191, mean_q: 4.824694\n",
      "[23 37 17 26 13 71 11 27 63 97]\n",
      " 47565/50001: episode: 5285, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 37.000, mean reward:  4.111 [ 2.000,  8.000], mean action: 40.222 [11.000, 97.000],  loss: 6.558274, mae: 2.583289, mean_q: 4.692327\n",
      "[77 95 49 84 14 13 13 97 85 23]\n",
      " 47574/50001: episode: 5286, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 52.556 [13.000, 97.000],  loss: 5.281778, mae: 2.590167, mean_q: 4.796095\n",
      "[39 37 12  7 54 60 48 60 54 28]\n",
      " 47583/50001: episode: 5287, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 12.000, mean reward:  1.333 [-10.000,  8.000], mean action: 40.000 [7.000, 60.000],  loss: 6.591409, mae: 2.652128, mean_q: 4.935061\n",
      "[94 78 47 46 43 32 99 84 93 34]\n",
      " 47592/50001: episode: 5288, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 41.000, mean reward:  4.556 [ 2.000, 10.000], mean action: 61.778 [32.000, 99.000],  loss: 7.554792, mae: 2.634799, mean_q: 4.873951\n",
      "[18 89 22 64 79 90 33 88 57 74]\n",
      " 47601/50001: episode: 5289, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 66.222 [22.000, 90.000],  loss: 5.891000, mae: 2.616196, mean_q: 4.834802\n",
      "[83 24 16 29 97 49 24 92 98 88]\n",
      " 47610/50001: episode: 5290, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 27.000, mean reward:  3.000 [-10.000,  8.000], mean action: 57.444 [16.000, 98.000],  loss: 9.583610, mae: 2.593681, mean_q: 4.772662\n",
      "[49 98 16 87 93 92 95  1 87 63]\n",
      " 47619/50001: episode: 5291, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 27.000, mean reward:  3.000 [-10.000,  8.000], mean action: 70.222 [1.000, 98.000],  loss: 6.896087, mae: 2.542379, mean_q: 4.701366\n",
      "[45 84  8 81  8 53 47 32 56 98]\n",
      " 47628/50001: episode: 5292, duration: 0.065s, episode steps:   9, steps per second: 140, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 51.889 [8.000, 98.000],  loss: 7.438712, mae: 2.494394, mean_q: 4.639526\n",
      "[38 91 41  2 35 99 97 74  8 49]\n",
      " 47637/50001: episode: 5293, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 55.111 [2.000, 99.000],  loss: 8.339397, mae: 2.565111, mean_q: 4.745694\n",
      "[80 14 94 88 44 13 70  2 23 55]\n",
      " 47646/50001: episode: 5294, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 36.000, mean reward:  4.000 [ 2.000,  6.000], mean action: 44.778 [2.000, 94.000],  loss: 7.130190, mae: 2.551521, mean_q: 4.777209\n",
      "[35 46 76 41 31 14 66 81 12 26]\n",
      " 47655/50001: episode: 5295, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 39.000, mean reward:  4.333 [ 1.000,  7.000], mean action: 43.667 [12.000, 81.000],  loss: 6.169657, mae: 2.602734, mean_q: 4.812230\n",
      "[31 41 57 56 95 68 34  5 11 13]\n",
      " 47664/50001: episode: 5296, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 42.222 [5.000, 95.000],  loss: 7.475626, mae: 2.625262, mean_q: 4.854604\n",
      "[77 95 66  1 69 13 32  4 18 33]\n",
      " 47673/50001: episode: 5297, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 35.000, mean reward:  3.889 [ 3.000,  5.000], mean action: 36.778 [1.000, 95.000],  loss: 5.523022, mae: 2.640508, mean_q: 4.877508\n",
      "[51 82  2 76 13  2  9  1  4 76]\n",
      " 47682/50001: episode: 5298, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 10.000, mean reward:  1.111 [-10.000,  6.000], mean action: 29.444 [1.000, 82.000],  loss: 7.579927, mae: 2.642757, mean_q: 4.823104\n",
      "[60 98 11 16 82 66 75 66 40  0]\n",
      " 47691/50001: episode: 5299, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 21.000, mean reward:  2.333 [-10.000,  5.000], mean action: 50.444 [0.000, 98.000],  loss: 6.766743, mae: 2.707571, mean_q: 4.967454\n",
      "[93  9 34 93 37 76 50 28 59 27]\n",
      " 47700/50001: episode: 5300, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward: 27.000, mean reward:  3.000 [-10.000,  6.000], mean action: 45.889 [9.000, 93.000],  loss: 4.777248, mae: 2.648199, mean_q: 4.835575\n",
      "[16 34 47 41  2 92 13 36 56 87]\n",
      " 47709/50001: episode: 5301, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 35.000, mean reward:  3.889 [ 1.000,  6.000], mean action: 45.333 [2.000, 92.000],  loss: 7.517314, mae: 2.653878, mean_q: 4.845520\n",
      "[54 34 48 46 37  1 99 13 37 90]\n",
      " 47718/50001: episode: 5302, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward: 27.000, mean reward:  3.000 [-10.000,  6.000], mean action: 45.000 [1.000, 99.000],  loss: 6.231483, mae: 2.638143, mean_q: 4.870807\n",
      "[46  0 27 31 85 60 74 88 24 98]\n",
      " 47727/50001: episode: 5303, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 43.000, mean reward:  4.778 [ 2.000,  7.000], mean action: 54.111 [0.000, 98.000],  loss: 8.734615, mae: 2.648215, mean_q: 4.867407\n",
      "[18 60  8  9 47 82  1 95 47 56]\n",
      " 47736/50001: episode: 5304, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 29.000, mean reward:  3.222 [-10.000,  8.000], mean action: 45.000 [1.000, 95.000],  loss: 5.730025, mae: 2.602180, mean_q: 4.817366\n",
      "[11 91 50 56 16 48 95 27 57 24]\n",
      " 47745/50001: episode: 5305, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 51.556 [16.000, 95.000],  loss: 5.253775, mae: 2.710039, mean_q: 4.954190\n",
      "[43 45 50 98 86 68 98 41 98 26]\n",
      " 47754/50001: episode: 5306, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward:  8.000, mean reward:  0.889 [-10.000,  5.000], mean action: 67.778 [26.000, 98.000],  loss: 9.136244, mae: 2.761208, mean_q: 5.060402\n",
      "[68 50 32 69  2 80 46  1 96 57]\n",
      " 47763/50001: episode: 5307, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 37.000, mean reward:  4.111 [ 2.000,  6.000], mean action: 48.111 [1.000, 96.000],  loss: 7.958860, mae: 2.594426, mean_q: 4.828125\n",
      "[50 62 40 77 37 62 23 49 94 32]\n",
      " 47772/50001: episode: 5308, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 52.889 [23.000, 94.000],  loss: 5.773658, mae: 2.568915, mean_q: 4.702837\n",
      "[22 34  7 14 11 41 78 81 14 88]\n",
      " 47781/50001: episode: 5309, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 25.000, mean reward:  2.778 [-10.000,  9.000], mean action: 40.889 [7.000, 88.000],  loss: 6.332589, mae: 2.624254, mean_q: 4.859152\n",
      "[65 51 25 81 86  1 13 80 93 19]\n",
      " 47790/50001: episode: 5310, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 37.000, mean reward:  4.111 [ 3.000,  6.000], mean action: 49.889 [1.000, 93.000],  loss: 7.594913, mae: 2.613719, mean_q: 4.791147\n",
      "[48 10 16 80 99 37 66 64 48 31]\n",
      " 47799/50001: episode: 5311, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 50.111 [10.000, 99.000],  loss: 7.809582, mae: 2.569467, mean_q: 4.756702\n",
      "[54 30 55  1 85 57 48 40 79 93]\n",
      " 47808/50001: episode: 5312, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 40.000, mean reward:  4.444 [ 3.000,  6.000], mean action: 54.222 [1.000, 93.000],  loss: 6.506771, mae: 2.593651, mean_q: 4.782711\n",
      "[25 88 82 13 38 76 13 99 57  4]\n",
      " 47817/50001: episode: 5313, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 52.222 [4.000, 99.000],  loss: 5.517892, mae: 2.666472, mean_q: 4.904904\n",
      "[65 86 41 97 23 89  1 66  3 13]\n",
      " 47826/50001: episode: 5314, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 44.000, mean reward:  4.889 [ 3.000,  9.000], mean action: 46.556 [1.000, 97.000],  loss: 7.953379, mae: 2.708116, mean_q: 4.995844\n",
      "[23 35 91 34 62 49 97 20 48 98]\n",
      " 47835/50001: episode: 5315, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 59.333 [20.000, 98.000],  loss: 9.234213, mae: 2.650759, mean_q: 4.942846\n",
      "[30 62 13  2 98 41 90 74 31 50]\n",
      " 47844/50001: episode: 5316, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 43.000, mean reward:  4.778 [ 3.000,  8.000], mean action: 51.222 [2.000, 98.000],  loss: 8.103313, mae: 2.579913, mean_q: 4.823387\n",
      "[65 37 28  1 86  9 79 93 50 50]\n",
      " 47853/50001: episode: 5317, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 28.000, mean reward:  3.111 [-10.000,  8.000], mean action: 48.111 [1.000, 93.000],  loss: 7.531979, mae: 2.560553, mean_q: 4.772551\n",
      "[91 50 97 95 88 63 20 13 17 14]\n",
      " 47862/50001: episode: 5318, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 38.000, mean reward:  4.222 [ 2.000,  6.000], mean action: 50.778 [13.000, 97.000],  loss: 6.458596, mae: 2.550652, mean_q: 4.742095\n",
      "[62 88 24 82 60 56 47 97 76 28]\n",
      " 47871/50001: episode: 5319, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 43.000, mean reward:  4.778 [ 4.000,  7.000], mean action: 62.000 [24.000, 97.000],  loss: 8.738000, mae: 2.528258, mean_q: 4.676096\n",
      "[99 68 28 80 45 41 10 63 10 13]\n",
      " 47880/50001: episode: 5320, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 24.000, mean reward:  2.667 [-10.000,  9.000], mean action: 39.778 [10.000, 80.000],  loss: 5.314677, mae: 2.538017, mean_q: 4.761160\n",
      "[94 66 30 41 63 14 52  2 14 14]\n",
      " 47889/50001: episode: 5321, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward:  9.000, mean reward:  1.000 [-10.000,  5.000], mean action: 32.889 [2.000, 66.000],  loss: 6.198648, mae: 2.599797, mean_q: 4.812822\n",
      "[99  8  9 49 82 95 27 95  1 97]\n",
      " 47898/50001: episode: 5322, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 29.000, mean reward:  3.222 [-10.000,  7.000], mean action: 51.444 [1.000, 97.000],  loss: 11.091145, mae: 2.579664, mean_q: 4.804591\n",
      "[19 95  7 31 30 84 96 53 13 28]\n",
      " 47907/50001: episode: 5323, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 41.000, mean reward:  4.556 [ 2.000,  8.000], mean action: 48.556 [7.000, 96.000],  loss: 6.119035, mae: 2.528737, mean_q: 4.644739\n",
      "[51 73 41 36 75 31 89 84 50 90]\n",
      " 47916/50001: episode: 5324, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 63.222 [31.000, 90.000],  loss: 5.408814, mae: 2.542810, mean_q: 4.669715\n",
      "[68 53 88 37  1 95 63 64 63 46]\n",
      " 47925/50001: episode: 5325, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 26.000, mean reward:  2.889 [-10.000,  6.000], mean action: 56.667 [1.000, 95.000],  loss: 7.430885, mae: 2.592604, mean_q: 4.771313\n",
      "[16 46 13 99 13 23 79 18 29  9]\n",
      " 47934/50001: episode: 5326, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 36.556 [9.000, 99.000],  loss: 6.628767, mae: 2.617070, mean_q: 4.803808\n",
      "[ 3 82 48 13 13 51 79 42  1 23]\n",
      " 47943/50001: episode: 5327, duration: 0.078s, episode steps:   9, steps per second: 116, episode reward: 26.000, mean reward:  2.889 [-10.000,  6.000], mean action: 39.111 [1.000, 82.000],  loss: 6.478849, mae: 2.631102, mean_q: 4.858275\n",
      "[ 5 54 37 98 94 60 63 95 64  9]\n",
      " 47952/50001: episode: 5328, duration: 0.113s, episode steps:   9, steps per second:  79, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 63.778 [9.000, 98.000],  loss: 8.455245, mae: 2.612485, mean_q: 4.785069\n",
      "[50 93 34 64 93 41 12 95 51 14]\n",
      " 47961/50001: episode: 5329, duration: 0.112s, episode steps:   9, steps per second:  80, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 55.222 [12.000, 95.000],  loss: 7.327400, mae: 2.613465, mean_q: 4.811656\n",
      "[23 62 88 12 88 32 54 99 79 96]\n",
      " 47970/50001: episode: 5330, duration: 0.113s, episode steps:   9, steps per second:  80, episode reward: 21.000, mean reward:  2.333 [-10.000,  6.000], mean action: 67.778 [12.000, 99.000],  loss: 8.810353, mae: 2.625203, mean_q: 4.791376\n",
      "[45 27 41 97 82 75 46 94 98 12]\n",
      " 47979/50001: episode: 5331, duration: 0.114s, episode steps:   9, steps per second:  79, episode reward: 45.000, mean reward:  5.000 [ 3.000,  9.000], mean action: 63.556 [12.000, 98.000],  loss: 6.964840, mae: 2.587617, mean_q: 4.779463\n",
      "[46 92 21 49 97 49 83 28 24 83]\n",
      " 47988/50001: episode: 5332, duration: 0.113s, episode steps:   9, steps per second:  80, episode reward: 10.000, mean reward:  1.111 [-10.000,  7.000], mean action: 58.444 [21.000, 97.000],  loss: 6.657041, mae: 2.570754, mean_q: 4.729910\n",
      "[ 0 34 99 14 78 31 45 88 57 91]\n",
      " 47997/50001: episode: 5333, duration: 0.069s, episode steps:   9, steps per second: 131, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 59.667 [14.000, 99.000],  loss: 6.402508, mae: 2.600733, mean_q: 4.877711\n",
      "[37 92 77 69 82 42 88 98 31 95]\n",
      " 48006/50001: episode: 5334, duration: 0.071s, episode steps:   9, steps per second: 126, episode reward: 43.000, mean reward:  4.778 [ 3.000, 10.000], mean action: 74.889 [31.000, 98.000],  loss: 9.658070, mae: 2.637128, mean_q: 4.875157\n",
      "[ 4 24 57 53 47 41 73 54 17 58]\n",
      " 48015/50001: episode: 5335, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 33.000, mean reward:  3.667 [ 2.000,  6.000], mean action: 47.111 [17.000, 73.000],  loss: 4.387336, mae: 2.582654, mean_q: 4.757724\n",
      "[37 34 37 28 14 12 47 34 40 88]\n",
      " 48024/50001: episode: 5336, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 11.000, mean reward:  1.222 [-10.000,  6.000], mean action: 37.111 [12.000, 88.000],  loss: 6.554104, mae: 2.575741, mean_q: 4.790670\n",
      "[24 69 40 28 92 85  4 51 49 33]\n",
      " 48033/50001: episode: 5337, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 32.000, mean reward:  3.556 [ 3.000,  5.000], mean action: 50.111 [4.000, 92.000],  loss: 8.190963, mae: 2.615417, mean_q: 4.803373\n",
      "[65 34 89 82 14 99 37 34 95 31]\n",
      " 48042/50001: episode: 5338, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 29.000, mean reward:  3.222 [-10.000,  8.000], mean action: 57.222 [14.000, 99.000],  loss: 6.361884, mae: 2.630193, mean_q: 4.885108\n",
      "[12 20 41 18 71 31 57 52 93 27]\n",
      " 48051/50001: episode: 5339, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 39.000, mean reward:  4.333 [ 2.000,  7.000], mean action: 45.556 [18.000, 93.000],  loss: 8.398540, mae: 2.569350, mean_q: 4.802907\n",
      "[33 74 16 50 99 83 41 28 61 68]\n",
      " 48060/50001: episode: 5340, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward: 36.000, mean reward:  4.000 [ 3.000,  6.000], mean action: 57.778 [16.000, 99.000],  loss: 9.288923, mae: 2.578405, mean_q: 4.810236\n",
      "[18 82 15 79 23 58 39  1 59 10]\n",
      " 48069/50001: episode: 5341, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 36.000, mean reward:  4.000 [ 2.000,  7.000], mean action: 40.667 [1.000, 82.000],  loss: 8.185308, mae: 2.540812, mean_q: 4.760030\n",
      "[ 2 34  1 88  9 32  1 57 68 12]\n",
      " 48078/50001: episode: 5342, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 26.000, mean reward:  2.889 [-10.000,  8.000], mean action: 33.556 [1.000, 88.000],  loss: 8.016529, mae: 2.510821, mean_q: 4.756489\n",
      "[88 75 82 28 97 27 13  2 60 50]\n",
      " 48087/50001: episode: 5343, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 44.000, mean reward:  4.889 [ 3.000,  8.000], mean action: 48.222 [2.000, 97.000],  loss: 8.778707, mae: 2.461923, mean_q: 4.570261\n",
      "[47 83 49  4 30 64 37 64 71 23]\n",
      " 48096/50001: episode: 5344, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 47.222 [4.000, 83.000],  loss: 6.980864, mae: 2.463918, mean_q: 4.551702\n",
      "[ 3 24 97  1 24  1 66  1  4 20]\n",
      " 48105/50001: episode: 5345, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: -5.000, mean reward: -0.556 [-10.000,  5.000], mean action: 26.444 [1.000, 97.000],  loss: 5.456545, mae: 2.435364, mean_q: 4.557557\n",
      "[ 3  5 31 30 48 82  3 76 57 37]\n",
      " 48114/50001: episode: 5346, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 29.000, mean reward:  3.222 [-10.000, 10.000], mean action: 41.000 [3.000, 82.000],  loss: 8.365445, mae: 2.528196, mean_q: 4.607588\n",
      "[92 98 50 46 11 77 10 13 24  1]\n",
      " 48123/50001: episode: 5347, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 36.667 [1.000, 98.000],  loss: 6.924454, mae: 2.568332, mean_q: 4.685446\n",
      "[96  5 24 79 31 77  9 54 17 89]\n",
      " 48132/50001: episode: 5348, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 33.000, mean reward:  3.667 [ 2.000,  6.000], mean action: 42.778 [5.000, 89.000],  loss: 7.194127, mae: 2.629610, mean_q: 4.812926\n",
      "[54 42 31 93 12 67  9 95 14 89]\n",
      " 48141/50001: episode: 5349, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 50.222 [9.000, 95.000],  loss: 6.739831, mae: 2.646337, mean_q: 4.932313\n",
      "[ 2  4  2 80 32 25 81 13 24 81]\n",
      " 48150/50001: episode: 5350, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward:  6.000, mean reward:  0.667 [-10.000,  6.000], mean action: 38.000 [2.000, 81.000],  loss: 8.952610, mae: 2.716414, mean_q: 5.024636\n",
      "[ 1 35 90 93 97 79 95  2  4 34]\n",
      " 48159/50001: episode: 5351, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 46.000, mean reward:  5.111 [ 3.000,  8.000], mean action: 58.778 [2.000, 97.000],  loss: 5.730903, mae: 2.669261, mean_q: 4.883265\n",
      "[72 27 13 93 37 60  9 35 97 79]\n",
      " 48168/50001: episode: 5352, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 43.000, mean reward:  4.778 [ 3.000,  7.000], mean action: 50.000 [9.000, 97.000],  loss: 6.893986, mae: 2.624625, mean_q: 4.810571\n",
      "[16 28 88 82 88 15 37 34 14 14]\n",
      " 48177/50001: episode: 5353, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 12.000, mean reward:  1.333 [-10.000,  7.000], mean action: 44.444 [14.000, 88.000],  loss: 10.114712, mae: 2.641818, mean_q: 4.854935\n",
      "[11 32 16  2 73 32 60 48 98 34]\n",
      " 48186/50001: episode: 5354, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 29.000, mean reward:  3.222 [-10.000,  9.000], mean action: 43.889 [2.000, 98.000],  loss: 8.095982, mae: 2.641298, mean_q: 4.956911\n",
      "[80 42 48 19 56  4 13 10  2  1]\n",
      " 48195/50001: episode: 5355, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 21.667 [1.000, 56.000],  loss: 8.812543, mae: 2.496221, mean_q: 4.666072\n",
      "[46 24 60 79 50  8 87 93 82 12]\n",
      " 48204/50001: episode: 5356, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 42.000, mean reward:  4.667 [ 3.000,  9.000], mean action: 55.000 [8.000, 93.000],  loss: 7.114259, mae: 2.421887, mean_q: 4.521110\n",
      "[82 49 56 38 59 42 78 82 24 62]\n",
      " 48213/50001: episode: 5357, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 54.444 [24.000, 82.000],  loss: 7.399157, mae: 2.500335, mean_q: 4.626112\n",
      "[43 93 47 79 63 15 64 28 40 45]\n",
      " 48222/50001: episode: 5358, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 35.000, mean reward:  3.889 [ 2.000,  6.000], mean action: 52.667 [15.000, 93.000],  loss: 7.780311, mae: 2.505945, mean_q: 4.697874\n",
      "[10 17 46 31 93 57 30  1 93 46]\n",
      " 48231/50001: episode: 5359, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 10.000, mean reward:  1.111 [-10.000,  7.000], mean action: 46.000 [1.000, 93.000],  loss: 9.185107, mae: 2.444177, mean_q: 4.585011\n",
      "[72  5 50  2 99 34 74 27 53 83]\n",
      " 48240/50001: episode: 5360, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 47.444 [2.000, 99.000],  loss: 7.761243, mae: 2.415988, mean_q: 4.485070\n",
      "[89 13 47 52 47 88 13 95 98 12]\n",
      " 48249/50001: episode: 5361, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 15.000, mean reward:  1.667 [-10.000,  7.000], mean action: 51.667 [12.000, 98.000],  loss: 8.369562, mae: 2.397729, mean_q: 4.452550\n",
      "[92 37 23 90 50 97  6  1 99 75]\n",
      " 48258/50001: episode: 5362, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 40.000, mean reward:  4.444 [ 3.000,  6.000], mean action: 53.111 [1.000, 99.000],  loss: 8.046438, mae: 2.379834, mean_q: 4.439970\n",
      "[32  5 79 42 69 91 74 84  2  2]\n",
      " 48267/50001: episode: 5363, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward: 21.000, mean reward:  2.333 [-10.000,  6.000], mean action: 49.778 [2.000, 91.000],  loss: 8.873144, mae: 2.398592, mean_q: 4.498558\n",
      "[99 75 28 47  1 89 79 34 64 74]\n",
      " 48276/50001: episode: 5364, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 54.556 [1.000, 89.000],  loss: 6.458546, mae: 2.465599, mean_q: 4.642982\n",
      "[79 96 47 12 27  2 81 82 34 30]\n",
      " 48285/50001: episode: 5365, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 40.000, mean reward:  4.444 [ 3.000,  8.000], mean action: 45.667 [2.000, 96.000],  loss: 6.786485, mae: 2.498455, mean_q: 4.657800\n",
      "[97 34 66  4  4 14 31  1 72 85]\n",
      " 48294/50001: episode: 5366, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 20.000, mean reward:  2.222 [-10.000,  5.000], mean action: 34.556 [1.000, 85.000],  loss: 7.464826, mae: 2.538066, mean_q: 4.685145\n",
      "[79 59 47 31 99 59 13 28 30 69]\n",
      " 48303/50001: episode: 5367, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 48.333 [13.000, 99.000],  loss: 5.006613, mae: 2.471725, mean_q: 4.522556\n",
      "[10 21 31 68 20 69 59 53 44 16]\n",
      " 48312/50001: episode: 5368, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 33.000, mean reward:  3.667 [ 3.000,  5.000], mean action: 42.333 [16.000, 69.000],  loss: 5.153040, mae: 2.559504, mean_q: 4.800696\n",
      "[79 95  9 64 98  1 57 12 42 57]\n",
      " 48321/50001: episode: 5369, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 48.333 [1.000, 98.000],  loss: 6.813154, mae: 2.596561, mean_q: 4.836635\n",
      "[80 90 37 27 28 61 81 95 13 34]\n",
      " 48330/50001: episode: 5370, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 45.000, mean reward:  5.000 [ 2.000,  8.000], mean action: 51.778 [13.000, 95.000],  loss: 7.014839, mae: 2.623652, mean_q: 4.928607\n",
      "[ 1 35 41 95 67 63 42 37 88 79]\n",
      " 48339/50001: episode: 5371, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 43.000, mean reward:  4.778 [ 3.000,  7.000], mean action: 60.778 [35.000, 95.000],  loss: 9.105164, mae: 2.641189, mean_q: 4.914316\n",
      "[30 11 89 55 84 23 57 17 12 37]\n",
      " 48348/50001: episode: 5372, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 42.000, mean reward:  4.667 [ 3.000,  9.000], mean action: 42.778 [11.000, 89.000],  loss: 6.184896, mae: 2.630014, mean_q: 4.841106\n",
      "[73 31 40 90 99 28 84 27 97 12]\n",
      " 48357/50001: episode: 5373, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 43.000, mean reward:  4.778 [ 3.000,  8.000], mean action: 56.444 [12.000, 99.000],  loss: 5.328294, mae: 2.717807, mean_q: 4.974150\n",
      "[45 37 57 53 97 76 67 53 96 34]\n",
      " 48366/50001: episode: 5374, duration: 0.069s, episode steps:   9, steps per second: 130, episode reward: 26.000, mean reward:  2.889 [-10.000, 10.000], mean action: 63.333 [34.000, 97.000],  loss: 7.269270, mae: 2.648481, mean_q: 4.894250\n",
      "[63  0 41 28 86 30 85 85 34 34]\n",
      " 48375/50001: episode: 5375, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 11.000, mean reward:  1.222 [-10.000,  9.000], mean action: 47.000 [0.000, 86.000],  loss: 6.762799, mae: 2.639202, mean_q: 4.910382\n",
      "[98 63 75  2 54 90 62 50 57 96]\n",
      " 48384/50001: episode: 5376, duration: 0.071s, episode steps:   9, steps per second: 128, episode reward: 37.000, mean reward:  4.111 [ 3.000,  7.000], mean action: 61.000 [2.000, 96.000],  loss: 7.083203, mae: 2.609982, mean_q: 4.794223\n",
      "[76 92 13 47 97 84 62 60 89 50]\n",
      " 48393/50001: episode: 5377, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 66.000 [13.000, 97.000],  loss: 8.375386, mae: 2.638089, mean_q: 4.845800\n",
      "[53 99 66 53 82 57 20  2 59 84]\n",
      " 48402/50001: episode: 5378, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 58.000 [2.000, 99.000],  loss: 9.170797, mae: 2.544363, mean_q: 4.695650\n",
      "[23 42 31 97 55 32 14 89 55 13]\n",
      " 48411/50001: episode: 5379, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 25.000, mean reward:  2.778 [-10.000,  9.000], mean action: 47.556 [13.000, 97.000],  loss: 7.298278, mae: 2.521695, mean_q: 4.645171\n",
      "[98 82 61 49 68 93 59 16 88 75]\n",
      " 48420/50001: episode: 5380, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 41.000, mean reward:  4.556 [ 2.000,  8.000], mean action: 65.667 [16.000, 93.000],  loss: 4.738225, mae: 2.434294, mean_q: 4.492994\n",
      "[85 51 35 34 26 57 31 67 24 28]\n",
      " 48429/50001: episode: 5381, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 39.222 [24.000, 67.000],  loss: 7.950524, mae: 2.498761, mean_q: 4.616481\n",
      "[19 41 93 88  5 63 18 86 12 97]\n",
      " 48438/50001: episode: 5382, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 55.889 [5.000, 97.000],  loss: 4.588137, mae: 2.578201, mean_q: 4.776358\n",
      "[68 37 18 31  8 61 41  4 74 79]\n",
      " 48447/50001: episode: 5383, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 39.000, mean reward:  4.333 [ 2.000,  8.000], mean action: 39.222 [4.000, 79.000],  loss: 5.909583, mae: 2.636998, mean_q: 4.868910\n",
      "[87  1 31 27 88 85 28 24  1 88]\n",
      " 48456/50001: episode: 5384, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 10.000, mean reward:  1.111 [-10.000,  5.000], mean action: 41.444 [1.000, 88.000],  loss: 8.023344, mae: 2.637874, mean_q: 4.855444\n",
      "[81 28 48 67 58 87 28 95 98 12]\n",
      " 48465/50001: episode: 5385, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 27.000, mean reward:  3.000 [-10.000,  8.000], mean action: 57.889 [12.000, 98.000],  loss: 6.963774, mae: 2.738407, mean_q: 5.072137\n",
      "[58 41 24 31 12  4 20 75 31 12]\n",
      " 48474/50001: episode: 5386, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward:  9.000, mean reward:  1.000 [-10.000,  5.000], mean action: 27.778 [4.000, 75.000],  loss: 8.427952, mae: 2.667882, mean_q: 4.926313\n",
      "[36 89 98 10 24 27  8 47 47 24]\n",
      " 48483/50001: episode: 5387, duration: 0.101s, episode steps:   9, steps per second:  89, episode reward: 12.000, mean reward:  1.333 [-10.000,  7.000], mean action: 41.556 [8.000, 98.000],  loss: 6.475294, mae: 2.620283, mean_q: 4.826972\n",
      "[63  1 91  9 74 60 14 48 60 98]\n",
      " 48492/50001: episode: 5388, duration: 0.083s, episode steps:   9, steps per second: 109, episode reward: 26.000, mean reward:  2.889 [-10.000,  6.000], mean action: 50.556 [1.000, 98.000],  loss: 5.568814, mae: 2.562157, mean_q: 4.748569\n",
      "[38 14 28 50 81 42 88 14 64 12]\n",
      " 48501/50001: episode: 5389, duration: 0.070s, episode steps:   9, steps per second: 129, episode reward: 28.000, mean reward:  3.111 [-10.000,  8.000], mean action: 43.667 [12.000, 88.000],  loss: 8.535045, mae: 2.554574, mean_q: 4.824693\n",
      "[16 11 74 37  1 50 22 63 12 30]\n",
      " 48510/50001: episode: 5390, duration: 0.069s, episode steps:   9, steps per second: 131, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 33.333 [1.000, 74.000],  loss: 7.498997, mae: 2.520508, mean_q: 4.727577\n",
      "[24 10 86 30 14 85 52 56 38 20]\n",
      " 48519/50001: episode: 5391, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 34.000, mean reward:  3.778 [ 2.000,  6.000], mean action: 43.444 [10.000, 86.000],  loss: 7.933395, mae: 2.495531, mean_q: 4.644673\n",
      "[85 60 66 28 42 31 93 76  4  2]\n",
      " 48528/50001: episode: 5392, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 40.000, mean reward:  4.444 [ 3.000,  6.000], mean action: 44.667 [2.000, 93.000],  loss: 10.048991, mae: 2.502952, mean_q: 4.696744\n",
      "[73 83 84 28 97 31 89  6 79 90]\n",
      " 48537/50001: episode: 5393, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 65.222 [6.000, 97.000],  loss: 6.466490, mae: 2.464190, mean_q: 4.630353\n",
      "[19 32 96  9 24 84  9 98 28 12]\n",
      " 48546/50001: episode: 5394, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 27.000, mean reward:  3.000 [-10.000,  8.000], mean action: 43.556 [9.000, 98.000],  loss: 7.150296, mae: 2.431070, mean_q: 4.645935\n",
      "[99 37 81 27 90 60  2 88  9 79]\n",
      " 48555/50001: episode: 5395, duration: 0.066s, episode steps:   9, steps per second: 135, episode reward: 40.000, mean reward:  4.444 [ 2.000,  7.000], mean action: 52.556 [2.000, 90.000],  loss: 7.017281, mae: 2.457839, mean_q: 4.527670\n",
      "[45 68 99 79 37  4 24 66 82 47]\n",
      " 48564/50001: episode: 5396, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 43.000, mean reward:  4.778 [ 3.000,  7.000], mean action: 56.222 [4.000, 99.000],  loss: 6.582418, mae: 2.514995, mean_q: 4.789752\n",
      "[95 20 61 82 56 31 62 50 98 32]\n",
      " 48573/50001: episode: 5397, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 41.000, mean reward:  4.556 [ 2.000,  7.000], mean action: 54.667 [20.000, 98.000],  loss: 6.014656, mae: 2.504841, mean_q: 4.714399\n",
      "[95 95 47 41 43 37 81 57 30 28]\n",
      " 48582/50001: episode: 5398, duration: 0.069s, episode steps:   9, steps per second: 130, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 51.000 [28.000, 95.000],  loss: 7.139956, mae: 2.565186, mean_q: 4.799363\n",
      "[44 17 37 91 23 74 14 39 13 31]\n",
      " 48591/50001: episode: 5399, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 37.667 [13.000, 91.000],  loss: 5.289225, mae: 2.549637, mean_q: 4.787504\n",
      "[94 65 47 45 82 90  5 52 78 79]\n",
      " 48600/50001: episode: 5400, duration: 0.069s, episode steps:   9, steps per second: 131, episode reward: 38.000, mean reward:  4.222 [ 2.000,  8.000], mean action: 60.333 [5.000, 90.000],  loss: 9.170002, mae: 2.551716, mean_q: 4.659019\n",
      "[23 20 41 93 27 44 13 84 27 79]\n",
      " 48609/50001: episode: 5401, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 47.556 [13.000, 93.000],  loss: 6.044815, mae: 2.546610, mean_q: 4.704450\n",
      "[24  1 62 79 94 60 96 46 74 12]\n",
      " 48618/50001: episode: 5402, duration: 0.069s, episode steps:   9, steps per second: 131, episode reward: 42.000, mean reward:  4.667 [ 3.000,  9.000], mean action: 58.222 [1.000, 96.000],  loss: 6.615011, mae: 2.569323, mean_q: 4.753171\n",
      "[39 84 98 56 86 97 13 22 58 92]\n",
      " 48627/50001: episode: 5403, duration: 0.068s, episode steps:   9, steps per second: 133, episode reward: 38.000, mean reward:  4.222 [ 3.000,  6.000], mean action: 67.333 [13.000, 98.000],  loss: 9.298046, mae: 2.642649, mean_q: 4.836253\n",
      "[20 48 37 37 82 34 63 27 37 69]\n",
      " 48636/50001: episode: 5404, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward:  9.000, mean reward:  1.000 [-10.000,  6.000], mean action: 48.222 [27.000, 82.000],  loss: 7.821233, mae: 2.608340, mean_q: 4.761621\n",
      "[ 2 33 13 79 37 77 27 98 60 82]\n",
      " 48645/50001: episode: 5405, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 56.222 [13.000, 98.000],  loss: 9.689421, mae: 2.547331, mean_q: 4.731238\n",
      "[35 84 31 64 54 85 66  4 95 32]\n",
      " 48654/50001: episode: 5406, duration: 0.069s, episode steps:   9, steps per second: 131, episode reward: 44.000, mean reward:  4.889 [ 3.000,  9.000], mean action: 57.222 [4.000, 95.000],  loss: 7.138228, mae: 2.493839, mean_q: 4.640013\n",
      "[80 85 86 47 47 46 59 50 79 12]\n",
      " 48663/50001: episode: 5407, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 31.000, mean reward:  3.444 [-10.000,  8.000], mean action: 56.778 [12.000, 86.000],  loss: 7.231565, mae: 2.517662, mean_q: 4.623991\n",
      "[45  9 88  1 18 62 52 49 57  6]\n",
      " 48672/50001: episode: 5408, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 38.000 [1.000, 88.000],  loss: 5.425254, mae: 2.536675, mean_q: 4.629659\n",
      "[89 72 86 95  0 57 48 48 89 79]\n",
      " 48681/50001: episode: 5409, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 11.000, mean reward:  1.222 [-10.000,  7.000], mean action: 63.778 [0.000, 95.000],  loss: 6.593056, mae: 2.635941, mean_q: 4.787097\n",
      "[ 5  6 76 88 92 24 21 76 52 50]\n",
      " 48690/50001: episode: 5410, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 28.000, mean reward:  3.111 [-10.000, 10.000], mean action: 53.889 [6.000, 92.000],  loss: 6.221678, mae: 2.674828, mean_q: 4.793853\n",
      "[95 44 92 85 88 20 72 13 31 42]\n",
      " 48699/50001: episode: 5411, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 36.000, mean reward:  4.000 [ 2.000,  7.000], mean action: 54.111 [13.000, 92.000],  loss: 6.067444, mae: 2.710273, mean_q: 4.942263\n",
      "[19 76 16 20 13 59 98 23 11 13]\n",
      " 48708/50001: episode: 5412, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 36.556 [11.000, 98.000],  loss: 6.976590, mae: 2.683672, mean_q: 4.877853\n",
      "[84 27 88 97 95 82 38 95 88 12]\n",
      " 48717/50001: episode: 5413, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 12.000, mean reward:  1.333 [-10.000,  9.000], mean action: 69.111 [12.000, 97.000],  loss: 8.345682, mae: 2.736235, mean_q: 4.943029\n",
      "[45 34 97 79 95 37 14 89 23 28]\n",
      " 48726/50001: episode: 5414, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 42.000, mean reward:  4.667 [ 3.000,  6.000], mean action: 55.111 [14.000, 97.000],  loss: 8.881490, mae: 2.590829, mean_q: 4.818048\n",
      "[72 47 79 21 28 14 13 66 67 66]\n",
      " 48735/50001: episode: 5415, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 44.556 [13.000, 79.000],  loss: 5.616215, mae: 2.547218, mean_q: 4.683835\n",
      "[73 34 13 47 95 46 34 89 56 48]\n",
      " 48744/50001: episode: 5416, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 51.333 [13.000, 95.000],  loss: 6.598521, mae: 2.541076, mean_q: 4.729183\n",
      "[82 23 24 67 66 42 34 95 40 98]\n",
      " 48753/50001: episode: 5417, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 44.000, mean reward:  4.889 [ 3.000,  8.000], mean action: 54.333 [23.000, 98.000],  loss: 8.299914, mae: 2.498931, mean_q: 4.602480\n",
      "[87 33 56 82 10 97 24 76 16 31]\n",
      " 48762/50001: episode: 5418, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 47.222 [10.000, 97.000],  loss: 6.781215, mae: 2.517498, mean_q: 4.645548\n",
      "[34 31 37  4 27 66 62 97 79 48]\n",
      " 48771/50001: episode: 5419, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 40.000, mean reward:  4.444 [ 3.000,  7.000], mean action: 50.111 [4.000, 97.000],  loss: 8.070049, mae: 2.528285, mean_q: 4.636629\n",
      "[46  4 13 96 27 50 35 34 82 63]\n",
      " 48780/50001: episode: 5420, duration: 0.065s, episode steps:   9, steps per second: 137, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 44.889 [4.000, 96.000],  loss: 8.089168, mae: 2.602511, mean_q: 4.768671\n",
      "[93 55 41 14 27 19 46 34 27 45]\n",
      " 48789/50001: episode: 5421, duration: 0.071s, episode steps:   9, steps per second: 126, episode reward: 22.000, mean reward:  2.444 [-10.000,  8.000], mean action: 34.222 [14.000, 55.000],  loss: 6.286440, mae: 2.611773, mean_q: 4.795177\n",
      "[20 48 92 21 34 91 36  4 37 66]\n",
      " 48798/50001: episode: 5422, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 41.000, mean reward:  4.556 [ 2.000,  8.000], mean action: 47.667 [4.000, 92.000],  loss: 6.997762, mae: 2.528180, mean_q: 4.708474\n",
      "[97 61 62 86 48 32 86 63 51 66]\n",
      " 48807/50001: episode: 5423, duration: 0.068s, episode steps:   9, steps per second: 133, episode reward: 26.000, mean reward:  2.889 [-10.000,  8.000], mean action: 61.667 [32.000, 86.000],  loss: 6.158834, mae: 2.507566, mean_q: 4.594761\n",
      "[89 37 13 80 81 12 53 50  2 37]\n",
      " 48816/50001: episode: 5424, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 40.556 [2.000, 81.000],  loss: 8.036392, mae: 2.575012, mean_q: 4.713273\n",
      "[29 92 32 66 17 91 48 57 88 21]\n",
      " 48825/50001: episode: 5425, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 56.889 [17.000, 92.000],  loss: 9.997484, mae: 2.530339, mean_q: 4.680787\n",
      "[76 75 92 54  4 11 23  1 63 47]\n",
      " 48834/50001: episode: 5426, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 41.111 [1.000, 92.000],  loss: 7.685991, mae: 2.509200, mean_q: 4.671963\n",
      "[86 37 11 33 27 57 60 60 24  1]\n",
      " 48843/50001: episode: 5427, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 28.000, mean reward:  3.111 [-10.000,  8.000], mean action: 34.444 [1.000, 60.000],  loss: 6.619567, mae: 2.532979, mean_q: 4.618900\n",
      "[55 57 17 98 50  1 90 24 52  2]\n",
      " 48852/50001: episode: 5428, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 42.000, mean reward:  4.667 [ 3.000,  6.000], mean action: 43.444 [1.000, 98.000],  loss: 6.182072, mae: 2.609468, mean_q: 4.776110\n",
      "[27 87 37 89 10 98 96 96 87 88]\n",
      " 48861/50001: episode: 5429, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 11.000, mean reward:  1.222 [-10.000,  9.000], mean action: 76.444 [10.000, 98.000],  loss: 7.443234, mae: 2.625687, mean_q: 4.772421\n",
      "[79 54 81 19 34 13  1 16 63 47]\n",
      " 48870/50001: episode: 5430, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 36.444 [1.000, 81.000],  loss: 7.552691, mae: 2.710970, mean_q: 4.885370\n",
      "[47 95 49 40 53 66 37  4 21 93]\n",
      " 48879/50001: episode: 5431, duration: 0.071s, episode steps:   9, steps per second: 127, episode reward: 36.000, mean reward:  4.000 [ 2.000,  6.000], mean action: 50.889 [4.000, 95.000],  loss: 6.946848, mae: 2.603377, mean_q: 4.728487\n",
      "[41 68 67 16 86 32 12 59  5  5]\n",
      " 48888/50001: episode: 5432, duration: 0.073s, episode steps:   9, steps per second: 124, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 38.889 [5.000, 86.000],  loss: 8.096360, mae: 2.645574, mean_q: 4.791364\n",
      "[10 86 37 95 73 27 27 48 34 37]\n",
      " 48897/50001: episode: 5433, duration: 0.073s, episode steps:   9, steps per second: 123, episode reward: 13.000, mean reward:  1.444 [-10.000,  8.000], mean action: 51.556 [27.000, 95.000],  loss: 6.226641, mae: 2.605334, mean_q: 4.734699\n",
      "[60 44  6 31 99 54  9 58 92  0]\n",
      " 48906/50001: episode: 5434, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward: 35.000, mean reward:  3.889 [ 2.000,  5.000], mean action: 43.667 [0.000, 99.000],  loss: 5.996664, mae: 2.542904, mean_q: 4.613605\n",
      "[72 27 95  5  3 47 50 27  1 97]\n",
      " 48915/50001: episode: 5435, duration: 0.070s, episode steps:   9, steps per second: 129, episode reward: 28.000, mean reward:  3.111 [-10.000,  7.000], mean action: 39.111 [1.000, 97.000],  loss: 7.091331, mae: 2.559260, mean_q: 4.611757\n",
      "[42 42 94 43 59 34 27 66 14 82]\n",
      " 48924/50001: episode: 5436, duration: 0.069s, episode steps:   9, steps per second: 131, episode reward: 28.000, mean reward:  3.111 [-10.000,  7.000], mean action: 51.222 [14.000, 94.000],  loss: 9.131680, mae: 2.642693, mean_q: 4.861634\n",
      "[20 32 11 64 97 37 67 68 27 40]\n",
      " 48933/50001: episode: 5437, duration: 0.070s, episode steps:   9, steps per second: 128, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 49.222 [11.000, 97.000],  loss: 7.082055, mae: 2.627854, mean_q: 4.804747\n",
      "[70 28 13 95 35 13 16  1 20 12]\n",
      " 48942/50001: episode: 5438, duration: 0.070s, episode steps:   9, steps per second: 128, episode reward: 27.000, mean reward:  3.000 [-10.000,  8.000], mean action: 25.889 [1.000, 95.000],  loss: 6.903618, mae: 2.591316, mean_q: 4.718795\n",
      "[99 33 19 95 31 60  9 95 43 57]\n",
      " 48951/50001: episode: 5439, duration: 0.071s, episode steps:   9, steps per second: 128, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 49.111 [9.000, 95.000],  loss: 9.205896, mae: 2.569148, mean_q: 4.669483\n",
      "[74 52 31 13 90 95 34 55  9 45]\n",
      " 48960/50001: episode: 5440, duration: 0.070s, episode steps:   9, steps per second: 128, episode reward: 36.000, mean reward:  4.000 [ 2.000,  6.000], mean action: 47.111 [9.000, 95.000],  loss: 10.288457, mae: 2.463595, mean_q: 4.564389\n",
      "[57 44 80 82 56  4 49  1 88 74]\n",
      " 48969/50001: episode: 5441, duration: 0.069s, episode steps:   9, steps per second: 131, episode reward: 43.000, mean reward:  4.778 [ 2.000,  8.000], mean action: 53.111 [1.000, 88.000],  loss: 7.766591, mae: 2.445050, mean_q: 4.501991\n",
      "[40 98 45 66 37 71 20 90 48 34]\n",
      " 48978/50001: episode: 5442, duration: 0.072s, episode steps:   9, steps per second: 126, episode reward: 44.000, mean reward:  4.889 [ 2.000, 10.000], mean action: 56.556 [20.000, 98.000],  loss: 5.858228, mae: 2.426113, mean_q: 4.411676\n",
      "[25 82 82 14 15 79 88 95 63  1]\n",
      " 48987/50001: episode: 5443, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward: 31.000, mean reward:  3.444 [-10.000,  8.000], mean action: 57.667 [1.000, 95.000],  loss: 6.741317, mae: 2.463323, mean_q: 4.488850\n",
      "[51 71 11 18 22 28 82 63 48 62]\n",
      " 48996/50001: episode: 5444, duration: 0.069s, episode steps:   9, steps per second: 130, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 45.000 [11.000, 82.000],  loss: 7.884094, mae: 2.629529, mean_q: 4.789282\n",
      "[10 31 41  7 12 79 75 50  9 37]\n",
      " 49005/50001: episode: 5445, duration: 0.070s, episode steps:   9, steps per second: 128, episode reward: 45.000, mean reward:  5.000 [ 2.000,  9.000], mean action: 37.889 [7.000, 79.000],  loss: 7.379849, mae: 2.670709, mean_q: 4.900753\n",
      "[97 22 27 12 60 11  9 31 80 50]\n",
      " 49014/50001: episode: 5446, duration: 0.072s, episode steps:   9, steps per second: 124, episode reward: 38.000, mean reward:  4.222 [ 3.000,  9.000], mean action: 33.556 [9.000, 80.000],  loss: 6.208324, mae: 2.670175, mean_q: 4.813387\n",
      "[ 0 87 93 82 31  3 68  1 28 47]\n",
      " 49023/50001: episode: 5447, duration: 0.068s, episode steps:   9, steps per second: 133, episode reward: 45.000, mean reward:  5.000 [ 3.000,  8.000], mean action: 48.889 [1.000, 93.000],  loss: 7.109474, mae: 2.766313, mean_q: 4.963238\n",
      "[84 41 82 92 95 69 94 16  5 40]\n",
      " 49032/50001: episode: 5448, duration: 0.069s, episode steps:   9, steps per second: 130, episode reward: 40.000, mean reward:  4.444 [ 3.000,  6.000], mean action: 59.333 [5.000, 95.000],  loss: 5.027467, mae: 2.732045, mean_q: 4.937006\n",
      "[35 86 38 19 78 20 12 99 50 40]\n",
      " 49041/50001: episode: 5449, duration: 0.072s, episode steps:   9, steps per second: 124, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 49.111 [12.000, 99.000],  loss: 7.855352, mae: 2.749059, mean_q: 4.935250\n",
      "[ 5 34 94 82 44  5 48 68 89 76]\n",
      " 49050/50001: episode: 5450, duration: 0.069s, episode steps:   9, steps per second: 130, episode reward: 21.000, mean reward:  2.333 [-10.000,  5.000], mean action: 60.000 [5.000, 94.000],  loss: 5.233088, mae: 2.809271, mean_q: 4.973162\n",
      "[70 10 68 32 95 78 30 66  1 70]\n",
      " 49059/50001: episode: 5451, duration: 0.070s, episode steps:   9, steps per second: 129, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 50.000 [1.000, 95.000],  loss: 7.299866, mae: 2.787630, mean_q: 5.119774\n",
      "[80 59 77 69 58 85 42 42 31  1]\n",
      " 49068/50001: episode: 5452, duration: 0.073s, episode steps:   9, steps per second: 123, episode reward: 26.000, mean reward:  2.889 [-10.000,  9.000], mean action: 51.556 [1.000, 85.000],  loss: 7.975292, mae: 2.727963, mean_q: 4.887007\n",
      "[25 91 37 97 37 20 48 95 30 34]\n",
      " 49077/50001: episode: 5453, duration: 0.075s, episode steps:   9, steps per second: 120, episode reward: 31.000, mean reward:  3.444 [-10.000,  9.000], mean action: 54.333 [20.000, 97.000],  loss: 6.184446, mae: 2.657937, mean_q: 4.824504\n",
      "[90 34 27 42 96 54 13 52  1 14]\n",
      " 49086/50001: episode: 5454, duration: 0.069s, episode steps:   9, steps per second: 131, episode reward: 41.000, mean reward:  4.556 [ 3.000,  7.000], mean action: 37.000 [1.000, 96.000],  loss: 7.956485, mae: 2.709111, mean_q: 4.914038\n",
      "[47 48 88 13 28 57 50 98 35  9]\n",
      " 49095/50001: episode: 5455, duration: 0.070s, episode steps:   9, steps per second: 128, episode reward: 37.000, mean reward:  4.111 [ 3.000,  5.000], mean action: 47.333 [9.000, 98.000],  loss: 6.758353, mae: 2.665222, mean_q: 4.815517\n",
      "[83 68 64 75 27 17 42 73 93 89]\n",
      " 49104/50001: episode: 5456, duration: 0.070s, episode steps:   9, steps per second: 128, episode reward: 37.000, mean reward:  4.111 [ 2.000,  7.000], mean action: 60.889 [17.000, 93.000],  loss: 7.363691, mae: 2.697781, mean_q: 4.976660\n",
      "[91 63 64 88 92 40 76 28 54 30]\n",
      " 49113/50001: episode: 5457, duration: 0.072s, episode steps:   9, steps per second: 124, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 59.444 [28.000, 92.000],  loss: 10.064092, mae: 2.647647, mean_q: 4.890180\n",
      "[49 51 54  7 93 13 21 57 77 47]\n",
      " 49122/50001: episode: 5458, duration: 0.071s, episode steps:   9, steps per second: 127, episode reward: 39.000, mean reward:  4.333 [ 2.000,  8.000], mean action: 46.667 [7.000, 93.000],  loss: 8.659573, mae: 2.562177, mean_q: 4.721936\n",
      "[44  9 25 30 51 53 71 31 97 52]\n",
      " 49131/50001: episode: 5459, duration: 0.072s, episode steps:   9, steps per second: 125, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 46.556 [9.000, 97.000],  loss: 7.802099, mae: 2.509581, mean_q: 4.599312\n",
      "[88 41 90 98 20 32 48 95  7 87]\n",
      " 49140/50001: episode: 5460, duration: 0.070s, episode steps:   9, steps per second: 128, episode reward: 38.000, mean reward:  4.222 [ 1.000,  8.000], mean action: 57.556 [7.000, 98.000],  loss: 7.356064, mae: 2.572091, mean_q: 4.620162\n",
      "[90 59  9 97 88 51 50 53 85 12]\n",
      " 49149/50001: episode: 5461, duration: 0.070s, episode steps:   9, steps per second: 128, episode reward: 41.000, mean reward:  4.556 [ 3.000,  8.000], mean action: 56.000 [9.000, 97.000],  loss: 7.738729, mae: 2.583087, mean_q: 4.767413\n",
      "[94 50 12 23 13 49 59 75 68 37]\n",
      " 49158/50001: episode: 5462, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward: 39.000, mean reward:  4.333 [ 3.000,  9.000], mean action: 42.889 [12.000, 75.000],  loss: 7.375356, mae: 2.492087, mean_q: 4.621357\n",
      "[11 72 46 94 41 95 98 34 87 63]\n",
      " 49167/50001: episode: 5463, duration: 0.072s, episode steps:   9, steps per second: 125, episode reward: 41.000, mean reward:  4.556 [ 2.000,  7.000], mean action: 70.000 [34.000, 98.000],  loss: 6.775095, mae: 2.472883, mean_q: 4.522337\n",
      "[76 40 58  9 97  5 28 56 93 40]\n",
      " 49176/50001: episode: 5464, duration: 0.073s, episode steps:   9, steps per second: 123, episode reward: 24.000, mean reward:  2.667 [-10.000,  6.000], mean action: 47.333 [5.000, 97.000],  loss: 8.108240, mae: 2.554324, mean_q: 4.659960\n",
      "[26 41 89 15  1 16 18 67 60 38]\n",
      " 49185/50001: episode: 5465, duration: 0.069s, episode steps:   9, steps per second: 131, episode reward: 36.000, mean reward:  4.000 [ 2.000,  7.000], mean action: 38.333 [1.000, 89.000],  loss: 6.554430, mae: 2.490137, mean_q: 4.584494\n",
      "[27 95 98 42 79 57 48 32 95 47]\n",
      " 49194/50001: episode: 5466, duration: 0.074s, episode steps:   9, steps per second: 122, episode reward: 27.000, mean reward:  3.000 [-10.000,  7.000], mean action: 65.889 [32.000, 98.000],  loss: 7.391662, mae: 2.542284, mean_q: 4.685141\n",
      "[88 32 56  4 77 23 59 93 30 98]\n",
      " 49203/50001: episode: 5467, duration: 0.070s, episode steps:   9, steps per second: 129, episode reward: 37.000, mean reward:  4.111 [ 3.000,  7.000], mean action: 52.444 [4.000, 98.000],  loss: 7.394799, mae: 2.551053, mean_q: 4.710623\n",
      "[80 19  1 37 34 33 86 24 67 12]\n",
      " 49212/50001: episode: 5468, duration: 0.070s, episode steps:   9, steps per second: 129, episode reward: 42.000, mean reward:  4.667 [ 3.000,  9.000], mean action: 34.778 [1.000, 86.000],  loss: 6.409330, mae: 2.511447, mean_q: 4.612131\n",
      "[23 60 81 89 31 63 24 88 24 90]\n",
      " 49221/50001: episode: 5469, duration: 0.072s, episode steps:   9, steps per second: 125, episode reward: 25.000, mean reward:  2.778 [-10.000,  6.000], mean action: 61.111 [24.000, 90.000],  loss: 7.015297, mae: 2.560671, mean_q: 4.723470\n",
      "[71 34 68 98 54 46  1 51 40 48]\n",
      " 49230/50001: episode: 5470, duration: 0.069s, episode steps:   9, steps per second: 131, episode reward: 43.000, mean reward:  4.778 [ 3.000,  7.000], mean action: 48.889 [1.000, 98.000],  loss: 5.560832, mae: 2.579704, mean_q: 4.693794\n",
      "[74 98 79 24 50  4 14 88 49 31]\n",
      " 49239/50001: episode: 5471, duration: 0.069s, episode steps:   9, steps per second: 130, episode reward: 40.000, mean reward:  4.444 [ 3.000,  6.000], mean action: 48.556 [4.000, 98.000],  loss: 6.530038, mae: 2.568645, mean_q: 4.704633\n",
      "[62 16 76 50 24 12 34 66 89 57]\n",
      " 49248/50001: episode: 5472, duration: 0.069s, episode steps:   9, steps per second: 130, episode reward: 44.000, mean reward:  4.889 [ 3.000,  7.000], mean action: 47.111 [12.000, 89.000],  loss: 7.175256, mae: 2.611374, mean_q: 4.710171\n",
      "[18 50 54 88 93 20 34 30 13 70]\n",
      " 49257/50001: episode: 5473, duration: 0.070s, episode steps:   9, steps per second: 128, episode reward: 41.000, mean reward:  4.556 [ 2.000,  8.000], mean action: 50.222 [13.000, 93.000],  loss: 8.092798, mae: 2.583215, mean_q: 4.648849\n",
      "[53 45 50  1 83 24 79 73 32 67]\n",
      " 49266/50001: episode: 5474, duration: 0.071s, episode steps:   9, steps per second: 128, episode reward: 37.000, mean reward:  4.111 [ 2.000,  7.000], mean action: 50.444 [1.000, 83.000],  loss: 4.762491, mae: 2.611119, mean_q: 4.775286\n",
      "[44 69 34 34 73 42 88 53 27 34]\n",
      " 49275/50001: episode: 5475, duration: 0.067s, episode steps:   9, steps per second: 133, episode reward:  9.000, mean reward:  1.000 [-10.000,  6.000], mean action: 50.444 [27.000, 88.000],  loss: 7.709180, mae: 2.586895, mean_q: 4.787291\n",
      "[44 31 87 72 24  2 95  1 33 10]\n",
      " 49284/50001: episode: 5476, duration: 0.073s, episode steps:   9, steps per second: 123, episode reward: 41.000, mean reward:  4.556 [ 2.000,  8.000], mean action: 39.444 [1.000, 95.000],  loss: 8.209464, mae: 2.640634, mean_q: 4.832488\n",
      "[41 93 11 85  2 48 97 48 66 71]\n",
      " 49293/50001: episode: 5477, duration: 0.069s, episode steps:   9, steps per second: 131, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 57.889 [2.000, 97.000],  loss: 10.555161, mae: 2.607062, mean_q: 4.790171\n",
      "[71 24  5 79 20 37 79 13 33 12]\n",
      " 49302/50001: episode: 5478, duration: 0.070s, episode steps:   9, steps per second: 129, episode reward: 26.000, mean reward:  2.889 [-10.000,  7.000], mean action: 33.556 [5.000, 79.000],  loss: 10.806171, mae: 2.499628, mean_q: 4.554436\n",
      "[34 11 51 92 37 23 50 40 27 40]\n",
      " 49311/50001: episode: 5479, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 41.222 [11.000, 92.000],  loss: 9.449061, mae: 2.445539, mean_q: 4.496334\n",
      "[23 97 95 42 84 15 74 34 48 66]\n",
      " 49320/50001: episode: 5480, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward: 43.000, mean reward:  4.778 [ 2.000,  8.000], mean action: 61.667 [15.000, 97.000],  loss: 6.607879, mae: 2.423594, mean_q: 4.491926\n",
      "[78 40 14 57 27 25 84  3 24 42]\n",
      " 49329/50001: episode: 5481, duration: 0.070s, episode steps:   9, steps per second: 128, episode reward: 37.000, mean reward:  4.111 [ 2.000,  7.000], mean action: 35.111 [3.000, 84.000],  loss: 6.116022, mae: 2.413463, mean_q: 4.433798\n",
      "[ 7 94 20 75 34 24 39 84 50 34]\n",
      " 49338/50001: episode: 5482, duration: 0.070s, episode steps:   9, steps per second: 129, episode reward: 26.000, mean reward:  2.889 [-10.000,  9.000], mean action: 50.444 [20.000, 94.000],  loss: 6.615897, mae: 2.548315, mean_q: 4.649183\n",
      "[93 13 76 16 34 23 56 88 51 19]\n",
      " 49347/50001: episode: 5483, duration: 0.070s, episode steps:   9, steps per second: 129, episode reward: 37.000, mean reward:  4.111 [ 2.000,  6.000], mean action: 41.778 [13.000, 88.000],  loss: 6.891980, mae: 2.634948, mean_q: 4.778400\n",
      "[54 41 74 50 60 85 67 25  4  3]\n",
      " 49356/50001: episode: 5484, duration: 0.069s, episode steps:   9, steps per second: 131, episode reward: 35.000, mean reward:  3.889 [ 3.000,  6.000], mean action: 45.444 [3.000, 85.000],  loss: 8.382284, mae: 2.620209, mean_q: 4.737524\n",
      "[77 89 95 80 27 63  1 32  1 75]\n",
      " 49365/50001: episode: 5485, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: 28.000, mean reward:  3.111 [-10.000,  6.000], mean action: 51.444 [1.000, 95.000],  loss: 6.119005, mae: 2.603685, mean_q: 4.720723\n",
      "[23 41 56 57 81 97 95 97 93 18]\n",
      " 49374/50001: episode: 5486, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward: 25.000, mean reward:  2.778 [-10.000,  7.000], mean action: 70.556 [18.000, 97.000],  loss: 7.014494, mae: 2.590805, mean_q: 4.703560\n",
      "[72 69 30  1 58 98 56 91 56 50]\n",
      " 49383/50001: episode: 5487, duration: 0.069s, episode steps:   9, steps per second: 130, episode reward: 28.000, mean reward:  3.111 [-10.000,  9.000], mean action: 56.556 [1.000, 98.000],  loss: 5.634840, mae: 2.634551, mean_q: 4.782894\n",
      "[90 76 97 10 13 28 68 95 42 95]\n",
      " 49392/50001: episode: 5488, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward: 26.000, mean reward:  2.889 [-10.000,  8.000], mean action: 58.222 [10.000, 97.000],  loss: 8.384088, mae: 2.677991, mean_q: 4.752923\n",
      "[64 41 12 27 67 95 59  1 74 32]\n",
      " 49401/50001: episode: 5489, duration: 0.069s, episode steps:   9, steps per second: 130, episode reward: 43.000, mean reward:  4.778 [ 3.000,  7.000], mean action: 45.333 [1.000, 95.000],  loss: 6.128145, mae: 2.648792, mean_q: 4.790693\n",
      "[40 31 62 55 82 40 34 23  5 71]\n",
      " 49410/50001: episode: 5490, duration: 0.071s, episode steps:   9, steps per second: 127, episode reward: 22.000, mean reward:  2.444 [-10.000,  7.000], mean action: 44.778 [5.000, 82.000],  loss: 6.716825, mae: 2.688720, mean_q: 4.890491\n",
      "[59 66 28 75 34  6 81 34 34  4]\n",
      " 49419/50001: episode: 5491, duration: 0.069s, episode steps:   9, steps per second: 130, episode reward:  8.000, mean reward:  0.889 [-10.000,  6.000], mean action: 40.222 [4.000, 81.000],  loss: 4.353084, mae: 2.674711, mean_q: 4.858236\n",
      "[77 50 54  2  1 86 21 22 40 83]\n",
      " 49428/50001: episode: 5492, duration: 0.068s, episode steps:   9, steps per second: 133, episode reward: 37.000, mean reward:  4.111 [ 3.000,  6.000], mean action: 39.889 [1.000, 86.000],  loss: 7.616445, mae: 2.761978, mean_q: 5.034087\n",
      "[ 4 34 53 95 54 60 56 60 75 83]\n",
      " 49437/50001: episode: 5493, duration: 0.072s, episode steps:   9, steps per second: 125, episode reward: 22.000, mean reward:  2.444 [-10.000,  5.000], mean action: 63.333 [34.000, 95.000],  loss: 6.036844, mae: 2.769593, mean_q: 5.001368\n",
      "[ 5 87 30 42 18 33 42 88 14 33]\n",
      " 49446/50001: episode: 5494, duration: 0.071s, episode steps:   9, steps per second: 127, episode reward: 12.000, mean reward:  1.333 [-10.000,  7.000], mean action: 43.000 [14.000, 88.000],  loss: 6.142555, mae: 2.771207, mean_q: 5.050417\n",
      "[71 96 30 48 53 41 90 24 88 80]\n",
      " 49455/50001: episode: 5495, duration: 0.071s, episode steps:   9, steps per second: 127, episode reward: 43.000, mean reward:  4.778 [ 3.000,  8.000], mean action: 61.111 [24.000, 96.000],  loss: 8.035773, mae: 2.726477, mean_q: 4.983213\n",
      "[30 36 30 42 26 50 97 98 12 12]\n",
      " 49464/50001: episode: 5496, duration: 0.075s, episode steps:   9, steps per second: 120, episode reward: 16.000, mean reward:  1.778 [-10.000,  8.000], mean action: 44.778 [12.000, 98.000],  loss: 5.339636, mae: 2.613093, mean_q: 4.802835\n",
      "[75 60 62 46 14 97 83 53  1 66]\n",
      " 49473/50001: episode: 5497, duration: 0.075s, episode steps:   9, steps per second: 119, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 53.556 [1.000, 97.000],  loss: 10.558958, mae: 2.600963, mean_q: 4.774562\n",
      "[28 37 93 84 95 85 57 13  5 82]\n",
      " 49482/50001: episode: 5498, duration: 0.070s, episode steps:   9, steps per second: 129, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 61.222 [5.000, 95.000],  loss: 5.878692, mae: 2.511466, mean_q: 4.621249\n",
      "[84 54 67  8  2 19 10 46 54 67]\n",
      " 49491/50001: episode: 5499, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward:  7.000, mean reward:  0.778 [-10.000,  6.000], mean action: 36.333 [2.000, 67.000],  loss: 9.005316, mae: 2.518600, mean_q: 4.729970\n",
      "[ 3  3 28 73 45 18 59 98 89 12]\n",
      " 49500/50001: episode: 5500, duration: 0.079s, episode steps:   9, steps per second: 114, episode reward: 29.000, mean reward:  3.222 [-10.000, 10.000], mean action: 47.222 [3.000, 98.000],  loss: 6.635767, mae: 2.472758, mean_q: 4.539539\n",
      "[22 48 44 93 77 20 42 30 62 75]\n",
      " 49509/50001: episode: 5501, duration: 0.070s, episode steps:   9, steps per second: 128, episode reward: 38.000, mean reward:  4.222 [ 2.000,  8.000], mean action: 54.556 [20.000, 93.000],  loss: 5.324726, mae: 2.534955, mean_q: 4.676470\n",
      "[31 41 42 95 77 32 20 67 77 14]\n",
      " 49518/50001: episode: 5502, duration: 0.075s, episode steps:   9, steps per second: 120, episode reward: 24.000, mean reward:  2.667 [-10.000,  7.000], mean action: 51.667 [14.000, 95.000],  loss: 7.447802, mae: 2.552703, mean_q: 4.750395\n",
      "[39 20  4 74 50 44  4 42 88 87]\n",
      " 49527/50001: episode: 5503, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward: 23.000, mean reward:  2.556 [-10.000,  7.000], mean action: 45.889 [4.000, 88.000],  loss: 6.437547, mae: 2.590230, mean_q: 4.676855\n",
      "[10 30 59 54 48 45 24  4 23 66]\n",
      " 49536/50001: episode: 5504, duration: 0.070s, episode steps:   9, steps per second: 129, episode reward: 41.000, mean reward:  4.556 [ 2.000,  8.000], mean action: 39.222 [4.000, 66.000],  loss: 7.198040, mae: 2.636832, mean_q: 4.882474\n",
      "[65 93 93 50 28 57 86 73  1 37]\n",
      " 49545/50001: episode: 5505, duration: 0.072s, episode steps:   9, steps per second: 124, episode reward: 30.000, mean reward:  3.333 [-10.000,  9.000], mean action: 57.556 [1.000, 93.000],  loss: 6.356069, mae: 2.586990, mean_q: 4.693417\n",
      "[12 83 85 57 47 89 48 62  4 31]\n",
      " 49554/50001: episode: 5506, duration: 0.071s, episode steps:   9, steps per second: 128, episode reward: 40.000, mean reward:  4.444 [ 3.000,  6.000], mean action: 56.222 [4.000, 89.000],  loss: 9.432943, mae: 2.647515, mean_q: 4.929708\n",
      "[29 67 81 91 28 57 48 13 66 61]\n",
      " 49563/50001: episode: 5507, duration: 0.070s, episode steps:   9, steps per second: 129, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 56.889 [13.000, 91.000],  loss: 8.315235, mae: 2.638806, mean_q: 4.823614\n",
      "[95 95 92 28 63 32 83 92 78 13]\n",
      " 49572/50001: episode: 5508, duration: 0.071s, episode steps:   9, steps per second: 127, episode reward:  9.000, mean reward:  1.000 [-10.000,  9.000], mean action: 64.000 [13.000, 95.000],  loss: 6.565255, mae: 2.587061, mean_q: 4.766600\n",
      "[89 50 44 42 10 63 64 37 88 12]\n",
      " 49581/50001: episode: 5509, duration: 0.071s, episode steps:   9, steps per second: 127, episode reward: 46.000, mean reward:  5.111 [ 2.000,  9.000], mean action: 45.556 [10.000, 88.000],  loss: 8.343279, mae: 2.620309, mean_q: 4.870197\n",
      "[38 41 54 31 10 33 44  9 68 36]\n",
      " 49590/50001: episode: 5510, duration: 0.072s, episode steps:   9, steps per second: 126, episode reward: 34.000, mean reward:  3.778 [ 2.000,  5.000], mean action: 36.222 [9.000, 68.000],  loss: 7.050151, mae: 2.517207, mean_q: 4.694736\n",
      "[86  5 30 32 75 48 37 50 13  7]\n",
      " 49599/50001: episode: 5511, duration: 0.073s, episode steps:   9, steps per second: 123, episode reward: 41.000, mean reward:  4.556 [ 1.000,  7.000], mean action: 33.000 [5.000, 75.000],  loss: 6.516922, mae: 2.502429, mean_q: 4.656205\n",
      "[76 98 59 79 27 37 13 41 27 34]\n",
      " 49608/50001: episode: 5512, duration: 0.072s, episode steps:   9, steps per second: 125, episode reward: 30.000, mean reward:  3.333 [-10.000,  9.000], mean action: 46.111 [13.000, 98.000],  loss: 6.945239, mae: 2.556273, mean_q: 4.698352\n",
      "[38 56  2 34 19  8 47 40 24 85]\n",
      " 49617/50001: episode: 5513, duration: 0.069s, episode steps:   9, steps per second: 130, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 35.000 [2.000, 85.000],  loss: 7.539560, mae: 2.555110, mean_q: 4.719793\n",
      "[97 75 11 84 96 97 52 34 48 75]\n",
      " 49626/50001: episode: 5514, duration: 0.070s, episode steps:   9, steps per second: 128, episode reward: 10.000, mean reward:  1.111 [-10.000,  8.000], mean action: 63.556 [11.000, 97.000],  loss: 9.040218, mae: 2.611910, mean_q: 4.849892\n",
      "[86  3 82 74 61 33 55 19 64  5]\n",
      " 49635/50001: episode: 5515, duration: 0.071s, episode steps:   9, steps per second: 127, episode reward: 37.000, mean reward:  4.111 [ 3.000,  7.000], mean action: 44.000 [3.000, 82.000],  loss: 5.696865, mae: 2.602246, mean_q: 4.763031\n",
      "[55 62 37 29 47 74 95 50 37 79]\n",
      " 49644/50001: episode: 5516, duration: 0.071s, episode steps:   9, steps per second: 127, episode reward: 28.000, mean reward:  3.111 [-10.000,  7.000], mean action: 56.667 [29.000, 95.000],  loss: 7.026476, mae: 2.583269, mean_q: 4.717913\n",
      "[67 37 37  2 13 19 82 66 24 13]\n",
      " 49653/50001: episode: 5517, duration: 0.070s, episode steps:   9, steps per second: 129, episode reward: 10.000, mean reward:  1.111 [-10.000,  5.000], mean action: 32.556 [2.000, 82.000],  loss: 6.303972, mae: 2.631530, mean_q: 4.850872\n",
      "[93 33 46 65 24 57 92 89  9 34]\n",
      " 49662/50001: episode: 5518, duration: 0.070s, episode steps:   9, steps per second: 128, episode reward: 43.000, mean reward:  4.778 [ 2.000, 10.000], mean action: 49.889 [9.000, 92.000],  loss: 5.596241, mae: 2.648574, mean_q: 4.810701\n",
      "[ 6 83 34 14 30 69 74 85 75 28]\n",
      " 49671/50001: episode: 5519, duration: 0.071s, episode steps:   9, steps per second: 126, episode reward: 42.000, mean reward:  4.667 [ 3.000,  8.000], mean action: 54.667 [14.000, 85.000],  loss: 7.826309, mae: 2.697186, mean_q: 4.934539\n",
      "[82 95 82 79 97 97 48 53 46 97]\n",
      " 49680/50001: episode: 5520, duration: 0.071s, episode steps:   9, steps per second: 127, episode reward: -2.000, mean reward: -0.222 [-10.000,  5.000], mean action: 77.111 [46.000, 97.000],  loss: 7.911386, mae: 2.695695, mean_q: 4.855893\n",
      "[63 59 11 89 95 30 14 34 79 46]\n",
      " 49689/50001: episode: 5521, duration: 0.072s, episode steps:   9, steps per second: 125, episode reward: 44.000, mean reward:  4.889 [ 3.000,  7.000], mean action: 50.778 [11.000, 95.000],  loss: 5.549688, mae: 2.697370, mean_q: 4.874448\n",
      "[17 41 45 44 93 41 90 93 17 27]\n",
      " 49698/50001: episode: 5522, duration: 0.070s, episode steps:   9, steps per second: 128, episode reward: -2.000, mean reward: -0.222 [-10.000,  8.000], mean action: 54.556 [17.000, 93.000],  loss: 9.306209, mae: 2.659278, mean_q: 4.843885\n",
      "[21 68 69 24 34 14 57 93 71 88]\n",
      " 49707/50001: episode: 5523, duration: 0.071s, episode steps:   9, steps per second: 127, episode reward: 41.000, mean reward:  4.556 [ 2.000,  9.000], mean action: 57.556 [14.000, 93.000],  loss: 6.656731, mae: 2.612401, mean_q: 4.783415\n",
      "[58 96 33 79 48 32 83 10  8 57]\n",
      " 49716/50001: episode: 5524, duration: 0.073s, episode steps:   9, steps per second: 123, episode reward: 35.000, mean reward:  3.889 [ 3.000,  6.000], mean action: 49.556 [8.000, 96.000],  loss: 7.118750, mae: 2.628849, mean_q: 4.852381\n",
      "[67 24 84 27 76 34 62 10 88  4]\n",
      " 49725/50001: episode: 5525, duration: 0.070s, episode steps:   9, steps per second: 128, episode reward: 43.000, mean reward:  4.778 [ 3.000,  7.000], mean action: 45.444 [4.000, 88.000],  loss: 7.715261, mae: 2.611252, mean_q: 4.724021\n",
      "[ 9 13 95 79 31 75 79  2 74 52]\n",
      " 49734/50001: episode: 5526, duration: 0.072s, episode steps:   9, steps per second: 125, episode reward: 25.000, mean reward:  2.778 [-10.000,  5.000], mean action: 55.556 [2.000, 95.000],  loss: 8.078954, mae: 2.571570, mean_q: 4.697424\n",
      "[29 98 65 51 51 28 56 97 41 82]\n",
      " 49743/50001: episode: 5527, duration: 0.070s, episode steps:   9, steps per second: 128, episode reward: 26.000, mean reward:  2.889 [-10.000,  6.000], mean action: 63.222 [28.000, 98.000],  loss: 7.731409, mae: 2.511884, mean_q: 4.629656\n",
      "[21 67 88 47 46 63 51 89 86 74]\n",
      " 49752/50001: episode: 5528, duration: 0.072s, episode steps:   9, steps per second: 125, episode reward: 39.000, mean reward:  4.333 [ 3.000,  7.000], mean action: 67.889 [46.000, 89.000],  loss: 6.618620, mae: 2.577891, mean_q: 4.690777\n",
      "[59 18 41 50 49 27 28 61 40 63]\n",
      " 49761/50001: episode: 5529, duration: 0.073s, episode steps:   9, steps per second: 124, episode reward: 39.000, mean reward:  4.333 [ 2.000,  6.000], mean action: 41.889 [18.000, 63.000],  loss: 5.898078, mae: 2.595378, mean_q: 4.679746\n",
      "[39 42 11 13 93 69 37 35 48 57]\n",
      " 49770/50001: episode: 5530, duration: 0.071s, episode steps:   9, steps per second: 126, episode reward: 42.000, mean reward:  4.667 [ 3.000,  7.000], mean action: 45.000 [11.000, 93.000],  loss: 6.069371, mae: 2.682752, mean_q: 4.874642\n",
      "[40 37 36 11 84 66 84 46 38 97]\n",
      " 49779/50001: episode: 5531, duration: 0.071s, episode steps:   9, steps per second: 127, episode reward: 25.000, mean reward:  2.778 [-10.000,  9.000], mean action: 55.444 [11.000, 97.000],  loss: 6.610569, mae: 2.675742, mean_q: 4.879615\n",
      "[26 96 34 69 57 62 20 27 23 93]\n",
      " 49788/50001: episode: 5532, duration: 0.071s, episode steps:   9, steps per second: 126, episode reward: 40.000, mean reward:  4.444 [ 3.000,  6.000], mean action: 53.444 [20.000, 96.000],  loss: 5.706912, mae: 2.679202, mean_q: 4.868556\n",
      "[ 0 53 24 49 22 51 79 28 16  9]\n",
      " 49797/50001: episode: 5533, duration: 0.072s, episode steps:   9, steps per second: 125, episode reward: 39.000, mean reward:  4.333 [ 3.000,  6.000], mean action: 36.778 [9.000, 79.000],  loss: 8.106695, mae: 2.666838, mean_q: 4.832145\n",
      "[78 37 46 60 13 15  2 50 45 68]\n",
      " 49806/50001: episode: 5534, duration: 0.071s, episode steps:   9, steps per second: 126, episode reward: 37.000, mean reward:  4.111 [ 2.000,  6.000], mean action: 37.333 [2.000, 68.000],  loss: 7.022455, mae: 2.633932, mean_q: 4.851457\n",
      "[27 86 40 25 82  6  6 58 12 50]\n",
      " 49815/50001: episode: 5535, duration: 0.071s, episode steps:   9, steps per second: 127, episode reward: 30.000, mean reward:  3.333 [-10.000,  9.000], mean action: 40.556 [6.000, 86.000],  loss: 7.777547, mae: 2.641451, mean_q: 4.797029\n",
      "[33 85 31 19 40 51  4  1 12 80]\n",
      " 49824/50001: episode: 5536, duration: 0.071s, episode steps:   9, steps per second: 127, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 35.889 [1.000, 85.000],  loss: 7.872070, mae: 2.600991, mean_q: 4.723882\n",
      "[41 28 34 34 50 32 91  8 50 32]\n",
      " 49833/50001: episode: 5537, duration: 0.072s, episode steps:   9, steps per second: 125, episode reward: -6.000, mean reward: -0.667 [-10.000,  5.000], mean action: 39.889 [8.000, 91.000],  loss: 8.975302, mae: 2.625880, mean_q: 4.822803\n",
      "[34 24 10 11 94 90 58 54  2 16]\n",
      " 49842/50001: episode: 5538, duration: 0.072s, episode steps:   9, steps per second: 124, episode reward: 34.000, mean reward:  3.778 [ 3.000,  6.000], mean action: 39.889 [2.000, 94.000],  loss: 10.166326, mae: 2.616225, mean_q: 4.835169\n",
      "[80 94 53 24 97 75 15 59 82 79]\n",
      " 49851/50001: episode: 5539, duration: 0.071s, episode steps:   9, steps per second: 127, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 64.222 [15.000, 97.000],  loss: 7.611175, mae: 2.549079, mean_q: 4.708275\n",
      "[85 15 46 99 96 40 34 82 13 48]\n",
      " 49860/50001: episode: 5540, duration: 0.069s, episode steps:   9, steps per second: 130, episode reward: 44.000, mean reward:  4.889 [ 2.000,  8.000], mean action: 52.556 [13.000, 99.000],  loss: 7.838442, mae: 2.538967, mean_q: 4.724154\n",
      "[16 32 98 46 82 44 97 19 32 63]\n",
      " 49869/50001: episode: 5541, duration: 0.070s, episode steps:   9, steps per second: 128, episode reward: 23.000, mean reward:  2.556 [-10.000,  6.000], mean action: 57.000 [19.000, 98.000],  loss: 6.787008, mae: 2.526549, mean_q: 4.670559\n",
      "[44 54 82 33 27 39  2 65  4 49]\n",
      " 49878/50001: episode: 5542, duration: 0.072s, episode steps:   9, steps per second: 125, episode reward: 37.000, mean reward:  4.111 [ 2.000,  7.000], mean action: 39.444 [2.000, 82.000],  loss: 6.534779, mae: 2.576295, mean_q: 4.686842\n",
      "[30  4 82 19 34 97 10 27 24 37]\n",
      " 49887/50001: episode: 5543, duration: 0.075s, episode steps:   9, steps per second: 120, episode reward: 44.000, mean reward:  4.889 [ 3.000,  8.000], mean action: 37.111 [4.000, 97.000],  loss: 7.593510, mae: 2.563644, mean_q: 4.693536\n",
      "[47 95 86 37 90 85 39 98 12 40]\n",
      " 49896/50001: episode: 5544, duration: 0.071s, episode steps:   9, steps per second: 127, episode reward: 40.000, mean reward:  4.444 [ 2.000,  8.000], mean action: 64.667 [12.000, 98.000],  loss: 7.945369, mae: 2.619979, mean_q: 4.861541\n",
      "[85 48 93 59 86 12 14 85 39 12]\n",
      " 49905/50001: episode: 5545, duration: 0.075s, episode steps:   9, steps per second: 120, episode reward:  5.000, mean reward:  0.556 [-10.000,  6.000], mean action: 49.778 [12.000, 93.000],  loss: 6.473339, mae: 2.604298, mean_q: 4.803107\n",
      "[77 27 74 82 84 75 34 37  1 14]\n",
      " 49914/50001: episode: 5546, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward: 43.000, mean reward:  4.778 [ 3.000,  7.000], mean action: 47.556 [1.000, 84.000],  loss: 8.802589, mae: 2.587149, mean_q: 4.773844\n",
      "[ 1 57 32 21 25 18 35 20 57 41]\n",
      " 49923/50001: episode: 5547, duration: 0.069s, episode steps:   9, steps per second: 130, episode reward: 20.000, mean reward:  2.222 [-10.000,  7.000], mean action: 34.000 [18.000, 57.000],  loss: 6.693894, mae: 2.544412, mean_q: 4.670308\n",
      "[84 84  9 79  1  1 21 95 82 18]\n",
      " 49932/50001: episode: 5548, duration: 0.071s, episode steps:   9, steps per second: 127, episode reward: 13.000, mean reward:  1.444 [-10.000,  8.000], mean action: 43.333 [1.000, 95.000],  loss: 6.357510, mae: 2.534660, mean_q: 4.620415\n",
      "[28 41 82 13 95 42 13 62 46 46]\n",
      " 49941/50001: episode: 5549, duration: 0.072s, episode steps:   9, steps per second: 125, episode reward: 10.000, mean reward:  1.111 [-10.000,  6.000], mean action: 48.889 [13.000, 95.000],  loss: 8.154127, mae: 2.509826, mean_q: 4.538105\n",
      "[92 99 98 91 14 50 79 77 64 32]\n",
      " 49950/50001: episode: 5550, duration: 0.069s, episode steps:   9, steps per second: 130, episode reward: 38.000, mean reward:  4.222 [ 3.000,  7.000], mean action: 67.111 [14.000, 99.000],  loss: 7.858853, mae: 2.622754, mean_q: 4.830762\n",
      "[31 24 76 71 79 79 31 37 86  4]\n",
      " 49959/50001: episode: 5551, duration: 0.073s, episode steps:   9, steps per second: 123, episode reward: 10.000, mean reward:  1.111 [-10.000,  7.000], mean action: 54.111 [4.000, 86.000],  loss: 6.485792, mae: 2.656388, mean_q: 4.818526\n",
      "[90 37 97 75 50 60 14 66 88 57]\n",
      " 49968/50001: episode: 5552, duration: 0.070s, episode steps:   9, steps per second: 129, episode reward: 41.000, mean reward:  4.556 [ 3.000,  6.000], mean action: 60.444 [14.000, 97.000],  loss: 5.792528, mae: 2.685479, mean_q: 4.911220\n",
      "[95 60 12 79 47 29 50 11  4 88]\n",
      " 49977/50001: episode: 5553, duration: 0.075s, episode steps:   9, steps per second: 120, episode reward: 38.000, mean reward:  4.222 [ 2.000,  7.000], mean action: 42.222 [4.000, 88.000],  loss: 9.008332, mae: 2.630562, mean_q: 4.773245\n",
      "[95 88 74 64  2 12 24 27 78 21]\n",
      " 49986/50001: episode: 5554, duration: 0.069s, episode steps:   9, steps per second: 130, episode reward: 35.000, mean reward:  3.889 [ 3.000,  5.000], mean action: 43.333 [2.000, 88.000],  loss: 6.463104, mae: 2.584894, mean_q: 4.724029\n",
      "[45 42  1 21 51 32 83 24 76 83]\n",
      " 49995/50001: episode: 5555, duration: 0.070s, episode steps:   9, steps per second: 128, episode reward: 22.000, mean reward:  2.444 [-10.000,  6.000], mean action: 45.889 [1.000, 83.000],  loss: 6.782535, mae: 2.581485, mean_q: 4.762315\n",
      "done, took 356.838 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-4), metrics=['mae'])\n",
    "history=dqn.fit(env, nb_steps=50001, visualize=False, verbose=2, action_repetition=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dqn.save_weights('models/dqn_0.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.09208588  2.465918    2.3802023   1.4971184   3.326894    2.2722988\n",
      "  1.6132427   1.4397268   2.178078    2.6207886   1.7620492   0.3722487\n",
      "  3.7532783   3.0413268   2.9067812   1.4383929   2.437113    0.7436111\n",
      "  0.25330162  1.8442116   2.5616512   1.633022    1.4879366   2.713372\n",
      "  2.4821937  -0.1127215   1.7409394   2.8187227   3.1395254   0.91047806\n",
      "  1.6949698   2.6761456   2.7240229   1.7270445   3.2656097   2.383215\n",
      "  1.2673366   3.1763463   1.4264318   1.2995583   1.8948374   2.5872586\n",
      "  2.2416797   1.6559376   1.1183548   1.2832756   2.3957334  -0.07838779\n",
      "  3.435488    1.220594    2.4905043   2.0279372   2.180113    1.7514144\n",
      "  1.6581522   1.4356804   0.24983841  2.970135    0.9382737   1.6611669\n",
      "  1.7461276   0.8250588   1.7703555   2.154612    2.302318    1.0308644\n",
      "  2.9652028   1.2757845   2.1942852   1.8686374   0.78799444  1.906012\n",
      "  1.5075873   1.2972778   1.7702136   1.9561983   1.1630626  -1.1309474\n",
      "  1.3732708   2.536018    1.4050657   1.6231196   2.1816835   2.2558603\n",
      "  2.0657864   2.109546    1.7286747   2.0767434   2.759117    2.4742007\n",
      "  2.5605717  -0.19025971  1.5300341   2.2585702   1.718558    1.7346426\n",
      "  1.6723678   2.616055    2.7384856   0.7038618 ]\n",
      "12\n",
      "48\n",
      "Episode = [77, 20, 47, 97, 88, 97, 95, 50, 12, 12]\n",
      "Reward = 43.98652696609497\n"
     ]
    }
   ],
   "source": [
    "a=[[ np.random.randint(0,100), np.random.randint(0,100), np.random.randint(0,100), np.random.randint(0,100), -1, -1, -1, -1, -1, -1]]\n",
    "a=[[ np.random.randint(0,100), np.random.randint(0,100), -1, -1, -1, -1, -1, -1, -1, -1]]\n",
    "t=0\n",
    "idx=2\n",
    "test= [[88, 41, 49, 42, 69, 43, 81, 6, 20, -1]]\n",
    "\n",
    "test_reward=dqn.compute_q_values(test)\n",
    "max_v=max(test_reward)\n",
    "print(test_reward)\n",
    "rrr=test_reward.tolist()\n",
    "index = rrr.index(max_v)\n",
    "print(index)\n",
    "rrr[index]=-100\n",
    "max_v=max(rrr)\n",
    "print(rrr.index(max_v))\n",
    "reward=[]\n",
    "\n",
    "for i in range(idx,10):\n",
    "        z=dqn.compute_q_values(a)\n",
    "        max_v=max(z)\n",
    "        t+=max_v\n",
    "        #print(max_v)\n",
    "        z=z.tolist()\n",
    "        index = z.index(max_v)\n",
    "        a[0][i]=index\n",
    "res=a[0]\n",
    "reward.append(t)\n",
    "    \n",
    "    \n",
    "print(\"Episode =\", a[0])\n",
    "print(\"Reward =\", t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/notebook/utils.py:280: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  return LooseVersion(v) >= LooseVersion(check)\n",
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maayush-singharoy\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/wandb/sdk/lib/ipython.py:46: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import HTML, display  # type: ignore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/aayush/git/rl_recsys/src/recsim/trial/wandb/run-20230216_182247-g5fr6t1x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/aayush-singharoy/rl_recsys-src_recsim_trial/runs/g5fr6t1x\" target=\"_blank\">giddy-plasma-100</a></strong> to <a href=\"https://wandb.ai/aayush-singharoy/rl_recsys-src_recsim_trial\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/aayush-singharoy/rl_recsys-src_recsim_trial\" target=\"_blank\">https://wandb.ai/aayush-singharoy/rl_recsys-src_recsim_trial</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/aayush-singharoy/rl_recsys-src_recsim_trial/runs/g5fr6t1x\" target=\"_blank\">https://wandb.ai/aayush-singharoy/rl_recsys-src_recsim_trial/runs/g5fr6t1x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN\n",
      "Input state = [[62, 41, 13, 2, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [62, 41, 13, 2, 95, 50, 34, 79, 40, 57]\n",
      "Reward = 32.20171880722046\n",
      "Input state = [[76, 77, 8, 34, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [76, 77, 8, 34, 13, 50, 47, 88, 98, 48]\n",
      "Reward = 26.900935173034668\n",
      "Input state = [[1, 80, 85, 10, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [1, 80, 85, 10, 34, 13, 47, 97, 75, 50]\n",
      "Reward = 29.456263303756714\n",
      "Input state = [[20, 2, 99, 18, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [20, 2, 99, 18, 1, 95, 50, 13, 37, 66]\n",
      "Reward = 30.122969388961792\n",
      "Input state = [[39, 1, 73, 85, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [39, 1, 73, 85, 37, 97, 13, 95, 12, 66]\n",
      "Reward = 34.0535044670105\n",
      "Input state = [[7, 36, 69, 5, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [7, 36, 69, 5, 95, 34, 48, 32, 40, 50]\n",
      "Reward = 25.10898232460022\n",
      "Input state = [[42, 73, 93, 1, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [42, 73, 93, 1, 95, 13, 34, 75, 50, 79]\n",
      "Reward = 27.22035312652588\n",
      "Input state = [[86, 96, 6, 61, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [86, 96, 6, 61, 13, 97, 48, 1, 88, 50]\n",
      "Reward = 30.640183448791504\n",
      "Input state = [[22, 7, 98, 96, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [22, 7, 98, 96, 34, 97, 13, 50, 66, 88]\n",
      "Reward = 34.6900110244751\n",
      "Input state = [[58, 82, 35, 77, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [58, 82, 35, 77, 34, 97, 13, 95, 48, 12]\n",
      "Reward = 30.228697299957275\n",
      "Input state = [[79, 83, 5, 2, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [79, 83, 5, 2, 37, 50, 97, 48, 13, 32]\n",
      "Reward = 29.89322566986084\n",
      "Input state = [[12, 71, 87, 78, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [12, 71, 87, 78, 95, 13, 75, 88, 14, 98]\n",
      "Reward = 30.638559341430664\n",
      "Input state = [[74, 77, 51, 42, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [74, 77, 51, 42, 97, 13, 1, 57, 40, 47]\n",
      "Reward = 30.537909984588623\n",
      "Input state = [[52, 56, 77, 9, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [52, 56, 77, 9, 95, 34, 1, 13, 57, 50]\n",
      "Reward = 26.658817291259766\n",
      "Input state = [[20, 21, 39, 85, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [20, 21, 39, 85, 34, 13, 50, 88, 12, 14]\n",
      "Reward = 24.00625777244568\n",
      "Input state = [[50, 83, 30, 2, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [50, 83, 30, 2, 34, 13, 48, 60, 31, 98]\n",
      "Reward = 22.64234709739685\n",
      "Input state = [[44, 58, 47, 18, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [44, 58, 47, 18, 34, 13, 88, 32, 50, 48]\n",
      "Reward = 22.746663331985474\n",
      "Input state = [[56, 5, 45, 76, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [56, 5, 45, 76, 37, 97, 95, 50, 12, 66]\n",
      "Reward = 31.044172286987305\n",
      "Input state = [[15, 99, 73, 62, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [15, 99, 73, 62, 34, 13, 47, 88, 31, 14]\n",
      "Reward = 28.566791534423828\n",
      "Input state = [[84, 13, 52, 86, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [84, 13, 52, 86, 37, 97, 95, 50, 12, 66]\n",
      "Reward = 32.00205898284912\n",
      "Input state = [[68, 27, 78, 84, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [68, 27, 78, 84, 95, 97, 13, 37, 50, 34]\n",
      "Reward = 31.713067531585693\n",
      "Input state = [[14, 62, 86, 41, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [14, 62, 86, 41, 95, 13, 88, 50, 34, 32]\n",
      "Reward = 29.461835861206055\n",
      "Input state = [[27, 15, 88, 79, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [27, 15, 88, 79, 95, 37, 74, 50, 48, 12]\n",
      "Reward = 29.203880548477173\n",
      "Input state = [[28, 77, 86, 84, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [28, 77, 86, 84, 95, 13, 34, 57, 50, 47]\n",
      "Reward = 32.5947322845459\n",
      "Input state = [[75, 91, 15, 39, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [75, 91, 15, 39, 97, 34, 13, 95, 40, 79]\n",
      "Reward = 31.80296492576599\n",
      "Input state = [[33, 21, 56, 17, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [33, 21, 56, 17, 95, 46, 13, 34, 79, 50]\n",
      "Reward = 25.828423500061035\n",
      "Input state = [[40, 92, 27, 15, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [40, 92, 27, 15, 34, 13, 47, 60, 31, 50]\n",
      "Reward = 23.99429750442505\n",
      "Input state = [[30, 93, 6, 27, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [30, 93, 6, 27, 97, 50, 48, 1, 88, 34]\n",
      "Reward = 32.80989074707031\n",
      "Input state = [[10, 70, 94, 83, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [10, 70, 94, 83, 95, 13, 75, 88, 14, 24]\n",
      "Reward = 31.192083835601807\n",
      "Input state = [[5, 38, 82, 87, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [5, 38, 82, 87, 34, 13, 88, 97, 14, 48]\n",
      "Reward = 26.198164701461792\n",
      "Input state = [[34, 65, 28, 0, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [34, 65, 28, 0, 37, 50, 48, 95, 98, 13]\n",
      "Reward = 24.094037532806396\n",
      "Input state = [[44, 36, 7, 82, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [44, 36, 7, 82, 50, 13, 48, 37, 34, 28]\n",
      "Reward = 23.190038919448853\n",
      "Input state = [[24, 61, 92, 22, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [24, 61, 92, 22, 95, 13, 88, 50, 34, 32]\n",
      "Reward = 28.556795597076416\n",
      "Input state = [[90, 15, 30, 96, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [90, 15, 30, 96, 37, 47, 50, 13, 31, 12]\n",
      "Reward = 29.156383752822876\n",
      "Input state = [[86, 75, 8, 38, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [86, 75, 8, 38, 13, 50, 47, 88, 34, 48]\n",
      "Reward = 26.519161462783813\n",
      "Input state = [[11, 42, 52, 63, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [11, 42, 52, 63, 95, 34, 13, 1, 88, 50]\n",
      "Reward = 29.42051410675049\n",
      "Input state = [[65, 20, 75, 68, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [65, 20, 75, 68, 95, 97, 13, 37, 66, 50]\n",
      "Reward = 31.588659286499023\n",
      "Input state = [[23, 42, 57, 65, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [23, 42, 57, 65, 95, 13, 1, 97, 98, 40]\n",
      "Reward = 26.674322366714478\n",
      "Input state = [[38, 22, 23, 70, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [38, 22, 23, 70, 37, 32, 50, 13, 34, 12]\n",
      "Reward = 22.362815856933594\n",
      "Input state = [[42, 66, 80, 75, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [42, 66, 80, 75, 95, 13, 57, 34, 50, 32]\n",
      "Reward = 28.39980673789978\n",
      "Input state = [[56, 83, 56, 2, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [56, 83, 56, 2, 34, 13, 50, 4, 5, 31]\n",
      "Reward = 23.608337879180908\n",
      "Input state = [[34, 81, 77, 13, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [34, 81, 77, 13, 95, 50, 88, 4, 32, 57]\n",
      "Reward = 24.98455262184143\n",
      "Input state = [[82, 99, 17, 29, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [82, 99, 17, 29, 97, 34, 13, 95, 40, 79]\n",
      "Reward = 33.023773193359375\n",
      "Input state = [[40, 79, 56, 16, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [40, 79, 56, 16, 34, 13, 88, 4, 2, 42]\n",
      "Reward = 22.966439723968506\n",
      "Input state = [[36, 59, 0, 47, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [36, 59, 0, 47, 2, 1, 37, 34, 48, 50]\n",
      "Reward = 22.924537420272827\n",
      "Input state = [[73, 3, 89, 69, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [73, 3, 89, 69, 95, 37, 66, 50, 12, 48]\n",
      "Reward = 28.749570608139038\n",
      "Input state = [[89, 84, 32, 70, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [89, 84, 32, 70, 97, 1, 2, 57, 50, 75]\n",
      "Reward = 28.953314781188965\n",
      "Input state = [[54, 94, 0, 34, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [54, 94, 0, 34, 97, 50, 48, 1, 88, 51]\n",
      "Reward = 32.911954402923584\n",
      "Input state = [[42, 14, 46, 37, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [42, 14, 46, 37, 95, 74, 13, 34, 1, 57]\n",
      "Reward = 24.93254804611206\n",
      "Input state = [[98, 31, 90, 11, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [98, 31, 90, 11, 97, 57, 95, 12, 4, 66]\n",
      "Reward = 26.665897369384766\n",
      "Input state = [[57, 7, 69, 93, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [57, 7, 69, 93, 37, 97, 95, 34, 66, 12]\n",
      "Reward = 33.55490303039551\n",
      "Input state = [[60, 63, 91, 20, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [60, 63, 91, 20, 95, 34, 1, 97, 40, 79]\n",
      "Reward = 29.94063663482666\n",
      "Input state = [[68, 93, 23, 89, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [68, 93, 23, 89, 2, 1, 95, 34, 48, 13]\n",
      "Reward = 34.60839366912842\n",
      "Input state = [[27, 7, 96, 86, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [27, 7, 96, 86, 34, 97, 13, 95, 12, 40]\n",
      "Reward = 36.55522584915161\n",
      "Input state = [[78, 21, 79, 24, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [78, 21, 79, 24, 95, 60, 13, 34, 1, 57]\n",
      "Reward = 24.43230128288269\n",
      "Input state = [[46, 1, 2, 92, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [46, 1, 2, 92, 37, 51, 88, 13, 50, 12]\n",
      "Reward = 34.28833723068237\n",
      "Input state = [[38, 54, 83, 7, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [38, 54, 83, 7, 95, 34, 1, 13, 88, 47]\n",
      "Reward = 29.266135215759277\n",
      "Input state = [[43, 85, 19, 96, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [43, 85, 19, 96, 2, 34, 88, 50, 48, 13]\n",
      "Reward = 30.46559238433838\n",
      "Input state = [[37, 65, 17, 6, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [37, 65, 17, 6, 34, 50, 48, 1, 95, 51]\n",
      "Reward = 21.54191541671753\n",
      "Input state = [[59, 4, 76, 82, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [59, 4, 76, 82, 37, 97, 95, 12, 66, 50]\n",
      "Reward = 35.2431583404541\n",
      "Input state = [[62, 13, 81, 58, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [62, 13, 81, 58, 95, 37, 41, 66, 12, 4]\n",
      "Reward = 23.785135746002197\n",
      "Input state = [[37, 19, 41, 62, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [37, 19, 41, 62, 95, 46, 13, 34, 1, 12]\n",
      "Reward = 23.673158168792725\n",
      "Input state = [[38, 91, 60, 16, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [38, 91, 60, 16, 34, 13, 47, 97, 75, 50]\n",
      "Reward = 26.44530963897705\n",
      "Input state = [[5, 65, 46, 35, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [5, 65, 46, 35, 34, 13, 50, 88, 75, 48]\n",
      "Reward = 24.38745903968811\n",
      "Input state = [[88, 69, 1, 70, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [88, 69, 1, 70, 47, 97, 95, 37, 13, 48]\n",
      "Reward = 27.643617153167725\n",
      "Input state = [[32, 59, 38, 92, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [32, 59, 38, 92, 95, 13, 1, 88, 50, 12]\n",
      "Reward = 27.783693552017212\n",
      "Input state = [[60, 80, 34, 0, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [60, 80, 34, 0, 2, 13, 37, 90, 75, 50]\n",
      "Reward = 24.849576950073242\n",
      "Input state = [[89, 90, 49, 4, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [89, 90, 49, 4, 2, 13, 95, 97, 48, 50]\n",
      "Reward = 27.61431336402893\n",
      "Input state = [[55, 18, 81, 88, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [55, 18, 81, 88, 95, 97, 37, 50, 66, 12]\n",
      "Reward = 36.75066566467285\n",
      "Input state = [[23, 91, 79, 45, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [23, 91, 79, 45, 34, 13, 47, 97, 5, 75]\n",
      "Reward = 28.448237895965576\n",
      "Input state = [[19, 82, 54, 42, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [19, 82, 54, 42, 34, 13, 50, 75, 31, 14]\n",
      "Reward = 23.8334059715271\n",
      "Input state = [[51, 54, 9, 6, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [51, 54, 9, 6, 34, 50, 95, 48, 13, 88]\n",
      "Reward = 24.530104637145996\n",
      "Input state = [[67, 18, 97, 2, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [67, 18, 97, 2, 95, 57, 13, 1, 34, 50]\n",
      "Reward = 25.16589331626892\n",
      "Input state = [[1, 62, 68, 12, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [1, 62, 68, 12, 34, 13, 47, 88, 75, 50]\n",
      "Reward = 26.553964138031006\n",
      "Input state = [[12, 0, 22, 32, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [12, 0, 22, 32, 37, 46, 95, 50, 48, 13]\n",
      "Reward = 23.252891540527344\n",
      "Input state = [[41, 48, 6, 81, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [41, 48, 6, 81, 37, 88, 95, 34, 13, 28]\n",
      "Reward = 25.871846437454224\n",
      "Input state = [[37, 47, 82, 10, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [37, 47, 82, 10, 95, 34, 1, 13, 4, 57]\n",
      "Reward = 23.279332637786865\n",
      "Input state = [[96, 45, 38, 20, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [96, 45, 38, 20, 97, 95, 13, 1, 34, 37]\n",
      "Reward = 28.647265434265137\n",
      "Input state = [[67, 94, 30, 16, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [67, 94, 30, 16, 93, 50, 95, 88, 40, 98]\n",
      "Reward = 29.826396465301514\n",
      "Input state = [[91, 34, 33, 45, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [91, 34, 33, 45, 97, 95, 13, 37, 1, 57]\n",
      "Reward = 29.78044033050537\n",
      "Input state = [[11, 92, 45, 52, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [11, 92, 45, 52, 34, 13, 47, 75, 31, 14]\n",
      "Reward = 25.016387462615967\n",
      "Input state = [[80, 19, 4, 88, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [80, 19, 4, 88, 50, 37, 13, 95, 12, 31]\n",
      "Reward = 28.858890056610107\n",
      "Input state = [[68, 4, 27, 2, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [68, 4, 27, 2, 95, 42, 50, 79, 12, 32]\n",
      "Reward = 30.897634506225586\n",
      "Input state = [[19, 23, 32, 90, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [19, 23, 32, 90, 34, 13, 50, 88, 12, 14]\n",
      "Reward = 24.4109206199646\n",
      "Input state = [[73, 3, 90, 67, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [73, 3, 90, 67, 95, 97, 37, 50, 66, 12]\n",
      "Reward = 37.123046875\n",
      "Input state = [[56, 70, 99, 15, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [56, 70, 99, 15, 95, 13, 97, 32, 4, 34]\n",
      "Reward = 29.37078046798706\n",
      "Input state = [[29, 48, 9, 31, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [29, 48, 9, 31, 2, 93, 95, 50, 88, 13]\n",
      "Reward = 29.1727774143219\n",
      "Input state = [[75, 60, 97, 68, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [75, 60, 97, 68, 95, 57, 1, 13, 2, 4]\n",
      "Reward = 29.90814781188965\n",
      "Input state = [[2, 99, 51, 87, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [2, 99, 51, 87, 34, 13, 50, 75, 31, 14]\n",
      "Reward = 28.787197828292847\n",
      "Input state = [[61, 79, 46, 28, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [61, 79, 46, 28, 13, 97, 50, 95, 1, 12]\n",
      "Reward = 31.140982151031494\n",
      "Input state = [[27, 21, 86, 53, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [27, 21, 86, 53, 95, 97, 13, 37, 66, 12]\n",
      "Reward = 30.563246726989746\n",
      "Input state = [[35, 56, 39, 78, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [35, 56, 39, 78, 95, 97, 13, 34, 1, 37]\n",
      "Reward = 28.618982315063477\n",
      "Input state = [[96, 55, 99, 88, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [96, 55, 99, 88, 95, 97, 13, 1, 34, 2]\n",
      "Reward = 32.237629890441895\n",
      "Input state = [[76, 65, 91, 10, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [76, 65, 91, 10, 95, 34, 1, 97, 56, 79]\n",
      "Reward = 30.03119945526123\n",
      "Input state = [[82, 92, 38, 14, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [82, 92, 38, 14, 2, 97, 48, 50, 95, 51]\n",
      "Reward = 33.06030225753784\n",
      "Input state = [[2, 59, 30, 82, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [2, 59, 30, 82, 34, 50, 48, 13, 4, 32]\n",
      "Reward = 25.543210744857788\n",
      "Input state = [[44, 76, 37, 74, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [44, 76, 37, 74, 95, 13, 1, 97, 98, 32]\n",
      "Reward = 29.981131076812744\n",
      "Input state = [[33, 37, 40, 86, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [33, 37, 40, 86, 95, 97, 13, 34, 50, 48]\n",
      "Reward = 29.570108890533447\n",
      "Input state = [[41, 41, 89, 56, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [41, 41, 89, 56, 95, 97, 13, 1, 34, 2]\n",
      "Reward = 30.385398387908936\n",
      "Input state = [[26, 63, 45, 63, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [26, 63, 45, 63, 95, 13, 88, 50, 98, 32]\n",
      "Reward = 30.589590549468994\n"
     ]
    }
   ],
   "source": [
    "# a=env.observation_space.sample().ravel()\n",
    "# print(a)\n",
    "import wandb\n",
    "wandb.init()\n",
    "b=[]\n",
    "\n",
    "\n",
    "reward=[]\n",
    "print('DQN')\n",
    "\n",
    "for j in range(0, 100):\n",
    "    t=0\n",
    "    if(j<0):\n",
    "        idx=1\n",
    "        a=[[ np.random.randint(0,100), -1, -1, -1, -1, -1, -1, -1, -1, -1]]\n",
    "        b.append(a[0][0])\n",
    "        print(\"Input state =\", a)\n",
    "    else:\n",
    "        idx=4\n",
    "        a=[[ np.random.randint(0,100), np.random.randint(0,100), np.random.randint(0,100), np.random.randint(0,100), -1, -1, -1, -1, -1, -1]]\n",
    "        b.append(a[0][0])\n",
    "        b.append(a[0][1])\n",
    "        b.append(a[0][2])\n",
    "        b.append(a[0][3])\n",
    "        print(\"Input state =\", a)\n",
    "\n",
    "    for i in range(idx,10):\n",
    "        z=dqn.compute_q_values(a)\n",
    "        max_v=max(z)\n",
    "        t+=max_v\n",
    "        #print(max_v)\n",
    "        z=z.tolist()\n",
    "        index = z.index(max_v)\n",
    "        while index in a[0]:\n",
    "            t-=max_v\n",
    "            # print(\"Nooo\")\n",
    "            z[index]=-100000000\n",
    "            max_v=max(z)\n",
    "            t+=max_v\n",
    "        #print(max_v)\n",
    "            index = z.index(max_v)\n",
    "        a[0][i]=index\n",
    "    res=a[0]\n",
    "    reward.append(t)\n",
    "    \n",
    "    \n",
    "    print(\"Episode =\", a[0])\n",
    "    print(\"Reward =\", t)\n",
    "xx=[i for i in range(0,100)]\n",
    "data = [[x, y] for (x, y) in zip(xx, reward)]\n",
    "table = wandb.Table(data=data, columns = [\"episode comp\", \"total reward\"])\n",
    "wandb.log({\"custom_plot\" : wandb.plot.line(table, \"episode comp\",\"total reward\",\n",
    "           title=\"greedy vs rl\")})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:g5fr6t1x) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "339be2d75bd149c9a81e35cc53c8969b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.011 MB of 0.014 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.820987…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">giddy-plasma-100</strong> at: <a href=\"https://wandb.ai/aayush-singharoy/rl_recsys-src_recsim_trial/runs/g5fr6t1x\" target=\"_blank\">https://wandb.ai/aayush-singharoy/rl_recsys-src_recsim_trial/runs/g5fr6t1x</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230216_182247-g5fr6t1x/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:g5fr6t1x). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/wandb/sdk/lib/ipython.py:58: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d56be6427874fb3add0ac39f5436640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016669018426910043, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/aayush/git/rl_recsys/src/recsim/trial/wandb/run-20230216_182249-x9p0kbe8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/aayush-singharoy/rl_recsys-src_recsim_trial/runs/x9p0kbe8\" target=\"_blank\">fresh-puddle-101</a></strong> to <a href=\"https://wandb.ai/aayush-singharoy/rl_recsys-src_recsim_trial\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/aayush-singharoy/rl_recsys-src_recsim_trial\" target=\"_blank\">https://wandb.ai/aayush-singharoy/rl_recsys-src_recsim_trial</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/aayush-singharoy/rl_recsys-src_recsim_trial/runs/x9p0kbe8\" target=\"_blank\">https://wandb.ai/aayush-singharoy/rl_recsys-src_recsim_trial/runs/x9p0kbe8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy Algo\n",
      "Input state = [62, 41, 13, 2]\n",
      "Episode = [62, 41, 13, 2, 95, 12, 34, 66, 50, 37]\n",
      "Reward = 43.0\n",
      "Input state = [76, 77, 8, 34]\n",
      "Episode = [76, 77, 8, 34, 95, 12, 37, 66, 50, 13]\n",
      "Reward = 45.0\n",
      "Input state = [1, 80, 85, 10]\n",
      "Episode = [1, 80, 85, 10, 95, 34, 12, 37, 50, 13]\n",
      "Reward = 49.0\n",
      "Input state = [20, 2, 99, 18]\n",
      "Episode = [20, 2, 99, 18, 95, 34, 13, 50, 12, 37]\n",
      "Reward = 47.0\n",
      "Input state = [39, 1, 73, 85]\n",
      "Episode = [39, 1, 73, 85, 95, 34, 50, 12, 37, 66]\n",
      "Reward = 51.0\n",
      "Input state = [7, 36, 69, 5]\n",
      "Episode = [7, 36, 69, 5, 95, 34, 12, 37, 50, 13]\n",
      "Reward = 51.0\n",
      "Input state = [42, 73, 93, 1]\n",
      "Episode = [42, 73, 93, 1, 95, 34, 12, 37, 66, 50]\n",
      "Reward = 49.0\n",
      "Input state = [86, 96, 6, 61]\n",
      "Episode = [86, 96, 6, 61, 34, 95, 12, 37, 50, 13]\n",
      "Reward = 55.0\n",
      "Input state = [22, 7, 98, 96]\n",
      "Episode = [22, 7, 98, 96, 95, 34, 13, 50, 12, 37]\n",
      "Reward = 49.0\n",
      "Input state = [58, 82, 35, 77]\n",
      "Episode = [58, 82, 35, 77, 95, 34, 12, 37, 50, 66]\n",
      "Reward = 49.0\n",
      "Input state = [79, 83, 5, 2]\n",
      "Episode = [79, 83, 5, 2, 95, 34, 66, 50, 12, 37]\n",
      "Reward = 48.0\n",
      "Input state = [12, 71, 87, 78]\n",
      "Episode = [12, 71, 87, 78, 95, 34, 66, 37, 13, 50]\n",
      "Reward = 51.0\n",
      "Input state = [74, 77, 51, 42]\n",
      "Episode = [74, 77, 51, 42, 95, 34, 50, 12, 37, 66]\n",
      "Reward = 48.0\n",
      "Input state = [52, 56, 77, 9]\n",
      "Episode = [52, 56, 77, 9, 95, 34, 12, 37, 66, 50]\n",
      "Reward = 48.0\n",
      "Input state = [20, 21, 39, 85]\n",
      "Episode = [20, 21, 39, 85, 95, 34, 50, 12, 37, 66]\n",
      "Reward = 53.0\n",
      "Input state = [50, 83, 30, 2]\n",
      "Episode = [50, 83, 30, 2, 95, 34, 66, 12, 37, 13]\n",
      "Reward = 47.0\n",
      "Input state = [44, 58, 47, 18]\n",
      "Episode = [44, 58, 47, 18, 95, 34, 12, 37, 13, 50]\n",
      "Reward = 49.0\n",
      "Input state = [56, 5, 45, 76]\n",
      "Episode = [56, 5, 45, 76, 95, 34, 66, 37, 12, 50]\n",
      "Reward = 53.0\n",
      "Input state = [15, 99, 73, 62]\n",
      "Episode = [15, 99, 73, 62, 95, 34, 13, 50, 12, 37]\n",
      "Reward = 52.0\n",
      "Input state = [84, 13, 52, 86]\n",
      "Episode = [84, 13, 52, 86, 95, 34, 12, 37, 66, 50]\n",
      "Reward = 49.0\n",
      "Input state = [68, 27, 78, 84]\n",
      "Episode = [68, 27, 78, 84, 95, 34, 66, 37, 12, 88]\n",
      "Reward = 51.0\n",
      "Input state = [14, 62, 86, 41]\n",
      "Episode = [14, 62, 86, 41, 95, 12, 34, 66, 50, 13]\n",
      "Reward = 52.0\n",
      "Input state = [27, 15, 88, 79]\n",
      "Episode = [27, 15, 88, 79, 95, 34, 66, 37, 12, 50]\n",
      "Reward = 47.0\n",
      "Input state = [28, 77, 86, 84]\n",
      "Episode = [28, 77, 86, 84, 34, 95, 12, 37, 13, 50]\n",
      "Reward = 50.0\n",
      "Input state = [75, 91, 15, 39]\n",
      "Episode = [75, 91, 15, 39, 95, 34, 37, 12, 50, 13]\n",
      "Reward = 54.0\n",
      "Input state = [33, 21, 56, 17]\n",
      "Episode = [33, 21, 56, 17, 95, 34, 66, 50, 12, 37]\n",
      "Reward = 53.0\n",
      "Input state = [40, 92, 27, 15]\n",
      "Episode = [40, 92, 27, 15, 95, 34, 66, 37, 12, 50]\n",
      "Reward = 52.0\n",
      "Input state = [30, 93, 6, 27]\n",
      "Episode = [30, 93, 6, 27, 34, 95, 66, 12, 37, 50]\n",
      "Reward = 50.0\n",
      "Input state = [10, 70, 94, 83]\n",
      "Episode = [10, 70, 94, 83, 34, 95, 12, 37, 13, 50]\n",
      "Reward = 52.0\n",
      "Input state = [5, 38, 82, 87]\n",
      "Episode = [5, 38, 82, 87, 95, 34, 12, 37, 66, 50]\n",
      "Reward = 52.0\n",
      "Input state = [34, 65, 28, 0]\n",
      "Episode = [34, 65, 28, 0, 95, 12, 37, 66, 13, 50]\n",
      "Reward = 48.0\n",
      "Input state = [44, 36, 7, 82]\n",
      "Episode = [44, 36, 7, 82, 95, 34, 12, 37, 66, 50]\n",
      "Reward = 57.0\n",
      "Input state = [24, 61, 92, 22]\n",
      "Episode = [24, 61, 92, 22, 95, 34, 66, 50, 12, 37]\n",
      "Reward = 51.0\n",
      "Input state = [90, 15, 30, 96]\n",
      "Episode = [90, 15, 30, 96, 95, 34, 66, 13, 50, 12]\n",
      "Reward = 51.0\n",
      "Input state = [86, 75, 8, 38]\n",
      "Episode = [86, 75, 8, 38, 95, 34, 12, 37, 66, 50]\n",
      "Reward = 53.0\n",
      "Input state = [11, 42, 52, 63]\n",
      "Episode = [11, 42, 52, 63, 95, 34, 66, 50, 12, 37]\n",
      "Reward = 50.0\n",
      "Input state = [65, 20, 75, 68]\n",
      "Episode = [65, 20, 75, 68, 95, 34, 13, 50, 12, 37]\n",
      "Reward = 52.0\n",
      "Input state = [23, 42, 57, 65]\n",
      "Episode = [23, 42, 57, 65, 95, 34, 66, 37, 12, 50]\n",
      "Reward = 52.0\n",
      "Input state = [38, 22, 23, 70]\n",
      "Episode = [38, 22, 23, 70, 95, 34, 12, 37, 66, 50]\n",
      "Reward = 52.0\n",
      "Input state = [42, 66, 80, 75]\n",
      "Episode = [42, 66, 80, 75, 95, 34, 50, 12, 37, 13]\n",
      "Reward = 48.0\n",
      "Input state = [56, 83, 56, 2]\n",
      "Episode = [56, 83, 56, 2, 95, 34, 66, 37, 12, 50]\n",
      "Reward = 48.0\n",
      "Input state = [34, 81, 77, 13]\n",
      "Episode = [34, 81, 77, 13, 95, 50, 12, 37, 66, 88]\n",
      "Reward = 42.0\n",
      "Input state = [82, 99, 17, 29]\n",
      "Episode = [82, 99, 17, 29, 95, 34, 12, 37, 66, 50]\n",
      "Reward = 51.0\n",
      "Input state = [40, 79, 56, 16]\n",
      "Episode = [40, 79, 56, 16, 95, 34, 66, 37, 12, 50]\n",
      "Reward = 47.0\n",
      "Input state = [36, 59, 0, 47]\n",
      "Episode = [36, 59, 0, 47, 95, 34, 12, 37, 50, 13]\n",
      "Reward = 49.0\n",
      "Input state = [73, 3, 89, 69]\n",
      "Episode = [73, 3, 89, 69, 95, 34, 66, 12, 37, 13]\n",
      "Reward = 53.0\n",
      "Input state = [89, 84, 32, 70]\n",
      "Episode = [89, 84, 32, 70, 34, 95, 12, 37, 66, 13]\n",
      "Reward = 53.0\n",
      "Input state = [54, 94, 0, 34]\n",
      "Episode = [54, 94, 0, 34, 95, 12, 37, 13, 50, 66]\n",
      "Reward = 47.0\n",
      "Input state = [42, 14, 46, 37]\n",
      "Episode = [42, 14, 46, 37, 95, 34, 66, 12, 50, 13]\n",
      "Reward = 43.0\n",
      "Input state = [98, 31, 90, 11]\n",
      "Episode = [98, 31, 90, 11, 95, 12, 34, 66, 37, 13]\n",
      "Reward = 48.0\n",
      "Input state = [57, 7, 69, 93]\n",
      "Episode = [57, 7, 69, 93, 95, 34, 66, 50, 12, 37]\n",
      "Reward = 51.0\n",
      "Input state = [60, 63, 91, 20]\n",
      "Episode = [60, 63, 91, 20, 95, 34, 13, 50, 12, 37]\n",
      "Reward = 49.0\n",
      "Input state = [68, 93, 23, 89]\n",
      "Episode = [68, 93, 23, 89, 95, 12, 34, 66, 50, 13]\n",
      "Reward = 49.0\n",
      "Input state = [27, 7, 96, 86]\n",
      "Episode = [27, 7, 96, 86, 95, 34, 12, 37, 66, 50]\n",
      "Reward = 53.0\n",
      "Input state = [78, 21, 79, 24]\n",
      "Episode = [78, 21, 79, 24, 95, 34, 66, 50, 12, 37]\n",
      "Reward = 49.0\n",
      "Input state = [46, 1, 2, 92]\n",
      "Episode = [46, 1, 2, 92, 95, 34, 66, 37, 12, 50]\n",
      "Reward = 44.0\n",
      "Input state = [38, 54, 83, 7]\n",
      "Episode = [38, 54, 83, 7, 95, 34, 12, 37, 66, 50]\n",
      "Reward = 57.0\n",
      "Input state = [43, 85, 19, 96]\n",
      "Episode = [43, 85, 19, 96, 95, 34, 50, 12, 37, 66]\n",
      "Reward = 51.0\n",
      "Input state = [37, 65, 17, 6]\n",
      "Episode = [37, 65, 17, 6, 95, 34, 66, 12, 50, 13]\n",
      "Reward = 52.0\n",
      "Input state = [59, 4, 76, 82]\n",
      "Episode = [59, 4, 76, 82, 95, 34, 66, 12, 37, 50]\n",
      "Reward = 49.0\n",
      "Input state = [62, 13, 81, 58]\n",
      "Episode = [62, 13, 81, 58, 95, 34, 12, 37, 50, 66]\n",
      "Reward = 46.0\n",
      "Input state = [37, 19, 41, 62]\n",
      "Episode = [37, 19, 41, 62, 95, 12, 34, 66, 50, 13]\n",
      "Reward = 49.0\n",
      "Input state = [38, 91, 60, 16]\n",
      "Episode = [38, 91, 60, 16, 95, 34, 12, 37, 66, 50]\n",
      "Reward = 50.0\n",
      "Input state = [5, 65, 46, 35]\n",
      "Episode = [5, 65, 46, 35, 95, 34, 66, 37, 12, 50]\n",
      "Reward = 51.0\n",
      "Input state = [88, 69, 1, 70]\n",
      "Episode = [88, 69, 1, 70, 95, 34, 12, 37, 66, 50]\n",
      "Reward = 50.0\n",
      "Input state = [32, 59, 38, 92]\n",
      "Episode = [32, 59, 38, 92, 95, 34, 66, 50, 12, 37]\n",
      "Reward = 52.0\n",
      "Input state = [60, 80, 34, 0]\n",
      "Episode = [60, 80, 34, 0, 95, 12, 37, 13, 50, 66]\n",
      "Reward = 47.0\n",
      "Input state = [89, 90, 49, 4]\n",
      "Episode = [89, 90, 49, 4, 95, 34, 66, 12, 37, 13]\n",
      "Reward = 48.0\n",
      "Input state = [55, 18, 81, 88]\n",
      "Episode = [55, 18, 81, 88, 95, 34, 50, 12, 37, 66]\n",
      "Reward = 50.0\n",
      "Input state = [23, 91, 79, 45]\n",
      "Episode = [23, 91, 79, 45, 95, 34, 66, 37, 12, 88]\n",
      "Reward = 50.0\n",
      "Input state = [19, 82, 54, 42]\n",
      "Episode = [19, 82, 54, 42, 95, 34, 66, 50, 12, 37]\n",
      "Reward = 52.0\n",
      "Input state = [51, 54, 9, 6]\n",
      "Episode = [51, 54, 9, 6, 95, 34, 66, 50, 12, 37]\n",
      "Reward = 51.0\n",
      "Input state = [67, 18, 97, 2]\n",
      "Episode = [67, 18, 97, 2, 95, 34, 66, 50, 12, 37]\n",
      "Reward = 48.0\n",
      "Input state = [1, 62, 68, 12]\n",
      "Episode = [1, 62, 68, 12, 95, 34, 66, 37, 13, 50]\n",
      "Reward = 44.0\n",
      "Input state = [12, 0, 22, 32]\n",
      "Episode = [12, 0, 22, 32, 95, 34, 66, 37, 13, 50]\n",
      "Reward = 46.0\n",
      "Input state = [41, 48, 6, 81]\n",
      "Episode = [41, 48, 6, 81, 95, 34, 66, 37, 12, 50]\n",
      "Reward = 50.0\n",
      "Input state = [37, 47, 82, 10]\n",
      "Episode = [37, 47, 82, 10, 95, 34, 12, 50, 66, 13]\n",
      "Reward = 45.0\n",
      "Input state = [96, 45, 38, 20]\n",
      "Episode = [96, 45, 38, 20, 95, 34, 13, 50, 12, 37]\n",
      "Reward = 54.0\n",
      "Input state = [67, 94, 30, 16]\n",
      "Episode = [67, 94, 30, 16, 95, 34, 66, 37, 12, 50]\n",
      "Reward = 51.0\n",
      "Input state = [91, 34, 33, 45]\n",
      "Episode = [91, 34, 33, 45, 95, 13, 66, 37, 12, 50]\n",
      "Reward = 49.0\n",
      "Input state = [11, 92, 45, 52]\n",
      "Episode = [11, 92, 45, 52, 95, 34, 66, 12, 37, 13]\n",
      "Reward = 50.0\n",
      "Input state = [80, 19, 4, 88]\n",
      "Episode = [80, 19, 4, 88, 95, 34, 12, 37, 66, 50]\n",
      "Reward = 45.0\n",
      "Input state = [68, 4, 27, 2]\n",
      "Episode = [68, 4, 27, 2, 95, 34, 66, 12, 37, 13]\n",
      "Reward = 44.0\n",
      "Input state = [19, 23, 32, 90]\n",
      "Episode = [19, 23, 32, 90, 95, 34, 66, 50, 12, 37]\n",
      "Reward = 48.0\n",
      "Input state = [73, 3, 90, 67]\n",
      "Episode = [73, 3, 90, 67, 95, 34, 13, 66, 37, 12]\n",
      "Reward = 51.0\n",
      "Input state = [56, 70, 99, 15]\n",
      "Episode = [56, 70, 99, 15, 95, 34, 13, 37, 12, 50]\n",
      "Reward = 52.0\n",
      "Input state = [29, 48, 9, 31]\n",
      "Episode = [29, 48, 9, 31, 95, 34, 66, 50, 12, 37]\n",
      "Reward = 49.0\n",
      "Input state = [75, 60, 97, 68]\n",
      "Episode = [75, 60, 97, 68, 95, 34, 12, 50, 13, 37]\n",
      "Reward = 45.0\n",
      "Input state = [2, 99, 51, 87]\n",
      "Episode = [2, 99, 51, 87, 95, 34, 50, 12, 37, 66]\n",
      "Reward = 46.0\n",
      "Input state = [61, 79, 46, 28]\n",
      "Episode = [61, 79, 46, 28, 95, 34, 66, 37, 12, 50]\n",
      "Reward = 48.0\n",
      "Input state = [27, 21, 86, 53]\n",
      "Episode = [27, 21, 86, 53, 95, 34, 66, 50, 12, 37]\n",
      "Reward = 51.0\n",
      "Input state = [35, 56, 39, 78]\n",
      "Episode = [35, 56, 39, 78, 95, 34, 66, 37, 12, 50]\n",
      "Reward = 53.0\n",
      "Input state = [96, 55, 99, 88]\n",
      "Episode = [96, 55, 99, 88, 95, 34, 12, 37, 50, 13]\n",
      "Reward = 46.0\n",
      "Input state = [76, 65, 91, 10]\n",
      "Episode = [76, 65, 91, 10, 95, 34, 12, 37, 66, 50]\n",
      "Reward = 52.0\n",
      "Input state = [82, 92, 38, 14]\n",
      "Episode = [82, 92, 38, 14, 95, 34, 66, 12, 37, 50]\n",
      "Reward = 49.0\n",
      "Input state = [2, 59, 30, 82]\n",
      "Episode = [2, 59, 30, 82, 95, 12, 34, 66, 50, 13]\n",
      "Reward = 50.0\n",
      "Input state = [44, 76, 37, 74]\n",
      "Episode = [44, 76, 37, 74, 95, 34, 12, 50, 13, 66]\n",
      "Reward = 47.0\n",
      "Input state = [33, 37, 40, 86]\n",
      "Episode = [33, 37, 40, 86, 95, 34, 12, 50, 13, 66]\n",
      "Reward = 47.0\n",
      "Input state = [41, 41, 89, 56]\n",
      "Episode = [41, 41, 89, 56, 95, 12, 34, 66, 37, 13]\n",
      "Reward = 51.0\n",
      "Input state = [26, 63, 45, 63]\n",
      "Episode = [26, 63, 45, 63, 95, 34, 66, 50, 12, 37]\n",
      "Reward = 54.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "wandb.init()\n",
    "nitems = len(ru)\n",
    "t=0\n",
    "dist = D.flatten()\n",
    "print(\"Greedy Algo\")\n",
    "idx=0\n",
    "p=0\n",
    "reward=[]\n",
    "for v in range(0,100):\n",
    "    t=0\n",
    "    if(j<0):\n",
    "        idx=0\n",
    "        state=[b[j]]\n",
    "        print(\"Input state =\", state)\n",
    "    else:\n",
    "        state=[b[4*v], b[4*v+1], b[4*v+2], b[4*v+3]]\n",
    "        idx=3\n",
    "        print(\"Input state =\", state)\n",
    "    for k in range(idx,9):\n",
    "        a=[0]*100\n",
    "        for j in range(0, 100):\n",
    "            # if j in state:\n",
    "            #     a[j]=-100000\n",
    "            # else:\n",
    "            #     a[j]=getReward1(ru, D, state, j)\n",
    "            a[j]=ru[j][0]\n",
    "            # k=len(state)\n",
    "            # if j in state:\n",
    "            #     a[j]+=-10\n",
    "            # else:\n",
    "            #     a[j]+=dist[(state[k-1])*nitems + j]\n",
    "            for i in range(0, len(state)):\n",
    "                if j in state:\n",
    "                    a[j]+=-20\n",
    "                else:\n",
    "                    a[j]+= (1/((len(state)-i)+1)) * dist[(state[i])*nitems + j]\n",
    "        \n",
    "        max_v=max(a)\n",
    "        \n",
    "        index = a.index(max_v)\n",
    "        t+=max_v\n",
    "        t=np.ceil(t)\n",
    "        # state[idx]=index\n",
    "        # idx+=1\n",
    "        state.append(index)\n",
    "    print(\"Episode =\", state)\n",
    "    print(\"Reward =\",t)\n",
    "    reward.append(t)\n",
    "xx=[i for i in range(0,100)]\n",
    "\n",
    "data = [[x, y] for (x, y) in zip(xx, reward)]\n",
    "data = [[x, y] for (x, y) in zip(xx, reward)]\n",
    "table = wandb.Table(data=data, columns = [\"episode comp\", \"total reward\"])\n",
    "wandb.log({\"custom_plot\" : wandb.plot.line(table, \"episode comp\",\"total reward\",\n",
    "           title=\"Greedy vs rl\")})\n",
    "# reward_dict={\"episode_reward\":reward}\n",
    "# reward_df=pd.DataFrame(reward_dict)\n",
    "# reward_table=wandb.Table(data=reward_df, columns=[\"rewards\"])\n",
    "# wandb.log({'reward_per_session':wandb.plot.line(reward_table, xx, reward)})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 100)\n",
      "100\n",
      "Testing for 10 episodes ...\n",
      "[98 34 47 97 88  2  2  2  2  2]\n",
      "Episode 1: reward: -19.000, steps: 9\n",
      "[74 95 37 34  5 97 48 50  1  1]\n",
      "Episode 2: reward: 27.000, steps: 9\n",
      "[58 34 13  2  2  2 13 88 13 50]\n",
      "Episode 3: reward: -16.000, steps: 9\n",
      "[ 7 12 37 95 34 13 75 50 50 12]\n",
      "Episode 4: reward: 14.000, steps: 9\n",
      "[90 34 47 97 88  2  2  2  2  2]\n",
      "Episode 5: reward: -19.000, steps: 9\n",
      "[59 34 13  2  2  2 13 88 13 50]\n",
      "Episode 6: reward: -16.000, steps: 9\n",
      "[38 13 37 95 37 13 50 50 50 12]\n",
      "Episode 7: reward: -15.000, steps: 9\n",
      "[ 2 34 37 95 34 13 13 50 50 12]\n",
      "Episode 8: reward: -1.000, steps: 9\n",
      "[59 34 13  2  2  2 13 88 13 50]\n",
      "Episode 9: reward: -16.000, steps: 9\n",
      "[78 95 37  2  2  2  2 13 90 50]\n",
      "Episode 10: reward: 1.000, steps: 9\n",
      "-6.0\n"
     ]
    }
   ],
   "source": [
    "print(model.output_shape)\n",
    "print(actions)\n",
    "results = dqn.test(env, nb_episodes=10, visualize=False)\n",
    "print(np.mean(results.history['episode_reward'])) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 1000 # number of episodes to run\n",
    "BATCH_SIZE = 32 # batch size for the replay buffer\n",
    "MEMORY_SIZE = 10000 # size of the replay buffer\n",
    "GAMMA = 0.95 # discount factor for future rewards\n",
    "EPSILON_MAX = 1.0 # maximum exploration rate\n",
    "EPSILON_MIN = 0.01 # minimum exploration rate\n",
    "EPSILON_DECAY = 0.995 # decay rate for exploration rate\n",
    "LEARNING_RATE = 0.001 # learning rate for the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
    "        self.epsilon = EPSILON_MAX\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = self._build_model().to(self.device)\n",
    "        self.target_model = self._build_model().to(self.device)\n",
    "    \n",
    "    def _build_model(self):\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(self.state_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, self.action_size)\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.model(state)\n",
    "        self.model.train()\n",
    "        return np.argmax(action_values.cpu().data.numpy())\n",
    "    \n",
    "    def replay(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        minibatch = random.sample\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, states, actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(states[1], 24)\n",
    "        self.fc2 = nn.Linear(24, 24)\n",
    "        self.fc3 = nn.Linear(24, 24)\n",
    "        self.fc4 = nn.Linear(24, actions)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "    # def __init__(self, state_size, action_size):\n",
    "    #     super(DQN, self).__init__()\n",
    "    #     self.fc1 = nn.Linear(state_size[1], 32)\n",
    "    #     self.fc2 = nn.Linear(32, 64)\n",
    "    #     self.fc3 = nn.Linear(64, action_size)\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     x = torch.relu(self.fc1(x))\n",
    "    #     x = torch.relu(self.fc2(x))\n",
    "    #     x = self.fc3(x)\n",
    "    #     return x\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, state_size, action_size, gamma, epsilon, epsilon_min, epsilon_decay, learning_rate, memory_size, batch_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.learning_rate = learning_rate\n",
    "        self.memory_size = memory_size\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.model = DQN(state_size, action_size)\n",
    "        self.target_model = DQN(state_size, action_size)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.memory = []\n",
    "        self.steps = 0\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(self.action_size)\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model(state)\n",
    "            return torch.argmax(q_values).item()\n",
    "    \n",
    "    def q_values(self,state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model(state)\n",
    "            return q_values\n",
    "\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if len(self.memory) > self.memory_size:\n",
    "            self.memory.pop(0)\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        samples = np.random.choice(len(self.memory), self.batch_size, replace=False)\n",
    "        states, actions, rewards, next_states, dones = zip(*[self.memory[i] for i in samples])\n",
    "\n",
    "        states = torch.from_numpy(np.vstack(states)).float()\n",
    "        actions = torch.from_numpy(np.vstack(actions)).long()\n",
    "        rewards = torch.from_numpy(np.vstack(rewards)).float()\n",
    "        next_states = torch.from_numpy(np.vstack(next_states)).float()\n",
    "        dones = torch.from_numpy(np.vstack(dones).astype(np.uint8)).float()\n",
    "\n",
    "        q_values = self.model(states).gather(1, actions)\n",
    "        next_q_values = self.target_model(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        targets = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "\n",
    "        loss = nn.MSELoss()(q_values, targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.steps += 1\n",
    "        if self.steps % 10 == 0:\n",
    "            self.update_target_model()\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "# Example usage\n",
    "env = CustomEnv1()\n",
    "agent = Agent(state_size=(1,10), action_size=100, gamma=0.0, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995, learning_rate=0.001, memory_size=10000, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31 42 52 18 66 17  4 56 61 59]\n",
      "[90 71 63 66 45 27 36 75 70 13]\n",
      "[92 21 90 69 26 13 14  8 38 37]\n",
      "[90 66 37 74 80 25 41 79 40 86]\n",
      "[94 25 60 35 18 78 24 80 26 46]\n",
      "[33 34 47 67 66 42  8 77  2  7]\n",
      "[36 24 84 90 21 13 77 54 95 77]\n",
      "[16 98 47 91 69 45 90  0 56 19]\n",
      "[39 42 68  2 17 75  7 24 80 54]\n",
      "[15 86 63 25 14  3 16 12 37 96]\n",
      "[62  4 21 60 97 37 48 37 13 51]\n",
      "[80 46 77 97 43 37 21 45 91 33]\n",
      "[11 39 17 37 31  6 54 13 69 37]\n",
      "[49 80  2 54 87 40 75 34 13 68]\n",
      "[13 55  2  2  2 21 37 95 23 59]\n",
      "[67 96 82 61 21 34 34 34 48  1]\n",
      "[48 92  6 13 76 37 99 37 36 93]\n",
      "[10 62 24 13 59 19 13 10 11  1]\n",
      "[87 62 37 99 13 16 13 13 13 51]\n",
      "[27 52 23  7 30 48 17 56 18 46]\n",
      "[ 0 24  8 23 44 42 56 11 56 24]\n",
      "[70 48 89 55  1 29  1  1 18 95]\n",
      "[ 8 48 22 56 95  4  4 36 52 93]\n",
      "[30 77 10 36  9 95 63 39 94 35]\n",
      "[64 46 23 23 23 23 23 96 74 46]\n",
      "[69 46 23 23 95 97 45 82 24 46]\n",
      "[79 95 97 87 37 37 32 37 87  7]\n",
      "[60 95 33 87 95  4  4  4  4 76]\n",
      "[31 95 33 97 33 33 14 75 19 19]\n",
      "[68 19 58 14 14 60 97 67  4 14]\n",
      "[80 93 97 14 92 75 89 14 14 14]\n",
      "[56 31 11 97 42 94 97 13  1 13]\n",
      "[75 97 84 81 42 16 97  6  6  6]\n",
      "[12 31 94 90 95 66 18 18 18 18]\n",
      "[96 31 97  8 83 77 77 70 77 52]\n",
      "[90 97 97  8  8  8 89 82 22 82]\n",
      "[63 97 94 59 87 29 17 17 17 17]\n",
      "[16 95 94 60 95 91 31 52 31 31]\n",
      "[61 31 65 82 95 19 19 90 59 59]\n",
      "[96 42 42  9 42 98 82 61 98 40]\n",
      "[57 62 94 82 42 90 90 79 52 75]\n",
      "[22 75 94 82 42 87 63 82 52 75]\n",
      "[60 62 94 82 89 90 52 56 52 82]\n",
      "[45 40 75 42 89 62 52 52 28 52]\n",
      "[76 95 30 25 94 24 32 24 15 24]\n",
      "[37 30 75 87 95 19 90 63 76 95]\n",
      "[82 63 75 66 89 90 56 63 41 75]\n",
      "[38 72 30 89 42 89 98 63 61 40]\n",
      "[86  2 94 66 89 89 67 63 41 67]\n",
      "[57 13 97 66 60 51 90 67 67 66]\n",
      "[46 66 94 66 89 19 19 19 66 38]\n",
      "[36 95 94 27 27  1  1  1 73 89]\n",
      "[98 20 97 27 89  1  1 47  1  1]\n",
      "[12 13 75 27 27 27 90 42 41 42]\n",
      "[29 13 97 27 89  3  3  3  3  3]\n",
      "[91 60 97 89 89 89  1  1 62 97]\n",
      "[84 13 97 89 83 90 90 97 34 13]\n",
      "[90 40 97 89 89  5 90 63 63 13]\n",
      "[39 41 75 90 95 90 63 28 37 63]\n",
      "[ 5 13 90 95 95 90 63 63 34 41]\n",
      "[22 95 90 60 95 63 28 28 41 38]\n",
      "[75 28 94 90 89 90 28 28 28 90]\n",
      "[92 94 94 94 89 89 94 28 28 28]\n",
      "[51 13 83  9 33 94 17 56 41 60]\n",
      "[58 13 75  9 58 60 60 41 41 41]\n",
      "[84 60 75 89 89 89 21 21 21 94]\n",
      "[20 21 75 48 89  8  8  8  8  8]\n",
      "[99 40 97 82 89 89  8  8  8 94]\n",
      "[64 13 97 82 89 89 21 99 13 13]\n",
      "[79 48 97 82 89 89 21 21 47 82]\n",
      "[99 97 75 24 48 21 40 40 40 97]\n",
      "[26 30 97 95 63 47 69 83 42 42]\n",
      "[31 95 95 60 95 83 62 62 62 83]\n",
      "[81  2 97 21 89 60  2 33 60  2]\n",
      "[51 13 97 48 89 69  2  2  2  2]\n",
      "[99 34 97 89 89 89 21 21  5  5]\n",
      "[ 0 34 95 12 12 95 56 87 37 37]\n",
      "[66 47 75 48 48 89 60 20 20 25]\n",
      "[86 94 75 94 48  5  5 89 75 75]\n",
      "[93 30 97 48 48 48 82 20 20 47]\n",
      "[19 13 97 95 63 69 83 20 83 83]\n",
      "[98 40 97 47 48 47 47 47 47 82]\n",
      "[66 13 97 47 63 47 47 47 47 47]\n",
      "[57 13 97 47 63 47 47 47 32 47]\n",
      "[94 40 97 47 48 47 47 47 47 97]\n",
      "[31 13 97 95 63 63 63 86 70 98]\n",
      "[92 37 97 82 86 82 47 74 40 97]\n",
      "[42 13 97 86 63 63 86 74 32 40]\n",
      "[15 34 52 48 63 86 32 87 34 60]\n",
      "[10 22 15 15 26 25 25 87 18 18]\n",
      "[86 13 97 97 86 86 47 74 32 40]\n",
      "[ 6 34 52 15 15 15 40 74 52 52]\n",
      "[15 34 52 15 15 15 40 74 30 30]\n",
      "[69 97 97 97 86 32 74 74 74 13]\n",
      "[ 3 51 46 16 76 32 32 32 32 32]\n",
      "[30 34 97 32 76 82 74 74 32 32]\n",
      "[25 42 90 97 63 32 40 34 40 40]\n",
      "[90 40 97 65 76 82 40 74 40 97]\n",
      "[64 37 97 82 63 82 40 74 40 40]\n",
      "[92 40 97 82 76 82 82 34 34 34]\n",
      "[18 98 13 13 13 13 13 40 14 13]\n",
      "[10 98 13 13 72 60 54 34 34 40]\n",
      "[14 13 97 76 63 76 82 34 34 34]\n",
      "[38 98 74 76 76 82 12 47 56 82]\n",
      "[34 34 97 82 63 76 82 87 34 34]\n",
      "[99 30 97 82 86 82 82 47 87 13]\n",
      "[64 13 97 48 21 21 48 55 63 98]\n",
      "[44 74 74 48 69 82 75 87 56 60]\n",
      "[29 13 97 48 10 48 48 47 63 98]\n",
      "[ 6 56 46 16 95 75 75 87 60 13]\n",
      "[ 2 98 95 16 16 16 16 47 37 89]\n",
      "[19 31 97 97 63 69 75 87 60 60]\n",
      "[40 47 97 97 63 55 75 87 32 32]\n",
      "[32 60 59 97 63 55 47 87 60 13]\n",
      "[26 37 97 69 63 13 13 13 13 13]\n",
      "[46 13 97 48 63 13 13 13 13 13]\n",
      "[ 6 28 95 69 63 42 93 87 32 40]\n",
      "[93 79 97 47 47 47 47 47 47 47]\n",
      "[58 79 97 47 63 47 47 53 56 48]\n",
      "[87 24 97 47 63 47 47 47 47 48]\n",
      "[30 13 97 63 63 33 33 56 32 32]\n",
      "[87 42 97 47 63 47 47 47 47 48]\n",
      "[97 32 97 47 63 47 47 47 47 48]\n",
      "[57 35 97 63 63 32 32 55 32 32]\n",
      "[85 79 97 47 32 47 47 87 63 48]\n",
      "[43 41 97 63 63 32 45 87 32 42]\n",
      "[54 31 97 63 63 45 45 30 60 48]\n",
      "[31 46 97 63 63 45 47 87 32 42]\n",
      "[18 98 74 56 63 47 47 87 56 46]\n",
      "[79 87 97 52 63 47 47 87 42 48]\n",
      "[99 31 97 48 45 48 47 47 47 48]\n",
      "[67 74 97 48 48 47 47 47 56 48]\n",
      "[14 35 74 69 63 45 75 87 40 31]\n",
      "[48 98 74 48 45 47 47 87 30 37]\n",
      "[15 13 97 63 74 45 45 87 74 53]\n",
      "[88 56 97 97 63 47 47  1  1  1]\n",
      "[36 47 97 69 63 55 47 87 74 53]\n",
      "[96 79 97 48 52 47 47 47 52 27]\n",
      "[99 41 47 48 32 45 47 47 47 48]\n",
      "[95 41 47 48 32 45 47 87 97 31]\n",
      "[45 37 97 63 63 33 93 87 30 88]\n",
      "[ 8 13 75 63 63 33 99 87 40 31]\n",
      "[86 48 97 48 63 93 47 87 16 16]\n",
      "[17 13 97 63 63 33 32 16 33 16]\n",
      "[30 75 32 52 32 45 41 87 74 40]\n",
      "[31 13 48 63 63 34 34 34 41 31]\n",
      "[77 12 48 48 45 45 62 87 40 40]\n",
      "[45 13 48 48 63 93 41 87 88 13]\n",
      "[78 79 97 86 63 62 47 87 41 48]\n",
      "[49 79 32 97 42 32 41 41 41 31]\n",
      "[96 87 32 48 32 45 62 41 41 32]\n",
      "[36 31 48 48 63 41 41 87 40 88]\n",
      "[19 74 74 30 63 93 87 87 56 56]\n",
      "[98  2 48 48 45 45 62 87 40 40]\n",
      "[17 98 74 52 30 30 59 87 74 74]\n",
      "[85 12 48 48 45 45 62 87 74 74]\n",
      "[81 42 48 48 45 45 62 87 88 88]\n",
      "[57 88 97 48 45 93 67 30 87 67]\n",
      "[ 5 88 46 46 22 59 67 30 66 47]\n",
      "[51 37 97 63 39 63 32 30 11 11]\n",
      "[35 40 32 48 32 32 59 30 66 47]\n",
      "[30 37 32 48 63 93 11 30 88 47]\n",
      "[24 79 74 89 63 59 30 30 30 11]\n",
      "[63 47 58 48 63 93 30 30 30 60]\n",
      "[85 12 48 48 63 45 62 30 34 30]\n",
      "[77 30 97 63 63 63 93 30 87 79]\n",
      "[34 40 12 48 42 62 66 66 88 40]\n",
      "[82  6 48 48 63 93 62 66 88 80]\n",
      "[15 40 12 59 42 99 66 66 58 40]\n",
      "[19 37 12 83 98 99 11 42 47 47]\n",
      "[82 12 48 48 45 62 62 34 34 88]\n",
      "[98 88 83 48 83 62 62 59 88 27]\n",
      "[73 87 83 48 62 99 62 66 88 97]\n",
      "[90 47 97 48 45 62 62 30 59 76]\n",
      "[ 3 13 59 59 90 99 88 87 88 47]\n",
      "[96 74 97 27 45 27 27 62 97 48]\n",
      "[98  6 48 48 45 45 62 34 57  6]\n",
      "[14 47 37 59 32 62 66 19 66 55]\n",
      "[44 35 97 45 63 93 88 88 88 79]\n",
      "[22 95 95 89 63 45 59 88 74 94]\n",
      "[95 79 97 48 45 45 45 59 88 48]\n",
      "[ 7 79 37 59 37 59 88 66 74 58]\n",
      "[31 79 12 12 83 88 88 88 88 40]\n",
      "[91 31 97 45 45 45 45 59 88 48]\n",
      "[62 97 97 48 45 45 59 88 56 37]\n",
      "[49  6 97 63 63 45 93 88 74 74]\n",
      "[20 10 97 63 63 93 88 88 74 40]\n",
      "[81 80 97 48 45 45 45 88 63 89]\n",
      "[70 97 97 89 45 45 45 88 74 31]\n",
      "[64 83 97 89 32 45 45 88 74 31]\n",
      "[91 31 97 89 32 32 59 76 88 47]\n",
      "[38 13 97 59 63 93 88 88 88 47]\n",
      "[80 37 97 89 32 59 87 66 88 94]\n",
      "[25 79 97 89 63 59 88 66 88 47]\n",
      "[40 28 97 59 63 93 88 66 88 79]\n",
      "[52 88 97 89 42 59 62 66 88 47]\n",
      "[83 88 97 27 76 27 27 59 59 89]\n",
      "[65 13 97 59 59 59 59 59 88 98]\n",
      "[31 80 59 89 42 59 59 88 74 74]\n",
      "[68 12 31 48 32 59 62 88 74 74]\n",
      "[91 37 97 59 59 59 59 59 88 48]\n",
      "[77 24 97 89 59 59 76 76 58 58]\n",
      "[20 24 97 59 90 93 88 66 66 79]\n",
      "[98 74 48 27 48 76 47 66 97 97]\n",
      "[80 31 48 48 32 76 62 88 88 37]\n",
      "[82 37 48 27 76 89 47 66 88 97]\n",
      "[74 78 27 27 48 76 47 66 97 97]\n",
      "[58 13 48 48 32 45 76 88 58 74]\n",
      "[80 31 48 48 32 76 62 88 88 37]\n",
      "[24 86 75 89 22 22 22 76 74 74]\n",
      "[84 48 48 48 76 76 62 66 88 13]\n",
      "[31 66 12 97 42 83 88 88 88 75]\n",
      "[15 47 75 59 90 93 88 66 66 79]\n",
      "[26 47 97 59 63 93 59 88 74 47]\n",
      "[31 47 97 59 63 93 59 88 88 47]\n",
      "[88 62 48 27 76 62 59 88 88 97]\n",
      "[74 31 48 48 59 76 62 88 88 98]\n",
      "[29 88 97 89 32 90 88 88 74 58]\n",
      "[52 13 48 48 32 59 62 88 58 94]\n",
      "[ 8 37 75 59 37 32 88 88 58 20]\n",
      "[95 79 48 27 48 48 27 59 97 97]\n",
      "[28  1 48 59 32 37 88 88 58 94]\n",
      "[12 88 75 89 32 32 59 88 74 94]\n",
      "[67 37 48 48 32 59 59 88 58 75]\n",
      "[ 4 13 75 59 37 32 88 88 58 20]\n",
      "[22 66 75 59 32 59 88 88 74 58]\n",
      "[67 88 45 48 45 59 59 88 34 97]\n",
      "[ 7 78 75 47 45 59 59 88 74 20]\n",
      "[36 13 48 59 45 45 34 88 58 20]\n",
      "[17 47 97 59 45 45 59 59 88 47]\n",
      "[69 13 48 48 57 59 62 88 88 98]\n",
      "[ 7 53 75 59 45 45 59 88 74 47]\n",
      "[23 13 48 59 32 37 88 88 58 58]\n",
      "[94 12 48 48 57 59 62 88 58 62]\n",
      "[95 30 48 27 76 45 45 59 47 27]\n",
      "[47 66 45 27 86 27 47 79 97 97]\n",
      "[12 89 75 89 45 90 88 88 74 58]\n",
      "[ 0 79 75 90 37 90 88 88 74 58]\n",
      "[15 98 75 89 45 59 88 88 58 75]\n",
      "[81 13 48 27 45 76 62 88 58 75]\n",
      "[64 37 45 27 86 62 59 88 47 97]\n",
      "[70 13 45 27 45 59 62 74 60 89]\n",
      "[55 13 45 45 32 59 76 88 74 75]\n",
      "[65 79 45 27 86 27 47 79 97 97]\n",
      "[98 31 45 27 48 76 47 88 97 97]\n",
      "[33 88 95 86 45 75 22 88 74 57]\n",
      "[79 31 45 27 57 45 62 88 97 89]\n",
      "[50 88 47 27 45 45 59 88 37 37]\n",
      "[63 61 57 27 86 62 47 66 88 27]\n",
      "[63 13 31 27 57 45 62 34 34 34]\n",
      "[54 47 45 27 86 45 47 66 84 97]\n",
      "[66 46 45 27 86 45 47 66 88 97]\n",
      "[17 31 75 59 74 93 88 66 66 61]\n",
      "[20 88 95 45 45 45 59 66 88 97]\n",
      "[78 93 45 27 45 45 47 74 59 44]\n",
      "[45 57 45 27 45 45 59 74 58 89]\n",
      "[45 88 47 27 86 47 59 66 66 13]\n",
      "[96 12 31 27 32 62 62 74 40 31]\n",
      "[27 13 27 59 32 32 34 74 94 31]\n",
      "[43 66 47 27 86 40 59 66 66 97]\n",
      "[90 63 12 27 57 62 47 74 40 75]\n",
      "[11 20 75 59 74 88 88 74 74 74]\n",
      "[10 98 75 59 57 59 88 74 74 74]\n",
      "[97 88 62 27 86 27 24 59 27 27]\n",
      "[53  3 48 59 57 88 88 66 88 94]\n",
      "[99 12 27 27 48 62 47 74 40 75]\n",
      "[38 47 40 27 86 40 47 59 13 13]\n",
      "[86 40 47 27 86 47 47 59 47 13]\n",
      "[98 34 47 27 48 48 47 74 97 30]\n",
      "[98 30 47 48 32 86 47 74 47 40]\n",
      "[ 5 20 47 59 95 88 88 61 47 31]\n",
      "[19 58 47 86 57 88 88 70 74 95]\n",
      "[55 98 47 40 47 59 47 88 34 37]\n",
      "[59 12 31 48 57 59 34 74 58 75]\n",
      "[76 13 31 48 32 57 62 74 37 75]\n",
      "[84 46 57 48 57 59 59 74 60 37]\n",
      "[98 60 30 48 32 57 62 88 34 57]\n",
      "[41 31 57 59 57 45 35 88 74 75]\n",
      "[27 88 47 47 47 47 47 88 75 63]\n",
      "[22 37 47 59 32 88 88 74 74 75]\n",
      "[41 63 47 89 32 45 88 88 75 75]\n",
      "[53 13 48 89 32 37 88 74 74 75]\n",
      "[77 53 30 48 32 59 47 74 40 75]\n",
      "[78 59 30 45 32 59 47 88 75 75]\n",
      "[85 30 47 48 32 59 59 74 58 75]\n",
      "[27 74 47 59 45 59 88 74 74 95]\n",
      "[24 97 47 47 47 59 88 74 74 32]\n",
      "[30 58 47 59 32 59 88 74 74 34]\n",
      "[57 57 45 48 48 59 59 88 74 34]\n",
      "[67 47 48 48 32 59 59 88 58 75]\n",
      "[20 37 47 59 32 74 88 74 74 95]\n",
      "[ 4  1 75 47 47 88 88 88 74 69]\n",
      "[34 13 48 59 74 88 88 74 34 94]\n",
      "[39 95 47 45 45 59 59 88 34 34]\n",
      "[89 89 45 48 48 30 59 76 59 62]\n",
      "[81 95 45 48 48 62 59 74 34 40]\n",
      "[37 88 47 59 45 59 59 88 34 34]\n",
      "[33 89 47 59 45 45 59 88 34 34]\n",
      "[49 37 48 48 32 59 59 74 74 34]\n",
      "[78  2 48 48 32 45 59 74 74 34]\n",
      "[78  2 48 48 32 59 59 74 74 34]\n",
      "[67 88 45 48 48 59 59 74 59 89]\n",
      "[74 40 48 48 32 59 59 74 74 34]\n",
      "[91 35 48 48 32 59 59 74 40 40]\n",
      "[49 88 47 89 45 45 45 57 62 47]\n",
      "[92 60 48 48 32 30 59 57 59 30]\n",
      "[44 86 47 59 48 59 59 88 34 34]\n",
      "[17 79 47 47 47 59 59 88 74 34]\n",
      "[39 88 47 59 48 59 59 88 34 34]\n",
      "[ 2 95 48 47 47 45 59 88 34 34]\n",
      "[76 58 48 48 48 59 59 57 88 27]\n",
      "[88 13 48 48 32 59 59 74 74 34]\n",
      "[74 63 48 48 48 59 59 57 88 95]\n",
      "[67 58 48 48 48 59 59 57 88 95]\n",
      "[87 47 48 48 32 59 59 57 88 27]\n",
      "[54  2 48 32 32 59 57 57 74 75]\n",
      "[41 57 47 59 57 59 57 57 74 95]\n",
      "[16 20 47 57 74 88 88 74 74 95]\n",
      "[49 20 48 89 74 88 88 57 74 95]\n",
      "[32 97 47 59 45 59 57 57 74  1]\n",
      "[49 46 48 96 32 45 57 57 47 31]\n",
      "[13 60 47 89 45 45 88 57 58 95]\n",
      "[48 46 45 48 32 59 57 57 74 37]\n",
      "[50 13 48 32 32 57 57 57 74 95]\n",
      "[91 31 62 48 32 57 57 57 88 62]\n",
      "[15 95 47 47 47 59 88 57 74 22]\n",
      "[ 5 95 47 47 47 45 88 57 74 95]\n",
      "[49 12 48 57 32 57 57 57 74 95]\n",
      "[19 46 47 59 32 88 88 88 74 95]\n",
      "[32 95 47 47 48 59 57 88 58 95]\n",
      "[29 95 47 47 45 59 57 88 58 95]\n",
      "[54 95 47 45 48 62 57 88 58 83]\n",
      "[95 56 48 48 48 62 62 57 88 27]\n",
      "[74 47 48 48 32 62 62 57 88 95]\n",
      "[86 89 48 48 48 62 40 57 62 40]\n",
      "[91 13 48 48 48 62 62 57 74 75]\n",
      "[ 8 37 48 97 58 88 88 74 88 95]\n",
      "[78  2 72 32 32 32 57 57 47 47]\n",
      "[49 88 47 59 48 62 57 57 88 95]\n",
      "[33  2 48 32 88 62 88 57 13  2]\n",
      "[30 31 48 57 32 57 57 57 74 95]\n",
      "[67 13 48 48 32 57 57 57 74 34]\n",
      "[23 57 48 59 74 57 88 57 74 95]\n",
      "[ 3 78 48 48 48 90 88 88 74 52]\n",
      "[55 97 48 59 48 57 57 57 88 95]\n",
      "[95 14 48 48 48 57 57 57 74 75]\n",
      "[76 46 48 48 48 57 57 74 74 34]\n",
      "[32 28 48 57 74 35 62 57 62 31]\n",
      "[20  2 48 88 95 34 34 34 34 93]\n",
      "[45 13 48 48 32 59 57 74 74 34]\n",
      "[14 89 48 48 48 59 88 88 74 95]\n",
      "[23 48 40 59 74 88 88 74 74 34]\n",
      "[21 13 48 59 88 88 88 57 34 34]\n",
      "[46 37 48 48 32 59 57 74 74 34]\n",
      "[14 78 48 48 48 59 88 88 74 34]\n",
      "[34 46 40 48 48 59 88 74 74 34]\n",
      "[47 13 48 48 32 59 59 57 74 34]\n",
      "[65 13 48 48 48 59 62 74 74 34]\n",
      "[32 75 40 40 48 59 59 74 74 34]\n",
      "[50 97 40 40 48 59 59 74 74 95]\n",
      "[78 13 62 48 32 59 59 57 74 95]\n",
      "[56 60 40 40 48 62 59 74 74 34]\n",
      "[ 1 72 32 48 48 88 88 74 22 22]\n",
      "[53 78 40 62 48 62 59 74 40 95]\n",
      "[87 13 62 48 32 59 59 74 74 75]\n",
      "[14 46 47 59 74 88 88 57 74 22]\n",
      "[ 8 22 47 59 88 88 88 61 84 95]\n",
      "[88 88 48 62 48 62 59 74 40 27]\n",
      "[20 31 47 59 74 88 88 57 74 95]\n",
      "[58 61 45 27 48 62 59 57 88 97]\n",
      "[ 2 95 48 40 48 88 88 88 74 95]\n",
      "[41 95 47 40 48 62 59 88 58 34]\n",
      "[96 13 62 48 32 62 62 74 40 75]\n",
      "[17 31 47 59 74 88 88 57 58 95]\n",
      "[51 95 47 62 48 62 59 88 40 34]\n",
      "[41 34 62 59 74 59 57 57 47 12]\n",
      "[30 13 32 59 74 88 88 57 40 95]\n",
      "[23 72 47 59 74 59 88 57 60 95]\n",
      "[80 97 45 45 48 62 40 57 40 40]\n",
      "[75 88 45 45 48 62 40 74 40 27]\n",
      "[47 34 48 30 32 59 59 57 88 34]\n",
      "[19 34 40 59 88 62 88 57 13 13]\n",
      "[57 12 62 32 32 59 59 57 47 12]\n",
      "[ 7 88 48 48 48 59 88 57 74  1]\n",
      "[26 72 47 59 15 74 88 88 58 34]\n",
      "[39 32 32 87 33 48 34 42 47 95]\n",
      "[39 34 48 59 32 59 59 57 74 34]\n",
      "[ 9 46 48 59 74 35 88 57 62 95]\n",
      "[31 95 47 40 48 59 40 57 58 89]\n",
      "[ 9 40 47 90 88 34 34 95 95 95]\n",
      "[15 13 48 59 88 34 34 62 47 95]\n",
      "[10 47 47 89 45 74 58 57  5 58]\n",
      "[55 46 48 27 32 40 40 57 47 60]\n",
      "[37 12 48 32 74 40 40 57 47 94]\n",
      "[ 5 95 47 40 45 40 88 57 74 34]\n",
      "[71 34 48 27 32 30 40 57 59 34]\n",
      "[33 47 40 40 48 59 57 57 74 34]\n",
      "[ 5 34 40 90 95 34 34 41 34 95]\n",
      "[65 46 48 27 32 30 40 57 93 34]\n",
      "[43 79 40 40 48 40 40 57 40 27]\n",
      "[58 46 45 27 32 40 40 57 60 37]\n",
      "[94 86 40 40 48 62 40 74 40 27]\n",
      "[31 13 40 32 74 62 40 57 47 95]\n",
      "[63 34 40 27 32 71 40 57 60 27]\n",
      "[21 52 40 89 48 95 88 41 41  1]\n",
      "[66 95 40 40 48 62 40 74 56 27]\n",
      "[58 95 40 40 48 62 40 57 56 89]\n",
      "[24 95 48 48 48 59 57 57 74 89]\n",
      "[ 5 57 48 90 48 74 88 57 41 41]\n",
      "[42 89 48 48 48 59 57 57 74 89]\n",
      "[ 3 95 48 48 48 61 88 57 74  1]\n",
      "[66 46 48 32 32 30 62 57 60 37]\n",
      "[ 3 95 48 48 48 90  1  1  1  1]\n",
      "[92  1 48 48 32 57 57 74 74 34]\n",
      "[52 95 48 48 48 62 57 57 74 95]\n",
      "[ 2 95 48 48 48 90  1  1  1  1]\n",
      "[ 7  1 48 47 95 88 88 57 47 13]\n",
      "[ 5 46 48 90 88 34 34 41 62 95]\n",
      "[69 28 48 48 32 59 57 57 74 34]\n",
      "[38 37 48 30 59 57 57 57 41 60]\n",
      "[ 1 57 48 59 74 57 74 57 41 87]\n",
      "[10 34 48 59 74 34 34 57 74 95]\n",
      "[20 97 48 59 74 57 57 57 74 34]\n",
      "[ 4 46 46 59 74 57 57 57 74 95]\n",
      "[23 97 45 59 74 57 57 57 74 34]\n",
      "[41 74 48 59 74 57 21 57 74 13]\n",
      "[57 57 48 30 59 57 57 57 41 60]\n",
      "[28 57 48 59 74 57 57 57 47 47]\n",
      "[29 95 48 59 74 57 57 57 41 57]\n",
      "[70 46 48 32 32 30 62 57 60 37]\n",
      "[ 4  1 75 47 95 93 57 57 74 86]\n",
      "[75 95 48 48 16 62 62 57 83 34]\n",
      "[56 46 48 30 32 57 57 57 60 34]\n",
      "[29 34 48 22 59 57 57 57 60 31]\n",
      "[48 89 48 59 59 59 57 57 57 95]\n",
      "[65 95 48 48 59 62 57 57 59 13]\n",
      "[98 60 48 48 30 30 62 57 83 34]\n",
      "[34 88 45 59 59 57 57 57 74 95]\n",
      "[35 13 48 59 74 34 62 57 47 98]\n",
      "[42 95 45 59 45 59 59 57 74 34]\n",
      "[ 4 41 46 59 74 79 74 57 74 34]\n",
      "[87 37 48 48 32 59 62 74 95 34]\n",
      "[68 34 48 48 32 59 62 74 74 34]\n",
      "[68 61 48 48 59 62 57 57 41 37]\n",
      "[55 95 45 59 45 59 59 74 27 27]\n",
      "[29 95 46 46 45 59 74 74 74 34]\n",
      "[42 37 48 22 59 48 57 57 60 34]\n",
      "[12 46 46 59 74 45 37 57 47 95]\n",
      "[93 12 48 48 48 59 31 57 47 31]\n",
      "[92 97 48 48 48 62 59 74 59 37]\n",
      "[84 12 48 48 32 59 62 74 74 34]\n",
      "[70 95 48 59 59 62 59 57 83 13]\n",
      "[42 78 48 27 59 59 57 57 60 97]\n",
      "[40 40 48 59 74 62 62 57 62 95]\n",
      "[18 88 46 59 59 59 74 74 74 34]\n",
      "[85 27 48 48 59 59 62 74 74 13]\n",
      "[40 97 46 46 59 59 57 74 74 34]\n",
      "[96 56 48 48 48 62 62 74 60 13]\n",
      "[58 57 48 89 33 45 62 51 62 95]\n",
      "[59 98 48 97 48 59 79 74 62 13]\n",
      "[94 13 48 48 48 59 62 74 60 27]\n",
      "[28 34 48 89 47 45 34 87 47 95]\n",
      "[19 61 47 59 74 79 57 57 74  1]\n",
      "[18 88 47 59 45 59 57 74 74 34]\n",
      "[86 97 48 48 48 62 59 74 93 13]\n",
      "[ 9 95 47 59 45 59 74 74 74 34]\n",
      "[ 3 95 48 59 45 79 74 74 74 34]\n",
      "[44 95 48 97 45 45 95 87 40 34]\n",
      "[87 95 48 48 48 62 59 74 59 34]\n",
      "[78 87 48 48 48 62 59 74 59 27]\n",
      "[37 97 48 46 45 59 74 74 74 34]\n",
      "[73 37 48 48 48 59 59 74 60 37]\n",
      "[ 0 61 48 59 74 45 37 57 74 95]\n",
      "[20 61 48 59 74 45 57 87 74 95]\n",
      "[98 37 48 48 48 62 62 74 60 13]\n",
      "[ 1  1 75 47 37 88 74 57 74 95]\n",
      "[99 60 48 48 48 62 62 74 60 13]\n",
      "[64 37 48 89 98 34 34 62 62 13]\n",
      "[84 47 48 48 48 62 62 74 60 13]\n",
      "[43 97 48 59 48 59 74 74 60 34]\n",
      "[62 97 48 48 48 62 57 74 60 34]\n",
      "[ 6  1 75 47 88 88 74 79 74 31]\n",
      "[96 93 48 48 48 62 35 74 59 27]\n",
      "[54 37 48 48 48 59 57 74 60 34]\n",
      "[95 56 48 48 48 62 62 74 60 13]\n",
      "[43 95 48 48 74 52 59 74 60 34]\n",
      "[64 88 48 48 48 62 59 74 60 34]\n",
      "[85 60 48 48 48 62 62 74 60 34]\n",
      "[ 6 95 47 74 45 45 37 51 45 34]\n",
      "[96 13 48 48 48 31 31 51 45 31]\n",
      "[60 75 48 22 59 62 57 57 60 97]\n",
      "[67 59 48 82 33 59 62 51 62 95]\n",
      "[ 9 95 47 59 45 59 74 74 51 34]\n",
      "[97 13 48 48 48 31 31 51 93 13]\n",
      "[75 57 48 48 48 62 62 74 60 34]\n",
      "[44 47 48 22 59 48 57 57 60 13]\n",
      "[35 13 48 22 59 48 57 57 60 95]\n",
      "[31 13 48 22 74 48 57 79 31 95]\n",
      "[16 88 47 59 45 59 51 51 74 34]\n",
      "[45 95 48 59 45 59 59 51 74 34]\n",
      "[24 88 47 59 45 59 74 74 74 34]\n",
      "[23 28 48 59 74 45 62 51 62 95]\n",
      "[97 56 48 82 48 59 62 51 62 95]\n",
      "[15 95 47 59 45 45 59 51 51 34]\n",
      "[43 34 48 22 59 93 57 57 60 51]\n",
      "[ 1 28 48 47 88 93 42 42 66  1]\n",
      "[25 88 47 59 45 59 74 74 74 34]\n",
      "[99 59 48 48 48 40 40 62 61 27]\n",
      "[12 95 47 47 45 45 74 74 74 34]\n",
      "[46 88 48 48 48 59 59 74 74 34]\n",
      "[75 82 48 48 48 57 59 74 93 34]\n",
      "[78 13 48 22 48 93 57 51 60 34]\n",
      "[94 95 48 48 48 57 40 57 93 27]\n",
      "[69 13 48 22 22 82 57 19 19 19]\n",
      "[25 17 48 59 47 45 34 87 95 34]\n",
      "[64 56 48 22 16 16 57 74 56 34]\n",
      "[ 5 47 47 59 45 45 74 74 74 95]\n",
      "[95 34 48 22 22 22 82 74 75 56]\n",
      "[45 90 48 22 48 22 57 74 74 34]\n",
      "[78 13 48 22 22 22 57 74 82 34]\n",
      "[74 22 48 22 22 57 57 74 59 34]\n",
      "[39 47 48 22 22 57 74 74 74 34]\n",
      "[63 37 48 22 22 57 59 74 59 34]\n",
      "[30 46 48 22 59 93 74 74 74 34]\n",
      "[66 98 48 22 48 27 59 74 59 34]\n",
      "[32 24 48 22 59 93 74 57 60 52]\n",
      "[67 95 48 82 48 57 59 74 27 13]\n",
      "[18 79 47 59 48 90 74 74 74 34]\n",
      "[20 88 47 59 48 90 74 74 74 34]\n",
      "[79 74 27 82 48 45 45 12 13 13]\n",
      "[ 2 34 47 59 88 34 34 62 62 95]\n",
      "[80 46 32 82 48 48 31 13 13 13]\n",
      "[54 79 74 22 59 82 57 74 47 97]\n",
      "[61 47 74 22 22 82 57 57 47  1]\n",
      "[18 28 48 59 74 34 34 12 12 12]\n",
      "[73 75 74 82 74 59 57 74 74 13]\n",
      "[40 79 47 59 59 59 59 57 74 56]\n",
      "[66 47 32 32 32 57 59 74 75 34]\n",
      "[84 12 82 82 74 59 57 74 74 95]\n",
      "[27 32 74 59 74 79 57 57 74 95]\n",
      "[74 95 74 57 59 57 57 57 74 13]\n",
      "[25 95 47 59 74 57 57 57 74 34]\n",
      "[46 34 32 27 74 27 57 57 82 34]\n",
      "[76  2 82 82 74 57 57 74 74 95]\n",
      "[12 39 47 59 74 88 88 57 74 34]\n",
      "[ 9 37 47 59 74 34 74 87 47 95]\n",
      "[72 24 27 82 74 34 42 62 31 31]\n",
      "[51 63 74 82 74 59 57 74 74 95]\n",
      "[47 13 27 82 47 34 95 32 32 42]\n",
      "[50  7 27 82 95 34 34 93 95 34]\n",
      "[ 8 95 46 46 59 59 79 74 87 85]\n",
      "[34 95 46 59 59 59 79 74 74 34]\n",
      "[38 95 46 59 74 57 79 74 74 34]\n",
      "[ 8 95 46 59 59 90 74 74 32 34]\n",
      "[86 24 82 82 74 59 57 74 74 95]\n",
      "[80 59 82 82 74 59 57 74 74 31]\n",
      "[44 13 82 59 74 93 57 74 74 95]\n",
      "[ 0 48 48 59 59 67 88 74 74 95]\n",
      "[57 90 48 82 74 59 59 74 74 95]\n",
      "[42 34 82 59 74 57 57 57 74 95]\n",
      "[71 12 27 27 32 57 93 74 95 34]\n",
      "[58 47 82 57 74 57 57 74 74 95]\n",
      "[28 95 46 46 48 79 74 74 32 34]\n",
      "[ 9 90 46 46 48 90 74 74 32 34]\n",
      "[53 46 82 57 74 79 57 57 74 95]\n",
      "[99 34 27 27 74 27 40 40 27 27]\n",
      "[44 46 82 82 74 79 57 79 74 95]\n",
      "[71 95 82 30 57 30 40 57 59 27]\n",
      "[15  8 27 59 95 34 34 87 95 95]\n",
      "[76 57 82 82 74 57 57 74 74 95]\n",
      "[55 37 82 82 74 59 57 74 74 95]\n",
      "[ 5 90 46 46 59 90 88 74 32 34]\n",
      "[60 95 82 82 74 57 57 57 74 13]\n",
      "[76 28 27 82 33 57 79 74 34 34]\n",
      "[ 8 34 46 59 59 58 88 87 58 95]\n",
      "[75 86 82 82 74 57 57 74 74 13]\n",
      "[66 78 82 89 74 59 59 74 74 47]\n",
      "[23 52 46 82 74 59 37 88 32 95]\n",
      "[62 67 82 82 74 59 59 74 74 95]\n",
      "[69 71 82 82 74 59 59 74 74 95]\n",
      "[65 56 82 82 74 59 59 74 74 95]\n",
      "[11 95 46 59 59 90 88 74 32 34]\n",
      "[41 95 47 46 59 57 59 74 27 34]\n",
      "[81 95 82 82 74 59 59 74 74 13]\n",
      "[21 24 82 59 59 88 59 59 74 95]\n",
      "[ 9 95 46 59 59 59 88 74 87 34]\n",
      "[87 95 82 82 74 59 59 74 93 13]\n",
      "[ 6 47 46 59 59 79 88 74 74 34]\n",
      "[40 95 47 46 59 57 59 74 27 87]\n",
      "[75 34 82 89 74 57 59 74 74 95]\n",
      "[ 4 47 46 59 59 79 88 74 74 34]\n",
      "[49 88 48 46 74 79 59 74 60 34]\n",
      "[91 47 82 16 16 16 30 13 16 16]\n",
      "[86 34 82 82 74 59 59 74 74 95]\n",
      "[56 95 48 82 59 59 59 88 40 13]\n",
      "[41 28 82 59 74 79 59 59 74 95]\n",
      "[97 13 82 79 74 59 59 74 74 95]\n",
      "[89 34 82 16 16 57 79 59 60  4]\n",
      "[67 46 82 79 74 57 59 74 74 95]\n",
      "[58 95 48 46 59 59 59 74 27 13]\n",
      "[24 13 82 59 95 93 93 79 74 95]\n",
      "[ 5 60 46 59 59 79 88 74 74 34]\n",
      "[13 32 46 59 74 93 87 74 74 95]\n",
      "[54 82 82 82 74 59 59 74 74 95]\n",
      "[70 13 46 82 74 59 62 87 32 95]\n",
      "[ 4 52 46 59 59 37 37 88 47 95]\n",
      "[91 47 82 82 74 59 59 74 74 95]\n",
      "[42 82 46 46 59 59 59 72 27 34]\n",
      "[82 95 82 82 74 59 59 74 74 13]\n",
      "[64 95 82 82 74 59 59 74 74 13]\n",
      "[40 95 46 46 59 59 59 74 27 34]\n",
      "[ 1 34 46 79 88 66 37 66 32 95]\n",
      "[15 95 46 46 59 79 88 74 32 34]\n",
      "[14 95 46 46 59 79 88 74 87 34]\n",
      "[23 89 46 46 59 79 88 74 87 34]\n",
      "[91 34 82 79 74 79 59 74 74 95]\n",
      "[85 28 82 79 74 79 59 74 74 95]\n",
      "[54 37 82 79 74 79 57 74 47 47]\n",
      "[ 8 34 48 79 88 37 37 60 27 47]\n",
      "[10 47 48 79 88 37 37 88 47 28]\n",
      "[50 34 82 79 74 79 57 74 74 95]\n",
      "[75 48 82 79 74 79 57 74 74 47]\n",
      "[68 13 82 79 74 79 62 74 74 95]\n",
      "[ 0 20 48 79 95 93 37 60 27 32]\n",
      "[52 66 82 79 74 79 74 74 74 95]\n",
      "[73 95 48 82 33 74 74 74 27 13]\n",
      "[28 60 48 79 74 42 37 74 27 47]\n",
      "[ 2 95 48 79 57 90 88 74 32 32]\n",
      "[81 12 82 46 79 82 57 74 95 95]\n",
      "[79 79 82 82 74 57 57 74 83 32]\n",
      "[36 95 48 46 12 79 79 74 32 34]\n",
      "[47 95 48 46 57 57 57 74 27 95]\n",
      "[86 59 82 82 74 57 57 74 74 47]\n",
      "[39 13 82 46 59 57 57 79 95 95]\n",
      "[37 13 82 46 59 88 57 79 95 95]\n",
      "[58 34 82 57 74 57 57 74 74 95]\n",
      "[58 12 82 57 57 57 57 74 95 95]\n",
      "[34 82 48 46 57 57 57 74 27 34]\n",
      "[61 95 82 57 74 57 57 74 42 13]\n",
      "[90 74 82 82 74 57 57 74 83 13]\n",
      "[25 37 48 57 74 59 37 74 74 95]\n",
      "[48 47 82 57 74 57 57 74 74 95]\n",
      "[17 44 48 57 74 57 37 88 32 95]\n",
      "[ 6 95 48 57 57 90 88 74 32 32]\n",
      "[50 27 82 57 57 57 57 74 95 95]\n",
      "[92 13 82 46 79 57 57 74 95 95]\n",
      "[65 95 48 82 74 57 57 74 27 13]\n",
      "[97 37 82 82 74 57 57 74 74 95]\n",
      "[35 88 48 46 57 79 57 74 27 37]\n",
      "[77 47 82 79 74 79 57 74 74 95]\n",
      "[ 7 79 48 46 57 10 90 79 28 28]\n",
      "[41 95 48 46 46 79 79 74 27 32]\n",
      "[45 12 82 46 95 93 40 91 95 95]\n",
      "[93 34 82 89 74 79 57 74 74 95]\n",
      "[25 34 48 89 88 57 37 88 27 95]\n",
      "[52 34 82 46 74 93 74 79 95 34]\n",
      "[68 88 48 82 59 59 59 88 82 34]\n",
      "[76 60 82 89 74 59 59 74 74 47]\n",
      "[84 13 82 46 79 93 93 74 60 95]\n",
      "[20 28 48 79 88 57 37 74 27 95]\n",
      "[13 34 48 79 88 14 62 60 27 27]\n",
      "[ 2 34 48 79 88 57 37 88 27 28]\n",
      "[49 82 48 46 59 93 88 74 95 34]\n",
      "[28 89 48 46 59 79 88 74 74 34]\n",
      "[83 11 82 46 46 46 59 74 95 34]\n",
      "[52 66 48 46 59 93 79 74 95 34]\n",
      "[89 88 82 82 59 59 59 74 93 13]\n",
      "[18 42 48 46 59 42 62 74 27 95]\n",
      "[75 75 82 79 59 59 59 74 74 31]\n",
      "[40 34 82 79 59 59 59 79 74 95]\n",
      "[ 8 82 48 59 59 59  0 88 58 95]\n",
      "[26 28 82 79 59 88 62 79 32 95]\n",
      "[29 20 82 59 59 88 57 79 95 95]\n",
      "[18 60 48 59 59 59 37 74 38 95]\n",
      "[84 83 82 89 74 59 59 74 74 13]\n",
      "[99 12 82 89 74 59 59 74 74 95]\n",
      "[77 27 82 46 79 46 57 74 95 95]\n",
      "[38 89 48 46 59 79 59 74 27 37]\n",
      "[24 34 48 79 12 88 95 74 74 34]\n",
      "[55 82 48 46 74 93 59 74 27 37]\n",
      "[60 95 48 46 12 59 59 88 34 34]\n",
      "[23 79 48 46 59 46 59 74 27 34]\n",
      "[31 12 82 47 47 46 59 79 95 95]\n",
      "[85 12 48 46 48 59 62 74 95 34]\n",
      "[23  5 48 46 59 90 74 79 95 34]\n",
      "[19 95 48 59 59 59 37 74 27 96]\n",
      "[64 79 48 46 59 59 59 74 95 34]\n",
      "[84 79 82 89 74 59 59 74 93 13]\n",
      "[12 11 48 59 88 93 37 74 74 95]\n",
      "[65 34 82 46 79 46 93 79 60 34]\n",
      "[44  3 48 46 67 93 62 74 95 34]\n",
      "[54 14 82 79 67 90 62 74 74 95]\n",
      "[91 79 30 82 33 74 62 37 95 34]\n",
      "[15 17 48 79 47 88 95 74 74 95]\n",
      "[49 79 48 46 12 74 59 74 95 34]\n",
      "[79 48 82 46 79 46 59 74 88 95]\n",
      "[41 97 48 46 12 12 59 62 46 34]\n",
      "[44 48 48 46 74 42 62 74 82 34]\n",
      "[78 22 48 48 12 59 59 74 95 34]\n",
      "[68 95 48 46 12 74 59 74 74 34]\n",
      "[33 82 48 46 12 59 62 74 82 83]\n",
      "[74 95 48 46 12 74 79 74 40 34]\n",
      "[77 95 48 48 12 74 42 74 74 34]\n",
      "[70 79 48 48 12 74 79 74 82 34]\n",
      "[ 2 75 48 46 59 90 88 74 27 34]\n",
      "[14 59 48 46 74 93 74 74 27 95]\n",
      "[49 13 82 79 67 79 62 74 74 95]\n",
      "[22  4 82 47 75 93 74 79 74 31]\n",
      "[71 18 82 79 74 79 62 74 27 27]\n",
      "[29 66 48 46 74 93 88 74 27 34]\n",
      "[41 95 48 46 74 79 59 74 27 37]\n",
      "[19 95 48 46 59 79 88 74 27 34]\n",
      "[28 79 48 46 74 79 88 74 27 95]\n",
      "[14 95 48 46 59 90 88 74 27 34]\n",
      "[32 67 48 46 74 79 88 74 27 95]\n",
      "[37 17 82 79 67 88 37 74 74 95]\n",
      "[ 0 79 48 79 24 24 37 37 95 17]\n",
      "[ 4 27 48 79 47 88 37 37 27 95]\n",
      "[61 13 82 79 67 79 62 74 27 95]\n",
      "[54 95 48 46 12 74 79 74 40 34]\n",
      "[24 40 48 79 12 88 37 74 95 95]\n",
      "[36 28 82 79 67 79 62 74 27 32]\n",
      "[41 79 48 79 74 59 62 74 27 13]\n",
      "[ 1 79 48 79 67 88 88 74 27 34]\n",
      "[20 66 48 79 74 59 37 74 27 95]\n",
      "[77 98 48 46 79 40 40 74 13 13]\n",
      "[85 12 82 79 79 79 62 74 74 95]\n",
      "[76 95 48 46 79 40 40 74 13 13]\n",
      "[20 82 48 59 59 59 59 74 27 95]\n",
      "[89 47 82 79 74 79 62 74 74 95]\n",
      "[55 79 48 46 59 42 59 74 27 13]\n",
      "[39 79 48 46 59 42 59 74 27 13]\n",
      "[95 82 82 79 74 59 59 74 93 13]\n",
      "[20 40 48 79 12 88 37 74 95 34]\n",
      "[15 79 48 59 59 59 37 74 27 27]\n",
      "[71 47 82 79 74 59 59 74 27 27]\n",
      "[92 95 82 46 59 42 42 74 46 93]\n",
      "[15 90 48 42 59 46 59 74 27 14]\n",
      "[55 13 48 46 12 59 59 74 95 34]\n",
      "[71 13 48 46 12 74 59 74 95 34]\n",
      "[ 6 28 48 79 47 88 37 74 27 95]\n",
      "[50 74 73 79 74 59 59 74 27 27]\n",
      "[92 34 82 48 74 79 62 74 95 95]\n",
      "[49 40 48 79 74 59 62 74 27 95]\n",
      "[64 27 48 48 74 42 62 74 60 34]\n",
      "[52 13 48 48 74 93 62 74 27 13]\n",
      "[13 34 48 80 88 62 95 74 82 34]\n",
      "[82 82 48 48 12 74 42 74 12 12]\n",
      "[88 37 48 48 12 74 59 74 95 34]\n",
      "[ 5 21 48 79 47 88 95 74 27 95]\n",
      "[26 82 48 46 74 29 57 59 27 13]\n",
      "[ 3 31 48 79 88 62 95 27 27 27]\n",
      "[58 34 48 48 74 93 62 74 27 95]\n",
      "[27 14 48 79 47 88 95 74 27 95]\n",
      "[69 46 48 48 74 93 62 74 27 95]\n",
      "[39 75 48 46 74 93 88 74 60 34]\n",
      "[61 48 48 48 74 93 62 74 60 34]\n",
      "[45 79 48 46 74 93 88 74 60 22]\n",
      "[76 34 48 48 74 93 62 74 60 34]\n",
      "[ 9 79 48 59 57 90 88 74 95 34]\n",
      "[40 13 48 79 73 67 62 74 27 95]\n",
      "[40 28 48 79 74 57 62 74 27 93]\n",
      "[87 34 69 48 79 48 93 74 60 34]\n",
      "[77 13 48 48 32 79 62 74 95 34]\n",
      "[ 9 95 48 59 57 90 88 74 27 32]\n",
      "[10 95 48 59 57 90 88 74 27 32]\n",
      "[57 95 48 46 74 48 79 74 27 27]\n",
      "[14 88 48 79 57 90 37 74 27 32]\n",
      "[ 6 22 48 79 47 88 95 74 27 95]\n",
      "[42 74 48 79 74 57 37 74 27 47]\n",
      "[30 95 48 79 57 90 37 74 27 95]\n",
      "[ 3 13 48 79 88 95 37 27 27 27]\n",
      "[96 88 48 48 74 48 93 74 93 34]\n",
      "[45 88 48 79 74 57 59 74 27 13]\n",
      "[41 24 48 79 74 90 62 74 27 95]\n",
      "[ 0 34 48 79 88 67 37 37 27 27]\n",
      "[90 95 48 48 74 40 79 74 93 34]\n",
      "[90 95 48 48 74 82 79 74 95 34]\n",
      "[90 79 48 48 74 82 79 74 95 34]\n",
      "[57 95 48 79 74 57 22 74 93  1]\n",
      "[88 95 48 48 74 82 79 74 95 34]\n",
      "[50 95 48 79 74 57 22 74 95 47]\n",
      "[30 79 48 79 74 90 37 88 27 95]\n",
      "[ 3 95 48 79 88 90 88 74 37 34]\n",
      "[97 82 48 48 74 82 79 74 95 34]\n",
      "[96 98 48 48 74 40 40 74 82 13]\n",
      "[88 74 48 48 74 40 40 74 93 13]\n",
      "[54 60 48 79 74 79 62 74 95 31]\n",
      "[75 46 48 27 79 48 79 74 60 34]\n",
      "[16 52 48 79 88 88 37 88 27 95]\n",
      "[26 31 48 79 74 88 37 74 27 95]\n",
      "[ 5 34 48 79 88 88 37 74 27 95]\n",
      "[82 12 82 79 79 79 62 74 74 95]\n",
      "[ 1 79 32 74 88 90 88 74 60 34]\n",
      "[34 95 48 79 74 59 88 74 27 60]\n",
      "[63 79 48 46 74 79 74 74 60 34]\n",
      "[83 60 82 79 74 79 74 74 27 27]\n",
      "[38  1 48 79 47 88 95 74 27 95]\n",
      "[96 95 48 48 33 33 79 88 13 13]\n",
      "[54 14 48 79 74 88 62 74 27 95]\n",
      "[32 24 48 79 47 88 74 74 27 95]\n",
      "[29 79 48 79 74 16 90 74 40 13]\n",
      "[81 13 48 79 74 59 62 62 27 95]\n",
      "[44 95 48 79 74 59 59 74 27 13]\n",
      "[35 82 48 79 74 59 59 74 27 13]\n",
      "[15 95 48 79 74 90 88 74 27 34]\n",
      "[62 60 48 79 74 59 62 74 27 13]\n",
      "[31 59 48 79 74 59 62 74 27 95]\n",
      "[44 95 48 79 74 59 59 74 27 13]\n",
      "[10 66 48 79 88 88 37 74 27 95]\n",
      "[ 6 60 48 79 88 88 37 74 27 95]\n",
      "[82 82 48 48 33 33 59 74 44 13]\n",
      "[61 82 48 79 74 59 62 74 13 13]\n",
      "[ 6 47 48 79 88 88 62 74 62 95]\n",
      "[35 82 48 79 74 59 59 74 27 13]\n",
      "[94 33 48 48 33 74 62 74 95 34]\n",
      "[45 82 48 79 74 74 74 74 13 13]\n",
      "[73 13 48 79 74 59 22 62 62 95]\n",
      "[74 82 48 48 74 57 79 74 13 13]\n",
      "[69 34 48 79 74 59 62 74 27 95]\n",
      "[75 98 48 79 74 74 74 74 27 13]\n",
      "[17 82 48 79 74 90 37 74 27 95]\n",
      "[66 95 48 79 74 74 74 74 27 95]\n",
      "[72 91 48 79 74 74 62 74 27 13]\n",
      "[14 95 48 79 74 90 37 74 27 34]\n",
      "[ 1 66 32 79 88 90 37 74 95 34]\n",
      "[41 82 48 79 74 59 37 74 27 47]\n",
      "[83 13 48 48 32 74 62 74 95 34]\n",
      "[ 3 69 32 32 88 88 88 74 34 34]\n",
      "[44 90 48 79 74 59 37 74 27 95]\n",
      "[23 90 48 79 74 90 37 74 27 34]\n",
      "[45 95 48 79 74 79 37 74 27 95]\n",
      "[49 82 48 79 74 57 37 74 27 47]\n",
      "[46 47 48 79 74 57 62 74 27 95]\n",
      "[14 79 48 79 57 57 37 74 27 40]\n",
      "[67 95 48 48 74 40 79 74 95 34]\n",
      "[30 37 48 79 74 57 62 74 40 95]\n",
      "[77 91 48 48 74 57 62 74 95 34]\n",
      "[84 88 48 48 74 32 62 74 95 13]\n",
      "[77 13 48 48 32 74 62 74 55 34]\n",
      "[36 28 48 79 74 57 62 62 62 47]\n",
      "[89 28 48 48 32 74 62 74 95 34]\n",
      "[87  4 48 48 32 74 62 74 95 34]\n",
      "[36 47 48 79 74 59 62 74 27 62]\n",
      "[23 95 48 79 74 57 37 74 27  1]\n",
      "[46 88 48 79 74 57 37 74 27 95]\n",
      "[64 95 48 79 74 57 74 74 13 13]\n",
      "[64 95 48 79 74 57 74 74 27 13]\n",
      "[49 95 48 79 74 57 37 74 27 95]\n",
      "[83 95 48 48 74 40 79 74 95 34]\n",
      "[81  3 48 79 74 57 62 74 27 95]\n",
      "[72 13 48 79 74 57 62 74 27 95]\n",
      "[76 79 48 79 74 74 74 74 93 34]\n",
      "[ 2 34 79 79 88 88 37 88 62 95]\n",
      "[56 63 48 79 74 74 37 74 95 34]\n",
      "[36 95 48 79 74 79 37 74 27 95]\n",
      "[13 95 17 48 74 90 37 74 34 34]\n",
      "[95 37 48 27 79 40 62  0 62 79]\n",
      "[89 95 48 79 74 74 74 74 13 13]\n",
      "[72 67 48 79 74 74 62 74 93 91]\n",
      "[83 95 48 79 74 74 74 74 93 13]\n",
      "[33 67 48 79 74 59 37 74 62 47]\n",
      "[42 34 48 79 25 74 37 74 95 95]\n",
      "[37 98 48 79 74 74 37 74 27 13]\n",
      "[33 95 48 79 74 74 37 74 27 13]\n",
      "[25 79 48 79 74 59 37 74 27 95]\n",
      "[ 5 66 32 79 95 95 37 74 95 34]\n",
      "[53 95 48 79 74 74 74 74 13 13]\n",
      "[77 62 82 79 74 79  2 74 62 95]\n",
      "[43 67 48 79 74 59 37 37 37 13]\n",
      "[ 4 79 32 79 37 37 37 37 95 34]\n",
      "[93 37 82 79 74 79 62 74 95 95]\n",
      "[60 13 48 79 74 90 62 62 62 95]\n",
      "[28 40 48 79 74 57 37 37 93 21]\n",
      "[45 88 48 79 74 79 74 74 60 34]\n",
      "[71 82 48 79 74 79 74 74 93 34]\n",
      "[66 98 48 79 74 74 74 74 13 13]\n",
      "[46 95 48 79 74 57 59 74 94 34]\n",
      "[94 37 93 79 74 79 59 74 95 95]\n",
      "[ 0 90 32 79 37 37 37 37 95 34]\n",
      "[61 79 48 79 74 74 59 74 95 34]\n",
      "[24 59 48 79 74 59 37 37 93  1]\n",
      "[53 82 48 79 74 74 59 74 95 34]\n",
      "[73 75 48 79 74 74 59 74 95 34]\n",
      "[15 88 32 79 37 95 37 37 37 34]\n",
      "[95 95 82 79 74 74 74 74 95 13]\n",
      "[11 82 32 79 37 37 37 37 95 34]\n",
      "[ 5 40 32 79 88 57 37 37 60 11]\n",
      "[67 88 48 79 74 74 74 74 95 34]\n",
      "[38 98 48 79 74 74 37 74 95 34]\n",
      "[ 6 79 32 79 37 95 37 37 95 34]\n",
      "[96 34 93 79 74 79 74 74 95 95]\n",
      "[51 95 48 79 74 74 74 74 95 60]\n",
      "[74 13 93 79  1 74 74 74 95 34]\n",
      "[38 95 48 79 74 74 59 74 95 34]\n",
      "[61 57 48 79 74 74 59 74 95 14]\n",
      "[83 13 93 79 74 79 74 74 95 95]\n",
      "[33 66 48 79 74 59 59 74 93 31]\n",
      "[46 22 93 79 88 88 59 74 11 47]\n",
      "[46 22 93 79 57 79 74 74 95 95]\n",
      "[ 7 79 48 79 57 57 95 74 40 31]\n",
      "[74 79 48 48 74 42 79 74 93 34]\n",
      "[44 24 93 79 57 88 57 74 47 95]\n",
      "[74  4 93 79 57 88 57 74 95 95]\n",
      "[50 75 48 79 74 57 59 74 93 14]\n",
      "[60 94 48 79 74 57 57 74 93 14]\n",
      "[80 57 82 79 74 79 57 74 95 95]\n",
      "[44 95 48 79 74 57 57 74 93 14]\n",
      "[87 99 48 48 74 32 79 42 42 13]\n",
      "[99  4 93 79 74 79 57 74 95 95]\n",
      "[ 4 34 32 79 88 57 95 57 62 31]\n",
      "[97 93 82 79 74 74 74 74 42 93]\n",
      "[82 37 82 79 74 79 62 74 42 95]\n",
      "[15 47 48 79 95 93 95 74 82 34]\n",
      "[34 82 48 79 74 57 59 74 93 34]\n",
      "[47 79 48 79 74 57 59 74 93  1]\n",
      "[56 95 48 79 74 57 57 74 93  1]\n",
      "[62 88 48 79 74 57 57 74 93  1]\n",
      "[48 34 93 79 74 57 57 74 42 47]\n",
      "[62 88 48 79 74 57 57 74 93  1]\n",
      "[14 60 48 79 57 57 95 74 93 31]\n",
      "[80 90 48 48 79 32 40 40 42 27]\n",
      "[55 24 48 32 79 93 42 74 95 34]\n",
      "[ 3 28 48 79 88 57 93 57 62 60]\n",
      "[37 90 48 79 74 57 57 59 93  1]\n",
      "[38  1 48 32 57 93 62 74 95 34]\n",
      "[57 34 48 27 79 93 42 27 27 27]\n",
      "[42 28 48 79 57 57 95 59 93 31]\n",
      "[31 79 48 79 57 57 95 74 93 34]\n",
      "[89 27 48 94 74 57 93 62 93 13]\n",
      "[34 95 48 79 57 79 95 74 95 34]\n",
      "[81 22 48 27 79 48 93 40 60 82]\n",
      "[26 24 48 79 98 57 93 62 62 60]\n",
      "[35 82 48 79 57 57 95 74 95 34]\n",
      "[ 9 13 48 79 47 57 95 85 93 31]\n",
      "[31 75 48 79 57 57 95 74 93 34]\n",
      "[83 88 48 48 79 32 11 79 57 93]\n",
      "[82 13 48 48 32 74 57 98 95 34]\n",
      "[31 27 48 79 57 57 57 98 95 31]\n",
      "[91 40 48 32 48 32 93 57 95 34]\n",
      "[66 98 48 48 79 79 79 74 95 34]\n",
      "[88  4 48 79 57 57 95 95 95 34]\n",
      "[47 82 48 79 74 57 95 74 93 34]\n",
      "[14 13 48 79 47 57 95 85 62 95]\n",
      "[63 52 48 27 79 48 79 74 60 34]\n",
      "[85 98 48 48 79 79 79 74 95 34]\n",
      "[91 98 48 48 79 79 79 74 95 34]\n",
      "[96 95 48 48 79 40 79 74 95 34]\n",
      "[78  4 48 79 57 57 59 98 95 34]\n",
      "[73 60 48 27 79 40 28 79 37 37]\n",
      "[35 34 48 79 57 57 95 74 57 69]\n",
      "[46 60 48 79 74 57 57 74 93  1]\n",
      "[47 79 48 79 74 79 59 74 94  1]\n",
      "[97 93 48 48 48 32 32 27 27 27]\n",
      "[20 31 48 79 98 57 57 57 93  1]\n",
      "[44 75 48 79 74 79 59 74 93  1]\n",
      "[93 82 48 48 48 32 32 79 57 34]\n",
      "[70 79 48 48 79 40 40 40 66 13]\n",
      "[18 59 48 79 57 57 57 74 93 31]\n",
      "[82 23 48 27 79 93 77 40 60 60]\n",
      "[35 60 48 79 74 57 57 74 93 31]\n",
      "[79 59 77 27 79 40 40 40 27 27]\n",
      "[16 13 26 79 47 57 95 85 60 69]\n",
      "[46 34 77 79 79 90 59 74 93 31]\n",
      "[91  4 77 79 79 79 59 74 95 98]\n",
      "[60 67 48 79 74 79 59 98 95 34]\n",
      "[55 34 77 79 79 90 59 74 93 98]\n",
      "[80 79 77 79 79 79 74 74 42 42]\n",
      "[44 98 48 79 74 57 57 74 93 31]\n",
      "[15 79 32 79 57 57 95 74 40 31]\n",
      "[49 34 77 79 79 90 59 74 93 27]\n",
      "[82 60 77 79 79 79 59 74 93 12]\n",
      "[ 5 95 32 79 57 57 57 85 40 31]\n",
      "[22 60 48 79 57 57 95 74 93 31]\n",
      "[84 22 77 79 79 90 62 98 47 95]\n",
      "[72 40 77 79 79 90 98 74 93 95]\n",
      "[66  4 77 79 57 90 57 74 95 67]\n",
      "[68 40 77 79 79 90 98 74 93 95]\n",
      "[42 82 48 79 74 79 59 74 93 31]\n",
      "[52 98 36 79 74 57 57 74 40 13]\n",
      "[84 90 77 27 79 40 40 27 27 27]\n",
      "[63 67 77 27 79 40 40 40 27 27]\n",
      "[62 95 48 79 74 57 57 74 93 13]\n",
      "[86 34 77 79 79 29 79 74 93  2]\n",
      "[11 60 32 79 95 95 57 74 93 31]\n",
      "[46 60 48 79 74 59 22 98 95 31]\n",
      "[27 34 48 79 95 57 57 57 93 31]\n",
      "[27 59 48 79 95 93 59 74 93 31]\n",
      "[71  4 77 79 57 90 59 74 95 34]\n",
      "[88  4 77 79 57 57 59 74 95 34]\n",
      "[ 1 31 32 79 47 67 95 85 40 62]\n",
      "[52 34 77 79 79 90 59 74 93 95]\n",
      "[28 82 48 79 57 57 57 74 40 31]\n",
      "[57 98 48 79 74 57 57 74 77 13]\n",
      "[56 98 48 79 74 57 57 74 51 13]\n",
      "[40 34 77 79 79 90 59 74 95 98]\n",
      "[39  4 77 79 46 90 95 74 77 95]\n",
      "[64 95 48 79 74 57 57 74 77 13]\n",
      "[43 60 48 79 74 42 57 98 95 31]\n",
      "[22 90 48 79 57 57 95 74 77 31]\n",
      "[83 34 48 94 74 57 59 98 95 31]\n",
      "[ 3 97 92 79 57 90 59 74 32 32]\n",
      "[47 82 48 79 74 79 22 98 95 31]\n",
      "[16 67 77 79 57 90 59 74 77 95]\n",
      "[28 60 48 79 95 57 59 59 93 21]\n",
      "[61 98 48 79 74 74 59 74 93 13]\n",
      "[66 22 77 79 79 90 59 74 77 95]\n",
      "[78 77 77 79 74 74 74 74 93 13]\n",
      "[82 82 77 79 74 74 74 74 42 93]\n",
      "[39 32 77 79 79 90 59 74 77 95]\n",
      "[95 82 77 79 74 74 98 74 42 37]\n",
      "[62 82 48 79 74 74 59 74 77 13]\n",
      "[ 5 66 77 79 57 90 59 74 77 31]\n",
      "[46 13 77 79 47 90 59 74 95 31]\n",
      "[93 37 77 71 79 79 98 74 95 95]\n",
      "[32 37 77 79 47 90 74 74 95 31]\n",
      "[34 79 48 79 74 57 59 74 93 31]\n",
      "[26 23 77 79 47 90 59 74 95 31]\n",
      "[ 7 28 77 79 47 90 59 74 95 31]\n",
      "[26  4 77 59 47 90 59 74 95 31]\n",
      "[90 98 77 79 74 74 59 74 37 13]\n",
      "[83 77 77 79 74 74 59 74 37 47]\n",
      "[93 11 77 79 57 57 59 74 95 95]\n",
      "[71  4 77 23 79 42 98 40 82 82]\n",
      "[ 9  6 77 47 86 88 59 74 95 95]\n",
      "[37 47 48 79 74 57 59 93 95  1]\n",
      "[27 95 77 74 57 79 74 74 77 34]\n",
      "[99 98 77 79 74 74 59 74 42 13]\n",
      "[40 21 77 79 57 90 59 74 95 98]\n",
      "[17 79 77 79 57 57 57 74 77 31]\n",
      "[78 40 77 79 74 57 57 74 42 47]\n",
      "[95 21 77 79 57 57 59 74 95 95]\n",
      "[39 47 77 79 57 57 57 74 95 98]\n",
      "[ 1 97 32 32 57 57 57 74 97 34]\n",
      "[99 98 77 79 74 74 98 74 95 34]\n",
      "[29 82 77 79 57 79 98 74 60 95]\n",
      "[39  4 77 79 47 95 95 74 95 95]\n",
      "[ 0 47 27 66 95 57 57 93 60  1]\n",
      "[53 34 77 79 79 90 98 74 93 95]\n",
      "[62 95 77 79 74 98 98 74 37 37]\n",
      "[79 77 77 71 74 98 98 74 37 37]\n",
      "[83  4 77 90 57 57 95 74 95 95]\n",
      "[50  4 77 46 47 46 57 74 95 95]\n",
      "[15 28 77 79 47 95 57 74 95 95]\n",
      "[ 1 22 27 79 47 57 57 85 95 94]\n",
      "[14 28 77 79 47 95 57 74 95 95]\n",
      "[98 13 77 79 57 57 59 74 95 95]\n",
      "[51 34 77 79 57 90 59 74 95 95]\n",
      "[32 82 77 79 57 79 59 74 95 95]\n",
      "[32 95 77 79 57 79 74 74 77 34]\n",
      "[15 97 77 79 57 79 59 74 77 34]\n",
      "[93 13 77 79 32 57 59 74 95 34]\n",
      "[92 98 77 79 74 74 42 98 95 13]\n",
      "[73 47 77 79 74 79 98 74 46 37]\n",
      "[ 3 47 77 79 57 46 95 74 77 95]\n",
      "[82 22 77 79 32 79 59 74 95 95]\n",
      "[13 34 77 79 47 47 95 74 95 95]\n",
      "[ 7 31 77 79 47 47 95 74 95 95]\n",
      "[12 24 77 79 47 47 95 74 95 95]\n",
      "[78  1 77 79 47 57 59 74 95 95]\n",
      "[78 24 77 79 32 79 59 74 95 34]\n",
      "[26 48 77 79 79 90 59 74 95 95]\n",
      "[ 3 40 77 79 47 46 95 74 95 95]\n",
      "[33 47 77 79 47 79 59 74 95 95]\n",
      "[53 95 77 79 79 79 74 74 95  2]\n",
      "[57 13 77 79 47 47 95 74 95 95]\n",
      "[67 82 77 79 32 74 74 74 95 95]\n",
      "[82 95 77 79 79 79 42 74 95 13]\n",
      "[54 93 77 79 74 79 74 74 95 13]\n",
      "[97 51 77 32 79 77 98 40 60 97]\n",
      "[64 98 77 79 79 79 74 74 56 13]\n",
      "[93 32 77 79 32 79 59 74 95 34]\n",
      "[61 37 77 79 79 90 59 74 95 95]\n",
      "[58 13 60 79 95 57 57 59 57 47]\n",
      "[25 28 77 79 47 46 95 74 95 95]\n",
      "[45 47 77 79 79 79 59 74 57 95]\n",
      "[34 28 77 79 79 57 57 74 57 95]\n",
      "[36 18 77 79 47 95 59 74 95 95]\n",
      "[68 42 77 79 74 79 59 74 95 47]\n",
      "[98  4 77 79 79 57 59 74 95 95]\n",
      "[92 56 77 79 74 79 59 74 95 47]\n",
      "[66 77 77 79 74 79 59 74 95 12]\n",
      "[52 67 77 74 74 79 59 74 95  2]\n",
      "[17 34 77 79 47 95 59 74 95 95]\n",
      "[48 98 77 74 74 79 74 74 95 34]\n",
      "[40 95 77 74 74 79 59 74 95 12]\n",
      "[70 95 48 56 74 57 59 74 95 13]\n",
      "[83 21 77 79 74 90 59 74 95 95]\n",
      "[10 95 77 74 57 90 59 74 77 34]\n",
      "[82  4 77 79 57 57 59 74 95 95]\n",
      "[12 28 77 79 47 95 59 74 95 95]\n",
      "[81 56 77 71 74 90 59 74 95 95]\n",
      "[61 22 77 79 57 95 59 74 95 95]\n",
      "[ 5 95 32 74 95 95 59 74 95 34]\n",
      "[69 34 77 79 74 90 59 74 95 95]\n",
      "[32 28 77 79 47 95 59 74 95 95]\n",
      "[51 23 77 79 79 95 59 74 95 95]\n",
      "[44 75 77 74 74 90 74 74 95 34]\n",
      "[76 82 77 74 74 74 74 74 95 34]\n",
      "[53  1 77 79 47 95 95 40 95 23]\n",
      "[38 34 77 79 79 93 59 74 95 95]\n",
      "[69 88 77 74 74 79 98 74 46 95]\n",
      "[66 95 77 74 74 74 98 74 46 95]\n",
      "[51  4 77 79 47 95 95 74 95 95]\n",
      "[68  4 77 79 57 46 95 74 95 34]\n",
      "[63 42 77 79 79 90 59 74 95  2]\n",
      "[19 98 77 88 57 90 23 74 95 47]\n",
      "[66  4 77 79 46 46 95 74 95 34]\n",
      "[16 47 77 79 57 46 95 74 77 98]\n",
      "[97 51 77 23 79 77 98 40 60  4]\n",
      "[52 23 77 79 79 46 93 74 95 69]\n",
      "[71 95 82 79 74 79 59 74 93 13]\n",
      "[66  4 77 79 79 95 57 74 95 95]\n",
      "[22 98 77 23 79 23 23 23 23 23]\n",
      "[49  4 77 79 47 46 95 74 95 95]\n",
      "[54 23 77 79 79 90 59 74 95 47]\n",
      "[66  4 77 79 57 79 59 74 95 75]\n",
      "[22 98 61 74 57 79 59 74 77 34]\n",
      "[49 66 82 74 74 90 74 74 95 13]\n",
      "[60  4 82 79 57 95 59 74 95 34]\n",
      "[27  4 77 79 47 95 95 74 95 31]\n",
      "[ 4 23 23 74 95 57 57 23 60  5]\n",
      "[74 34 77 98 74 59 59 74 95 47]\n",
      "[68 24 77 98 74 95 59 74 95 47]\n",
      "[ 1 95 32 74 74 95 95 74 34 34]\n",
      "[88 59 77 98 74 74 74 74 95 47]\n",
      "[52 22 77 59 57 90 74 74 95 95]\n",
      "[32 24 77 59 79 88 59 74 95 95]\n",
      "[17 41 77 79 57 95 59 74 95 31]\n",
      "[56 95 77 74 74 74 59 74 95 12]\n",
      "[19 34 77 79 57 95 59 74 95 31]\n",
      "[15 60 77 74 57 95 59 74 95 34]\n",
      "[19  1 77 59 47 88 59 74 95 98]\n",
      "[65 77 77 74 74 74 74 74 95 34]\n",
      "[72 13 77 79 74 95 59 74 95 95]\n",
      "[32 47 77 74 79 22 93 74 77  1]\n",
      "[28 88 77 74 74 90 59 74 95 34]\n",
      "[23 14 77 79 47 95 95 74 77 31]\n",
      "[28 12 77 79 95 88 57 74 57 95]\n",
      "[25 79 32 74 74 95 95 74 95 34]\n",
      "[22 27 77 79 95 95 57 74 95 95]\n",
      "[24 95 32 74 74 95 95 74 95 34]\n",
      "[38 75 77 74 74 90 22 74 95 95]\n",
      "[94 98 77 74 74 74 74 74 95 13]\n",
      "[59 95 77 74 74 74 74 74 95 12]\n",
      "[42 22 77 84 79 95 95 74 57 95]\n",
      "[93 98 77 98 74 74 98 74 46 42]\n",
      "[14  1 77 47 47 88 59 74 95 31]\n",
      "[32 34 77 79 79 95 59 74 95 32]\n",
      "[ 6 60 89 79 79 88 59 74 95 32]\n",
      "[55 40 77 79 79 90 59 74 95 47]\n",
      "[97 67 77 71 74 79 22 98 95 95]\n",
      "[77 88 77 71 74 79 55 74 95 13]\n",
      "[29 95 32 88 95 95 59 74 93  1]\n",
      "[59 41 77 79 79 90 98 74 46 95]\n",
      "[69 95 77 79 74 79 98 74 95 34]\n",
      "[43 42 77 79 95 95 59 74 95 95]\n",
      "[33 13 77 79 95 95 95 40 40 40]\n",
      "[56 40 77 79 74 95 59 74 95 95]\n",
      "[99 67 77 98 74 25 74 74 95 47]\n",
      "[61 34 77 79 74 95 95 74 95 31]\n",
      "[70 13 57 79 74 95 59 98 95 11]\n",
      "[57 13 77 79 79 95 95 74 95 95]\n",
      "[58 13 77 79 95 95 34 98 95 95]\n",
      "[26 97 32 88 95 95 59 74 60  1]\n",
      "[79 47 77 79 74 90 98 74 46 95]\n",
      "[48 40 77 79 95 90 59 74 95  2]\n",
      "[ 2 79 32 88 95 57 59 59 60  1]\n",
      "[61 95 89 74 74 79 98 74 46 60]\n",
      "[14 47 66 79 95 95 59 74 57 95]\n",
      "[40 13 77 79 47 95 95 74 95 31]\n",
      "[36 95 66 88 45 95 59 74 27 27]\n",
      "[41 95 66 88 95 90 59 74 60 13]\n",
      "[38 79 66 88 95 90 59 59 93  1]\n",
      "[ 1 47 66 79 95 95 59 74 57 34]\n",
      "[32 34 77 79 95 90 59 74 57 60]\n",
      "[26 13 77 79 47 95 95 74 57 60]\n",
      "[34  4 77 79 47 95 95 74 57 60]\n",
      "[84 13 77 79 57 95 59 74 95 34]\n",
      "[67 59 77 73 74 90 98 74 46 46]\n",
      "[32 89 66 88 57 95 59 74 95 34]\n",
      "[77 37 77 90 74 90 59 74 95 95]\n",
      "[64 34 77 79 95 90 98 74 46 46]\n",
      "[12 27 88 79 95 95 57 74 57 27]\n",
      "[12 27 88 79 47 95 95 74 57 95]\n",
      "[76 24 77 79 95 95 59 98 95 34]\n",
      "[73 40 77 90 95 90 59 98 95 94]\n",
      "[77 60 77 88 74 90 22 98 95 95]\n",
      "[79 74 77 88 74 90 22 98 95 95]\n",
      "[76 98 82 24 24 90 59 98  4  4]\n",
      "[39 13 77 79 47 95 95 98 95 34]\n",
      "[34 34 77 79 47 95 95 74 95 31]\n",
      "[ 3 79 88 79 88 90 59 59 59 47]\n",
      "[21 66 82 79 57 90 59 74 95 98]\n",
      "[ 0  5 75 94 47 47 95 85 32 95]\n",
      "[19 40 82 79 79 95 59 74 95 47]\n",
      "[81 37 77 79 95 90 98 74 57 13]\n",
      "[84 33 77 79 95 90 98 74 57 13]\n",
      "[49 34 77 79 79 95 95 74 57 95]\n",
      "[19 66 89 79 88 46 95 74 57 47]\n",
      "[55 34 77 79 79 95 95 74 95 31]\n",
      "[38 34 77 79 79 95 95 74 57 95]\n",
      "[20 95 89 79 88 90 90 74 95 34]\n",
      "[40 31 77 79 79 95 95 74 57 95]\n",
      "[ 0 79 77 79 88 90 59 74 95 34]\n",
      "[49 95 89 74 79 90 22 98 95 95]\n",
      "[64 13 77 79 79 95 59 74 95 95]\n",
      "[ 5 79 89 79 79 90 59 74 59 47]\n",
      "[ 9 59 89 79 79 46 46 59 59 88]\n",
      "[31 66 89 79 79 90 59 74 59 95]\n",
      "[82 98 82 79 74 90 22 98 95 42]\n",
      "[36 22 77 79 79 61 95 74 57 60]\n",
      "[85 88 77 89 74 74 22 74 95 12]\n",
      "[87 24 77 89 74 90 22 22 95 48]\n",
      "[99 22 77 89 74 90 22 22 59 95]\n",
      "[15 98 89 79 95 90 59 74 59 13]\n",
      "[10 59 89 79 95 90 59 59 57 47]\n",
      "[17 28 97 79 47 95 59 74 95 98]\n",
      "[14 59 97 79 79 90 59 74 57 60]\n",
      "[ 0 46 97 79 47 47 95 74 59 98]\n",
      "[42 82 97 79 79 90 98 74 57 60]\n",
      "[84 74 77 27 30 30 90 74 34 34]\n",
      "[30 13 77 79 95 95 57 74 57 47]\n",
      "[47 82 62 74 74 90 59 74 37 13]\n",
      "[70 95 89 74 79 90 22 74 95 13]\n",
      "[81 88 77 79 74 90 22 74 95 47]\n",
      "[19 89 97 79 95 90 59 74 59 47]\n",
      "[23 34 97 79 95 95 59 74 57 74]\n",
      "[19 95 97 79 95 90 59 74 95 47]\n",
      "[16 57 97 79 95 90 59 74 57 47]\n",
      "[ 8 47 97 79 47 95 59 74 57 47]\n",
      "[65 86 82 90 95 90 22 98 95 95]\n",
      "[ 7 59 97 79 95 95 59 74 57 47]\n",
      "[56 13 77 79 95 95 57 98 95 94]\n",
      "[72 57 77 79 95 95 22 98 95 95]\n",
      "[84 47 77 47 79 95 22 98 95 95]\n",
      "[64 88 77 79 95 95 95 74 46 24]\n",
      "[39 60 89 79 95 90 59 74 57  2]\n",
      "[91 40 77 47 79 95 59 53 97 47]\n",
      "[95 47 77 47 79 90 59 98 95 34]\n",
      "[29 47 89 79 95 95 57 74 95 47]\n",
      "[29 47 89 79 95 95 57 74 95 32]\n",
      "[38 47 77 79 95 95 57 59 57 60]\n",
      "[ 0 95 88 79 57 95 59 74 32 32]\n",
      "[97 95 77 79 74 95 59 98 95 34]\n",
      "[67 90 77 79 79 90 98 74 46 37]\n",
      "[48 66 89 79 79 90 98 74 57 60]\n",
      "[10 66 97 79 57 79 59 74 57 32]\n",
      "[91 90 77 79 74 74 98 74 95 34]\n",
      "[10 60 97 79 79 61 95 74 57 41]\n",
      "[74 95 77 79 74 79 98 74 46 37]\n",
      "[93 86 77 79 74 95 98 74 46 97]\n",
      "[56 74 77 79 79 90 98 74 46 37]\n",
      "[39 60 48 79 95 57 30 59 57 13]\n",
      "[57 41 77 79 95 90 59 74 57 95]\n",
      "[11 34 97 79 47 47 95 74 59 27]\n",
      "[16 34 97 79 95 95 57 59 57 74]\n",
      "[67 34 77 79 60 95 59 74 95 95]\n",
      "[34 41 77 79 95 95 59 59 57 60]\n",
      "[51 37 77 79 95 90 59 74 57 95]\n",
      "[ 9 75 97 79 79 88 59 74 57 47]\n",
      "[84 14 77 79 95 90 59 98 95 95]\n",
      "[36 75 97 79 79 90 59 74 95 47]\n",
      "[89 95 77 79 74 95 98 74 46 97]\n",
      "[ 0 88 32 77 95 24 88 17 47 19]\n",
      "[57 13 77 79 95 95 57 98 95 95]\n",
      "[96 86 77 79 74 90 98 74 46 37]\n",
      "[69 46 77 79 95 90 98 74 46 46]\n",
      "[36 95 48 88 95 95 59 74 46 13]\n",
      "[17 46 97 79 79 88 59 74 57 74]\n",
      "[50 34 77 79 95 90 59 74 95 34]\n",
      "[55 95 48 88 74 95 59 74 37 34]\n",
      "[24 41 97 79 95 95 57 74 57 95]\n",
      "[47 60 77 79 79 90 59 74 95 34]\n",
      "[49 95 97 79 79 90 98 74 46 13]\n",
      "[38 86 97 79 79 90 59 74 95 34]\n",
      "[51 24 77 79 95 95 59 74 95 34]\n",
      "[66 95 82 79 79 90 98 74 46 34]\n",
      "[16 24 89 79 95 95 57 74 57 60]\n",
      "[33 59 89 79 87 90 59 74 95 34]\n",
      "[98 51 77 79 79 90 98 74 46 97]\n",
      "[86 95 77 79 74 90 98 74 95 34]\n",
      "[89 40 77 79 79 90 98 74 46 97]\n",
      "[31 57 89 79 79 90 59 74 95 34]\n",
      "[38 22 77 79 79 95 59 74 95 95]\n",
      "[65 37 77 79 95 90 98 74 46 95]\n",
      "[45 37 77 79 79 90 59 74 57 95]\n",
      "[38 29 77 79 79 95 59 74 57 95]\n",
      "[99 74 77 79 74 74 98 74 46 24]\n",
      "[69 13 77 79 79 90 59 74 57 95]\n",
      "[30 40 77 79 79 90 74 74 57 60]\n",
      "[85 95 77 79 74 74 98 74 46 24]\n",
      "[45 40 77 79 79 90 98 74 46 60]\n",
      "[85 34 77 79 95 90 98 74 46 97]\n",
      "[75 95 77 74 36 74 74 74 95 34]\n",
      "[29 47 89 79 79 95 74 74 57 60]\n",
      "[41 34 77 79 95 90 74 74 57 13]\n",
      "[65 95 89 74 79 90 74 74 24 60]\n",
      "[93 95 77 79 74 74 98 74 95 34]\n",
      "[80 95 77 79 74 74 98 74 46 97]\n"
     ]
    }
   ],
   "source": [
    "num_episodes=1300\n",
    "max_steps=10\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    for step in range(max_steps):\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        agent.replay()\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[55 79 89 74 95 90 98 59 29 60]\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "\n",
    "while True:\n",
    "    action = agent.act(state)\n",
    "    q_values=agent.q_values(state)\n",
    "    while action in state:\n",
    "        k=torch.argmax(q_values).item()\n",
    "        q_values[0,k]=0\n",
    "        action=torch.argmax(q_values).item()\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    state = next_state\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[62 41 13  2 95 70 59 57 82 16]]\n",
      "tensor(42.7142)\n",
      "[[76 77  8 34 24 74 47 95 40 51]]\n",
      "tensor(50.6571)\n",
      "[[ 1 80 85 10 79 88 98 74 40 13]]\n",
      "tensor(59.7436)\n",
      "[[20  2 99 18 79 88 59 74 53 52]]\n",
      "tensor(51.3154)\n",
      "[[39  1 73 85 95 24 57 59 62 47]]\n",
      "tensor(42.2990)\n",
      "[[ 7 36 69  5 79 88 98 74 40 82]]\n",
      "tensor(49.6127)\n",
      "[[42 73 93  1 79 88 98 74 97 51]]\n",
      "tensor(52.5967)\n",
      "[[86 96  6 61 24 74 95 98 40 51]]\n",
      "tensor(60.3592)\n",
      "[[22  7 98 96 95 24 57 60 59 88]]\n",
      "tensor(52.4219)\n",
      "[[58 82 35 77 74 59 98 69 46 29]]\n",
      "tensor(48.3886)\n",
      "[[79 83  5  2 77 30 47 59 95 34]]\n",
      "tensor(42.7020)\n",
      "[[12 71 87 78 95 90 74 98 57 60]]\n",
      "tensor(53.1260)\n",
      "[[74 77 51 42 79 76 98 59 46 34]]\n",
      "tensor(40.9445)\n",
      "[[52 56 77  9 79 88 98 74 46 82]]\n",
      "tensor(44.3118)\n",
      "[[20 21 39 85 95 57 59 62 46  5]]\n",
      "tensor(43.4344)\n",
      "[[50 83 30  2 88 79 59 74 34 13]]\n",
      "tensor(42.9008)\n",
      "[[44 58 47 18 79 88 42 74 34 13]]\n",
      "tensor(37.5133)\n",
      "[[56  5 45 76 95 57 59 62 46 13]]\n",
      "tensor(36.4600)\n",
      "[[15 99 73 62 95 90 98 74 60 34]]\n",
      "tensor(64.4396)\n",
      "[[84 13 52 86 95 61 59 98 46 94]]\n",
      "tensor(41.1667)\n",
      "[[68 27 78 84 95 90 98 74 46 60]]\n",
      "tensor(50.7112)\n",
      "[[14 62 86 41 79 88 74 98 95 34]]\n",
      "tensor(52.5987)\n",
      "[[27 15 88 79 95 24 57 98 59 21]]\n",
      "tensor(45.2517)\n",
      "[[28 77 86 84 95 90 74 98 57 60]]\n",
      "tensor(52.8481)\n",
      "[[75 91 15 39 24 33 88 95 40 27]]\n",
      "tensor(50.3683)\n",
      "[[33 21 56 17 79 88 42 74 95 82]]\n",
      "tensor(40.2200)\n",
      "[[40 92 27 15 88 79 42 74 59 13]]\n",
      "tensor(46.7502)\n",
      "[[30 93  6 27 74 79 59 98 94 34]]\n",
      "tensor(48.7265)\n",
      "[[10 70 94 83 95 88 59 74 57 47]]\n",
      "tensor(58.7320)\n",
      "[[ 5 38 82 87 95 24 57 98 59 27]]\n",
      "tensor(45.9707)\n",
      "[[34 65 28  0 74 79 59 53 82 16]]\n",
      "tensor(39.6781)\n",
      "[[44 36  7 82 88 42 57 59 46  5]]\n",
      "tensor(42.3613)\n",
      "[[24 61 92 22 79 88 98 74 46 51]]\n",
      "tensor(54.6289)\n",
      "[[90 15 30 96 74 88 95 57 46 60]]\n",
      "tensor(54.3673)\n",
      "[[86 75  8 38 24 74 47 95 40 51]]\n",
      "tensor(53.3472)\n",
      "[[11 42 52 63 95 57 59 74 46 60]]\n",
      "tensor(38.1406)\n",
      "[[65 20 75 68 95 90 98 74 46 29]]\n",
      "tensor(47.9271)\n",
      "[[23 42 57 65 95 61 59 74 46 13]]\n",
      "tensor(37.8892)\n",
      "[[38 22 23 70 95 57 59 60 46  5]]\n",
      "tensor(37.1979)\n",
      "[[42 66 80 75 79 90 74 98 95 94]]\n",
      "tensor(51.1420)\n",
      "[[56 83 56  2 74 90 42 98 88 34]]\n",
      "tensor(47.0318)\n",
      "[[34 81 77 13 79 88 98 74 46 82]]\n",
      "tensor(50.3478)\n",
      "[[82 99 17 29 24 77 47 95 40 27]]\n",
      "tensor(51.0963)\n",
      "[[40 79 56 16 74 90 42 98 34 66]]\n",
      "tensor(42.8042)\n",
      "[[36 59  0 47 74 95 88 62 57 82]]\n",
      "tensor(43.1187)\n",
      "[[73  3 89 69 79 95 98 74 46 60]]\n",
      "tensor(56.0731)\n",
      "[[89 84 32 70 74 76 95 98 46 34]]\n",
      "tensor(52.3203)\n",
      "[[54 94  0 34 88 30 59 74 13 51]]\n",
      "tensor(35.9909)\n",
      "[[42 14 46 37 95 34 57 60 29 13]]\n",
      "tensor(29.2878)\n",
      "[[98 31 90 11 79 42 99 40 95 13]]\n",
      "tensor(33.7359)\n",
      "[[57  7 69 93 95 24 34 74 62 60]]\n",
      "tensor(44.4512)\n",
      "[[60 63 91 20 79 88 98 74 46 97]]\n",
      "tensor(47.7748)\n",
      "[[68 93 23 89 74 98 95 19 62 32]]\n",
      "tensor(56.9009)\n",
      "[[27  7 96 86 95 24 57 98 59 21]]\n",
      "tensor(50.9222)\n",
      "[[78 21 79 24 90 34 98 40 46 13]]\n",
      "tensor(35.2361)\n",
      "[[46  1  2 92 88 57 59 60 40  5]]\n",
      "tensor(44.2162)\n",
      "[[38 54 83  7 79 88 98 74 95 82]]\n",
      "tensor(50.2432)\n",
      "[[43 85 19 96 74 95 88 98 46 13]]\n",
      "tensor(54.5360)\n",
      "[[37 65 17  6 88 79 59 56 82 16]]\n",
      "tensor(43.2127)\n",
      "[[59  4 76 82 95 24 57 74 62 32]]\n",
      "tensor(39.2436)\n",
      "[[62 13 81 58 79 95 98 74 46 29]]\n",
      "tensor(49.6416)\n",
      "[[37 19 41 62 95 57 59 60 46 13]]\n",
      "tensor(33.8821)\n",
      "[[38 91 60 16 74 90 42 98 66 13]]\n",
      "tensor(49.7329)\n",
      "[[ 5 65 46 35 79 61 59 74 60 34]]\n",
      "tensor(45.6023)\n",
      "[[88 69  1 70 74 33 95 57 46 40]]\n",
      "tensor(51.6614)\n",
      "[[32 59 38 92 95 57 19 74 46  5]]\n",
      "tensor(46.4902)\n",
      "[[60 80 34  0 24 79 47 74 90 28]]\n",
      "tensor(38.6255)\n",
      "[[89 90 49  4 77 53 42 56 59 88]]\n",
      "tensor(36.8741)\n",
      "[[55 18 81 88 95 24 57 74 59 32]]\n",
      "tensor(40.4960)\n",
      "[[23 91 79 45 61 90 74 98 95 34]]\n",
      "tensor(51.7659)\n",
      "[[19 82 54 42 74 90 98 59 57 34]]\n",
      "tensor(46.8105)\n",
      "[[51 54  9  6 77 30 59 82 34 28]]\n",
      "tensor(32.2200)\n",
      "[[67 18 97  2 79 88 98 40 53 51]]\n",
      "tensor(46.6983)\n",
      "[[ 1 62 68 12 79 88 98 74 40 82]]\n",
      "tensor(54.3798)\n",
      "[[12  0 22 32 95 57 60 84 82 94]]\n",
      "tensor(35.9631)\n",
      "[[41 48  6 81 74 42 57 98 40 13]]\n",
      "tensor(45.0052)\n",
      "[[37 47 82 10 79 88 98 74 95 94]]\n",
      "tensor(50.1403)\n",
      "[[96 45 38 20 95 77 42 59 46 34]]\n",
      "tensor(36.3389)\n",
      "[[67 94 30 16 24 79 47 95 97 34]]\n",
      "tensor(48.2485)\n",
      "[[91 34 33 45 74 98 95 57 46 82]]\n",
      "tensor(44.4604)\n",
      "[[11 92 45 52 74 90 98 69 60 34]]\n",
      "tensor(50.6682)\n",
      "[[80 19  4 88 33 12 42 98 95 40]]\n",
      "tensor(56.0918)\n",
      "[[68  4 27  2 90 34 57 59 82 28]]\n",
      "tensor(32.6181)\n",
      "[[19 23 32 90 95 57 59 62 46  5]]\n",
      "tensor(44.7051)\n",
      "[[73  3 90 67 79 95 98 74 46 60]]\n",
      "tensor(55.7857)\n",
      "[[56 70 99 15 79 88 98 74 97 51]]\n",
      "tensor(51.6727)\n",
      "[[29 48  9 31 74 61 59 98 82 34]]\n",
      "tensor(34.0312)\n",
      "[[75 60 97 68 79 90 98 74 46 37]]\n",
      "tensor(56.4543)\n",
      "[[ 2 99 51 87 95 24 88 74 40  5]]\n",
      "tensor(50.9681)\n",
      "[[61 79 46 28 74 90 42 98 34 13]]\n",
      "tensor(41.1217)\n",
      "[[27 21 86 53 95 88 59 74 57 60]]\n",
      "tensor(46.6299)\n",
      "[[35 56 39 78 95 57 59 74 46 13]]\n",
      "tensor(42.8809)\n",
      "[[96 55 99 88 95 90 98 74 57 24]]\n",
      "tensor(55.9124)\n",
      "[[76 65 91 10 79 88 42 98 90 34]]\n",
      "tensor(45.9602)\n",
      "[[82 92 38 14 24 79 47 95 77 34]]\n",
      "tensor(44.5215)\n",
      "[[ 2 59 30 82 95 57 88 74 46 13]]\n",
      "tensor(46.8948)\n",
      "[[44 76 37 74 95 57 98 59 46 60]]\n",
      "tensor(38.6633)\n",
      "[[33 37 40 86 95 57 59 74 46  5]]\n",
      "tensor(44.3958)\n",
      "[[41 41 89 56 79 90 74 98 95 34]]\n",
      "tensor(51.3704)\n",
      "[[26 63 45 63 95 61 59 74 46 13]]\n",
      "tensor(44.0806)\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "state=np.array([[49,61,74, 48, -1, -1, -1, -1, -1, -1]])\n",
    "\n",
    "for v in range(0, 100):\n",
    "    state=np.array([[b[4*v], b[4*v+1], b[4*v+2], b[4*v+3], -1, -1, -1, -1, -1, -1]])\n",
    "    t=0\n",
    "    for i in (range(4,10)):\n",
    "        q_values=agent.q_values(state)\n",
    "        max_v=torch.max(q_values)\n",
    "        t+=max_v\n",
    "        k=torch.argmax(q_values).item()\n",
    "\n",
    "        while(k in state[0]):\n",
    "            t-=max_v\n",
    "            q_values[0,k]=0\n",
    "            k=torch.argmax(q_values).item()\n",
    "            max_v = torch.max(q_values)\n",
    "            t+=max_v\n",
    "        state[0][i]=k\n",
    "    print(state)\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEnv2(Env):\n",
    "    def __init__(self):\n",
    "        self.i=0\n",
    "        self.action_space = Discrete(100)\n",
    "        self.observation_space = Box(low=0, high=99, shape=(1,10), dtype=int64)\n",
    "        self.state = self.observation_space.sample().ravel()\n",
    "        \n",
    "    def step(self,action):\n",
    "        self.i+=1 \n",
    "        for j in range(self.i,10):\n",
    "            self.state[j]=-1\n",
    "        \n",
    "        self.next_state = self.observation_space.sample().ravel()\n",
    "        #self.next_state[0] = self.state[1]\n",
    "        #self.next_state[1]= action\n",
    "        \n",
    "        if action in self.state:\n",
    "            self.state[self.i]=action\n",
    "            reward = -10\n",
    "        else:  \n",
    "            print(self.i)\n",
    "            print(self.state)\n",
    "            self.state[self.i]=action \n",
    "            reward = getReward1(ru, D, self.state, action)\n",
    "        \n",
    "        if self.i==9:\n",
    "            done=True\n",
    "        else:\n",
    "            done=False\n",
    "        info={}\n",
    "\n",
    "        return self.state, reward, done, info\n",
    "    def render(self):\n",
    "        # Implement viz\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        self.i=0\n",
    "        self.state = self.observation_space.sample().ravel()\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, n_actions):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.actor = nn.Linear(64, n_actions)\n",
    "        self.critic = nn.Linear(64, 1)\n",
    "        self.log_std = nn.Parameter(torch.zeros(n_actions))\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.tanh(self.fc1(state))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        value = self.critic(x)\n",
    "        mean = self.actor(x)\n",
    "        std = torch.exp(self.log_std)\n",
    "        dist = torch.distributions.Normal(mean, std)\n",
    "        return dist, value\n",
    "\n",
    "def ppo(env_fn, epochs=200, batch_size=32, gamma=0.0, eps_clip=0.2):\n",
    "    env = CustomEnv2()\n",
    "    state_size=(1,10)\n",
    "    state_dim = np.shape(env.observation_space)[1]\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    model = ActorCritic(state_dim, n_actions)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            log_probs_old, value_old = model(torch.Tensor(state))\n",
    "            action = env.action_space.sample()\n",
    "            print(done)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            state = next_state\n",
    "\n",
    "        for _ in range(batch_size):\n",
    "            log_probs_old, value_old = model(torch.Tensor(state))\n",
    "            action = env.action_space.sample()\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            log_probs_new, value_new = model(torch.Tensor(next_state))\n",
    "            advantage = reward + gamma * (1 - done) * value_new - value_old\n",
    "            ratio = torch.exp(log_probs_new - log_probs_old)\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1 - eps_clip, 1 + eps_clip) * advantage\n",
    "            actor_loss = -torch.min(surr1, surr2).mean()\n",
    "            critic_loss = nn.MSELoss()(value_old, reward + gamma * (1 - done) * value_new)\n",
    "            loss = actor_loss + 0.5 * critic_loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print('Epoch:', epoch, 'Loss:', loss.item())\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "ray.tune in ray > 0.7.5 requires 'tabulate'. Please re-run 'pip install ray[tune]' or 'pip install ray[rllib]'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/git/rl_recsys/.venv/lib/python3.9/site-packages/ray/tune/progress_reporter.py:51\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mtabulate\u001b[39;00m \u001b[39mimport\u001b[39;00m tabulate\n\u001b[1;32m     52\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tabulate'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mppo\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mppo\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshutil\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n",
      "File \u001b[0;32m~/git/rl_recsys/.venv/lib/python3.9/site-packages/ray/rllib/__init__.py:7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mray\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_private\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39musage\u001b[39;00m \u001b[39mimport\u001b[39;00m usage_lib\n\u001b[1;32m      5\u001b[0m \u001b[39m# Note: do not introduce unnecessary library dependencies here, e.g. gym.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m# This file is imported from the tune module in order to register RLlib agents.\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mray\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrllib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menv\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase_env\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseEnv\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mray\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrllib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menv\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexternal_env\u001b[39;00m \u001b[39mimport\u001b[39;00m ExternalEnv\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mray\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrllib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menv\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmulti_agent_env\u001b[39;00m \u001b[39mimport\u001b[39;00m MultiAgentEnv\n",
      "File \u001b[0;32m~/git/rl_recsys/.venv/lib/python3.9/site-packages/ray/rllib/env/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mray\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrllib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menv\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase_env\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseEnv\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mray\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrllib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menv\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menv_context\u001b[39;00m \u001b[39mimport\u001b[39;00m EnvContext\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mray\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrllib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menv\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexternal_env\u001b[39;00m \u001b[39mimport\u001b[39;00m ExternalEnv\n",
      "File \u001b[0;32m~/git/rl_recsys/.venv/lib/python3.9/site-packages/ray/rllib/env/base_env.py:6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgym\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mray\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mray\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrllib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mannotations\u001b[39;00m \u001b[39mimport\u001b[39;00m Deprecated, DeveloperAPI, PublicAPI\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mray\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrllib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m AgentID, EnvID, EnvType, MultiAgentDict, MultiEnvDict\n\u001b[1;32m      9\u001b[0m \u001b[39mif\u001b[39;00m TYPE_CHECKING:\n",
      "File \u001b[0;32m~/git/rl_recsys/.venv/lib/python3.9/site-packages/ray/rllib/utils/__init__.py:29\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mray\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrllib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpre_checks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menv\u001b[39;00m \u001b[39mimport\u001b[39;00m check_env\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mray\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrllib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mschedules\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     23\u001b[0m     LinearSchedule,\n\u001b[1;32m     24\u001b[0m     PiecewiseSchedule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m     ConstantSchedule,\n\u001b[1;32m     28\u001b[0m )\n\u001b[0;32m---> 29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mray\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrllib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtest_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     30\u001b[0m     check,\n\u001b[1;32m     31\u001b[0m     check_compute_single_action,\n\u001b[1;32m     32\u001b[0m     check_train_results,\n\u001b[1;32m     33\u001b[0m     framework_iterator,\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mray\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtune\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m merge_dicts, deep_update\n\u001b[1;32m     38\u001b[0m \u001b[39m@DeveloperAPI\u001b[39m\n\u001b[1;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madd_mixins\u001b[39m(base, mixins, \u001b[39mreversed\u001b[39m\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/git/rl_recsys/.venv/lib/python3.9/site-packages/ray/rllib/utils/test_utils.py:28\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgym\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspaces\u001b[39;00m \u001b[39mimport\u001b[39;00m Box\n\u001b[1;32m     27\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mray\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mray\u001b[39;00m \u001b[39mimport\u001b[39;00m air, tune\n\u001b[1;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mray\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrllib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m try_import_jax, try_import_tf, try_import_torch\n\u001b[1;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mray\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrllib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menv\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwrappers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39matari_wrappers\u001b[39;00m \u001b[39mimport\u001b[39;00m is_atari, wrap_deepmind\n",
      "File \u001b[0;32m~/git/rl_recsys/.venv/lib/python3.9/site-packages/ray/tune/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mray\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtune\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39merror\u001b[39;00m \u001b[39mimport\u001b[39;00m TuneError\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mray\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtune\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtune\u001b[39;00m \u001b[39mimport\u001b[39;00m run_experiments, run\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mray\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtune\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msyncer\u001b[39;00m \u001b[39mimport\u001b[39;00m SyncConfig\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mray\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtune\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperiment\u001b[39;00m \u001b[39mimport\u001b[39;00m Experiment\n",
      "File \u001b[0;32m~/git/rl_recsys/.venv/lib/python3.9/site-packages/ray/tune/tune.py:17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mray\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtune\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39merror\u001b[39;00m \u001b[39mimport\u001b[39;00m TuneError\n\u001b[1;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mray\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtune\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperiment\u001b[39;00m \u001b[39mimport\u001b[39;00m Experiment, _convert_to_experiment_list\n\u001b[0;32m---> 17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mray\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtune\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprogress_reporter\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     18\u001b[0m     ProgressReporter,\n\u001b[1;32m     19\u001b[0m     _detect_reporter,\n\u001b[1;32m     20\u001b[0m     _detect_progress_metrics,\n\u001b[1;32m     21\u001b[0m     _prepare_progress_reporter_for_ray_client,\n\u001b[1;32m     22\u001b[0m     _stream_client_output,\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mray\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtune\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexecution\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mray_trial_executor\u001b[39;00m \u001b[39mimport\u001b[39;00m RayTrialExecutor\n\u001b[1;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mray\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtune\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mregistry\u001b[39;00m \u001b[39mimport\u001b[39;00m get_trainable_cls, is_function_trainable\n",
      "File \u001b[0;32m~/git/rl_recsys/.venv/lib/python3.9/site-packages/ray/tune/progress_reporter.py:53\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mtabulate\u001b[39;00m \u001b[39mimport\u001b[39;00m tabulate\n\u001b[1;32m     52\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[0;32m---> 53\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m     54\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mray.tune in ray > 0.7.5 requires \u001b[39m\u001b[39m'\u001b[39m\u001b[39mtabulate\u001b[39m\u001b[39m'\u001b[39m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     55\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease re-run \u001b[39m\u001b[39m'\u001b[39m\u001b[39mpip install ray[tune]\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     56\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpip install ray[rllib]\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     57\u001b[0m     )\n\u001b[1;32m     59\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     class_name \u001b[39m=\u001b[39m get_ipython()\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n",
      "\u001b[0;31mImportError\u001b[0m: ray.tune in ray > 0.7.5 requires 'tabulate'. Please re-run 'pip install ray[tune]' or 'pip install ray[rllib]'."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "import ray.rllib.agents.ppo as ppo\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "_config = tf.ConfigProto()\n",
    "_config.gpu_options.allow_growth = True\n",
    "tf.keras.backend.set_session(tf.Session(config=_config))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ModelCatalog.register_custom_model(\"state_feedback_model_v2\", StateFeedbackModel_v2)\n",
    "\n",
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "config[\"log_level\"] = \"ERROR\"\n",
    "config[\"num_workers\"] = 2\n",
    "config['seed'] = 8363\n",
    "config['model'] = {\"fcnet_hiddens\": [512, 512]}\n",
    "# config[\"model\"] = {\"custom_model\": \"state_feedback_model_v2\"}\n",
    "config[\"env_config\"] = {'type': 'dense', 'seed': 8363}\n",
    "\n",
    "\n",
    "print(config)\n",
    "\n",
    "agent = ppo.PPOTrainer(config, env=CustomEnv2)\n",
    "\n",
    "N_ITER = 300\n",
    "s = \"{:3d} reward {:6.2f}/{:6.2f}/{:6.2f} len {:6.2f}\"\n",
    "print(\"BEGIN TRAINING\")\n",
    "import time\n",
    "for n in range(1,N_ITER+1,1):\n",
    "    print(\"running EPOCH \", n)\n",
    "    t1=time.time()\n",
    "    result = agent.train()\n",
    "    print(time.time() - t1)\n",
    "    print(s.format(\n",
    "        n,\n",
    "        result[\"episode_reward_min\"],\n",
    "        result[\"episode_reward_mean\"],\n",
    "        result[\"episode_reward_max\"],\n",
    "        result[\"episode_len_mean\"],))\n",
    "\n",
    "    with open('ppo_training.txt','a') as handle:\n",
    "\n",
    "        my_string = '{} {} {} {} {}\\n'.format(n,\n",
    "        result[\"episode_reward_min\"],\n",
    "        result[\"episode_reward_mean\"],\n",
    "        result[\"episode_reward_max\"],\n",
    "        result[\"episode_len_mean\"])\n",
    "        handle.write(my_string)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "1\n",
      "[52 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "False\n",
      "2\n",
      "[52 80 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "False\n",
      "3\n",
      "[52 80 14 -1 -1 -1 -1 -1 -1 -1]\n",
      "False\n",
      "4\n",
      "[52 80 14 29 -1 -1 -1 -1 -1 -1]\n",
      "False\n",
      "5\n",
      "[52 80 14 29 75 -1 -1 -1 -1 -1]\n",
      "False\n",
      "6\n",
      "[52 80 14 29 75 71 -1 -1 -1 -1]\n",
      "False\n",
      "7\n",
      "[52 80 14 29 75 71 63 -1 -1 -1]\n",
      "False\n",
      "8\n",
      "[52 80 14 29 75 71 63 12 -1 -1]\n",
      "False\n",
      "9\n",
      "[52 80 14 29 75 71 63 12 69 -1]\n",
      "10\n",
      "[52 80 14 29 75 71 63 12 69 39]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 10 is out of bounds for axis 0 with size 10",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mppo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCustomEnv2\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [13], line 46\u001b[0m, in \u001b[0;36mppo\u001b[0;34m(env_fn, epochs, batch_size, gamma, eps_clip)\u001b[0m\n\u001b[1;32m     44\u001b[0m log_probs_old, value_old \u001b[38;5;241m=\u001b[39m model(torch\u001b[38;5;241m.\u001b[39mTensor(state))\n\u001b[1;32m     45\u001b[0m action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()\n\u001b[0;32m---> 46\u001b[0m next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m log_probs_new, value_new \u001b[38;5;241m=\u001b[39m model(torch\u001b[38;5;241m.\u001b[39mTensor(next_state))\n\u001b[1;32m     48\u001b[0m advantage \u001b[38;5;241m=\u001b[39m reward \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m done) \u001b[38;5;241m*\u001b[39m value_new \u001b[38;5;241m-\u001b[39m value_old\n",
      "Cell \u001b[0;32mIn [7], line 23\u001b[0m, in \u001b[0;36mCustomEnv2.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi)\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate)\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m=\u001b[39maction \n\u001b[1;32m     24\u001b[0m     reward \u001b[38;5;241m=\u001b[39m getReward1(ru, D, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, action)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m9\u001b[39m:\n",
      "\u001b[0;31mIndexError\u001b[0m: index 10 is out of bounds for axis 0 with size 10"
     ]
    }
   ],
   "source": [
    "ppo(CustomEnv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CustomEnv()\n",
    "state = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    dist, value = model(torch.Tensor(state))\n",
    "    action = dist.sample().item()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    state = next_state\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import rl.callbacks\n",
    "# class EpisodeLogger(rl.callbacks.Callback):\n",
    "#     def __init__(self):\n",
    "#         self.observations = {}\n",
    "#         self.rewards = {}\n",
    "#         self.actions = {}\n",
    "\n",
    "#     def on_episode_begin(self, episode, logs):\n",
    "#         self.observations[episode] = []\n",
    "#         self.rewards[episode] = []\n",
    "#         self.actions[episode] = []\n",
    "        \n",
    "#     def on_step_end(self, step, logs):\n",
    "#         episode = logs['episode']\n",
    "#         self.observations[episode].append(logs['observation'])\n",
    "#         print(logs['observation'])\n",
    "#         print(logs['reward'])\n",
    "#         print(logs['action'])\n",
    "#         self.rewards[episode].append(logs['reward'])\n",
    "#         self.actions[episode].append(logs['action'])\n",
    "\n",
    "# cb_ep = EpisodeLogger()\n",
    "# dqn.test(env, nb_episodes=10, visualize=False, callbacks=[cb_ep])\n",
    "\n",
    "\n",
    "# %matplotlib inline\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# for obs in cb_ep.rewards.values():\n",
    "#     plt.plot([o for o in obs])\n",
    "# plt.xlabel(\"step\")\n",
    "# plt.ylabel(\"reward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cb_ep' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [43], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m rewards\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m obs \u001b[38;5;129;01min\u001b[39;00m \u001b[43mcb_ep\u001b[49m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m      3\u001b[0m     rewards\u001b[38;5;241m.\u001b[39mappend(obs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cb_ep' is not defined"
     ]
    }
   ],
   "source": [
    "rewards=[]\n",
    "for obs in cb_ep.rewards.values():\n",
    "    rewards.append(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_reward=[]\n",
    "for i in rewards:\n",
    "    p=0\n",
    "    u=[]\n",
    "    for j in i:\n",
    "        j*=-1\n",
    "        p+=j\n",
    "        u.append(p)\n",
    "    cum_reward.append(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (12,) and (9,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [36], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m purchases \u001b[38;5;241m=\u001b[39m cum_reward[i]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(purchases)):\n\u001b[0;32m----> 7\u001b[0m     \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpurchases\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m     plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/git/rl_recsys/.venv/lib/python3.9/site-packages/matplotlib/pyplot.py:2728\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2726\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[39m.\u001b[39mplot)\n\u001b[1;32m   2727\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot\u001b[39m(\u001b[39m*\u001b[39margs, scalex\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, scaley\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> 2728\u001b[0m     \u001b[39mreturn\u001b[39;00m gca()\u001b[39m.\u001b[39;49mplot(\n\u001b[1;32m   2729\u001b[0m         \u001b[39m*\u001b[39;49margs, scalex\u001b[39m=\u001b[39;49mscalex, scaley\u001b[39m=\u001b[39;49mscaley,\n\u001b[1;32m   2730\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m({\u001b[39m\"\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\"\u001b[39;49m: data} \u001b[39mif\u001b[39;49;00m data \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m {}), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/git/rl_recsys/.venv/lib/python3.9/site-packages/matplotlib/axes/_axes.py:1662\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1419\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1420\u001b[0m \u001b[39mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1421\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1659\u001b[0m \u001b[39m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1660\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1661\u001b[0m kwargs \u001b[39m=\u001b[39m cbook\u001b[39m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[39m.\u001b[39mLine2D)\n\u001b[0;32m-> 1662\u001b[0m lines \u001b[39m=\u001b[39m [\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_lines(\u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39mdata, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)]\n\u001b[1;32m   1663\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m lines:\n\u001b[1;32m   1664\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m~/git/rl_recsys/.venv/lib/python3.9/site-packages/matplotlib/axes/_base.py:311\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m     this \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m],\n\u001b[1;32m    310\u001b[0m     args \u001b[39m=\u001b[39m args[\u001b[39m1\u001b[39m:]\n\u001b[0;32m--> 311\u001b[0m \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_plot_args(\n\u001b[1;32m    312\u001b[0m     this, kwargs, ambiguous_fmt_datakey\u001b[39m=\u001b[39;49mambiguous_fmt_datakey)\n",
      "File \u001b[0;32m~/git/rl_recsys/.venv/lib/python3.9/site-packages/matplotlib/axes/_base.py:504\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes\u001b[39m.\u001b[39myaxis\u001b[39m.\u001b[39mupdate_units(y)\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]:\n\u001b[0;32m--> 504\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y must have same first dimension, but \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    505\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhave shapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    506\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m y\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    507\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y can be no greater than 2D, but have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    508\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mshapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (12,) and (9,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "x=list(range(12))\n",
    "for i in range(7):\n",
    "    purchases = cum_reward[i]\n",
    "    for j in range(len(purchases)):\n",
    "        plt.plot(x,purchases)\n",
    "        plt.xlabel(\"step\")\n",
    "        plt.ylabel(\"reward\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4b1aae1df98fcdaba42211eb04fb1c10f1061d9efaa8b282c66397dab6b26e66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
